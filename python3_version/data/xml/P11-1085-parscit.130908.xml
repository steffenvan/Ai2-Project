<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.9856355">
Learning to Transform and Select Elementary Trees for Improved
Syntax-based Machine Translations
</title>
<author confidence="0.659354">
Bing Zhao†, and Young-Suk Lee†, and Xiaoqiang Luo†, and Liu Li‡
</author>
<note confidence="0.842391">
IBM T.J. Watson Research† and Carnegie Mellon University‡
</note>
<email confidence="0.702223">
{zhaob, ysuklee, xiaoluo}@us.ibm.com and liul@andrew.cmu.edu
</email>
<sectionHeader confidence="0.988893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997195">
We propose a novel technique of learning how to
transform the source parse trees to improve the trans-
lation qualities of syntax-based translation mod-
els using synchronous context-free grammars. We
transform the source tree phrasal structure into a
set of simpler structures, expose such decisions to
the decoding process, and find the least expensive
transformation operation to better model word re-
ordering. In particular, we integrate synchronous bi-
narizations, verb regrouping, removal of redundant
parse nodes, and incorporate a few important fea-
tures such as translation boundaries. We learn the
structural preferences from the data in a generative
framework. The syntax-based translation system in-
tegrating the proposed techniques outperforms the
best Arabic-English unconstrained system in NIST-
08 evaluations by 1.3 absolute BLEU, which is sta-
tistically significant.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980264705883">
Most syntax-based machine translation models with syn-
chronous context free grammar (SCFG) have been re-
lying on the off-the-shelf monolingual parse structures
to learn the translation equivalences for string-to-tree,
tree-to-string or tree-to-tree grammars. However, state-
of-the-art monolingual parsers are not necessarily well
suited for machine translation in terms of both labels
and chunks/brackets. For instance, in Arabic-to-English
translation, we find only 45.5% of Arabic NP-SBJ struc-
tures are mapped to the English NP-SBJ with machine
alignment and parse trees, and only 60.1% of NP-SBJs
are mapped with human alignment and parse trees as in
§ 2. The chunking is of more concern; at best only 57.4%
source chunking decisions are translated contiguously on
the target side. To translate the rest of the chunks one
has to frequently break the original structures. The main
issue lies in the strong assumption behind SCFG-style
nonterminals – each nonterminal (or variable) assumes a
source chunk should be rewritten into a contiguous chunk
in the target. Without integrating techniques to mod-
ify the parse structures, the SCFGs are not to be effec-
tive even for translating NP-SBJ in linguistically distant
language-pairs such as Arabic-English.
Such problems have been noted in previous literature.
Zollmann and Venugopal (2006) and Marcu et al. (2006)
used broken syntactic fragments to augment their gram-
mars to increase the rule coverage; while we learn opti-
mal tree fragments transformed from the original ones via
a generative framework, they enumerate the fragments
available from the original trees without learning pro-
cess. Mi and Huang (2008) introduced parse forests to
blur the chunking decisions to a certain degree, to ex-
pand search space and reduce parsing errors from 1-best
trees (Mi et al., 2008); others tried to use the parse trees
as soft constraints on top of unlabeled grammar such as
Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang
et al., 2010; Shen et al., 2010) without sufficiently lever-
aging rich tree context. Recent works tried more com-
plex approaches to integrate both parsing and decoding
in one single search space as in (Liu and Liu, 2010), at
the cost of huge search space. In (Zhang et al., 2009),
combinations of tree forest and tree-sequence (Zhang et
al., 2008) based approaches were carried out by adding
pseudo nodes and hyper edges into the forest. Overall,
the forest-based translation can reduce the risks from up-
stream parsing errors and expand the search space, but
it cannot sufficiently address the syntactic divergences
between various language-pairs. The tree sequence ap-
proach adds pseudo nodes and hyper edges to the forest,
which makes the forest even denser and harder for nav-
igation and search. As trees thrive in the search space,
especially with the pseudo nodes and edges being added
to the already dense forest, it is becoming harder to wade
through the deep forest for the best derivation path out.
We propose to simplify suitable subtrees to a reason-
able level, at which the correct reordering can be easily
identified. The transformed structure should be frequent
enough to have rich statistics for learning a model. In-
stead of creating pseudo nodes and edges and make the
forest dense, we transform a tree with a few simple oper-
ators; only meaningful frontier nodes, context nodes and
edges are kept to induce the correct reordering; such oper-
ations also enable the model to share the statistics among
all similar subtrees.
On the basis of our study on investigating the language
divergence between Arabic-English with human aligned
and parsed data, we integrate several simple statistical op-
erations, to transform parse trees adaptively to serve the
</bodyText>
<page confidence="0.971659">
846
</page>
<note confidence="0.979638">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 846–855,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999973825">
translation purpose better. For each source span in the
given sentence, a subgraph, corresponding to an elemen-
tary tree (in Eqn. 1), is proposed for PSCFG translation;
we apply a few operators to transform the subgraph into
some frequent subgraphs seen in the whole training data,
and thus introduce alternative similar translational equiv-
alences to explain the same source span with enriched
statistics and features. For instance, if we regroup two
adjacent nodes IV and NP-SBJ in the tree, we can ob-
tain the correct reordering pattern for verb-subject order,
which is not easily available otherwise. By finding a set
of similar elementary trees derived from the original ele-
mentary trees, statistics can be shared for robust learning.
We also investigate the features using the context be-
yond the phrasal subtree. This is to further disambiguate
the transformed subgraphs so that informative neighbor-
ing nodes and edges can influence the reordering prefer-
ences for each of the transformed trees. For instance, at
the beginning and end of a sentence, we do not expect
dramatic long distance reordering to happen; or under
SBAR context, the clause may prefer monotonic reorder-
ing for verb and subject. Such boundary features were
treated as hard constraints in previous literature in terms
of re-labeling (Huang and Knight, 2006) or re-structuring
(Wang et al., 2010). The boundary cases were not ad-
dressed in the previous literature for trees, and here we
include them in our feature sets for learning a MaxEnt
model to predict the transformations. We integrate the
neighboring context of the subgraph in our transforma-
tion preference predictions, and this improve translation
qualities further.
The rest of the paper is organized as follows: in sec-
tion 2, we analyze the projectable structures using hu-
man aligned and parsed data, to identify the problems for
SCFG in general; in section 3, our proposed approach
is explained in detail, including the statistical operators
using a MaxEnt model; in section 4, we illustrate the in-
tegration of the proposed approach in our decoder; in sec-
tion 5, we present experimental results; in section 6, we
conclude with discussions and future work.
</bodyText>
<sectionHeader confidence="0.954379" genericHeader="method">
2 The Projectable Structures
</sectionHeader>
<bodyText confidence="0.999805666666667">
A context-free style nonterminal in PSCFG rules means
the source span governed by the nonterminal should be
translated into a contiguous target chunk. A “projectable”
phrase-structure means that it is translated into a con-
tiguous span on the target side, and thus can be gener-
alized into a nonterminal in our PSCFG rule. We carried
out a controlled study on the projectable structures using
human annotated parse trees and word alignment for 5k
Arabic-English sentence-pairs.
In Table 1, the unlabeled F-measures with machine
alignment and parse trees show that, for only 48.71% of
the time, the boundaries introduced by the source parses
</bodyText>
<table confidence="0.999513461538462">
Alignment Parse Labels Accuracy
H H NP-SBJ 0.6011
PP 0.3436
NP 0.4832
unlabel 0.5739
M H NP-SBJ 0.5356
PP 0.2765
NP 0.3959
unlabel 0.5305
M M NP-SBJ 0.4555
PP 0.1935
NP 0.3556
unlabel 0.4871
</table>
<tableCaption confidence="0.940337">
Table 1: The labeled and unlabeled F-measures for projecting
the source nodes onto the target side via alignments and parse
trees; unlabeled F-measures show the bracketing accuracies for
translating a source span contiguously. H: human, M: machine.
</tableCaption>
<bodyText confidence="0.999072333333334">
are real translation boundaries that can be explained by a
nonterminal in PSCFG rule. Even for human parse and
alignment, the unlabeled F-measures are still as low as
57.39%. Such statistics show that we should not blindly
learn tree-to-string grammar; additional transformations
to manipulate the bracketing boundaries and labels ac-
cordingly have to be implemented to guarantee the reli-
ability of source-tree based syntax translation grammars.
The transformations could be as simple as merging two
adjacent nonterminals into one bracket to accommodate
non-contiguity on the target side, or lexicalizing those
words which have fork-style, many-to-many alignment,
or unaligned content words to enable the rest of the span
to be generalized into nonterminals. We illustrate several
cases using the tree in Figure 1.
</bodyText>
<figureCaption confidence="0.9652682">
Figure 1: Non-projectable structures in an SBAR tree with
human parses and alignment; there are non-projectable struc-
tures: the deleted nonterminals PRON (+hA), the many-to-
many alignment for IV(ttAlf) PREP(mn), fork-style alignment
for NOUN (Azmp).
</figureCaption>
<figure confidence="0.960819294117647">
In Figure 1, several non-projectable nodes were illus-
the millde east crisis
up
that make
SBAR
S
NP
WHNP VP PP−CLR
NP−SBJ
PRON IV
Alty ttAlf
PREP PRON NOUN
mn +hA Azmp
NOUN
ADJ
AlAwsT
Al$rq
</figure>
<page confidence="0.988296">
847
</page>
<bodyText confidence="0.997309515151515">
trated: the deleted nonterminals PRON (+hA), the many- where α is the target string, containing the terminals
to-many alignment for IV(ttAlf) PREP(mn), fork-style and/or nonterminals in a target language; — is the one-
alignment for NOUN (Azmp). Intuitively, it would be to-one alignment of the nonterminals between γ and α; t
good to glue the nodes NOUN(Al$rq) ADJ(AlAwsT) un- contains possible sequence of transform operations (to be
der the node of NP, because it is more frequent for moving explained later in this section) associated with each rule;
ADJ before NOUN in our training data. It should be eas- m� is a function of enumerating the neighborhood of the
ier to model the swapping of (NOUN ADJ) using the tree source elementary tree γ, and certain tree context (nodes
(NP NOUN, ADJ) instead of the original bigger tree of and edges) can be used to further disambiguate the re-
(NP-SBJ Azmp, NOUN, ADJ) with one lexicalized node. ordering or the given lexical choices. The interior nodes
Approaches in tree-sequence based grammar (Zhang et of γ.vi, however, are not necessarily informative for the
al., 2009) tried to address the bracketing problem by us- reordering decisions, like the unary nodes WHNP,VP, and
ing arbitrary pseudo nodes to weave a new “tree” back PP-CLR in Figure 1; while the frontier nodes γ.vf are the
into the forest for further grammar extractions. Such ap- ones directly executing the reordering decisions. We can
proach may improve grammar coverage, but the pseudo selectively cut off the interior nodes, which have no or
node labels would be arguably a worse choice to split only weak causal relations to the reordering decisions.
the already sparse data. Some of the interior nodes con- This will enable the frequency or derived probabilities
necting the frontier nodes might be very informative for for executing the reordering to be more focused. We call
modeling reordering. Also, due to the introduced pseudo such transformation operators t. We specified a few op-
nodes, it would need exponentially many nonterminals to erators for transforming an elementary tree γ, including
keep track of the matching tree-structures for translations. flattening tree operators such as removing interior nodes
The created pseudo node could easily block the informa- in vi, or grouping the children via binarizations.
tive neighbor nodes associated with the subgraph which Let’s use the trigram “Alty ttAlf mn” in Figure 1 as
could change the reordering nature. For instance, IV and an example, the immediate common parent for the span
NP-SBJ tends to swap at the beginning of a sentence, but is SBAR: γ.` = SBAR; the interior nodes are γ.vi =
it may prefer monotone if they share a common parent of {WHNP VP S PP-CLR}; the frontier nodes are γ.vf =
SBAR for a subclause. In this case, it is unnecessary to (x:PRON x:IV x:PREP). The edges γ.E (as highlighted
create a pseudo node “IV+SBJ” to block useful factors. in Figure 1) connect γ.vi and γ.vf into a subgraph for the
We propose to navigate through the forest, via simpli- given source ngram.
fying trees by grouping the nodes, cutting the branches, For any source span, we look up one elementary tree γ
and attaching connected neighboring informative nodes covering the span, then we select an operator t E T, to
to further disambiguate the derivation path. We apply ex- explore a set of similar elementary trees f(γ, m) = {γ&apos;}
plicit translation motivated operators, on a given mono- as simplified alternatives for translating that source tree
lingual elementary tree, to transform it into similar but (span) γ into an optimal target string α* accordingly. Our
simpler trees, and expose such statistical preferences to generative model is summarized in Eqn. 3:
the decoding process to select the best rewriting rule α* = arg max p.(α&apos;|γ&apos;)X
from the enriched grammar rule sets, for generating tar- �tET;ry&apos;E�t(&apos;y, fn)
get strings. pb(γ&apos;|�t,γ, m)X
3 Elementary Trees to String Grammar pc(flγ, m). (3)
We propose to use variations of an elementary tree, which In our generative scheme, for a given elementary tree
is a connected subgraph fitted in the original monolingual γ, we sample an operator (or a combination of operations)
parse tree. The subgraph is connected so that the frontiers t with the probability of pc(flγ); with operation t, we
(two or more) are connected by their immediate common transform γ into a set of simplified versions γ&apos; E f(γ, m)
parent. Let γ be a source elementary tree: with the probability of pb(γ&apos;|�t, γ); finally we select the
γ =&lt; `; vf, vi, E &gt;, (1) transformed version γ&apos; to generate the target string α&apos;
where vf is a set of frontier nodes which contain nonter- with a probability of p,,,(α&apos;|γ&apos;). Note here, γ&apos; and γ share
minals or words; vi are the interior nodes with source la- the same immediate common parent `, but not necessar-
bels/symbols; E is the set of edges connecting the nodes ily the frontier, or interior, or even neighbors. The frontier
v = vf +vi into a connected subgraph fitted in the source nodes can be merged, lexicalized, or even deleted in the
parse tree; ` is the immediate common parent of the fron- tree-to-string rule associated with γ&apos;, as long as the align-
tier nodes vf. Our proposed grammar rule is formulated ment for the nonterminals are book-kept in the deriva-
as follows: tions. To simplify the model, one can choose the operator
&lt; γ; α; fn; t &gt;, (2) t to be only one level, and the model using a single oper-
848 ator t is to be deterministic. Thus, the final set of models
to learn are pa(α0jγ0) for rule alignment, and the pref-
erence model pb(γ0jf, γ, fn), and the operator proposal
model pc(�tjγ, in), which in our case is a maximum en-
tropy model— the key model in our proposed approach
in this paper for transforming the original elementary tree
into similar trees for evaluating the reordering probabili-
ties.
Eqn. 3 significantly enriches reordering powers for
syntax-based machine translation. This is because it uses
all similar set of elementary trees to generate the best tar-
get strings. In the next section, we’ll first define the op-
erators conceptually, and then explain how we learn each
of the models.
</bodyText>
<subsectionHeader confidence="0.998818">
3.1 Model pa(α0jγ0)
</subsectionHeader>
<bodyText confidence="0.999855142857143">
A log linear model is applied here to approximate
pa(α0jγ0) a exp( A · f f ) via weighted combination (A) of
feature functions ff(α0, γ0), including relative frequen-
cies in both directions, and IBM Model-1 scores in both
directions as γ0 and α0 have lexical items within them.
We also employed a few binary features listed in the fol-
lowing table.
</bodyText>
<equation confidence="0.9610832">
γ0 is observed less than 2 times
(α0, γ0) deletes a src content word
(α0, γ0) deletes a src function word
(α0, γ0) over generates a tgt content word
(α0, γ0) over generates a tgt function word
</equation>
<tableCaption confidence="0.951751">
Table 2: Additional 5 Binary Features for pa(α&apos;|γ&apos;)
</tableCaption>
<subsectionHeader confidence="0.896444">
3.2 Model pb(γ0j�t, γ, m)
</subsectionHeader>
<bodyText confidence="0.999528">
pb(γ0j�t, γ, m) is our preference model. For instance us-
ing the operator t of cutting an unary interior node in
γ.vi, if γ.vi has more than one unary interior node, like
the SBAR tree in Figure 1, having three unary interior
node: WHNP, VP and PP-CLR, pb(γ0j�t, γ, m) specifies
which one should have more probabilities to be cut. In
our case, to make model simple, we simply choose his-
togram/frequency for modeling the choices.
</bodyText>
<subsectionHeader confidence="0.980407">
3.3 Model pc(�tjγ, m)
</subsectionHeader>
<bodyText confidence="0.870522166666667">
pc(�tjγ, m) is our operator proposal model. It ranks
the operators which are valid to be applied for the
given source tree γ together with its neighborhood m.
Here, in our approach, we applied a Maximum Entropy
model, which is also employed to train our Arabic parser:
pc(�tjγ, m) a exp λ� · ff(�t, γ, in). The feature sets we
use here are almost the same set we used to train our Ara-
bic parser; the only difference is the future space here is
operator categories, and we check bag-of-nodes for inte-
rior nodes and frontier nodes. The key feature categories
we used are listed as in the Table 3. The headtable used
in our training is manually built for Arabic.
bag-of-nodes γ.vi
bag-of-nodes and ngram of γ.vf
chunk-level features: left-child, right-child, etc.
lexical features: unigram and bigram
pos features: unigram and bigram
contextual features: surrounding words
</bodyText>
<tableCaption confidence="0.988981">
Table 3: Feature Features for learning p.(flγ, fn)
</tableCaption>
<subsectionHeader confidence="0.940145">
3.4 t: Tree Transformation Function
</subsectionHeader>
<bodyText confidence="0.9997895">
Obvious systematic linguistic divergences between
language-pairs could be handled by some simple oper-
ators such as using binarization to re-group contiguously
aligned children. Here, we start from the human aligned
and parsed data as used in section 2 to explore potential
useful operators.
</bodyText>
<subsectionHeader confidence="0.829556">
3.4.1 Binarizations
</subsectionHeader>
<bodyText confidence="0.999988260869565">
One of the simplest way for transforming a tree is via bi-
narization. Monolingual binarization chooses to re-group
children into smaller subtree with a suitable label for the
newly created root. We choose a function mapping to se-
lect the top-frequent label as the root for the grouped chil-
dren; if such label is not found we simply use the label of
the immediate common parent for γ. In decoding time,
we need to select trees from all possible binarizations,
while in the training time, we restrict the choices allowed
with the alignment constraint, that every grouped chil-
dren should be aligned contiguously on the target side.
Our goal is to simulate the synchronous binarization as
much as we can. In this paper, we applied the four ba-
sic operators for binarizing a tree: left-most, right-most
and additionally head-out left and head-out right for more
than three children. Two examples are given in Table 4,
in which we used LDC style representation for the trees.
With the proper binarization, the structure becomes
rich in sub-structures which allow certain reordering to
happen more likely than others. For instance for the sub-
tree (VP PV NP-SBJ), one would apply stronger statistics
from training data to support the swap of NP-SBJ and PV
for translation.
</bodyText>
<subsectionHeader confidence="0.930656">
3.4.2 Regrouping verbs
</subsectionHeader>
<bodyText confidence="0.983482">
Verbs are keys for reordering especially for Araic-English
with VSO translated into SVO. However, if the verb and
its relevant arguments for reordering are at different lev-
els in the tree, the reordering is difficult to model as more
interior nodes combinations will distract the distributions
and make the model less focused. We provide the fol-
lowing two operations specific for verb in VP trees as in
Table 5.
</bodyText>
<subsectionHeader confidence="0.483017">
3.4.3 Removing interior nodes and edges
</subsectionHeader>
<bodyText confidence="0.9790555">
For reordering patterns, keeping the deep tree structure
might not be the best choice. Sometimes it is not even
</bodyText>
<page confidence="0.989697">
849
</page>
<table confidence="0.999706666666667">
Binarization Operations Examples
right-most (NP Xnoun Xadj1 Xadj2) 7→ (NP Xnoun (ADJP Xadj1 Xadj2))
left-most (VP Xpv XNP-SBJ XSBAR) 7→ (VP (VP Xpv XNP-SBJ) XSBAR)
</table>
<tableCaption confidence="0.729572">
Table 4: Operators for binarizing the trees
</tableCaption>
<table confidence="0.995804333333333">
Operators for regroup verbs Examples
regroup verb (V P1 Xv (V P2 Y )) 7→ (V P1 (V P2 Xv Y ))
regroup verb and remove the top level VP (R (V P1 Xv (R2 Y ))) 7→ (R (R2 XvY ))
</table>
<tableCaption confidence="0.995885">
Table 5: Operators for manipulating the trees
</tableCaption>
<bodyText confidence="0.999879533333333">
possible due to the many-to-many alignment, insertions
and deletions of terminals. So, we introduce the oper-
ators to remove the interior nodes y.vi selectively; this
way, we can flatten the tree, remove irrelevant nodes and
edges, and can use more frequent observations of simpli-
fied structures to capture the reordering patterns. We use
two operators as shown in Table 6.
The second operator deletes all the interior nodes, la-
bels and edges; thus reordering will become a Hiero-alike
(Chiang, 2007) unlabeled rule, and additionally a spe-
cial glue rule: X1X2 → X1X2. This operator is neces-
sary, we need a scheme to automatically back off to the
meaningful glue or Hiero-alike rules, which may lead to a
cheaper derivation path for constructing a partial hypoth-
esis, at the decoding time.
</bodyText>
<figure confidence="0.618479">
NP
</figure>
<figureCaption confidence="0.99723">
Figure 2: A NP tree with an “inside-out” alignment. The nodes
“NP*” and “PP*” are not suitable for generalizing into NTs
used in PSCFG rules.
</figureCaption>
<bodyText confidence="0.998448928571429">
As shown in Table 1, NP brackets has only 35.56% of
time to be translated contiguously as an NP in machine
aligned &amp; parsed data. The NP tree in Figure 2 happens to
be an “inside-out” style alignment, and context free gram-
mar such as ITG (Wu, 1997) can not explain this structure
well without necessary lexicalization. Actually, the Ara-
bic tokens of “dfE Aly AlAnfjAr” form a combination
and is turned into English word “ignite” in an idiomatic
way. With lexicalization, a Hiero style rule “dfE X Aly
AlAnfjAr 7→ to ignite X” is potentially a better alterna-
tive for translating the NP tree. Our operators allow us
to back off to such Hiero-style rules to construct deriva-
tions, which share the immediate common parent NP, as
defined for the elementary tree, for the given source span.
</bodyText>
<subsectionHeader confidence="0.790621">
3.5 in: Neighboring Function
</subsectionHeader>
<bodyText confidence="0.9998888">
For a given elementary tree, we use function in to check
the context beyond the subgraph. This includes looking
the nodes and edges connected to the subgraph. Similar
to the features used in (Dyer et al., 2009), we check the
following three cases.
</bodyText>
<subsectionHeader confidence="0.68271">
3.5.1 Sentence boundaries
</subsectionHeader>
<bodyText confidence="0.999536666666667">
When the tree y frontier sets contain the left-most token,
right-most token, or both sides, we will add to the neigh-
boring nodes the corresponding decoration tags L (left),
R (right), and B (both), respectively. These decorations
are important especially when the reordering patterns for
the same trees are depending on the context. For instance,
at the beginning or end of a sentence, we do not expect
dramatic reordering – moving a token too far away in the
middle of the sentences.
</bodyText>
<sectionHeader confidence="0.35205" genericHeader="method">
3.5.2 SBAR/IP/PP/FRAG boundaries
</sectionHeader>
<bodyText confidence="0.9999676">
We check siblings of the root for y for a few special la-
bels, including SBAR, IP, PP, and FRAG. These labels
indicate a partial sentence or clause, and the reordering
patterns may get different distributions due to the posi-
tion relative to these nodes. For instance, the PV and SBJ
nodes under SBAR tends to have more monotone prefer-
ence for word reordering (Carpuat et al., 2010). We mark
the boundaries with position markers such as L-PP, to in-
dicate having a left sibling PP, R-IP for having a right
sibling IP, and C-SBAR to indicate the elementary tree is
a child of SBAR. These labels are selected mainly based
on our linguistic intuitions and errors in our translation
system. A data-driven approach might be more promis-
ing for identifying useful markups w.r.t specific reorder-
ing patterns.
</bodyText>
<subsectionHeader confidence="0.518882">
3.5.3 Translation boundaries
</subsectionHeader>
<bodyText confidence="0.9999432">
In the Figure 2, there are two special nodes under NP:
NP* and PP*. These two nodes are aligned in a “inside-
out” fashion, and none of them can be generalized into
a nonterminal to be rewritten in a PSCFG rule. In other
words, the phrasal brackets induced from NP* and PP*
</bodyText>
<figure confidence="0.996977">
AlAnfjAr
Aly
dfE
PREP
NOUN
DET+NOUN
DET+NOUN
NP*
NP
PP*
NP
AlAwDAE
to ignite the situation
</figure>
<page confidence="0.981116">
850
</page>
<table confidence="0.934183">
operators for removing nodes/edges Examples
remove unary nodes (R Xt1(R1 (R2 Xt2))) — (R Xt1(R2 Xt2)))
remove all labels (R (R1 Xt1(R2 Xt2))) — (R Xt2Xt1)
</table>
<tableCaption confidence="0.982995">
Table 6: Operators for simplifying the trees
</tableCaption>
<bodyText confidence="0.999925071428572">
are not translation boundaries, and to avoid translation
errors we should identify them by applying a PSCFG
rule on top of them. During training, we label nodes
with translation boundaries, as one additional function
tag; during decoding, we employ the MaxEnt model to
predict the translation boundary label probability for each
span associated with a subgraph y, and discourage deriva-
tions accordingly for using nonterminals over the non-
translation boundary span. The translation boundaries
over elementary trees have much richer representation
power. The previous works as in Xiong et al. (2010),
defined translation boundaries on phrase-decoder style
derivation trees due to the nature of their shift-reduce al-
gorithm, which is a special case in our model.
</bodyText>
<sectionHeader confidence="0.998339" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999959038461539">
Decoding using the proposed elementary tree to string
grammar naturally resembles bottom up chart parsing al-
gorithms. The key difference is at the grammar querying
step. Given a grammar G, and the input source parse tree
7r from a monolingual parser, we first construct the ele-
mentary tree for a source span, and then retrieve all the
relevant subgraphs seen in the given grammar through
the proposed operators. This step is called populating,
using the proposed operators to find all relevant elemen-
tary trees y which may have contributed to explain the
source span, and put them in the corresponding cells in
the chart. There would have been exponential number of
relevant elementary trees to search if we do not have any
restrictions in the populating step; we restrict the maxi-
mum number of interior nodes |y.vi |to be 3, and the size
of frontier nodes |y.vf |to be less than 6; additional prun-
ing for less frequent elementary trees is carried out.
After populating the elementary trees, we construct
the partial hypotheses bottom up, by rewriting the fron-
tier nodes of each elementary tree with the probabili-
ties(costs) for y → α* as in Eqn. 3. Our decoder (Zhao
and Al-Onaizan, 2008) is a template-based chart decoder
in C++. It generalizes over the dotted-product operator in
Earley style parser, to allow us to leverage many opera-
tors t E T as above-mentioned, such as binarizations, at
different levels for constructing partial hypothesis.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99929425">
In our experiments, we built our system using most of the
parallel training data available to us: 250M Arabic run-
ning tokens, corresponding to the “unconstrained” condi-
tion in NIST-MT08. We chose the testsets of newswire
and weblog genres from MT08 and DEV101. In partic-
ular, we choose MT08 to enable the comparison of our
results to the reported results in NIST evaluations. Our
training and test data is summarized in Table 5. For test-
ings, we have 129,908 tokens in our testsets. For lan-
guage models (LM), we used 6-gram LM trained with
10.3 billion English tokens, and also a shrinkage-based
LM (Chen, 2009) – “ModelM” (Chen and Chu, 2010;
Emami et al., 2010) with 150 word-clusters learnt from
2.1 million tokens.
From the parallel data, we extract phrase pairs(blocks)
and elementary trees to string grammar in various con-
figurations: basic tree-to-string rules (Tr2str), elementary
tree-to-string rules with boundaries t(elm2str+inc), and
with both t and in (elm2str+t + in). This is to evalu-
ate the operators’ effects at different levels for decoding.
To learn our MaxEnt models defined in § 3.3, we collect
the events during extracting elm2str grammar in training
time, and learn the model using improved iterative scal-
ing. We use the same training data as that used in training
our Arabic parser. There are 16 thousand human parse
trees with human alignment; additional 1 thousand hu-
man parse and aligned sent-pairs are used as unseen test
set to verify our MaxEnt models and parsers. For our
Arabic parser, we have a labeled F-measure of 78.4%,
and POS tag accuracy 94.9%. In particular, we’ll evaluate
model p,(�t|y, in) in Eqn. 3 for predicting the translation
boundaries in § 3.5.3 for projectable spans as detailed in
§ 5.1.
Our decoder (Zhao and Al-Onaizan, 2008) supports
grammars including monotone, ITG, Hiero, tree-to-
string, string-to-tree, and several mixtures of them (Lee
et al., 2010). We used 19 feature functions, mainly from
those used in phrase-based decoder like Moses (Koehn
et al., 2007), including two language models (one for a
6-gram LM, one for ModelM, one brevity penalty, IBM
Model-1 (Brown et al., 1993) style alignment probabil-
ities in both directions, relative frequency in both direc-
tions, word/rule counts, content/function word mismatch,
together with features on tr2str rule probabilities. We
use BLEU (Papineni et al., 2002) and TER (Snover et
al., 2006) to evaluate translation qualities. Our base-
line used basic elementary tree to string grammar without
any manipulations and boundary markers in the model,
</bodyText>
<footnote confidence="0.9697555">
1DEV10 are unseen testsets used in our GALE project. It was se-
lected from recently released LDC data LDC2010E43.v3.
</footnote>
<page confidence="0.992755">
851
</page>
<table confidence="0.999764">
Data Train MT08-NW MT08-WB Dev10-NW Dev10-WB
# Sents 8,032,837 813 547 1089 1059
# Tokens 349M(ar)/230M(en) 25,926 19,654 41,240 43,088
</table>
<tableCaption confidence="0.999688">
Table 7: Training and test data; using all training parallel training data for 4 test sets
</tableCaption>
<bodyText confidence="0.9999247">
and we achieved a BLEUr4n4 55.01 for MT08-NW, or
a cased BLEU of 53.31, which is close to the best offi-
cially reported result 53.85 for unconstrained systems.2
We expose the statistical decisions in Eqn. 3 as the rule
probability as one of the 19 dimensions, and use Sim-
plex Downhill algorithm with Armijo line search (Zhao
and Chen, 2009) to optimize the weight vector for de-
coding. The algorithm moves all dimensions at the same
time, and empirically achieved more stable results than
MER(Och, 2003) in many of our experiments.
</bodyText>
<subsectionHeader confidence="0.997779">
5.1 Predicting Projectable Structures
</subsectionHeader>
<bodyText confidence="0.999667578947368">
The projectable structure is important for our proposed
elementary tree to string grammar (elm2str). When a
span is predicted not to be a translation boundary, we
want the decoder to prefer alternative derivations out-
side of the immediate elementary tree, or more aggres-
sive manipulation of the trees, such as deleting inte-
rior nodes, to explore unlabeled grammar such as Hi-
ero style rules, with proper costs. We test separately
on predicting the projectable structures, like predicting
function tags in § 3.5.3, for each node in syntactic parse
tree. We use one thousand test sentences with two con-
ditions: human parses and machine parses. There are
totally 40,674 nodes excluding the sentence-level node.
The results are shown in Table 8. It showed our Max-
Ent model is very accurate using human trees: 94.5% of
accuracy, and about 84.7% of accuracy for using the ma-
chine parsed trees. Our accuracies are higher compared
with the 71+% accuracies reported in (Xiong et al., 2010)
for their phrasal decoder.
</bodyText>
<table confidence="0.978502">
Setups Accuracy
Human Parses 94.5%
Machine Parses 84.7%
</table>
<tableCaption confidence="0.999845">
Table 8: Accuracies of predicting projectable structures
</tableCaption>
<bodyText confidence="0.993108">
We zoom in the translation boundaries for MT08-NW,
in which we studied a few important frequent labels in-
cluding VP and NP-SBJ as in Table 9. According to our
MaxEnt model, 20% of times we should discourage a VP
tree to be translated contiguously; such VP trees have an
average span length of 16.9 tokens in MT08-NW. Simi-
lar statistics are 15.9% for S-tree with an average span of
13.8 tokens.
</bodyText>
<footnote confidence="0.9670465">
2See link: http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/mt08
official results v0.html
</footnote>
<table confidence="0.9998958">
Labels total NonProj Percent Avg.len
VP* 4479 920 20.5% 16.9
NP* 14164 825 5.8% 8.12
S* 3123 495 15.9% 13.8
NP-SBJ* 1284 53 4.12% 11.9
</table>
<tableCaption confidence="0.999933">
Table 9: The predicted projectable structures in MT08-NW
</tableCaption>
<bodyText confidence="0.998662428571429">
Using the predicted projectable structures for elm2str
grammar, together with the probability defined in Eqn. 3
as additional cost, the translation results in Table 11 show
it helps BLEU by 0.29 BLEU points (56.13 v.s. 55.84).
The boundary decisions penalize the derivation paths us-
ing nonterminals for non-projectable spans for partial hy-
pothesis construction.
</bodyText>
<table confidence="0.999701444444444">
Setups TER BLEUr4n4
Baseline 39.87 55.01
right-binz (rbz) 39.10 55.19
left-binz (lbz) 39.67 55.31
Head-out-left (hlbz) 39.56 55.50
Head-out-right (hrbz) 39.52 55.53
+all binzation (abz) 39.42 55.60
+regroup-verb 39.29 55.72
+deleting interior nodes γ.vi 38.98 55.84
</table>
<tableCaption confidence="0.999058">
Table 10: TER and BLEU for MT08-NW, using only t(-y)
</tableCaption>
<subsectionHeader confidence="0.998507">
5.2 Integrating t and m�
</subsectionHeader>
<bodyText confidence="0.998922">
We carried out a series of experiments to explore the im-
pacts using t and m� for elm2str grammar. We start from
transforming the trees via simple operator f(γ), and then
expand the function with more tree context to include the
neighboring functions: f(γ, fn).
</bodyText>
<table confidence="0.9990548">
Setups TER BLEUr4n4
Baseline w/ t 38.98 55.84
+ TM Boundaries 38.89 56.13
+ SENT Bound 38.63 56.46
all t(γ, in) 38.61 56.87
</table>
<tableCaption confidence="0.998811">
Table 11: TER and BLEU for MT08-NW, using f(-y, fn).
</tableCaption>
<bodyText confidence="0.896482666666667">
Experiments in Table 10 focus on testing operators es-
pecially binarizations for transforming the trees. In Ta-
ble 10, the four possible binarization methods all improve
</bodyText>
<page confidence="0.993436">
852
</page>
<table confidence="0.9998796">
Data MT08-NW MT08-WB Dev10-NW Dev10-WB
Tr2Str 55.01 39.19 37.33 41.77
elm2str+t 55.84 39.43 38.02 42.70
elm2str+inc 55.57 39.60 37.67 42.54
elm2str+t(-y, in) 56.87 39.82 38.62 42.75
</table>
<tableCaption confidence="0.995952">
Table 12: BLEU scores on various test sets; comparing elementary tree-to-string grammar (tr2str), transformation of the trees
</tableCaption>
<bodyText confidence="0.972112616666667">
(elm2str+t), using the neighboring function for boundaries ( elm2str+Tn), and combination of all together ( elm2str+ T(-y, fn)).
MT08-NW and MT08-WB have four references; Dev10-WB has three references, and Dev10-NW has one reference. BLEUn4
were reported.
over the baseline from +0.18 (via right-most binarization)
to +0.52 (via head-out-right) BLEU points. When we
combine all binarizations (abz), we did not see additive
gains over the best individual case – hrbz. Because during
our decoding time, we do not frequently see large number
of children (maximum at 6), and for smaller trees (with
three or four children), these operators will largely gen-
erate same transformed trees, and that explains the differ-
ences from these individual binarization are small. For
other languages, these binarization choices might give
larger differences. Additionally, regrouping the verbs is
marginally helpful for BLEU and TER. Upon close ex-
aminations, we found it is usually beneficial to group
verb (PV or IV) with its neighboring nodes for expressing
phrases like “have to do” and “will not only”. Deleting
the interior nodes helps on shrinking the trees, so that we
can translate it with more statistics and confidences. It
helps more on TER than BLEU for MT08-NW.
Table 11 extends Table 10 with neighboring function
to further disambiguate the reordering rule using the tree
context. Besides the translation boundary, the reorder-
ing decisions should be different with regard to the posi-
tions of the elementary tree relative to the sentence. At
the sentence-beginning one might expect more for mono-
tone decoding, while in the middle of the sentence, one
might expect more reorderings. Table 11 shows when we
add such boundary markups in our rules, an improvement
of 0.33 BLEU points were obtained (56.46 v.s. 56.13)
on top of the already improved setups. A close check
up showed that the sentence-begin/end markups signifi-
cantly reduced the leading “and” (from Arabic word w#)
in the decoding output. Also, the verb subject order un-
der SBAR seems to be more like monotone with a lead-
ing pronoun, rather than the general strong reordering of
moving verb after subject. Overall, our results showed
that such boundary conditions are helpful for executing
the correct reorderings. We conclude the investigation
with full function f(-y, in), which leads to a BLEUr4n4 of
56.87 (cased BLEUr4n4c 55.16), a significant improve-
ment of 1.77 BLEU point over a already strong baseline.
We apply the setups for several other NW and WEB
datasets to further verify the improvement. Shown in Ta-
ble 12, we apply separately the operators for t and in first,
then combine them as the final results. Varied improve-
ments were observed for different genres. On DEV10-
NW, we observed 1.29 BLEU points improvement, and
about 0.63 and 0.98 improved BLEU points for MT08-
WB and DEV10-WB, respectively. The improvements
for newwire are statistically significant. The improve-
ments for weblog are, however, only marginally better.
One possible reason is the parser quality for web genre is
reliable, as our training data is all in newswire. Regarding
to the individual operators proposed in this paper, we ob-
served consistent improvements of applying them across
all the datasets. The generative model in Eqn. 3 leverages
the operators further by selecting the best transformed
tree form for executing the reorderings.
</bodyText>
<subsectionHeader confidence="0.984121">
5.3 A Translation Example
</subsectionHeader>
<bodyText confidence="0.999982083333333">
To illustrate the advantages of the proposed grammar, we
use a testing case with long distance word reordering and
the source side parse trees. We compare the translation
from a strong phrasal decoder (DTM2) (Ittycheriah and
Roukos, 2007), which is one of the top systems in NIST-
08 evaluation for Arabic-English. The translations from
both decoders with the same training data (LM+TM) are
in Table 13. The highlighted parts in Figure 3 show that,
the rules on partial trees are effectively selected and ap-
plied for capturing long-distance word reordering, which
is otherwise rather difficult to get correct in a phrasal sys-
tem even with a MaxEnt reordering model.
</bodyText>
<sectionHeader confidence="0.996136" genericHeader="conclusions">
6 Discussions and Conclusions
</sectionHeader>
<bodyText confidence="0.983800307692308">
We proposed a framework to learn models to predict
how to transform an elementary tree into its simplified
forms for better executing the word reorderings. Two
types of operators were explored, including (a) trans-
forming the trees via binarizations, grouping or deleting
interior nodes to change the structures; and (b) neighbor-
ing boundary context to further disambiguate the reorder-
ing decisions. Significant improvements were observed
on top of a strong baseline system, and consistent im-
provements were observed across genres; we achieved a
cased BLEU of 55.16 for MT08-NW, which is signifi-
cantly better than the officially reported results in NIST
MT08 Arabic-English evaluations.
</bodyText>
<page confidence="0.996124">
853
</page>
<table confidence="0.996763">
Src Sent qAl AlAmyr EbdAlrHmn bn EbdAlEzyz nA}b wzyr AldfAE AlsEwdy AlsAbq fy tSryH SHAfy An +h
mtfA}l b# qdrp Almmlkp Ely AyjAd Hl l# Alm$klp .
Phrasal Decoder prince abdul rahman bin abdul aziz , deputy minister of defense former saudi said in a press statement
Elm2Str+t(-y, fn) that he was optimistic about the kingdom ’s ability to find a solution to the problem.
former saudi deputy defense minister prince abdul rahman bin abdul aziz said in a press statement
that he was optimistic of the kingdom ’s ability to find a solution to the problem.
</table>
<tableCaption confidence="0.996228">
Table 13: A translation example, comparing with phrasal decoder.
</tableCaption>
<figureCaption confidence="0.995452">
Figure 3: A testing case: illustrating the derivations from chart decoder. The left panel is source parse tree for the Arabic sentence
</figureCaption>
<bodyText confidence="0.9624956">
— the input to our decoder; the right panel is the English translation together with the simplified derivation tree and alignment
from our decoder output. Each “X” is a nonterminal in the grammar rule; a “Block” means a phrase pair is applied to rewrite a
nonterminal; “Glue” and “Hiero” means the unlabeled rules were chosen to explain the span as explained in § 3.4.3 ; “Tree” means
a labeled rule is applied for the span. For instance, for the source span [1,10], a rule is applied on a partial tree with PV and NP-SBJ;
for the span [18,23], a rule is backed off to an unlabeled rule (Hiero-alike); for the span [21,22], it is another partial tree of NPs.
Within the proposed framework, we also presented
several special cases including the translation boundaries
for nonterminals in SCFG for translation. We achieved
a high accuracy of 84.7% for predicting such bound-
aries using MaxEnt model on machine parse trees. Fu-
ture works aim at transforming such non-projectable trees
into projectable form (Eisner, 2003), driven by translation
rules from aligned data(Burkett et al., 2010), and infor-
mative features form both the source 3 and the target sides
(Shen et al., 2008) to enable the system to leverage more
</bodyText>
<footnote confidence="0.945012">
3The BLEU score on MT08-NW has been improved to 57.55 since
the acceptance of this paper, using the proposed technique but with our
GALE P5 data pipeline and setups.
</footnote>
<bodyText confidence="0.999259">
isomorphic trees, and avoid potential detour errors. We
are exploring the incremental decoding framework, like
(Huang and Mi, 2010), to improve pruning and speed.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998265">
This work was partially supported by the Defense Ad-
vanced Research Projects Agency under contract No.
HR0011-08-C-0110. The views and findings contained in
this material are those of the authors and do not necessar-
ily reflect the position or policy of the U.S. government
and no official endorsement should be inferred.
We are also very grateful to the three anonymous re-
viewers for their suggestions and comments.
</bodyText>
<page confidence="0.998401">
854
</page>
<sectionHeader confidence="0.989964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996824213675214">
Peter F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estimation. In
Computational Linguistics, volume 19(2), pages 263–331.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint pars-
ing and alignment with weakly synchronized grammars. In
Proceedings of HLT-NAACL, pages 127–135, Los Angeles,
California, June. Association for Computational Linguistics.
Marine Carpuat, Yuval Marton, and Nizar Habash. 2010.
Reordering matrix post-verbal subjects for arabic-to-english
smt. In 17th Confrence sur le Traitement Automatique des
Langues Naturelles, Montral, Canada, July.
Stanley F. Chen and Stephen M. Chu. 2010. Enhanced word
classing for model m. In Proceedings of Interspeech.
Stanley F. Chen. 2009. Shrinking exponential language mod-
els. In Proceedings of NAACL HLT,, pages 468–476.
David Chiang. 2007. Hierarchical phrase-based translation. In
Computational Linguistics, volume 33(2), pages 201–228.
David Chiang. 2010. Learning to translate with source and
target syntax. In Proc. ACL, pages 1443–1452.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip Resnik.
2009. The University of Maryland statistical machine trans-
lation system for the Fourth Workshop on Machine Trans-
lation. In Proceedings of the Fourth Workshop on Statisti-
cal Machine Translation, pages 145–149, Athens, Greece,
March.
Jason Eisner. 2003. Learning Non-Isomorphic tree mappings
for Machine Translation. In Proc. ACL-2003, pages 205–
208.
Ahmad Emami, Stanley F. Chen, Abe Ittycheriah, Hagen
Soltau, and Bing Zhao. 2010. Decoding with shrinkage-
based language models. In Proceedings of Interspeech.
Bryant Huang and Kevin Knight. 2006. Relabeling syntax
trees to improve syntax-based machine translation quality. In
Proc. NAACL-HLT, pages 240–247.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings of
EMNLP, pages 273–283, Cambridge, MA, October. Asso-
ciation for Computational Linguistics.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou. 2010.
Soft syntactic constraints for hierarchical phrase-based trans-
lation using latent syntactic distributions. In Proceedings of
the 2010 EMNLP, pages 138–147.
Abraham Ittycheriah and Salim Roukos. 2007. Direct transla-
tion model 2. In Proc of HLT-07, pages 57–64.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-
Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-
drej Bojar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine transla-
tion. In ACL, pages 177–180.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Con-
stituent reordering and syntax models for english-to-japanese
statistical machine translation. In Proceedings of Coling-
2010, pages 626–634, Beijing, China, August.
Yang Liu and Qun Liu. 2010. Joint parsing and translation.
In Proceedings of COLING 2010,, pages 707–715, Beijing,
China, August.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. Spmt: Statistical machine translation with
syntactified target language phraases. In Proceedings of
EMNLP-2006, pages 44–52.
Yuval Marton and Philip Resnik. 2008. Soft syntactic con-
straints for hierarchical phrased-based translation. In Pro-
ceedings of ACL-08: HLT, pages 1003–1011.
Haitao Mi and Liang Huang. 2008. Forest-based translation
rule extraction. In Proceedings of EMNLP 2008, pages 206–
214.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based
translation. In In Proceedings of ACL-HLT, pages 192–199.
Franz Josef Och. 2003. Minimum error rate training in Statis-
tical Machine Translation. In ACL-2003, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proc. of the ACL-02), pages 311–318,
Philadelphia, PA, July.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new
string-to-dependency machine translation algorithm with a
target dependency language model. In Proceedings of ACL-
08: HLT, pages 577–585, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Libin Shen, Bing Zhang, Spyros Matsoukas, Jinxi Xu, and
Ralph Weischedel. 2010. Statistical machine translation
with a factorized grammar. In Proceedings of the 2010
EMNLP, pages 616–625, Cambridge, MA, October. Asso-
ciation for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In AMTA.
W. Wang, J. May, K. Knight, and D. Marcu. 2010. Re-
structuring, re-labeling, and re-aligning for syntax-based sta-
tistical machine translation. In Computational Linguistics,
volume 36(2), pages 247–277.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. In Computational
Linguistics, volume 23(3), pages 377–403.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learn-
ing translation boundaries for phrase-based decoding. In
NAACL-HLT 2010, pages 136–144.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim
Tan, and Sheng Li. 2008. A tree sequence alignment-based
tree-to-tree translation model. In ACL-HLT, pages 559–567.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew Lim
Tan. 2009. Forest-based tree sequence to string translation
model. In Proc. of ACL 2009, pages 172–180.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing local
and non-local word-reordering patterns for syntax-based ma-
chine translation. In Proceedings of EMNLP, pages 572–
581, Honolulu, Hawaii, October.
Bing Zhao and Shengyuan Chen. 2009. A simplex armijo
downhill algorithm for optimizing statistical machine trans-
lation decoding parameters. In Proceedings of HLT-NAACL,
pages 21–24, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In Proc. of
NAACL 2006 - Workshop on SMT, pages 138–141.
</reference>
<page confidence="0.99852">
855
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933898">
<title confidence="0.999786">Learning to Transform and Select Elementary Trees for Syntax-based Machine Translations</title>
<author confidence="0.994304">T J Watson Carnegie Mellon</author>
<email confidence="0.999327">ysuklee,andliul@andrew.cmu.edu</email>
<abstract confidence="0.996809947368421">We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NISTevaluations by BLEU, which is statistically significant.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>19</volume>
<issue>2</issue>
<pages>263--331</pages>
<marker>Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent. J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. In Computational Linguistics, volume 19(2), pages 263–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>127--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="40139" citStr="Burkett et al., 2010" startWordPosition="6559" endWordPosition="6562">], a rule is applied on a partial tree with PV and NP-SBJ; for the span [18,23], a rule is backed off to an unlabeled rule (Hiero-alike); for the span [21,22], it is another partial tree of NPs. Within the proposed framework, we also presented several special cases including the translation boundaries for nonterminals in SCFG for translation. We achieved a high accuracy of 84.7% for predicting such boundaries using MaxEnt model on machine parse trees. Future works aim at transforming such non-projectable trees into projectable form (Eisner, 2003), driven by translation rules from aligned data(Burkett et al., 2010), and informative features form both the source 3 and the target sides (Shen et al., 2008) to enable the system to leverage more 3The BLEU score on MT08-NW has been improved to 57.55 since the acceptance of this paper, using the proposed technique but with our GALE P5 data pipeline and setups. isomorphic trees, and avoid potential detour errors. We are exploring the incremental decoding framework, like (Huang and Mi, 2010), to improve pruning and speed. Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency under contract No. HR0011-08-C-0110. The vi</context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In Proceedings of HLT-NAACL, pages 127–135, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
</authors>
<title>Reordering matrix post-verbal subjects for arabic-to-english smt.</title>
<date>2010</date>
<booktitle>In 17th Confrence sur le Traitement Automatique des Langues Naturelles,</booktitle>
<location>Montral, Canada,</location>
<contexts>
<context position="23371" citStr="Carpuat et al., 2010" startWordPosition="3839" endWordPosition="3842">tterns for the same trees are depending on the context. For instance, at the beginning or end of a sentence, we do not expect dramatic reordering – moving a token too far away in the middle of the sentences. 3.5.2 SBAR/IP/PP/FRAG boundaries We check siblings of the root for y for a few special labels, including SBAR, IP, PP, and FRAG. These labels indicate a partial sentence or clause, and the reordering patterns may get different distributions due to the position relative to these nodes. For instance, the PV and SBJ nodes under SBAR tends to have more monotone preference for word reordering (Carpuat et al., 2010). We mark the boundaries with position markers such as L-PP, to indicate having a left sibling PP, R-IP for having a right sibling IP, and C-SBAR to indicate the elementary tree is a child of SBAR. These labels are selected mainly based on our linguistic intuitions and errors in our translation system. A data-driven approach might be more promising for identifying useful markups w.r.t specific reordering patterns. 3.5.3 Translation boundaries In the Figure 2, there are two special nodes under NP: NP* and PP*. These two nodes are aligned in a “insideout” fashion, and none of them can be general</context>
</contexts>
<marker>Carpuat, Marton, Habash, 2010</marker>
<rawString>Marine Carpuat, Yuval Marton, and Nizar Habash. 2010. Reordering matrix post-verbal subjects for arabic-to-english smt. In 17th Confrence sur le Traitement Automatique des Langues Naturelles, Montral, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Stephen M Chu</author>
</authors>
<title>Enhanced word classing for model m.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="27261" citStr="Chen and Chu, 2010" startWordPosition="4488" endWordPosition="4491">uilt our system using most of the parallel training data available to us: 250M Arabic running tokens, corresponding to the “unconstrained” condition in NIST-MT08. We chose the testsets of newswire and weblog genres from MT08 and DEV101. In particular, we choose MT08 to enable the comparison of our results to the reported results in NIST evaluations. Our training and test data is summarized in Table 5. For testings, we have 129,908 tokens in our testsets. For language models (LM), we used 6-gram LM trained with 10.3 billion English tokens, and also a shrinkage-based LM (Chen, 2009) – “ModelM” (Chen and Chu, 2010; Emami et al., 2010) with 150 word-clusters learnt from 2.1 million tokens. From the parallel data, we extract phrase pairs(blocks) and elementary trees to string grammar in various configurations: basic tree-to-string rules (Tr2str), elementary tree-to-string rules with boundaries t(elm2str+inc), and with both t and in (elm2str+t + in). This is to evaluate the operators’ effects at different levels for decoding. To learn our MaxEnt models defined in § 3.3, we collect the events during extracting elm2str grammar in training time, and learn the model using improved iterative scaling. We use th</context>
</contexts>
<marker>Chen, Chu, 2010</marker>
<rawString>Stanley F. Chen and Stephen M. Chu. 2010. Enhanced word classing for model m. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Shrinking exponential language models.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL HLT,,</booktitle>
<pages>468--476</pages>
<contexts>
<context position="27230" citStr="Chen, 2009" startWordPosition="4484" endWordPosition="4485">In our experiments, we built our system using most of the parallel training data available to us: 250M Arabic running tokens, corresponding to the “unconstrained” condition in NIST-MT08. We chose the testsets of newswire and weblog genres from MT08 and DEV101. In particular, we choose MT08 to enable the comparison of our results to the reported results in NIST evaluations. Our training and test data is summarized in Table 5. For testings, we have 129,908 tokens in our testsets. For language models (LM), we used 6-gram LM trained with 10.3 billion English tokens, and also a shrinkage-based LM (Chen, 2009) – “ModelM” (Chen and Chu, 2010; Emami et al., 2010) with 150 word-clusters learnt from 2.1 million tokens. From the parallel data, we extract phrase pairs(blocks) and elementary trees to string grammar in various configurations: basic tree-to-string rules (Tr2str), elementary tree-to-string rules with boundaries t(elm2str+inc), and with both t and in (elm2str+t + in). This is to evaluate the operators’ effects at different levels for decoding. To learn our MaxEnt models defined in § 3.3, we collect the events during extracting elm2str grammar in training time, and learn the model using improv</context>
<context position="29859" citStr="Chen, 2009" startWordPosition="4909" endWordPosition="4910">ecently released LDC data LDC2010E43.v3. 851 Data Train MT08-NW MT08-WB Dev10-NW Dev10-WB # Sents 8,032,837 813 547 1089 1059 # Tokens 349M(ar)/230M(en) 25,926 19,654 41,240 43,088 Table 7: Training and test data; using all training parallel training data for 4 test sets and we achieved a BLEUr4n4 55.01 for MT08-NW, or a cased BLEU of 53.31, which is close to the best officially reported result 53.85 for unconstrained systems.2 We expose the statistical decisions in Eqn. 3 as the rule probability as one of the 19 dimensions, and use Simplex Downhill algorithm with Armijo line search (Zhao and Chen, 2009) to optimize the weight vector for decoding. The algorithm moves all dimensions at the same time, and empirically achieved more stable results than MER(Och, 2003) in many of our experiments. 5.1 Predicting Projectable Structures The projectable structure is important for our proposed elementary tree to string grammar (elm2str). When a span is predicted not to be a translation boundary, we want the decoder to prefer alternative derivations outside of the immediate elementary tree, or more aggressive manipulation of the trees, such as deleting interior nodes, to explore unlabeled grammar such as</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Stanley F. Chen. 2009. Shrinking exponential language models. In Proceedings of NAACL HLT,, pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>33</volume>
<issue>2</issue>
<pages>201--228</pages>
<contexts>
<context position="20951" citStr="Chiang, 2007" startWordPosition="3424" endWordPosition="3425"> and remove the top level VP (R (V P1 Xv (R2 Y ))) 7→ (R (R2 XvY )) Table 5: Operators for manipulating the trees possible due to the many-to-many alignment, insertions and deletions of terminals. So, we introduce the operators to remove the interior nodes y.vi selectively; this way, we can flatten the tree, remove irrelevant nodes and edges, and can use more frequent observations of simplified structures to capture the reordering patterns. We use two operators as shown in Table 6. The second operator deletes all the interior nodes, labels and edges; thus reordering will become a Hiero-alike (Chiang, 2007) unlabeled rule, and additionally a special glue rule: X1X2 → X1X2. This operator is necessary, we need a scheme to automatically back off to the meaningful glue or Hiero-alike rules, which may lead to a cheaper derivation path for constructing a partial hypothesis, at the decoding time. NP Figure 2: A NP tree with an “inside-out” alignment. The nodes “NP*” and “PP*” are not suitable for generalizing into NTs used in PSCFG rules. As shown in Table 1, NP brackets has only 35.56% of time to be translated contiguously as an NP in machine aligned &amp; parsed data. The NP tree in Figure 2 happens to b</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. In Computational Linguistics, volume 33(2), pages 201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to translate with source and target syntax.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="3136" citStr="Chiang, 2010" startWordPosition="469" endWordPosition="470">et al. (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. Overall, the forest-based translation can reduce the risks from upstream parsing errors and expand the search space, but it cannot sufficiently address the </context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to translate with source and target syntax. In Proc. ACL, pages 1443–1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Hendra Setiawan</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>The University of Maryland statistical machine translation system for the Fourth Workshop on Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>145--149</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="22409" citStr="Dyer et al., 2009" startWordPosition="3677" endWordPosition="3680"> English word “ignite” in an idiomatic way. With lexicalization, a Hiero style rule “dfE X Aly AlAnfjAr 7→ to ignite X” is potentially a better alternative for translating the NP tree. Our operators allow us to back off to such Hiero-style rules to construct derivations, which share the immediate common parent NP, as defined for the elementary tree, for the given source span. 3.5 in: Neighboring Function For a given elementary tree, we use function in to check the context beyond the subgraph. This includes looking the nodes and edges connected to the subgraph. Similar to the features used in (Dyer et al., 2009), we check the following three cases. 3.5.1 Sentence boundaries When the tree y frontier sets contain the left-most token, right-most token, or both sides, we will add to the neighboring nodes the corresponding decoration tags L (left), R (right), and B (both), respectively. These decorations are important especially when the reordering patterns for the same trees are depending on the context. For instance, at the beginning or end of a sentence, we do not expect dramatic reordering – moving a token too far away in the middle of the sentences. 3.5.2 SBAR/IP/PP/FRAG boundaries We check siblings </context>
</contexts>
<marker>Dyer, Setiawan, Marton, Resnik, 2009</marker>
<rawString>Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip Resnik. 2009. The University of Maryland statistical machine translation system for the Fourth Workshop on Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 145–149, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning Non-Isomorphic tree mappings for Machine Translation. In</title>
<date>2003</date>
<booktitle>Proc. ACL-2003,</booktitle>
<pages>205--208</pages>
<contexts>
<context position="40070" citStr="Eisner, 2003" startWordPosition="6551" endWordPosition="6552">applied for the span. For instance, for the source span [1,10], a rule is applied on a partial tree with PV and NP-SBJ; for the span [18,23], a rule is backed off to an unlabeled rule (Hiero-alike); for the span [21,22], it is another partial tree of NPs. Within the proposed framework, we also presented several special cases including the translation boundaries for nonterminals in SCFG for translation. We achieved a high accuracy of 84.7% for predicting such boundaries using MaxEnt model on machine parse trees. Future works aim at transforming such non-projectable trees into projectable form (Eisner, 2003), driven by translation rules from aligned data(Burkett et al., 2010), and informative features form both the source 3 and the target sides (Shen et al., 2008) to enable the system to leverage more 3The BLEU score on MT08-NW has been improved to 57.55 since the acceptance of this paper, using the proposed technique but with our GALE P5 data pipeline and setups. isomorphic trees, and avoid potential detour errors. We are exploring the incremental decoding framework, like (Huang and Mi, 2010), to improve pruning and speed. Acknowledgments This work was partially supported by the Defense Advanced</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning Non-Isomorphic tree mappings for Machine Translation. In Proc. ACL-2003, pages 205– 208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Stanley F Chen</author>
<author>Abe Ittycheriah</author>
<author>Hagen Soltau</author>
<author>Bing Zhao</author>
</authors>
<title>Decoding with shrinkagebased language models.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="27282" citStr="Emami et al., 2010" startWordPosition="4492" endWordPosition="4495">g most of the parallel training data available to us: 250M Arabic running tokens, corresponding to the “unconstrained” condition in NIST-MT08. We chose the testsets of newswire and weblog genres from MT08 and DEV101. In particular, we choose MT08 to enable the comparison of our results to the reported results in NIST evaluations. Our training and test data is summarized in Table 5. For testings, we have 129,908 tokens in our testsets. For language models (LM), we used 6-gram LM trained with 10.3 billion English tokens, and also a shrinkage-based LM (Chen, 2009) – “ModelM” (Chen and Chu, 2010; Emami et al., 2010) with 150 word-clusters learnt from 2.1 million tokens. From the parallel data, we extract phrase pairs(blocks) and elementary trees to string grammar in various configurations: basic tree-to-string rules (Tr2str), elementary tree-to-string rules with boundaries t(elm2str+inc), and with both t and in (elm2str+t + in). This is to evaluate the operators’ effects at different levels for decoding. To learn our MaxEnt models defined in § 3.3, we collect the events during extracting elm2str grammar in training time, and learn the model using improved iterative scaling. We use the same training data </context>
</contexts>
<marker>Emami, Chen, Ittycheriah, Soltau, Zhao, 2010</marker>
<rawString>Ahmad Emami, Stanley F. Chen, Abe Ittycheriah, Hagen Soltau, and Bing Zhao. 2010. Decoding with shrinkagebased language models. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryant Huang</author>
<author>Kevin Knight</author>
</authors>
<title>Relabeling syntax trees to improve syntax-based machine translation quality.</title>
<date>2006</date>
<booktitle>In Proc. NAACL-HLT,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="6425" citStr="Huang and Knight, 2006" startWordPosition="996" endWordPosition="999">be shared for robust learning. We also investigate the features using the context beyond the phrasal subtree. This is to further disambiguate the transformed subgraphs so that informative neighboring nodes and edges can influence the reordering preferences for each of the transformed trees. For instance, at the beginning and end of a sentence, we do not expect dramatic long distance reordering to happen; or under SBAR context, the clause may prefer monotonic reordering for verb and subject. Such boundary features were treated as hard constraints in previous literature in terms of re-labeling (Huang and Knight, 2006) or re-structuring (Wang et al., 2010). The boundary cases were not addressed in the previous literature for trees, and here we include them in our feature sets for learning a MaxEnt model to predict the transformations. We integrate the neighboring context of the subgraph in our transformation preference predictions, and this improve translation qualities further. The rest of the paper is organized as follows: in section 2, we analyze the projectable structures using human aligned and parsed data, to identify the problems for SCFG in general; in section 3, our proposed approach is explained i</context>
</contexts>
<marker>Huang, Knight, 2006</marker>
<rawString>Bryant Huang and Kevin Knight. 2006. Relabeling syntax trees to improve syntax-based machine translation quality. In Proc. NAACL-HLT, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Haitao Mi</author>
</authors>
<title>Efficient incremental decoding for tree-to-string translation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>273--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<marker>Huang, Mi, 2010</marker>
<rawString>Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings of EMNLP, pages 273–283, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Martin Cmejrek</author>
<author>Bowen Zhou</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 EMNLP,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="3156" citStr="Huang et al., 2010" startWordPosition="471" endWordPosition="474">used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. Overall, the forest-based translation can reduce the risks from upstream parsing errors and expand the search space, but it cannot sufficiently address the syntactic divergence</context>
</contexts>
<marker>Huang, Cmejrek, Zhou, 2010</marker>
<rawString>Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou. 2010. Soft syntactic constraints for hierarchical phrase-based translation using latent syntactic distributions. In Proceedings of the 2010 EMNLP, pages 138–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Direct translation model 2. In</title>
<date>2007</date>
<booktitle>Proc of HLT-07,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="37157" citStr="Ittycheriah and Roukos, 2007" startWordPosition="6069" endWordPosition="6072">the parser quality for web genre is reliable, as our training data is all in newswire. Regarding to the individual operators proposed in this paper, we observed consistent improvements of applying them across all the datasets. The generative model in Eqn. 3 leverages the operators further by selecting the best transformed tree form for executing the reorderings. 5.3 A Translation Example To illustrate the advantages of the proposed grammar, we use a testing case with long distance word reordering and the source side parse trees. We compare the translation from a strong phrasal decoder (DTM2) (Ittycheriah and Roukos, 2007), which is one of the top systems in NIST08 evaluation for Arabic-English. The translations from both decoders with the same training data (LM+TM) are in Table 13. The highlighted parts in Figure 3 show that, the rules on partial trees are effectively selected and applied for capturing long-distance word reordering, which is otherwise rather difficult to get correct in a phrasal system even with a MaxEnt reordering model. 6 Discussions and Conclusions We proposed a framework to learn models to predict how to transform an elementary tree into its simplified forms for better executing the word r</context>
</contexts>
<marker>Ittycheriah, Roukos, 2007</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2007. Direct translation model 2. In Proc of HLT-07, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris CallisonBurch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation. In</title>
<date>2007</date>
<booktitle>ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="28637" citStr="Koehn et al., 2007" startWordPosition="4709" endWordPosition="4712">rse and aligned sent-pairs are used as unseen test set to verify our MaxEnt models and parsers. For our Arabic parser, we have a labeled F-measure of 78.4%, and POS tag accuracy 94.9%. In particular, we’ll evaluate model p,(�t|y, in) in Eqn. 3 for predicting the translation boundaries in § 3.5.3 for projectable spans as detailed in § 5.1. Our decoder (Zhao and Al-Onaizan, 2008) supports grammars including monotone, ITG, Hiero, tree-tostring, string-to-tree, and several mixtures of them (Lee et al., 2010). We used 19 feature functions, mainly from those used in phrase-based decoder like Moses (Koehn et al., 2007), including two language models (one for a 6-gram LM, one for ModelM, one brevity penalty, IBM Model-1 (Brown et al., 1993) style alignment probabilities in both directions, relative frequency in both directions, word/rule counts, content/function word mismatch, together with features on tr2str rule probabilities. We use BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to evaluate translation qualities. Our baseline used basic elementary tree to string grammar without any manipulations and boundary markers in the model, 1DEV10 are unseen testsets used in our GALE project. It was sele</context>
</contexts>
<marker>Koehn, Hoang, Birch, CallisonBurch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris CallisonBurch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Bing Zhao</author>
<author>Xiaoqian Luo</author>
</authors>
<title>Constituent reordering and syntax models for english-to-japanese statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of Coling2010,</booktitle>
<pages>626--634</pages>
<location>Beijing, China,</location>
<contexts>
<context position="28527" citStr="Lee et al., 2010" startWordPosition="4691" endWordPosition="4694"> Arabic parser. There are 16 thousand human parse trees with human alignment; additional 1 thousand human parse and aligned sent-pairs are used as unseen test set to verify our MaxEnt models and parsers. For our Arabic parser, we have a labeled F-measure of 78.4%, and POS tag accuracy 94.9%. In particular, we’ll evaluate model p,(�t|y, in) in Eqn. 3 for predicting the translation boundaries in § 3.5.3 for projectable spans as detailed in § 5.1. Our decoder (Zhao and Al-Onaizan, 2008) supports grammars including monotone, ITG, Hiero, tree-tostring, string-to-tree, and several mixtures of them (Lee et al., 2010). We used 19 feature functions, mainly from those used in phrase-based decoder like Moses (Koehn et al., 2007), including two language models (one for a 6-gram LM, one for ModelM, one brevity penalty, IBM Model-1 (Brown et al., 1993) style alignment probabilities in both directions, relative frequency in both directions, word/rule counts, content/function word mismatch, together with features on tr2str rule probabilities. We use BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to evaluate translation qualities. Our baseline used basic elementary tree to string grammar without any man</context>
</contexts>
<marker>Lee, Zhao, Luo, 2010</marker>
<rawString>Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Constituent reordering and syntax models for english-to-japanese statistical machine translation. In Proceedings of Coling2010, pages 626–634, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>Joint parsing and translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING 2010,,</booktitle>
<pages>707--715</pages>
<location>Beijing, China,</location>
<contexts>
<context position="3362" citStr="Liu and Liu, 2010" startWordPosition="506" endWordPosition="509">he fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. Overall, the forest-based translation can reduce the risks from upstream parsing errors and expand the search space, but it cannot sufficiently address the syntactic divergences between various language-pairs. The tree sequence approach adds pseudo nodes and hyper edges to the forest, which makes the forest even denser and harder for navigation and search. As trees thrive in the </context>
</contexts>
<marker>Liu, Liu, 2010</marker>
<rawString>Yang Liu and Qun Liu. 2010. Joint parsing and translation. In Proceedings of COLING 2010,, pages 707–715, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>Spmt: Statistical machine translation with syntactified target language phraases.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-2006,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="2537" citStr="Marcu et al. (2006)" startWordPosition="368" endWordPosition="371">anslated contiguously on the target side. To translate the rest of the chunks one has to frequently break the original structures. The main issue lies in the strong assumption behind SCFG-style nonterminals – each nonterminal (or variable) assumes a source chunk should be rewritten into a contiguous chunk in the target. Without integrating techniques to modify the parse structures, the SCFGs are not to be effective even for translating NP-SBJ in linguistically distant language-pairs such as Arabic-English. Such problems have been noted in previous literature. Zollmann and Venugopal (2006) and Marcu et al. (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010;</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. Spmt: Statistical machine translation with syntactified target language phraases. In Proceedings of EMNLP-2006, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="3122" citStr="Marton and Resnik, 2008" startWordPosition="465" endWordPosition="468">nugopal (2006) and Marcu et al. (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. Overall, the forest-based translation can reduce the risks from upstream parsing errors and expand the search space, but it cannot sufficientl</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of ACL-08: HLT, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP 2008,</booktitle>
<pages>206--214</pages>
<contexts>
<context position="2836" citStr="Mi and Huang (2008)" startWordPosition="415" endWordPosition="418">s chunk in the target. Without integrating techniques to modify the parse structures, the SCFGs are not to be effective even for translating NP-SBJ in linguistically distant language-pairs such as Arabic-English. Such problems have been noted in previous literature. Zollmann and Venugopal (2006) and Marcu et al. (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations </context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP 2008, pages 206– 214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forest-based translation. In</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="2998" citStr="Mi et al., 2008" startWordPosition="443" endWordPosition="446"> distant language-pairs such as Arabic-English. Such problems have been noted in previous literature. Zollmann and Venugopal (2006) and Marcu et al. (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. Overall, the fores</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In In Proceedings of ACL-HLT, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in Statistical Machine Translation. In</title>
<date>2003</date>
<booktitle>ACL-2003,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="30021" citStr="Och, 2003" startWordPosition="4935" endWordPosition="4936">654 41,240 43,088 Table 7: Training and test data; using all training parallel training data for 4 test sets and we achieved a BLEUr4n4 55.01 for MT08-NW, or a cased BLEU of 53.31, which is close to the best officially reported result 53.85 for unconstrained systems.2 We expose the statistical decisions in Eqn. 3 as the rule probability as one of the 19 dimensions, and use Simplex Downhill algorithm with Armijo line search (Zhao and Chen, 2009) to optimize the weight vector for decoding. The algorithm moves all dimensions at the same time, and empirically achieved more stable results than MER(Och, 2003) in many of our experiments. 5.1 Predicting Projectable Structures The projectable structure is important for our proposed elementary tree to string grammar (elm2str). When a span is predicted not to be a translation boundary, we want the decoder to prefer alternative derivations outside of the immediate elementary tree, or more aggressive manipulation of the trees, such as deleting interior nodes, to explore unlabeled grammar such as Hiero style rules, with proper costs. We test separately on predicting the projectable structures, like predicting function tags in § 3.5.3, for each node in syn</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in Statistical Machine Translation. In ACL-2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL-02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="28988" citStr="Papineni et al., 2002" startWordPosition="4762" endWordPosition="4765">oder (Zhao and Al-Onaizan, 2008) supports grammars including monotone, ITG, Hiero, tree-tostring, string-to-tree, and several mixtures of them (Lee et al., 2010). We used 19 feature functions, mainly from those used in phrase-based decoder like Moses (Koehn et al., 2007), including two language models (one for a 6-gram LM, one for ModelM, one brevity penalty, IBM Model-1 (Brown et al., 1993) style alignment probabilities in both directions, relative frequency in both directions, word/rule counts, content/function word mismatch, together with features on tr2str rule probabilities. We use BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to evaluate translation qualities. Our baseline used basic elementary tree to string grammar without any manipulations and boundary markers in the model, 1DEV10 are unseen testsets used in our GALE project. It was selected from recently released LDC data LDC2010E43.v3. 851 Data Train MT08-NW MT08-WB Dev10-NW Dev10-WB # Sents 8,032,837 813 547 1089 1059 # Tokens 349M(ar)/230M(en) 25,926 19,654 41,240 43,088 Table 7: Training and test data; using all training parallel training data for 4 test sets and we achieved a BLEUr4n4 55.01 for MT08-NW, or a cased BLEU of 53.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of the ACL-02), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>577--585</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="40229" citStr="Shen et al., 2008" startWordPosition="6576" endWordPosition="6579">cked off to an unlabeled rule (Hiero-alike); for the span [21,22], it is another partial tree of NPs. Within the proposed framework, we also presented several special cases including the translation boundaries for nonterminals in SCFG for translation. We achieved a high accuracy of 84.7% for predicting such boundaries using MaxEnt model on machine parse trees. Future works aim at transforming such non-projectable trees into projectable form (Eisner, 2003), driven by translation rules from aligned data(Burkett et al., 2010), and informative features form both the source 3 and the target sides (Shen et al., 2008) to enable the system to leverage more 3The BLEU score on MT08-NW has been improved to 57.55 since the acceptance of this paper, using the proposed technique but with our GALE P5 data pipeline and setups. isomorphic trees, and avoid potential detour errors. We are exploring the incremental decoding framework, like (Huang and Mi, 2010), to improve pruning and speed. Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency under contract No. HR0011-08-C-0110. The views and findings contained in this material are those of the authors and do not necessaril</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL08: HLT, pages 577–585, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>Statistical machine translation with a factorized grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 EMNLP,</booktitle>
<pages>616--625</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="3176" citStr="Shen et al., 2010" startWordPosition="475" endWordPosition="478">c fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. Overall, the forest-based translation can reduce the risks from upstream parsing errors and expand the search space, but it cannot sufficiently address the syntactic divergences between various la</context>
</contexts>
<marker>Shen, Zhang, Matsoukas, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Bing Zhang, Spyros Matsoukas, Jinxi Xu, and Ralph Weischedel. 2010. Statistical machine translation with a factorized grammar. In Proceedings of the 2010 EMNLP, pages 616–625, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="29018" citStr="Snover et al., 2006" startWordPosition="4768" endWordPosition="4771"> supports grammars including monotone, ITG, Hiero, tree-tostring, string-to-tree, and several mixtures of them (Lee et al., 2010). We used 19 feature functions, mainly from those used in phrase-based decoder like Moses (Koehn et al., 2007), including two language models (one for a 6-gram LM, one for ModelM, one brevity penalty, IBM Model-1 (Brown et al., 1993) style alignment probabilities in both directions, relative frequency in both directions, word/rule counts, content/function word mismatch, together with features on tr2str rule probabilities. We use BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to evaluate translation qualities. Our baseline used basic elementary tree to string grammar without any manipulations and boundary markers in the model, 1DEV10 are unseen testsets used in our GALE project. It was selected from recently released LDC data LDC2010E43.v3. 851 Data Train MT08-NW MT08-WB Dev10-NW Dev10-WB # Sents 8,032,837 813 547 1089 1059 # Tokens 349M(ar)/230M(en) 25,926 19,654 41,240 43,088 Table 7: Training and test data; using all training parallel training data for 4 test sets and we achieved a BLEUr4n4 55.01 for MT08-NW, or a cased BLEU of 53.31, which is close to the best</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J May</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Restructuring, re-labeling, and re-aligning for syntax-based statistical machine translation.</title>
<date>2010</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>36</volume>
<issue>2</issue>
<pages>247--277</pages>
<contexts>
<context position="6463" citStr="Wang et al., 2010" startWordPosition="1002" endWordPosition="1005">stigate the features using the context beyond the phrasal subtree. This is to further disambiguate the transformed subgraphs so that informative neighboring nodes and edges can influence the reordering preferences for each of the transformed trees. For instance, at the beginning and end of a sentence, we do not expect dramatic long distance reordering to happen; or under SBAR context, the clause may prefer monotonic reordering for verb and subject. Such boundary features were treated as hard constraints in previous literature in terms of re-labeling (Huang and Knight, 2006) or re-structuring (Wang et al., 2010). The boundary cases were not addressed in the previous literature for trees, and here we include them in our feature sets for learning a MaxEnt model to predict the transformations. We integrate the neighboring context of the subgraph in our transformation preference predictions, and this improve translation qualities further. The rest of the paper is organized as follows: in section 2, we analyze the projectable structures using human aligned and parsed data, to identify the problems for SCFG in general; in section 3, our proposed approach is explained in detail, including the statistical op</context>
</contexts>
<marker>Wang, May, Knight, Marcu, 2010</marker>
<rawString>W. Wang, J. May, K. Knight, and D. Marcu. 2010. Restructuring, re-labeling, and re-aligning for syntax-based statistical machine translation. In Computational Linguistics, volume 36(2), pages 247–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="21633" citStr="Wu, 1997" startWordPosition="3548" endWordPosition="3549">operator is necessary, we need a scheme to automatically back off to the meaningful glue or Hiero-alike rules, which may lead to a cheaper derivation path for constructing a partial hypothesis, at the decoding time. NP Figure 2: A NP tree with an “inside-out” alignment. The nodes “NP*” and “PP*” are not suitable for generalizing into NTs used in PSCFG rules. As shown in Table 1, NP brackets has only 35.56% of time to be translated contiguously as an NP in machine aligned &amp; parsed data. The NP tree in Figure 2 happens to be an “inside-out” style alignment, and context free grammar such as ITG (Wu, 1997) can not explain this structure well without necessary lexicalization. Actually, the Arabic tokens of “dfE Aly AlAnfjAr” form a combination and is turned into English word “ignite” in an idiomatic way. With lexicalization, a Hiero style rule “dfE X Aly AlAnfjAr 7→ to ignite X” is potentially a better alternative for translating the NP tree. Our operators allow us to back off to such Hiero-style rules to construct derivations, which share the immediate common parent NP, as defined for the elementary tree, for the given source span. 3.5 in: Neighboring Function For a given elementary tree, we us</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. In Computational Linguistics, volume 23(3), pages 377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Learning translation boundaries for phrase-based decoding. In NAACL-HLT</title>
<date>2010</date>
<pages>136--144</pages>
<contexts>
<context position="24982" citStr="Xiong et al. (2010)" startWordPosition="4105" endWordPosition="4108">simplifying the trees are not translation boundaries, and to avoid translation errors we should identify them by applying a PSCFG rule on top of them. During training, we label nodes with translation boundaries, as one additional function tag; during decoding, we employ the MaxEnt model to predict the translation boundary label probability for each span associated with a subgraph y, and discourage derivations accordingly for using nonterminals over the nontranslation boundary span. The translation boundaries over elementary trees have much richer representation power. The previous works as in Xiong et al. (2010), defined translation boundaries on phrase-decoder style derivation trees due to the nature of their shift-reduce algorithm, which is a special case in our model. 4 Decoding Decoding using the proposed elementary tree to string grammar naturally resembles bottom up chart parsing algorithms. The key difference is at the grammar querying step. Given a grammar G, and the input source parse tree 7r from a monolingual parser, we first construct the elementary tree for a source span, and then retrieve all the relevant subgraphs seen in the given grammar through the proposed operators. This step is c</context>
<context position="31067" citStr="Xiong et al., 2010" startWordPosition="5104" endWordPosition="5107">mar such as Hiero style rules, with proper costs. We test separately on predicting the projectable structures, like predicting function tags in § 3.5.3, for each node in syntactic parse tree. We use one thousand test sentences with two conditions: human parses and machine parses. There are totally 40,674 nodes excluding the sentence-level node. The results are shown in Table 8. It showed our MaxEnt model is very accurate using human trees: 94.5% of accuracy, and about 84.7% of accuracy for using the machine parsed trees. Our accuracies are higher compared with the 71+% accuracies reported in (Xiong et al., 2010) for their phrasal decoder. Setups Accuracy Human Parses 94.5% Machine Parses 84.7% Table 8: Accuracies of predicting projectable structures We zoom in the translation boundaries for MT08-NW, in which we studied a few important frequent labels including VP and NP-SBJ as in Table 9. According to our MaxEnt model, 20% of times we should discourage a VP tree to be translated contiguously; such VP trees have an average span length of 16.9 tokens in MT08-NW. Similar statistics are 15.9% for S-tree with an average span of 13.8 tokens. 2See link: http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/mt08</context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Learning translation boundaries for phrase-based decoding. In NAACL-HLT 2010, pages 136–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In ACL-HLT,</booktitle>
<pages>559--567</pages>
<contexts>
<context position="3489" citStr="Zhang et al., 2008" startWordPosition="528" endWordPosition="531">the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. Overall, the forest-based translation can reduce the risks from upstream parsing errors and expand the search space, but it cannot sufficiently address the syntactic divergences between various language-pairs. The tree sequence approach adds pseudo nodes and hyper edges to the forest, which makes the forest even denser and harder for navigation and search. As trees thrive in the search space, especially with the pseudo nodes and edges being added to the already dense forest, it is becoming harder to wade</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In ACL-HLT, pages 559–567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
<author>Aiti Aw</author>
<author>Chew Lim Tan</author>
</authors>
<title>Forest-based tree sequence to string translation model.</title>
<date>2009</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>172--180</pages>
<contexts>
<context position="3421" citStr="Zhang et al., 2009" startWordPosition="518" endWordPosition="521">rning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al., 2010; Shen et al., 2010) without sufficiently leveraging rich tree context. Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in (Liu and Liu, 2010), at the cost of huge search space. In (Zhang et al., 2009), combinations of tree forest and tree-sequence (Zhang et al., 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. Overall, the forest-based translation can reduce the risks from upstream parsing errors and expand the search space, but it cannot sufficiently address the syntactic divergences between various language-pairs. The tree sequence approach adds pseudo nodes and hyper edges to the forest, which makes the forest even denser and harder for navigation and search. As trees thrive in the search space, especially with the pseudo nodes and edges be</context>
</contexts>
<marker>Zhang, Zhang, Li, Aw, Tan, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and Chew Lim Tan. 2009. Forest-based tree sequence to string translation model. In Proc. of ACL 2009, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Generalizing local and non-local word-reordering patterns for syntax-based machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>572--581</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="26345" citStr="Zhao and Al-Onaizan, 2008" startWordPosition="4335" endWordPosition="4338">pan, and put them in the corresponding cells in the chart. There would have been exponential number of relevant elementary trees to search if we do not have any restrictions in the populating step; we restrict the maximum number of interior nodes |y.vi |to be 3, and the size of frontier nodes |y.vf |to be less than 6; additional pruning for less frequent elementary trees is carried out. After populating the elementary trees, we construct the partial hypotheses bottom up, by rewriting the frontier nodes of each elementary tree with the probabilities(costs) for y → α* as in Eqn. 3. Our decoder (Zhao and Al-Onaizan, 2008) is a template-based chart decoder in C++. It generalizes over the dotted-product operator in Earley style parser, to allow us to leverage many operators t E T as above-mentioned, such as binarizations, at different levels for constructing partial hypothesis. 5 Experiments In our experiments, we built our system using most of the parallel training data available to us: 250M Arabic running tokens, corresponding to the “unconstrained” condition in NIST-MT08. We chose the testsets of newswire and weblog genres from MT08 and DEV101. In particular, we choose MT08 to enable the comparison of our res</context>
<context position="28398" citStr="Zhao and Al-Onaizan, 2008" startWordPosition="4673" endWordPosition="4676">grammar in training time, and learn the model using improved iterative scaling. We use the same training data as that used in training our Arabic parser. There are 16 thousand human parse trees with human alignment; additional 1 thousand human parse and aligned sent-pairs are used as unseen test set to verify our MaxEnt models and parsers. For our Arabic parser, we have a labeled F-measure of 78.4%, and POS tag accuracy 94.9%. In particular, we’ll evaluate model p,(�t|y, in) in Eqn. 3 for predicting the translation boundaries in § 3.5.3 for projectable spans as detailed in § 5.1. Our decoder (Zhao and Al-Onaizan, 2008) supports grammars including monotone, ITG, Hiero, tree-tostring, string-to-tree, and several mixtures of them (Lee et al., 2010). We used 19 feature functions, mainly from those used in phrase-based decoder like Moses (Koehn et al., 2007), including two language models (one for a 6-gram LM, one for ModelM, one brevity penalty, IBM Model-1 (Brown et al., 1993) style alignment probabilities in both directions, relative frequency in both directions, word/rule counts, content/function word mismatch, together with features on tr2str rule probabilities. We use BLEU (Papineni et al., 2002) and TER (</context>
</contexts>
<marker>Zhao, Al-Onaizan, 2008</marker>
<rawString>Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing local and non-local word-reordering patterns for syntax-based machine translation. In Proceedings of EMNLP, pages 572– 581, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Shengyuan Chen</author>
</authors>
<title>A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>21--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="29859" citStr="Zhao and Chen, 2009" startWordPosition="4907" endWordPosition="4910">ed from recently released LDC data LDC2010E43.v3. 851 Data Train MT08-NW MT08-WB Dev10-NW Dev10-WB # Sents 8,032,837 813 547 1089 1059 # Tokens 349M(ar)/230M(en) 25,926 19,654 41,240 43,088 Table 7: Training and test data; using all training parallel training data for 4 test sets and we achieved a BLEUr4n4 55.01 for MT08-NW, or a cased BLEU of 53.31, which is close to the best officially reported result 53.85 for unconstrained systems.2 We expose the statistical decisions in Eqn. 3 as the rule probability as one of the 19 dimensions, and use Simplex Downhill algorithm with Armijo line search (Zhao and Chen, 2009) to optimize the weight vector for decoding. The algorithm moves all dimensions at the same time, and empirically achieved more stable results than MER(Och, 2003) in many of our experiments. 5.1 Predicting Projectable Structures The projectable structure is important for our proposed elementary tree to string grammar (elm2str). When a span is predicted not to be a translation boundary, we want the decoder to prefer alternative derivations outside of the immediate elementary tree, or more aggressive manipulation of the trees, such as deleting interior nodes, to explore unlabeled grammar such as</context>
</contexts>
<marker>Zhao, Chen, 2009</marker>
<rawString>Bing Zhao and Shengyuan Chen. 2009. A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters. In Proceedings of HLT-NAACL, pages 21–24, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL 2006 - Workshop on SMT,</booktitle>
<pages>138--141</pages>
<contexts>
<context position="2513" citStr="Zollmann and Venugopal (2006)" startWordPosition="363" endWordPosition="366">% source chunking decisions are translated contiguously on the target side. To translate the rest of the chunks one has to frequently break the original structures. The main issue lies in the strong assumption behind SCFG-style nonterminals – each nonterminal (or variable) assumes a source chunk should be rewritten into a contiguous chunk in the target. Without integrating techniques to modify the parse structures, the SCFGs are not to be effective even for translating NP-SBJ in linguistically distant language-pairs such as Arabic-English. Such problems have been noted in previous literature. Zollmann and Venugopal (2006) and Marcu et al. (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al., 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Res</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. of NAACL 2006 - Workshop on SMT, pages 138–141.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>