<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020428">
<title confidence="0.997685">
Learning Structured Classifiers for Statistical Dependency Parsing
</title>
<author confidence="0.904274">
Qin Iris Wang
</author>
<affiliation confidence="0.91141">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.746873">
Edmonton, Canada T6G 2E8
</address>
<email confidence="0.998352">
wqin@cs.ualberta.ca
</email>
<sectionHeader confidence="0.993879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988555">
My research is focused on developing ma-
chine learning algorithms for inferring de-
pendency parsers from language data. By
investigating several approaches I have
developed a unifying perspective that al-
lows me to share advances between both
probabilistic and non-probabilistic meth-
ods. First, I describe a generative tech-
nique that uses a strictly lexicalised pars-
ing model, where all the parameters are
based on words and do not use any part-
of-speech (POS) tags nor grammatical cat-
egories. Then, I incorporate two ideas
from probabilistic parsing—word similar-
ity smoothing and local estimation—to
improve the large margin approach. Fi-
nally, I present a simpler and more ef-
ficient approach to training dependency
parsers by applying a boosting-like proce-
dure to standard training methods.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984575">
Over the past decade, there has been tremendous
progress on learning parsing models from treebank
data (Magerman, 1995; Collins, 1999; Charniak,
1997; Ratnaparkhi, 1999; Charniak, 2000; Wang
et al., 2005; McDonald et al., 2005). Most of the
early work in this area was based on postulating
generative probability models of language that in-
cluded parse structures (Magerman, 1995; Collins,
1997; Charniak, 1997). Learning in this context
consisted of estimating the parameters of the model
with simple likelihood based techniques, but incor-
porating various smoothing and back-off estimation
</bodyText>
<page confidence="0.840101">
5
</page>
<bodyText confidence="0.99997">
tricks to cope with the sparse data problems (Collins,
1997; Bikel, 2004). Subsequent research began to
focus more on conditional models of parse structure
given the input sentence, which allowed discrimi-
native training techniques such as maximum con-
ditional likelihood (i.e. “maximum entropy”) to be
applied (Ratnaparkhi, 1999; Charniak, 2000). Cur-
rently, the work on conditional parsing models ap-
pears to have culminated in large margin training
approaches (Taskar et al., 2004; McDonald et al.,
2005), which demonstrates the state of the art per-
formance in English dependency parsing.
Despite the realization that maximum margin
training is closely related to maximum conditional
likelihood for conditional models (McDonald et
al., 2005), a sufficiently unified view has not yet
been achieved that permits the easy exchange of
improvements between the probabilistic and non-
probabilistic approaches. For example, smoothing
methods have played a central role in probabilistic
approaches (Collins, 1997; Wang et al., 2005), and
yet they are not being used in current large margin
training algorithms. Another unexploited connec-
tion is that probabilistic approaches pay closer at-
tention to the individual errors made by each compo-
nent of a parse, whereas the training error minimized
in the large margin approach—the “structured mar-
gin loss” (McDonald et al., 2005)—is a coarse mea-
sure that only assesses the total error of an entire
parse rather than focusing on the error of any par-
ticular component. I have addressed both of these
issues, as well as others in my work.
</bodyText>
<sectionHeader confidence="0.993694" genericHeader="method">
2 Dependency Parsing Model
</sectionHeader>
<bodyText confidence="0.976627136363636">
Given a sentence , I consider the
problem of computing an accurate directed depen-
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 5–8,
Rochester, April 2007. c�2007 Association for Computational Linguistics
dency tree, , over . Note that consists of or-
dered pairs of words in such that each
word appears in at least one pair and each word has
in-degree at most one. Dependency trees are usually
assumed to be projective (no crossing arcs), which
means that if there is an arc , then is
an ancestor of all the words between and . Let
denote the set of all the directed, projective
trees that span .
From an input sentence , one would like to be
able to compute the best parse; that is, a projective
tree, , that obtains the highest “score”. In
particular, I follow Eisner (1996) and McDonald et
al. (2005) and assume that the score of a complete
spanning tree for a given sentence, whether prob-
abilistically motivated or not, can be decomposed as
a sum of local scores for each link (a word pair). In
which case, the parsing problem reduces to
</bodyText>
<equation confidence="0.773103">
s (1)
</equation>
<bodyText confidence="0.999882166666667">
where the score s can depend on any
measurable property of and within the tree
. This formulation is sufficiently general to capture
most dependency parsing models, including proba-
bilistic dependency models (Wang et al., 2005; Eis-
ner, 1996) as well as non-probabilistic models (Mc-
Donald et al., 2005; Wang et al., 2006).
For the purpose of learning, the score of each link
can be expressed as a weighted linear combination
of features
where are the weight parameters to be estimated
during training.
</bodyText>
<sectionHeader confidence="0.97261" genericHeader="method">
3 Lexicalised Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999975">
To learn an accurate dependency parser from data,
the first approach I investigated is based on a strictly
lexical parsing model where all the parameters are
based on words (Wang et al., 2005). The advantage
of this approach is that it does not rely on part-of-
speech tags nor grammatical categories. Further-
more, I based training on maximizing the condi-
tional probability of a parse tree given a sentence,
unlike most previous generative models (Magerman,
1995; Collins, 1997; Charniak, 1997), which focus
on maximizing the joint probability of the parse tree
and the sentence.
An efficient training algorithm can be achieved
by maximizing the conditional probability of each
parsing decision, hence minimizing a loss based
on each local link decision independently. Impor-
tantly, inter-dependence between links can still be
accommodated by exploiting dynamic features in
training—features that take into account the labels
of (some) of the surrounding components when pre-
dicting the label of a target component. To cope
with the sparse data problem, I use distributional
word similarity (Pereira et al., 1993; Grefenstette,
1994; Lin, 1998) to generalize the observed fre-
quency counts in the training corpus. The exper-
imental results on the Chinese Treebank 4.0 show
that the accuracy of the conditional model is 13.6%
higher than corresponding joint models, while sim-
ilarity smoothing also allows the strictly lexicalised
approach to outperform corresponding models based
on part-of-speech tags.
</bodyText>
<sectionHeader confidence="0.950592" genericHeader="method">
4 Extensions to Large Margin Parsing
</sectionHeader>
<bodyText confidence="0.999978166666667">
The approach presented above has a limitation: it
uses a local scoring function instead of a global scor-
ing function to compute the score for a candidate
tree. The structured large margin approach, on the
other hand, uses a global scoring function by mini-
mizing a training loss—the “structured margin loss”
(McDonald et al., 2005)—which is directly coordi-
nated with the global tree. However, the training
error minimized in the large margin approach is a
coarse measure that only assesses the total error of
an entire parse rather than focusing on the error of
any particular component. Also, smoothing meth-
ods, which have been widely used in probabilistic
approaches, are not currently being used in large
margin training algorithms. In the second approach,
I improve structured large margin training for pars-
ing in two ways (Wang et al., 2006). First, I incor-
porate local constraints that enforce the correctness
of each individual link, rather than just scoring the
global parse tree. Second, to cope with sparse data
and generalize to unseen words, I smooth the lexical
parameters according to their underlying word sim-
ilarities. To smooth parameters in the large margin
framework, I introduce the technique of Laplacian
</bodyText>
<equation confidence="0.970338">
s (2)
</equation>
<page confidence="0.977788">
6
</page>
<bodyText confidence="0.9998299">
regularization in large margin parsing. Finally, to
demonstrate the benefits of my approach, I recon-
sider the problem of parsing Chinese treebank data
using only lexical features, as in Section 3. My re-
sults improve current large margin approaches and
show that similarity smoothing combined with local
constraint enforcement leads to state of the art per-
formance, while only requiring word-based features
that do not rely on part-of-speech tags nor grammat-
ical categories in any way.
</bodyText>
<sectionHeader confidence="0.954128" genericHeader="method">
5 Training via Structured Boosting
</sectionHeader>
<bodyText confidence="0.9995191875">
Finally, I have recently demonstrated the somewhat
surprising result that state of the art dependency
parsing performance can be achieved through the
use of conventional, local classification methods. In
particular, I show how a simple form of structured
boosting can be used to improve the training of stan-
dard local classification methods, in the context of
structured predictions, without modifying the under-
lying training method (Wang et al., 2007). The ad-
vantage of this approach is that one can use off-the-
shelf classification techniques, such as support vec-
tor machines or logistic regression, to achieve com-
petitive parsing results with little additional effort.
The idea behind structured boosting is very sim-
ple. To produce an accurate parsing model, one
combines the local predictions of multiple weak pre-
dictors to obtain a score for each link, which a parser
can then use to compute the maximum score tree for
a given sentence. Structured boosting proceeds in
rounds. On each round a local “link predictor” is
trained merely to predict the existence and orienta-
tion of a link between two words given input fea-
tures encoding context—without worrying about co-
ordinating the predictions in a coherent global parse.
Once a weak predictor is learned, it is added to the
ensemble of weak hypotheses, the training corpus
is re-parsed using the new predictor, and the local
training contexts are re-weighted based on errors
made by the parser’s output. Thus, a wrapper ap-
proach is used to successively modify the training
data so that the training algorithm is encouraged to
facilitate improved global parsing accuracy.
</bodyText>
<tableCaption confidence="0.928729">
Table 1: Comparison with State of the Art (Depen-
dency Accuracy)
</tableCaption>
<table confidence="0.800545428571429">
Model Chinese English
Yamada&amp;Matsumoto 03 - 90.3
Nivre&amp;Scholz 04 - 87.3
Wang et al. 05 (Sec. 3) 79.9* -
McDonald et al. 05 - 90.9
McDonald&amp;Pereira 06 82.5* 91.5
Corston-Oliver et al. 06 73.3 90.8
Structured 86.6* 89.3
Boosting (Sec. 5) 77.6
Obtained with Chinese Treebank 4.0 using the data split re-
ported in Wang et al. (2005).
▼
Obtained with Chinese Treebank 5.0 using the data split re-
ported in Corston-Olivr et al. (2006).
</table>
<sectionHeader confidence="0.938019" genericHeader="method">
6 Current Results
</sectionHeader>
<bodyText confidence="0.994578857142857">
Table 1 compares my results1 with those obtained
by other researchers, on both English and Chinese
data.2 The English results are obtained using the
same standard training and test set splits from En-
glish Penn Treebank 3.0. The results on Chinese are
obtained on two different data sets, Chinese Tree-
bank 4.0 and Chinese Treebank 5.0 as noted.3
Table 1 shows that the results I am able to achieve
on English are competitive with the state of the art,
but are still behind the best results of (McDonald
and Pereira, 2006). However, perhaps surprisingly,
Table 1 also shows that the structured boosting ap-
proach actually surpasses state of the art accuracy on
Chinese parsing for both treebank collections.
</bodyText>
<sectionHeader confidence="0.99735" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.99907">
Although the three pieces of my work above look
very different superficially, they are actually closely
related by the “scoring” formulation and, more
</bodyText>
<footnote confidence="0.9973525">
1I did not include the results of the technique described in
Section 4, because we were only able to conveniently train on
sentences with less than or equal to 15 words.
2McDonald et al. (2005) have tried MIRA on Chinese Tree-
bank 4.0 with the same data split reported here, obtaining a
dependency accuracy score of 82.5 (Ryan McDonald, personal
communication).
3The results on Chinese Treebank 5.0 are generally worse
than on Chinese Treebank 4.0, since the former is a superset of
the latter, and moreover the additional sentences come entirely
from a Taiwanese Chinese source that is more difficult to parse
than the rest of the data.
</footnote>
<page confidence="0.999457">
7
</page>
<bodyText confidence="0.992846186046512">
specifically, by the equations introduced in Sec-
tion 2. In other words, they all compute a linear
classifier.4 The only differences among them are:
(1) What features are used? (2) How are the param-
eters estimated?
A general perspective I bring to my investigation
is the desire to delineate the effects of domain en-
gineering (choosing good features for representing
and learning parsing models) from the general ma-
chine learning principles (training criteria, regular-
ization and smoothing techniques) that permit good
results. In fact, combined features have been proved
to be useful in dependency parsing with support vec-
tor machines (Yamada and Matsumoto, 2003), and
I have already obtained some preliminary results on
generating useful feature combinations via boosting.
Therefore, I will consider combining all the projects
I presented above. That is, I plan to incorporate all
the useful features, the morphological features and
the combined features as discussed above, into the
training algorithms presented in Section 4 or Sec-
tion 5, to train a dependency parser globally. Then
I am going to augment the training with the exist-
ing smoothing and regularization techniques (as de-
scribed in Section 4), or new developed ones. I ex-
pect the resulting parser to have better performance
than those I have presented above.
There are a lot of other ideas which can be ex-
plored in my future work. First and most important,
I plan to investigate new advanced machine learning
methods (e.g., structured boosting or unsupervised
/ semi-supervised algorithms (Xu et al., 2006)) and
apply them to the dependency parsing problem gen-
erally, since the goal of my research is to learn nat-
ural language parsers in an elegant and principled
manner. Next, I am going to apply my approaches
to parse other languages, such as Czech, German,
Spanish and French, and analyze the performance
of my parsers on these different languages. Further-
more, I plan to apply my parsers in other domains
(e.g., biomedical data) (Blitzer et al., 2006) besides
treebank data, to investigate the effectiveness and
generality of my approaches.
</bodyText>
<footnote confidence="0.5714215">
4In general, for any probabilistic model, the product of prob-
abilities can be converted to sums of scores in the log space,
which makes the search identical to a score based discrimina-
tive model.
</footnote>
<sectionHeader confidence="0.923747" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999771276595745">
D. Bikel. 2004. Intricacies of Collins’ parsing model. Compu-
tational Linguistics, 30(4).
J. Blitzer, R. McDonald, and F. Pereira. 2006. Doman adap-
tation with structural correspondence learning. In Proc. of
EMNLP.
E. Charniak. 1997. Statistical parsing with a context-free gram-
mar and word statistics. In Proc. of AAAI, pages 598–603.
E. Charniak. 2000. A maximum entropy inspired parser. In
Proc. of North American ACL, pages 132–139.
M. Collins. 1997. Three generative, lexicalized models for
statistical parsing. In Proc. of ACL, pages 16–23.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
S. Corston-Oliver, A. Aue, K. Duh, and E. Ringger. 2006. Mul-
tilingual dependency parsing using Bayes’ point machines.
In Proc. of HLT/NAACL.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. of COLING.
G. Grefenstette. 1994. Corpus-derived first, second and third-
order word affinities. In Proc. of Euralex.
D. Lin. 1998. Automatic retrieval and clustering of similar
words. In Proc. of COLING/ACL, pages 768–774.
D. Magerman. 1995. Statistical decision-tree model for pars-
ing. In Proc. of ACL, pages 276–283.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-
margin training of dependency parsers. In Proc. of ACL.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional cluster-
ing of English words. In Proc. of ACL, pages 183–190.
A. Ratnaparkhi. 1999. Learning to parse natural language with
maximum entropy models. Mach. Learn., 34(1-3):151–175.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004. Max-margin parsing. In Proc. of EMNLP.
Q. Wang, D. Schuurmans, and D. Lin. 2005. Strictly lexical
dependency parsing. In Proc. of IWPT, pages 152–159.
Q. Wang, C. Cherry, D. Lizotte, and D. Schuurmans. 2006. Im-
proved large margin dependency parsing via local constraints
and Laplacian regularization. In Proc. of CoNLL.
Q. Wang, D. Lin, and D. Schuurmans. 2007. Simple training
of dependency parsers via structured boosting. In Proc. of
IJCAI, pages 1756–1762.
L. Xu, D. Wilkinson, F. Southey, and D. Schuurmans. 2006.
Discriminative unsupervised learning of structured predic-
tors. In Proc. of ICML.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of IWPT.
</reference>
<page confidence="0.998483">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.725009">
<title confidence="0.999989">Learning Structured Classifiers for Statistical Dependency Parsing</title>
<author confidence="0.901778">Qin Iris</author>
<affiliation confidence="0.99942">Department of Computing University of</affiliation>
<address confidence="0.851819">Edmonton, Canada T6G</address>
<email confidence="0.99429">wqin@cs.ualberta.ca</email>
<abstract confidence="0.99743980952381">My research is focused on developing machine learning algorithms for inferring dependency parsers from language data. By investigating several approaches I have developed a unifying perspective that allows me to share advances between both probabilistic and non-probabilistic methods. First, I describe a generative technique that uses a strictly lexicalised parsing model, where all the parameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1657" citStr="Bikel, 2004" startWordPosition="245" endWordPosition="246">ess on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. “maximum entropy”) to be applied (Ratnaparkhi, 1999; Charniak, 2000). Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005), which demonstrates the state of the art performance in English dependency parsing. Despite the realization that maximum margin training is closely related to maximum co</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Doman adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>J. Blitzer, R. McDonald, and F. Pereira. 2006. Doman adaptation with structural correspondence learning. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. of AAAI,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="1141" citStr="Charniak, 1997" startWordPosition="167" endWordPosition="168">strictly lexicalised parsing model, where all the parameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure g</context>
<context position="5268" citStr="Charniak, 1997" startWordPosition="840" endWordPosition="841"> combination of features where are the weight parameters to be estimated during training. 3 Lexicalised Dependency Parsing To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al., 2005). The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories. Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models (Magerman, 1995; Collins, 1997; Charniak, 1997), which focus on maximizing the joint probability of the parse tree and the sentence. An efficient training algorithm can be achieved by maximizing the conditional probability of each parsing decision, hence minimizing a loss based on each local link decision independently. Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training—features that take into account the labels of (some) of the surrounding components when predicting the label of a target component. To cope with the sparse data problem, I use distributional word similarity (Pere</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. of AAAI, pages 598–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of North American ACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="1176" citStr="Charniak, 2000" startWordPosition="171" endWordPosition="172"> where all the parameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allo</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum entropy inspired parser. In Proc. of North American ACL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1385" citStr="Collins, 1997" startWordPosition="205" endWordPosition="206">estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. “maximum entropy”) to be applied (Ratnaparkhi, 1999; Charniak, 2000). Currently, the work on conditional parsing models appea</context>
<context position="5251" citStr="Collins, 1997" startWordPosition="838" endWordPosition="839">weighted linear combination of features where are the weight parameters to be estimated during training. 3 Lexicalised Dependency Parsing To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al., 2005). The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories. Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models (Magerman, 1995; Collins, 1997; Charniak, 1997), which focus on maximizing the joint probability of the parse tree and the sentence. An efficient training algorithm can be achieved by maximizing the conditional probability of each parsing decision, hence minimizing a loss based on each local link decision independently. Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training—features that take into account the labels of (some) of the surrounding components when predicting the label of a target component. To cope with the sparse data problem, I use distributional word</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalized models for statistical parsing. In Proc. of ACL, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1125" citStr="Collins, 1999" startWordPosition="165" endWordPosition="166">ue that uses a strictly lexicalised parsing model, where all the parameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of p</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>A Aue</author>
<author>K Duh</author>
<author>E Ringger</author>
</authors>
<title>Multilingual dependency parsing using Bayes’ point machines.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL.</booktitle>
<marker>Corston-Oliver, Aue, Duh, Ringger, 2006</marker>
<rawString>S. Corston-Oliver, A. Aue, K. Duh, and E. Ringger. 2006. Multilingual dependency parsing using Bayes’ point machines. In Proc. of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="3971" citStr="Eisner (1996)" startWordPosition="623" endWordPosition="624"> Association for Computational Linguistics dency tree, , over . Note that consists of ordered pairs of words in such that each word appears in at least one pair and each word has in-degree at most one. Dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc , then is an ancestor of all the words between and . Let denote the set of all the directed, projective trees that span . From an input sentence , one would like to be able to compute the best parse; that is, a projective tree, , that obtains the highest “score”. In particular, I follow Eisner (1996) and McDonald et al. (2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to s (1) where the score s can depend on any measurable property of and within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006). For the</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Corpus-derived first, second and thirdorder word affinities.</title>
<date>1994</date>
<booktitle>In Proc. of</booktitle>
<pages>768--774</pages>
<contexts>
<context position="5904" citStr="Grefenstette, 1994" startWordPosition="935" endWordPosition="936">aximizing the joint probability of the parse tree and the sentence. An efficient training algorithm can be achieved by maximizing the conditional probability of each parsing decision, hence minimizing a loss based on each local link decision independently. Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training—features that take into account the labels of (some) of the surrounding components when predicting the label of a target component. To cope with the sparse data problem, I use distributional word similarity (Pereira et al., 1993; Grefenstette, 1994; Lin, 1998) to generalize the observed frequency counts in the training corpus. The experimental results on the Chinese Treebank 4.0 show that the accuracy of the conditional model is 13.6% higher than corresponding joint models, while similarity smoothing also allows the strictly lexicalised approach to outperform corresponding models based on part-of-speech tags. 4 Extensions to Large Margin Parsing The approach presented above has a limitation: it uses a local scoring function instead of a global scoring function to compute the score for a candidate tree. The structured large margin approa</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Corpus-derived first, second and thirdorder word affinities. In Proc. of Euralex. D. Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of COLING/ACL, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical decision-tree model for parsing.</title>
<date>1995</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="1110" citStr="Magerman, 1995" startWordPosition="163" endWordPosition="164">nerative technique that uses a strictly lexicalised parsing model, where all the parameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditio</context>
<context position="5236" citStr="Magerman, 1995" startWordPosition="836" endWordPosition="837"> expressed as a weighted linear combination of features where are the weight parameters to be estimated during training. 3 Lexicalised Dependency Parsing To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al., 2005). The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories. Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models (Magerman, 1995; Collins, 1997; Charniak, 1997), which focus on maximizing the joint probability of the parse tree and the sentence. An efficient training algorithm can be achieved by maximizing the conditional probability of each parsing decision, hence minimizing a loss based on each local link decision independently. Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training—features that take into account the labels of (some) of the surrounding components when predicting the label of a target component. To cope with the sparse data problem, I use dist</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. Magerman. 1995. Statistical decision-tree model for parsing. In Proc. of ACL, pages 276–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="10718" citStr="McDonald and Pereira, 2006" startWordPosition="1717" endWordPosition="1720"> with Chinese Treebank 5.0 using the data split reported in Corston-Olivr et al. (2006). 6 Current Results Table 1 compares my results1 with those obtained by other researchers, on both English and Chinese data.2 The English results are obtained using the same standard training and test set splits from English Penn Treebank 3.0. The results on Chinese are obtained on two different data sets, Chinese Treebank 4.0 and Chinese Treebank 5.0 as noted.3 Table 1 shows that the results I am able to achieve on English are competitive with the state of the art, but are still behind the best results of (McDonald and Pereira, 2006). However, perhaps surprisingly, Table 1 also shows that the structured boosting approach actually surpasses state of the art accuracy on Chinese parsing for both treebank collections. 7 Future Work Although the three pieces of my work above look very different superficially, they are actually closely related by the “scoring” formulation and, more 1I did not include the results of the technique described in Section 4, because we were only able to conveniently train on sentences with less than or equal to 15 words. 2McDonald et al. (2005) have tried MIRA on Chinese Treebank 4.0 with the same da</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online largemargin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1219" citStr="McDonald et al., 2005" startWordPosition="177" endWordPosition="180"> on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such</context>
<context position="2948" citStr="McDonald et al., 2005" startWordPosition="437" endWordPosition="440">a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al., 2005), and yet they are not being used in current large margin training algorithms. Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach—the “structured margin loss” (McDonald et al., 2005)—is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. I have addressed both of these issues, as well as others in my work. 2 Dependency Parsing Model Given a sentence , I consider the problem of computing an accurate directed depenProceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 5–8, Rochester, April 2007. c�2007 Association for Computational Linguistics dency tree, , over . Note that consists of ordered pairs of words in such that each word appears in at least one pair and each word has in-degree a</context>
<context position="4542" citStr="McDonald et al., 2005" startWordPosition="719" endWordPosition="723">est “score”. In particular, I follow Eisner (1996) and McDonald et al. (2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to s (1) where the score s can depend on any measurable property of and within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006). For the purpose of learning, the score of each link can be expressed as a weighted linear combination of features where are the weight parameters to be estimated during training. 3 Lexicalised Dependency Parsing To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al., 2005). The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories. Furthermore, I based training on maximizing the conditional probab</context>
<context position="6640" citStr="McDonald et al., 2005" startWordPosition="1049" endWordPosition="1052">hinese Treebank 4.0 show that the accuracy of the conditional model is 13.6% higher than corresponding joint models, while similarity smoothing also allows the strictly lexicalised approach to outperform corresponding models based on part-of-speech tags. 4 Extensions to Large Margin Parsing The approach presented above has a limitation: it uses a local scoring function instead of a global scoring function to compute the score for a candidate tree. The structured large margin approach, on the other hand, uses a global scoring function by minimizing a training loss—the “structured margin loss” (McDonald et al., 2005)—which is directly coordinated with the global tree. However, the training error minimized in the large margin approach is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. Also, smoothing methods, which have been widely used in probabilistic approaches, are not currently being used in large margin training algorithms. In the second approach, I improve structured large margin training for parsing in two ways (Wang et al., 2006). First, I incorporate local constraints that enforce the correctness of each individ</context>
<context position="11261" citStr="McDonald et al. (2005)" startWordPosition="1805" endWordPosition="1808"> the art, but are still behind the best results of (McDonald and Pereira, 2006). However, perhaps surprisingly, Table 1 also shows that the structured boosting approach actually surpasses state of the art accuracy on Chinese parsing for both treebank collections. 7 Future Work Although the three pieces of my work above look very different superficially, they are actually closely related by the “scoring” formulation and, more 1I did not include the results of the technique described in Section 4, because we were only able to conveniently train on sentences with less than or equal to 15 words. 2McDonald et al. (2005) have tried MIRA on Chinese Treebank 4.0 with the same data split reported here, obtaining a dependency accuracy score of 82.5 (Ryan McDonald, personal communication). 3The results on Chinese Treebank 5.0 are generally worse than on Chinese Treebank 4.0, since the former is a superset of the latter, and moreover the additional sentences come entirely from a Taiwanese Chinese source that is more difficult to parse than the rest of the data. 7 specifically, by the equations introduced in Section 2. In other words, they all compute a linear classifier.4 The only differences among them are: (1) Wh</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online largemargin training of dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="5884" citStr="Pereira et al., 1993" startWordPosition="931" endWordPosition="934">997), which focus on maximizing the joint probability of the parse tree and the sentence. An efficient training algorithm can be achieved by maximizing the conditional probability of each parsing decision, hence minimizing a loss based on each local link decision independently. Importantly, inter-dependence between links can still be accommodated by exploiting dynamic features in training—features that take into account the labels of (some) of the surrounding components when predicting the label of a target component. To cope with the sparse data problem, I use distributional word similarity (Pereira et al., 1993; Grefenstette, 1994; Lin, 1998) to generalize the observed frequency counts in the training corpus. The experimental results on the Chinese Treebank 4.0 show that the accuracy of the conditional model is 13.6% higher than corresponding joint models, while similarity smoothing also allows the strictly lexicalised approach to outperform corresponding models based on part-of-speech tags. 4 Extensions to Large Margin Parsing The approach presented above has a limitation: it uses a local scoring function instead of a global scoring function to compute the score for a candidate tree. The structured</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of English words. In Proc. of ACL, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<pages>34--1</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="1160" citStr="Ratnaparkhi, 1999" startWordPosition="169" endWordPosition="170">ised parsing model, where all the parameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sent</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Mach. Learn., 34(1-3):151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2063" citStr="Taskar et al., 2004" startWordPosition="303" endWordPosition="306">timating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. “maximum entropy”) to be applied (Ratnaparkhi, 1999; Charniak, 2000). Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005), which demonstrates the state of the art performance in English dependency parsing. Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al., 2005), and yet they are not being used in current large margin</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wang</author>
<author>D Schuurmans</author>
<author>D Lin</author>
</authors>
<title>Strictly lexical dependency parsing.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="1195" citStr="Wang et al., 2005" startWordPosition="173" endWordPosition="176">arameters are based on words and do not use any partof-speech (POS) tags nor grammatical categories. Then, I incorporate two ideas from probabilistic parsing—word similarity smoothing and local estimation—to improve the large margin approach. Finally, I present a simpler and more efficient approach to training dependency parsers by applying a boosting-like procedure to standard training methods. 1 Introduction Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005). Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997). Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation 5 tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004). Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative </context>
<context position="2606" citStr="Wang et al., 2005" startWordPosition="382" endWordPosition="385">to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005), which demonstrates the state of the art performance in English dependency parsing. Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al., 2005), a sufficiently unified view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. For example, smoothing methods have played a central role in probabilistic approaches (Collins, 1997; Wang et al., 2005), and yet they are not being used in current large margin training algorithms. Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach—the “structured margin loss” (McDonald et al., 2005)—is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. I have addressed both of these issues, as well as others in my work. 2 Dependency Parsing Model Given a sentence , I con</context>
<context position="4468" citStr="Wang et al., 2005" startWordPosition="707" endWordPosition="710">te the best parse; that is, a projective tree, , that obtains the highest “score”. In particular, I follow Eisner (1996) and McDonald et al. (2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to s (1) where the score s can depend on any measurable property of and within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006). For the purpose of learning, the score of each link can be expressed as a weighted linear combination of features where are the weight parameters to be estimated during training. 3 Lexicalised Dependency Parsing To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al., 2005). The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical cate</context>
<context position="10079" citStr="Wang et al. (2005)" startWordPosition="1606" endWordPosition="1609">re re-weighted based on errors made by the parser’s output. Thus, a wrapper approach is used to successively modify the training data so that the training algorithm is encouraged to facilitate improved global parsing accuracy. Table 1: Comparison with State of the Art (Dependency Accuracy) Model Chinese English Yamada&amp;Matsumoto 03 - 90.3 Nivre&amp;Scholz 04 - 87.3 Wang et al. 05 (Sec. 3) 79.9* - McDonald et al. 05 - 90.9 McDonald&amp;Pereira 06 82.5* 91.5 Corston-Oliver et al. 06 73.3 90.8 Structured 86.6* 89.3 Boosting (Sec. 5) 77.6 Obtained with Chinese Treebank 4.0 using the data split reported in Wang et al. (2005). ▼ Obtained with Chinese Treebank 5.0 using the data split reported in Corston-Olivr et al. (2006). 6 Current Results Table 1 compares my results1 with those obtained by other researchers, on both English and Chinese data.2 The English results are obtained using the same standard training and test set splits from English Penn Treebank 3.0. The results on Chinese are obtained on two different data sets, Chinese Treebank 4.0 and Chinese Treebank 5.0 as noted.3 Table 1 shows that the results I am able to achieve on English are competitive with the state of the art, but are still behind the best </context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2005</marker>
<rawString>Q. Wang, D. Schuurmans, and D. Lin. 2005. Strictly lexical dependency parsing. In Proc. of IWPT, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wang</author>
<author>C Cherry</author>
<author>D Lizotte</author>
<author>D Schuurmans</author>
</authors>
<title>Improved large margin dependency parsing via local constraints and Laplacian regularization.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="4562" citStr="Wang et al., 2006" startWordPosition="724" endWordPosition="727">lar, I follow Eisner (1996) and McDonald et al. (2005) and assume that the score of a complete spanning tree for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair). In which case, the parsing problem reduces to s (1) where the score s can depend on any measurable property of and within the tree . This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al., 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al., 2005; Wang et al., 2006). For the purpose of learning, the score of each link can be expressed as a weighted linear combination of features where are the weight parameters to be estimated during training. 3 Lexicalised Dependency Parsing To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al., 2005). The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories. Furthermore, I based training on maximizing the conditional probability of a parse tre</context>
<context position="7155" citStr="Wang et al., 2006" startWordPosition="1133" endWordPosition="1136">bal scoring function by minimizing a training loss—the “structured margin loss” (McDonald et al., 2005)—which is directly coordinated with the global tree. However, the training error minimized in the large margin approach is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. Also, smoothing methods, which have been widely used in probabilistic approaches, are not currently being used in large margin training algorithms. In the second approach, I improve structured large margin training for parsing in two ways (Wang et al., 2006). First, I incorporate local constraints that enforce the correctness of each individual link, rather than just scoring the global parse tree. Second, to cope with sparse data and generalize to unseen words, I smooth the lexical parameters according to their underlying word similarities. To smooth parameters in the large margin framework, I introduce the technique of Laplacian s (2) 6 regularization in large margin parsing. Finally, to demonstrate the benefits of my approach, I reconsider the problem of parsing Chinese treebank data using only lexical features, as in Section 3. My results impr</context>
</contexts>
<marker>Wang, Cherry, Lizotte, Schuurmans, 2006</marker>
<rawString>Q. Wang, C. Cherry, D. Lizotte, and D. Schuurmans. 2006. Improved large margin dependency parsing via local constraints and Laplacian regularization. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wang</author>
<author>D Lin</author>
<author>D Schuurmans</author>
</authors>
<title>Simple training of dependency parsers via structured boosting.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI,</booktitle>
<pages>1756--1762</pages>
<contexts>
<context position="8515" citStr="Wang et al., 2007" startWordPosition="1345" endWordPosition="1348">rformance, while only requiring word-based features that do not rely on part-of-speech tags nor grammatical categories in any way. 5 Training via Structured Boosting Finally, I have recently demonstrated the somewhat surprising result that state of the art dependency parsing performance can be achieved through the use of conventional, local classification methods. In particular, I show how a simple form of structured boosting can be used to improve the training of standard local classification methods, in the context of structured predictions, without modifying the underlying training method (Wang et al., 2007). The advantage of this approach is that one can use off-theshelf classification techniques, such as support vector machines or logistic regression, to achieve competitive parsing results with little additional effort. The idea behind structured boosting is very simple. To produce an accurate parsing model, one combines the local predictions of multiple weak predictors to obtain a score for each link, which a parser can then use to compute the maximum score tree for a given sentence. Structured boosting proceeds in rounds. On each round a local “link predictor” is trained merely to predict the</context>
</contexts>
<marker>Wang, Lin, Schuurmans, 2007</marker>
<rawString>Q. Wang, D. Lin, and D. Schuurmans. 2007. Simple training of dependency parsers via structured boosting. In Proc. of IJCAI, pages 1756–1762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Xu</author>
<author>D Wilkinson</author>
<author>F Southey</author>
<author>D Schuurmans</author>
</authors>
<title>Discriminative unsupervised learning of structured predictors.</title>
<date>2006</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="13277" citStr="Xu et al., 2006" startWordPosition="2130" endWordPosition="2133">s as discussed above, into the training algorithms presented in Section 4 or Section 5, to train a dependency parser globally. Then I am going to augment the training with the existing smoothing and regularization techniques (as described in Section 4), or new developed ones. I expect the resulting parser to have better performance than those I have presented above. There are a lot of other ideas which can be explored in my future work. First and most important, I plan to investigate new advanced machine learning methods (e.g., structured boosting or unsupervised / semi-supervised algorithms (Xu et al., 2006)) and apply them to the dependency parsing problem generally, since the goal of my research is to learn natural language parsers in an elegant and principled manner. Next, I am going to apply my approaches to parse other languages, such as Czech, German, Spanish and French, and analyze the performance of my parsers on these different languages. Furthermore, I plan to apply my parsers in other domains (e.g., biomedical data) (Blitzer et al., 2006) besides treebank data, to investigate the effectiveness and generality of my approaches. 4In general, for any probabilistic model, the product of pro</context>
</contexts>
<marker>Xu, Wilkinson, Southey, Schuurmans, 2006</marker>
<rawString>L. Xu, D. Wilkinson, F. Southey, and D. Schuurmans. 2006. Discriminative unsupervised learning of structured predictors. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="12370" citStr="Yamada and Matsumoto, 2003" startWordPosition="1983" endWordPosition="1986">duced in Section 2. In other words, they all compute a linear classifier.4 The only differences among them are: (1) What features are used? (2) How are the parameters estimated? A general perspective I bring to my investigation is the desire to delineate the effects of domain engineering (choosing good features for representing and learning parsing models) from the general machine learning principles (training criteria, regularization and smoothing techniques) that permit good results. In fact, combined features have been proved to be useful in dependency parsing with support vector machines (Yamada and Matsumoto, 2003), and I have already obtained some preliminary results on generating useful feature combinations via boosting. Therefore, I will consider combining all the projects I presented above. That is, I plan to incorporate all the useful features, the morphological features and the combined features as discussed above, into the training algorithms presented in Section 4 or Section 5, to train a dependency parser globally. Then I am going to augment the training with the existing smoothing and regularization techniques (as described in Section 4), or new developed ones. I expect the resulting parser to</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>