<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.9984065">
Text Summarizer in Use: Lessons Learned
from Real World Deployment and Evaluation
</title>
<author confidence="0.92276">
Mary Ellen Okurowski
Harold Wilson
Joaquin Urbina
</author>
<affiliation confidence="0.996378">
Department of Defense
</affiliation>
<address confidence="0.677516818181818">
9800 Savage Rd.
Fort Meade, MD. 20755
Ruth Colvin Clark
Clark Training &amp; Consulting
17801 CR 23
Dolores, Colorado 81323
Tony Taylor
SRA Corp.
4939 Ellcridge Landing
Suite #195
Linthicum, MD. 21090
</address>
<author confidence="0.902988">
Frank Krapcho
</author>
<affiliation confidence="0.851789">
Kathpol Technologies Inc.
</affiliation>
<address confidence="0.3311065">
6835 Deerpath Suite #102
Elkridge, MD. 21705
</address>
<sectionHeader confidence="0.983732" genericHeader="abstract">
1.0 Introduction
</sectionHeader>
<bodyText confidence="0.9999376">
Much of the historical and current
summarization literature has been technology-
centered with the questions posed and
answered having implications for technology
development. Though commercial
summarization products have appeared in the
market place and developers continue to
explore new summarization areas, few papers
have been user-centered, examining
summarization technology in-use. In this
paper, we show how applied work and the
knowledge gleaned about technology in-use
&apos;can temper theoretical considerations and
. motivate as well as direct development likely
to result in higher return on investment.
</bodyText>
<sectionHeader confidence="0.615363" genericHeader="keywords">
2.0 Background
</sectionHeader>
<bodyText confidence="0.998907675">
The importance of understanding the
function a summary serves for users is widely
acknowledged, and seminal works defining
summary types by functions (Paice, 1990;
Sparck-Jones, 1993) are frequently cited by
developers. Task orientation defines extrinsic
technology assessments, and the research
literature on how to assess performance for
machine generated summaries in an
experimental task scenario has grown
( Brandow et al., 1994; Morris et al., 1999;
Jing et al., 1998; Merlino and Maybury,
1999; Wasson, 1998; Tombros et al., 1998;
Firmin and Chrzanowski, 1999; and Mani et
al., 1999). An increasing number of research
papers on summarization systems now also
describe some type of extrinsic evaluative
task (e.g. Salton et al., 1999; Strzalkowski et
al., 1998). A number of factors (i.e.
characteristics of summaries, documents,
users, and tasks) have surfaced which have
implications for technology use. More
research assessing technology (or any aspect
of it) in-use on a user&apos;s own data even in a
development mode along the lines of
McKeown et al. (1998) is needed. While
experimentation designs involving subjects
performing short term controlled tasks may
yield results of statistical significance,
generalizability to the user community is
limited.
In addition, the level of user support
text summarization systems should provide
also continues to be speculative. More
interest lies in new areas of inquiry like
visualization and browsing techniques (e.g.,
Boguraev et al., 1998), multi-document
summarization ( e.g., McKeown and Radev,
1995), multi-media summarization (e.g.,
Merlino and Maybury, 1999), summarization
</bodyText>
<page confidence="0.998962">
49
</page>
<bodyText confidence="0.99885525">
of documents with graphics (e.g., Futrelle,
1998) and multi-lingual summarization (e.g.,
Cowie, 1998). But systematic user studies on
interface support, applicability of proposed
summarization features, or on the real-world
use of demonstration and prototype systems or
even commercial systems have not
materialized.
</bodyText>
<sectionHeader confidence="0.743947" genericHeader="introduction">
3.0 Overview
</sectionHeader>
<bodyText confidence="0.99993570967742">
This paper presents a user study of a _
summarization system and provides insights
on a number of technical issues relevant to the
summarization R&amp;D community that arise in,
the context of use, concerning technology
performance and user support. We describe
initial stages in the insertion of the SRA
summarizer in which (1) a large scale beta
test was conducted, and (2) analysis of tool
usage data, user surveys and observations, and
user requirements is leading to system
enhancements and more effective
summarization technology insertion. In our
user study, we begin with a brief description
of the task and technology (3.1). We then
describe the beta test methodology (3.2) and
&apos;analysis of tool usage data (3.3). We focus on
what we learned in our user-centered
approach about how technology performance
in a task and user support affect user
. acceptance (3.4) and what significant
technology-related modifications resulted
and what studies are in progress to measure
tool efficacy, summarization effectiveness,
and the impact of training on tool use (3.5).
Though work to enhance the text
summarization system is underway, we focus
in this paper on user-centered issues. Our
work is predicated on the belief that there is
no substitute for user generated data to guide
tool enhancement.
</bodyText>
<subsectionHeader confidence="0.983763">
3.1 Task and Technology
</subsectionHeader>
<bodyText confidence="0.999755195121951">
The task is indicative. Our users rely
on machine generated summaries (single
document, either generic or query-based, with
user adjustment of compression rates) to
judge relevance of full documents to their
information need. As an information analyst, -
our typical user routinely scans summaries to
stay current with fields of interest and
enhance domain knowledge. This scanning
task is one of many jobs an analyst performs
to support report writing for customers in
other Government agencies. Our goal is to
generate summaries that accelerate
eliminating or selecting documents without
misleading or causing a user to access the
original text unnecessarily.
The system in this user study is a
version of the SRA sentence extraction system
described in Aone et al. (1997, 1998, 1999).
Users retrieve documents from a database of
multiple text collections of reports and press.
Documents are generally written in a
journalistic style and average 2,000 characters
in length. The number of documents in a batch
may vary from a few to hundreds. Batches of
retrieved texts may be routinely routed to our
summary server or uploaded by the user. The
system is web-based and provides the
capability to tailor summary output by
creating multiple summary set-ups. User
options include: number of sentences
viewed, summary type applied and sorting,
other information viewed (e.g. title, date),
and high frequency document terms and
named entities viewed. Users can save, print
or view full text originals with summaries
appended. Viewed originals highlight
extracted sentences.
All system use is voluntary. Our users
are customers and, if dissatisfied, may elect to
scan data without our technology.
</bodyText>
<subsectionHeader confidence="0.998664">
3.2 Beta Test Methodology
</subsectionHeader>
<bodyText confidence="0.999418833333333">
In the fall of 1998, 90+ users were
recruited primarily through an IR system news
group and provided access to the SRA system
summarizer to replace their full text review
process of scanning concatenated files.
Procedural (how-to) training was optional, but
</bodyText>
<page confidence="0.992392">
50
</page>
<bodyText confidence="0.999945733333333">
approximately 70 users opted to receive a one-
on-one hands-on demonstration (about forty-
five minutes in length) on texts that the new
user had retrieved. The beta testing took place
over a six month period. With no stipulation
on the length of participation, many users
simply tried out the system a limited number
of times. Initial feedback gave us a clear
picture of the likelihood of continued use. Our
relatively low retention rate highlighted the
fact that the experimental conditions in
previous summary experiments may be
misleading and masked factors that do not
surface until users use a system in a daily,
work in a real-world setting.
</bodyText>
<subsectionHeader confidence="0.999891">
3.3 Analysis of Tool Usage Data
</subsectionHeader>
<bodyText confidence="0.999678785714286">
Usage data were collected for all
system users and analyzed through web logs:
These logs were a record of what users did on
their actual work data. For each user, our logs
provided a rich source of information: number
of summary batches, number of documents in
each, whether documents were viewed, and
set up features--summary type, summary lines
viewed, number of indicator (high frequency
signature terms) lines viewed, number of
entity (persons, places, organizations) lines
viewed, query terms). Table 1 below
illustrates the type of representative data
collected, questions of interest, and findings.
</bodyText>
<tableCaption confidence="0.995899">
Table 1: Questions of Interest, Tool Usage Data, Findings
</tableCaption>
<bodyText confidence="0.9769864">
Questions Data Finding
Were documents number of sum- Users routinely accessed our system to read
summarized? mary events machine generated summaries.
Did users actually number of current Most users did not appear to fully exploit the flex-
tailor the system? set-ups ibility of the system. The beta test population had
a median of only two set-up types active.
Did the users select type of summary Usage data indicated that about half the popula-
generic or query- tion selected generic and the other half query-
based summaries? based summaries. (Note: The default set-up was
the generic summarization.)
Is there a difference number of sen- The hypothesis of equal median number of sen-
among summary tences viewed by tences available for viewing sentences was tested.
types for the num- summary types The number of sentences viewed with generic
ber of sentences (generic, query- summary type (3) is significantly different from
viewed? based, lead) either query-based (5) or lead (6).
Do users choose to indicator/entity Users tended to retain indicator and entity prefer-
use indicators and preferences for ences when tailoring capabilities. (But users gen-
entities when tailor- non-default set-ups erally modified a default set-up in which both
ing browsing capa- (on or off) preferences have a line viewed.)
bility?
</bodyText>
<page confidence="0.996238">
51
</page>
<tableCaption confidence="0.999001">
Table 1: Questions of Interest, Tool Usage Data, Findings
</tableCaption>
<figureCaption confidence="0.419094384615385">
Questions Data Finding
Does training make training and tool A chi-squared test for independence between
a difference on sys- use data training and use reflected a significant relation-
tern use or user pro- ship (p value close to 0) i.e., training did impact
file type? Users the user&apos;s decision to use the system. However,
were categorized training did not make a difference across the three
(advanced, interrne- user profile types. A Fisher Exact test on a 3x2
diate, novice) on contingency table revealed that the relative num-
the basis of usage bets of trained and untrained users at the three
features with Harti- user profile types were the same (p-value=
gan&apos;s K-Means 0.1916) i.e., training and type are independent.
clustering algo- ,
rithm.
</figureCaption>
<bodyText confidence="0.999961090909091">
As we began to analyze the data, we
realized that we had only a record of use, but
were not sure of what motivated the use
patterns. Therefore, the team supplemented
tool usage data with an ori-line survey and
one-on-one observations to help us understand
and analyze the user behavior. These
additional data points motivated much of our
work described in 3.5. Throughout the six
month cycle we also collected and categorized
user requirements.
</bodyText>
<subsectionHeader confidence="0.882579">
3.4 Insights on Text Summarization
3.4.1 Technology Performance
</subsectionHeader>
<bodyText confidence="0.564179">
Insight 1: For user acceptance, technology
</bodyText>
<listItem confidence="0.78119">
•
</listItem>
<bodyText confidence="0.996706069767442">
performance must go beyond a good
summary. It requires an understanding of the
users&apos; work practices.
We learned that many factors in the
task environment affect technology
performance and user acceptance.
Underpinning much work in summarization is
the view that summaries are time savers. Mani
et at. (1999) report that summaries at a low
compression rate reduced decision making
time by 40% (categorization) and 50% (ad-
hoc) with relevance asessments almost as
accurate as the full text. Although evaluators
acknowledge the role of data presentation (
e.g., Firmin and Chrzanowski, 1999; Merlino
and Maybury, 1999), most studies use
summary system output as the metric for
evaluation. The question routinely posed
seems to be &amp;quot;Do summaries save the user
time without loss in accuracy?&amp;quot; However, we
confirmed observations on the integration of
summarization and retrieval technologies of
McKeown et al. (1998) and learned that
users are not likely to consider using
summaries as a time saver unless the
summaries are efficiently accessed. For our
users a tight coupling of retrieval and
summarization is pre-requisite. Batches
automatically routed to the summary server
available for user review were preferred over
those requiring the user to upload files for
summarization. Users pointed out that the
uploading took more time then they, were
willing to spend.
User needs and their work practices
often constrain how technology is applied.
For example, McKeown et al. (1998) focused
on the needs of physicians who want to
examine only data for patients with similar
characteristics to their own patients, and
Wasson (1998) focused on the needs of news
information customers who want to retrieve
documents likely to be on-topic. We too
</bodyText>
<page confidence="0.990788">
52
</page>
<bodyText confidence="0.99984725">
discovered that the user needs affect their
interest in summarization technology, but
from a more general perspective. Text
REtrieval Conferences (e.g., Harman, 1996)
have baselined system performance in terms
of two types of tasks--routing or ad-hoc. In
our environment the ad-hoc users were less
likely .to want a summary. They simply
wanted an answer to a question and did not
want to review summaries. If too many
documents were retrieved, they would simply
craft a more effective query.
Measuring the efficiency gains with a
real population was quite problematic for
technology in-use. We faced a number of
challenges. Note that in experimental
conditions, subjects perform . on full and
reduced versions. One challenge was to
baseline non-intrusively the current (non-
summary) full text review process. A second
was to measure both accuracy and efficiency
gains for users performing on the job. These
challenges were further exacerbated by the
fact that users in an indicative task primarily
use a summary to eliminate most documents.
They have developed effective skimming and
Scanning techniques and are already quite
efficient at this task.
In short, our experience showed that
technologists deploying single document
. summarization capability are likely be
constrained by the following factors:
</bodyText>
<listItem confidence="0.99832875">
• • the ease of technology use
• the type of user information need
• how effective the user performs the task
without the technology.
</listItem>
<subsectionHeader confidence="0.563623">
3.4.2 User Support
</subsectionHeader>
<bodyText confidence="0.994146">
Insight 2: Users require more than just a good
summary. They require the right level of
technology support.
Although the bulk of the research work
still continues to focus on summarization
algorithms, we now appreciate the importance
of user support to text summarization use.
The SRA software was quite robust and fast.
The task of judging relevance with a summary
(even a machine generated one) instead of the
full text version does not require a user to
acquire a fundamentally different work
practice. Yet our system was not apparently
sufficiently supporting tool navigation. One of
the reasons was that our on-line help was not
developed from a user perspective and was
rarely accessed. Another was that browse and
view features did not maximize performance.
For example, the interface employed a scroll
bar for viewing summaries rather than more
effective Next Or Previous buttons. Users
frequently asked the same questions, but we
were answering them individually.
Terminology clear to the technologists was
not understood by users. We also noticed that
though there were requirements for
improvement of summarization quality, many
requirements were associated with these user
support issues.
One of the more unexpected findings
was the under-utilization of tailoring features.
The system offered the user many ways to
tailor summaries to their individual needs, yet
most users simply relied on default set-ups.
Observations revealed little understanding of
the configurable features and how these
features corresponded to user needs to say
nothing of how the algorithm worked. Some
users did not understand the difference
between the two summary types or sorting
effects with query-based summary selection.
Non-traditional summary types--indicators
and named entities--did not appear to help
render a relevance judgment. We came to
understand that just because technologists sees
the value to these features does not mean that
a user will or that the features, in fact, have
utility.
</bodyText>
<sectionHeader confidence="0.897389" genericHeader="method">
3.5 Technology-related Modifications
</sectionHeader>
<footnote confidence="0.62743">
3.5.1 User-centered Changes to Technology
Work Practices
</footnote>
<page confidence="0.991662">
53
</page>
<listItem confidence="0.928564333333333">
, On technology performance, we
learned that
• seamless integration with an IR system
was preferred
• users with static queries were more likely
customers for a summary service
• gains in efficiency are hard to measure for
a task already efficiently performed in a
real-world situations.
</listItem>
<bodyText confidence="0.999952807228916">
In response, we have established a summary
service in which retrieval results are directly
routed to our summary server and await the
user. We plan to integrate the summarization
tool into the IR system. (Uploading batches
and then submission to the server is still an
option.) We also abandoned the naive idea
that data overload equates to summarization
requirements and realized that the technology
does not apply to all users. We have more
effectively selected users by profiling
characteristics of active users (e.g. daily
document viewing work practice, document
volume, static query use, etc.) and have
prioritized deployment to that population
which could most benefit from it.
In order to demonstrate tool
:summarization efficiency, we needed to
baseline full-text review. We considered, but
rejected a number of options--user self-report
and timing, observations, and even the
creation of a viewing tool to monitor and
document full text review. Instead, we
baselined full text scanning through
information retrieval logs for a subgroup of
users by tracking per document viewing time
for a month period. These users submit the
same queries daily and view their documents
through the IR system browser. For the
heaviest system users, 75% of the documents
were viewed in under 20 seconds per
document, but note that users vary widely
with a tendency to spend a much longer
browse time on a relatively small number of
documents. We then identified a subgroup of
these users and attempted to deploy the
summarizer to this baseline group to compare
scanning time required over a similar time
frame. We are currently analyzing these data.
System in a work environment is
considered a good indicator of tool utility, but
we wanted some gauge of summary quality
and also anticipated user concerns about an
emerging technology like automatic text
summarization. We compromised and
selected a method to measure the
effectiveness of our summaries that serves a
dual purpose--our users gain confidence in the
utility of the summaries and we can collect
and measure the effectiveness of the generic
summaries for some of our users on their data.
We initially piloted and now have
incorporated a data collection procedure into
our software. In our on-line training, we
guide users to explore tool capabilities
through a series of experiments or tasks. In the
first of these tasks, a user is asked to submit a
batch for summarization, then for each of five
to seven user-selected summaries to record
answers to the question:
&amp;quot;Is this document likely to be relevant to
me?&amp;quot;(based on the summary)
yes no
Then, the• user was directed to open the
original documents for each of the summaries
and record answers to the question:
&amp;quot;Is the document relevant to me?&amp;quot;
(after reading the original text)
yes no
In a prototype collection effort, we
asked users to review the first ten documents,
but in follow-on interviews the users
recommended review of fewer documents. We
understand the limits this places on
interpreting our data. Also, the on-line training
is optional so we are not able to collect these
data for all our users uniformly.
Most of the users tested exhibited both
high recall and precision, with six users
judging relevance correctly for all documents
(in Table 2 below). The False Negative error
was high for only one user, while the majority
of - the users exhibited no False Negative
</bodyText>
<page confidence="0.996214">
54
</page>
<bodyText confidence="0.999844333333333">
errors, a worse error to commit than wasting
time viewing irrelevant data, False Positive.
Across all the users, 79% of all relevant
documents and 81% of the irrelevant
documents were accurately categorized by
examination of the summary.
</bodyText>
<tableCaption confidence="0.982866">
Table 2: Relevance Classes by User
</tableCaption>
<table confidence="0.9995264375">
User True False True False
Positive Positive Negative Negative
1 5 0 0 0
2 5 0 0 0
3 4 0 0 1
4 1 4 0 . 0
5 . 5 0 0 1
1 4 0 0 2
7 7 0 &apos;0 0
8 4 0 3 0
9 5 0 0 2
, 10 0 0 7 0
11 2 0 3 0
&apos; 12 1 0 2 2
13 0 1 6 0
14 1 0 1 4
</table>
<subsectionHeader confidence="0.96011">
3.5.2 User-centered Changes in User
Support
</subsectionHeader>
<bodyText confidence="0.980182">
On user support , we learned that
</bodyText>
<listItem confidence="0.989438">
• our system did not effectively support user
tool navigation
• our users did not fully exploit system
tailorable features
</listItem>
<bodyText confidence="0.999751529411765">
In response, we addressed user support needs
from three different angles, each of which we
discuss below: incorporation of Electronic
Performance Support Systems, design and
implementation of procedural on-line training
and guided discovery training, and user
analysis of summary quality.
Electronic Performance Support
Systems (EPSS) is a widely acknowledged
strategy for on the job performance support.
Defined as &amp;quot;an optimized body of co-
ordinated on-line methods and resources that
enable and maintain a person&apos;s or an
organization&apos;s performance,&amp;quot; EPSS
interventions range from simple help systems
to intelligent wizard-types of support.
(Villachica and Stone, 1999; Gery 1991). We
elected to incorporate EPSS rather than
, classroom instruction. Based on an analysis of
tool usage data, user requirements, and user
observations, experts in interface design and
technology performance support prototyped
an EPSS enhanced interface. Active system
users reviewed these changes before
implementation. The on-line perfomance
support available at all times includes system
feature procedures, a term glossary, FAQ, and
a new interface design.
With incorporation of the EPSS, we
also addressed the under-utilization of the
configurable features. Although simple
technologies with few options such as
traditional telephones do not require
conceptual system understanding for effective
use, more complex systems with multiple
options are often underutilized when
supported with procedural training alone. We
decided to incorporate both procedural
training in a &amp;quot;Getting Started&amp;quot; tutorial and
conceptual training in &amp;quot;The Lab.&amp;quot; In &amp;quot;Getting
Started&amp;quot;, users learn basic system actions (e.g.,
creating set-ups, submitting batches for
summarization, viewing summaries). &amp;quot;The
Lab&amp;quot;, on the other hand, supports guided
discovery training in which users explore the
system through a series of experiments in
which they use their own data against various
tool options and record their observations.
Given our own experience with under-
utilization and research reporting difficulties
with unguided exploratory learning (Hsu et
</bodyText>
<page confidence="0.996732">
55
</page>
<bodyText confidence="0.999824577777778">
al., 1993; Tuovinen and Sweller, 1999), we
built on the work of de Mul and Van
Oostendorf (1996) and Van Oostendorf and de
Mul (1999) and their finding that task-oriented
exploratory support leads to more effective
learning of computer systems. We created a
series of experiments that the user conducts to
discover how the summarization technology
can best meet their needs. For example, users
are directed to change summary length and to
determine for themselves how the variation
affects their ability to judge relevance using
their data.
In February, we conducted a study of,
two groups, one with the EPSS and &amp;quot;Getting
Starting&amp;quot; Tutorial and a second with the same
level of support and additionally &amp;quot;The Lab&amp;quot;.
Earlier work by Kieras and Bovair (1984)
compared straight procedural training with
conceptual training and showed that the
conceptually trained users made more efficient
use of system features. The goal of our study
was to determine just what level of training
support the summarization technology
requires for effective use. Through surveys,
we planned to collect attitudes toward the tool
and training and through web logs, tool usage
data and option trials. We also planned to
assess the users&apos; understanding of the features
and benefits of the tool. We are currently
analyzing these data.
In addition to the EPSS and the on-line
training, we developed a method for taking
into account user assessment of our summary
quality in a systematic way. User feedback on
summarization quality during the beta test
was far too general and uneven. We recruited
two users to join our technology team and
become informed rather than the typical naive
users. They designed an analysis tool through
which they database problematic machine
generated summaries and assign them to error-
type categories. Though we expected users to
address issues like summary coherence, they
have identified categories like the following:
</bodyText>
<listItem confidence="0.989299166666667">
• sentence identification errors
• formatting errors
• sentence extraction due to the &amp;quot;rare&amp;quot; word
phenomena
• sentence extraction in &amp;quot;long&amp;quot; documents
• failure to identify abstracts when available
</listItem>
<bodyText confidence="0.99230125">
We expect that this approach can complement
a technology-driven one by helping us
prioritize changes we need based on
methodical data collection and analysis.
</bodyText>
<subsectionHeader confidence="0.836573">
4.0 Summary
</subsectionHeader>
<bodyText confidence="0.999910428571429">
Our experience with text summarization
technology in-use has been quite sobering. In
this paper, we have shown how beta testing an
emerging technology has helped us to
understand that for technology to enhance job
performance many factors besides the
algorithm need to be addressed.
</bodyText>
<sectionHeader confidence="0.915422" genericHeader="method">
5.0 References
</sectionHeader>
<reference confidence="0.997639608695652">
Aone, C., Gorlinsky, J. and Okurowski, M.E.
1997. Trainable, scalable summarization
using robust NLP. In Intelligent Scalable
Text Summarization. Madrid, Spain:
Association of Computational Linguistics,
pages 66-73.
Aone, C., Gorlinsky, J. and Okurowski, M.E.
1998. Trainable scalable summarization
using robust NLP and machine learning. In
Coling-ACL 98. Montreal, Quebec,
Canada, pages 62-66.
Aone, C., Gorlinsky, J., Larsen, B. and
Okurowski, M.E. 1999. A trainable
summarizer with knowledge acquired
from robust NLP techniques. In Mani, I.
and Maybury, M. (eds.), Advances in
Automatic Text Summarization. pages 71-
80, Cambridge, Massachusetts: MIT Press.
Boguraev, B., Kennedy, C., Bellamey, R.,
Brawer, S., Wong, Y.Y. and Swartz, J.
1998. Dynamic presentation of document
content for rapid on-line skimming.
Intelligent Text Summarization. (Papers
</reference>
<page confidence="0.991777">
56
</page>
<reference confidence="0.909449113924051">
from the 1998 AAAI Spring Symposium
Technical Report SS-98-06), pages 109-
118.
Brandow, R., Mitze, K. and Rau, L. 1994.
Automatic condensation of electronic
publications by sentence selection.
Information Processing and Management,
31(5):675-685.
Cowie, J., Mahesh, K., Nirenburg, S. and
Zajac, R., 1998. MINDS--Multi-lingual
INteractive document summarization.
Intelligent Text Summarization. (Papers
from the 1998 AAAI Spring Symposium
Technical Report SS-98-06), pages 122-
123.
de Mul, S. and van Oostendorp, H. 1996.
Learning user interfaces by exploration.
Acta Psychologica, 91:325-344.
Firmin, T. and Chrzanowski, M. 1999. An
evaluation of automatic text
summarization. In Mani, I. and Maybury,
M.(eds.), Advances in Automatic Text
Summarization. pages 325-336,
Cambridge, Massachusetts: MIT Press.
Futrelle, R. 1998. Summarization of
documents that include graphics.
Intelligent Text Summarization. (Papers
from the 1998 AAAI Spring Symposium
Technical Report SS-98-06), pages 92-
101.
Gery. , G. 1991. Electronic performance
support systems: How and why to remake
the workplace through the stratgic
application of technology. Tolland, MA:
Gery Performance Press.
Harman, D.K. 1996. The Fourth Text
REtrieval Conference (TREC-4). National
Institute of Standards and Technology
Special Publication, pages 500-236.
Hsu, J-F., Chapelle, C. and Thompson, 4.,
1993. Exploratory learning environments:
What are they and do students explore?
Journal Educational Computing Research,
9(1): 1-15.
Jing, H., McKeown, K., Barzilay, R. and
Elhadad, M. 1998. Summarization
evaluation methods: Experiments and
methods. Intelligent Text Summarization.
(Papers from the 1998 AAAI Spring
Symposium Technical Report SS-98-06),
pages 51-59.
Kieras, D.E. and Bovair, S. 1984. The role of
a mental model in learning to operate a
device. Cognitive Science, (8), 1-17.
Mani, I., House, D., Klein, G., Hirschman, L.,
Firmin, T. and Sundheim, B. 1999. The
TIPSTER SUMMAC Text Summarization
Evaluation. In Proceedings of EACL99
Ninth Conference of the European
Chapter of the Association for
Computational Linguistics. pages 77-83.
McKeown, K. and Radev, D. 1995.
Generating summaries of multiple news
articles. In Proceedings of the 18th Annual
International SIGIR Conference on
Research and Development in Information
Retrieval. pages 74-78.
McKeown, K. Jordan, D. and
Hatzivassiloglou, V. 1998. Generating
patient specific summaries on online
literature. Intelligent Text Summarization.
(Papers from the 1998. AAAI Spring
Symposium Technical Report SS-98-06),
pages 34-43.
Merlino, A. and Maybury, M. 1999. An
empirical study of the optimal presentation
of multi-media summaries of broadcast
news. In Mani, I. and Maybury, M. (eds.),
Advances in Automatic Text
</reference>
<page confidence="0.980937">
57
</page>
<reference confidence="0.999759">
Summarization. pages 391-401,
Cambridge, Massachusetts: MIT Press.
Morris, A., Kasper, G., and Adams, D. 1999.
The effects and limitations of automated
text condensing on reading comprehension
performance. In Mani, I. and Maybury,
M. (eds.), Advances in Automatic Text
Summarization. pages 305-323,
Cambridge, Massachusetts: MIT Press.
Paice, C.D., 1990. Constructing literature
abstracts by computer: Techniques and
prospects. Information Processing and
Management, 26(1): 171-186.
Sparck-Jones, K. 1993. What might be in a
summary? In Information Retrieval 93:
Von der Modellierung zur Anwendung,
pages 9-26.
Salton, G., Singhal, A., Mitra, M. and
Buckely, C. 1999. Automatic text
structuring and summarization. In Mani, I.
and Maybury, M. (eds.), Advances in
Automatic Text Summarization. pages 342-
355, Cambridge, Massachusetts: MIT
Press.
Strzalkowski, T., Wang, J. and Wise, B.,
1998. A robust practical text
summarization. Intelligent Text
„ Summarization. (Papers from the 1998
AAAI Spring Symposium Technical
Report SS-98-06), pages 26-33.
Tombros, A., and Sanderson, M. 1998.
Advantages of query-based summaries in
information retrieval. In Proceedings of
the 21st ACM SIGIR Conference
(SIGIR98). pages 2-10.
Tuovinen, J. and Sweller, J. 1999. A
comparison of cognitive load associated
with discovery learning and worked
examples. Journal of Educational
Psychology, 9(2):334-341.
Van Oostendorp, H. and de Mul, S. 1999.
Learning by exploring: Thinking aloud
while exploring an information system.
Instruction Science, 27:269-284.
Villachica, S.W. and Stone, D. 1999.
Performance support systems. In
Stolovitch, H.D. and Keeps, K.J., (eds.),
Handbook of Human Performance
Technology. San Francisco: Jossey-Bass
Pfeiffer.
Wasson, M. 1998. Using leading text for news
summaries: Evaluation results and
implications for commercial
summarization. In Coling-ACL 98.
Montreal, Quebec, Canada. pages 1364-
1368.
</reference>
<page confidence="0.999261">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.549682">
<title confidence="0.996048">Text Summarizer in Use: Lessons from Real World Deployment and Evaluation</title>
<author confidence="0.957377333333333">Mary Ellen Harold Joaquin</author>
<affiliation confidence="0.990479">Department of</affiliation>
<address confidence="0.953232">9800 Savage Fort Meade, MD. 20755</address>
<author confidence="0.996458">Ruth Colvin</author>
<affiliation confidence="0.973697">Clark Training &amp;</affiliation>
<address confidence="0.9733525">17801 CR Dolores, Colorado 81323</address>
<author confidence="0.962699">Tony Taylor</author>
<affiliation confidence="0.951181">SRA Corp.</affiliation>
<address confidence="0.983991666666667">4939 Ellcridge Landing Suite #195 Linthicum, MD. 21090</address>
<author confidence="0.932541">Frank</author>
<affiliation confidence="0.89573">Kathpol Technologies</affiliation>
<address confidence="0.990557">6835 Deerpath Suite Elkridge, MD. 21705</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>J Gorlinsky</author>
<author>M E Okurowski</author>
</authors>
<title>Trainable, scalable summarization using robust NLP.</title>
<date>1997</date>
<journal>Association of Computational Linguistics,</journal>
<booktitle>In Intelligent Scalable Text Summarization.</booktitle>
<pages>66--73</pages>
<location>Madrid, Spain:</location>
<contexts>
<context position="5136" citStr="Aone et al. (1997" startWordPosition="767" endWordPosition="770">ates) to judge relevance of full documents to their information need. As an information analyst, - our typical user routinely scans summaries to stay current with fields of interest and enhance domain knowledge. This scanning task is one of many jobs an analyst performs to support report writing for customers in other Government agencies. Our goal is to generate summaries that accelerate eliminating or selecting documents without misleading or causing a user to access the original text unnecessarily. The system in this user study is a version of the SRA sentence extraction system described in Aone et al. (1997, 1998, 1999). Users retrieve documents from a database of multiple text collections of reports and press. Documents are generally written in a journalistic style and average 2,000 characters in length. The number of documents in a batch may vary from a few to hundreds. Batches of retrieved texts may be routinely routed to our summary server or uploaded by the user. The system is web-based and provides the capability to tailor summary output by creating multiple summary set-ups. User options include: number of sentences viewed, summary type applied and sorting, other information viewed (e.g. t</context>
</contexts>
<marker>Aone, Gorlinsky, Okurowski, 1997</marker>
<rawString>Aone, C., Gorlinsky, J. and Okurowski, M.E. 1997. Trainable, scalable summarization using robust NLP. In Intelligent Scalable Text Summarization. Madrid, Spain: Association of Computational Linguistics, pages 66-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>J Gorlinsky</author>
<author>M E Okurowski</author>
</authors>
<title>Trainable scalable summarization using robust NLP and machine learning.</title>
<date>1998</date>
<booktitle>In Coling-ACL 98.</booktitle>
<pages>62--66</pages>
<location>Montreal, Quebec, Canada,</location>
<marker>Aone, Gorlinsky, Okurowski, 1998</marker>
<rawString>Aone, C., Gorlinsky, J. and Okurowski, M.E. 1998. Trainable scalable summarization using robust NLP and machine learning. In Coling-ACL 98. Montreal, Quebec, Canada, pages 62-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>J Gorlinsky</author>
<author>B Larsen</author>
<author>M E Okurowski</author>
</authors>
<title>A trainable summarizer with knowledge acquired from robust NLP techniques.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>71--80</pages>
<editor>In Mani, I. and Maybury, M. (eds.),</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<marker>Aone, Gorlinsky, Larsen, Okurowski, 1999</marker>
<rawString>Aone, C., Gorlinsky, J., Larsen, B. and Okurowski, M.E. 1999. A trainable summarizer with knowledge acquired from robust NLP techniques. In Mani, I. and Maybury, M. (eds.), Advances in Automatic Text Summarization. pages 71-80, Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>C Kennedy</author>
<author>R Bellamey</author>
<author>S Brawer</author>
<author>Y Y Wong</author>
<author>J Swartz</author>
</authors>
<title>Dynamic presentation of document content for rapid on-line skimming. Intelligent Text Summarization. (Papers from the</title>
<date>1998</date>
<tech>Technical Report SS-98-06),</tech>
<pages>109--118</pages>
<contexts>
<context position="2571" citStr="Boguraev et al., 1998" startWordPosition="375" endWordPosition="378">ich have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involving subjects performing short term controlled tasks may yield results of statistical significance, generalizability to the user community is limited. In addition, the level of user support text summarization systems should provide also continues to be speculative. More interest lies in new areas of inquiry like visualization and browsing techniques (e.g., Boguraev et al., 1998), multi-document summarization ( e.g., McKeown and Radev, 1995), multi-media summarization (e.g., Merlino and Maybury, 1999), summarization 49 of documents with graphics (e.g., Futrelle, 1998) and multi-lingual summarization (e.g., Cowie, 1998). But systematic user studies on interface support, applicability of proposed summarization features, or on the real-world use of demonstration and prototype systems or even commercial systems have not materialized. 3.0 Overview This paper presents a user study of a _ summarization system and provides insights on a number of technical issues relevant to </context>
</contexts>
<marker>Boguraev, Kennedy, Bellamey, Brawer, Wong, Swartz, 1998</marker>
<rawString>Boguraev, B., Kennedy, C., Bellamey, R., Brawer, S., Wong, Y.Y. and Swartz, J. 1998. Dynamic presentation of document content for rapid on-line skimming. Intelligent Text Summarization. (Papers from the 1998 AAAI Spring Symposium Technical Report SS-98-06), pages 109-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brandow</author>
<author>K Mitze</author>
<author>L Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1994</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>31--5</pages>
<contexts>
<context position="1512" citStr="Brandow et al., 1994" startWordPosition="213" endWordPosition="216">wledge gleaned about technology in-use &apos;can temper theoretical considerations and . motivate as well as direct development likely to result in higher return on investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1994</marker>
<rawString>Brandow, R., Mitze, K. and Rau, L. 1994. Automatic condensation of electronic publications by sentence selection. Information Processing and Management, 31(5):675-685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cowie</author>
<author>K Mahesh</author>
<author>S Nirenburg</author>
<author>R Zajac</author>
</authors>
<title>MINDS--Multi-lingual INteractive document summarization. Intelligent Text Summarization. (Papers from the</title>
<date>1998</date>
<tech>Technical Report SS-98-06),</tech>
<pages>122--123</pages>
<marker>Cowie, Mahesh, Nirenburg, Zajac, 1998</marker>
<rawString>Cowie, J., Mahesh, K., Nirenburg, S. and Zajac, R., 1998. MINDS--Multi-lingual INteractive document summarization. Intelligent Text Summarization. (Papers from the 1998 AAAI Spring Symposium Technical Report SS-98-06), pages 122-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S de Mul</author>
<author>H van Oostendorp</author>
</authors>
<title>Learning user interfaces by exploration.</title>
<date>1996</date>
<journal>Acta Psychologica,</journal>
<pages>91--325</pages>
<marker>de Mul, van Oostendorp, 1996</marker>
<rawString>de Mul, S. and van Oostendorp, H. 1996. Learning user interfaces by exploration. Acta Psychologica, 91:325-344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Firmin</author>
<author>M Chrzanowski</author>
</authors>
<title>An evaluation of automatic text summarization.</title>
<date>1999</date>
<booktitle>In Mani, I. and Maybury, M.(eds.), Advances in Automatic Text Summarization.</booktitle>
<pages>325--336</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="1645" citStr="Firmin and Chrzanowski, 1999" startWordPosition="235" endWordPosition="238">kely to result in higher return on investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involving subjects performing short term controlled tasks ma</context>
<context position="10943" citStr="Firmin and Chrzanowski, 1999" startWordPosition="1690" endWordPosition="1693">ght 1: For user acceptance, technology • performance must go beyond a good summary. It requires an understanding of the users&apos; work practices. We learned that many factors in the task environment affect technology performance and user acceptance. Underpinning much work in summarization is the view that summaries are time savers. Mani et at. (1999) report that summaries at a low compression rate reduced decision making time by 40% (categorization) and 50% (adhoc) with relevance asessments almost as accurate as the full text. Although evaluators acknowledge the role of data presentation ( e.g., Firmin and Chrzanowski, 1999; Merlino and Maybury, 1999), most studies use summary system output as the metric for evaluation. The question routinely posed seems to be &amp;quot;Do summaries save the user time without loss in accuracy?&amp;quot; However, we confirmed observations on the integration of summarization and retrieval technologies of McKeown et al. (1998) and learned that users are not likely to consider using summaries as a time saver unless the summaries are efficiently accessed. For our users a tight coupling of retrieval and summarization is pre-requisite. Batches automatically routed to the summary server available for use</context>
</contexts>
<marker>Firmin, Chrzanowski, 1999</marker>
<rawString>Firmin, T. and Chrzanowski, M. 1999. An evaluation of automatic text summarization. In Mani, I. and Maybury, M.(eds.), Advances in Automatic Text Summarization. pages 325-336, Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Futrelle</author>
</authors>
<title>Summarization of documents that include graphics. Intelligent Text Summarization. (Papers from the</title>
<date>1998</date>
<tech>Symposium Technical Report SS-98-06),</tech>
<pages>92--101</pages>
<publisher>AAAI Spring</publisher>
<contexts>
<context position="2763" citStr="Futrelle, 1998" startWordPosition="401" endWordPosition="402">eeded. While experimentation designs involving subjects performing short term controlled tasks may yield results of statistical significance, generalizability to the user community is limited. In addition, the level of user support text summarization systems should provide also continues to be speculative. More interest lies in new areas of inquiry like visualization and browsing techniques (e.g., Boguraev et al., 1998), multi-document summarization ( e.g., McKeown and Radev, 1995), multi-media summarization (e.g., Merlino and Maybury, 1999), summarization 49 of documents with graphics (e.g., Futrelle, 1998) and multi-lingual summarization (e.g., Cowie, 1998). But systematic user studies on interface support, applicability of proposed summarization features, or on the real-world use of demonstration and prototype systems or even commercial systems have not materialized. 3.0 Overview This paper presents a user study of a _ summarization system and provides insights on a number of technical issues relevant to the summarization R&amp;D community that arise in, the context of use, concerning technology performance and user support. We describe initial stages in the insertion of the SRA summarizer in whic</context>
</contexts>
<marker>Futrelle, 1998</marker>
<rawString>Futrelle, R. 1998. Summarization of documents that include graphics. Intelligent Text Summarization. (Papers from the 1998 AAAI Spring Symposium Technical Report SS-98-06), pages 92-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G</author>
</authors>
<title>Electronic performance support systems: How and why to remake the workplace through the stratgic application of technology.</title>
<date>1991</date>
<publisher>Gery Performance Press.</publisher>
<location>Tolland, MA:</location>
<marker>G, 1991</marker>
<rawString>Gery. , G. 1991. Electronic performance support systems: How and why to remake the workplace through the stratgic application of technology. Tolland, MA: Gery Performance Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Harman</author>
</authors>
<date>1996</date>
<booktitle>The Fourth Text REtrieval Conference (TREC-4). National Institute of Standards and Technology Special Publication,</booktitle>
<pages>500--236</pages>
<contexts>
<context position="12261" citStr="Harman, 1996" startWordPosition="1897" endWordPosition="1898">t that the uploading took more time then they, were willing to spend. User needs and their work practices often constrain how technology is applied. For example, McKeown et al. (1998) focused on the needs of physicians who want to examine only data for patients with similar characteristics to their own patients, and Wasson (1998) focused on the needs of news information customers who want to retrieve documents likely to be on-topic. We too 52 discovered that the user needs affect their interest in summarization technology, but from a more general perspective. Text REtrieval Conferences (e.g., Harman, 1996) have baselined system performance in terms of two types of tasks--routing or ad-hoc. In our environment the ad-hoc users were less likely .to want a summary. They simply wanted an answer to a question and did not want to review summaries. If too many documents were retrieved, they would simply craft a more effective query. Measuring the efficiency gains with a real population was quite problematic for technology in-use. We faced a number of challenges. Note that in experimental conditions, subjects perform . on full and reduced versions. One challenge was to baseline non-intrusively the curre</context>
</contexts>
<marker>Harman, 1996</marker>
<rawString>Harman, D.K. 1996. The Fourth Text REtrieval Conference (TREC-4). National Institute of Standards and Technology Special Publication, pages 500-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-F Hsu</author>
<author>C Chapelle</author>
<author>Thompson</author>
</authors>
<title>Exploratory learning environments: What are they and do students explore?</title>
<date>1993</date>
<journal>Journal Educational Computing Research,</journal>
<volume>9</volume>
<issue>1</issue>
<pages>1--15</pages>
<marker>Hsu, Chapelle, Thompson, 1993</marker>
<rawString>Hsu, J-F., Chapelle, C. and Thompson, 4., 1993. Exploratory learning environments: What are they and do students explore? Journal Educational Computing Research, 9(1): 1-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>K McKeown</author>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Summarization evaluation methods: Experiments and methods. Intelligent Text Summarization. (Papers from the</title>
<date>1998</date>
<tech>Technical Report SS-98-06),</tech>
<pages>51--59</pages>
<contexts>
<context position="1552" citStr="Jing et al., 1998" startWordPosition="221" endWordPosition="224"> temper theoretical considerations and . motivate as well as direct development likely to result in higher return on investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is neede</context>
</contexts>
<marker>Jing, McKeown, Barzilay, Elhadad, 1998</marker>
<rawString>Jing, H., McKeown, K., Barzilay, R. and Elhadad, M. 1998. Summarization evaluation methods: Experiments and methods. Intelligent Text Summarization. (Papers from the 1998 AAAI Spring Symposium Technical Report SS-98-06), pages 51-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Kieras</author>
<author>S Bovair</author>
</authors>
<title>The role of a mental model in learning to operate a device.</title>
<date>1984</date>
<journal>Cognitive Science,</journal>
<volume>8</volume>
<pages>1--17</pages>
<contexts>
<context position="22879" citStr="Kieras and Bovair (1984)" startWordPosition="3601" endWordPosition="3604">999) and their finding that task-oriented exploratory support leads to more effective learning of computer systems. We created a series of experiments that the user conducts to discover how the summarization technology can best meet their needs. For example, users are directed to change summary length and to determine for themselves how the variation affects their ability to judge relevance using their data. In February, we conducted a study of, two groups, one with the EPSS and &amp;quot;Getting Starting&amp;quot; Tutorial and a second with the same level of support and additionally &amp;quot;The Lab&amp;quot;. Earlier work by Kieras and Bovair (1984) compared straight procedural training with conceptual training and showed that the conceptually trained users made more efficient use of system features. The goal of our study was to determine just what level of training support the summarization technology requires for effective use. Through surveys, we planned to collect attitudes toward the tool and training and through web logs, tool usage data and option trials. We also planned to assess the users&apos; understanding of the features and benefits of the tool. We are currently analyzing these data. In addition to the EPSS and the on-line traini</context>
</contexts>
<marker>Kieras, Bovair, 1984</marker>
<rawString>Kieras, D.E. and Bovair, S. 1984. The role of a mental model in learning to operate a device. Cognitive Science, (8), 1-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirschman</author>
<author>T Firmin</author>
<author>B Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL99 Ninth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>77--83</pages>
<contexts>
<context position="1669" citStr="Mani et al., 1999" startWordPosition="240" endWordPosition="243"> investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involving subjects performing short term controlled tasks may yield results of stati</context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Firmin, Sundheim, 1999</marker>
<rawString>Mani, I., House, D., Klein, G., Hirschman, L., Firmin, T. and Sundheim, B. 1999. The TIPSTER SUMMAC Text Summarization Evaluation. In Proceedings of EACL99 Ninth Conference of the European Chapter of the Association for Computational Linguistics. pages 77-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>D Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual International SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<pages>74--78</pages>
<contexts>
<context position="2634" citStr="McKeown and Radev, 1995" startWordPosition="383" endWordPosition="386">sing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involving subjects performing short term controlled tasks may yield results of statistical significance, generalizability to the user community is limited. In addition, the level of user support text summarization systems should provide also continues to be speculative. More interest lies in new areas of inquiry like visualization and browsing techniques (e.g., Boguraev et al., 1998), multi-document summarization ( e.g., McKeown and Radev, 1995), multi-media summarization (e.g., Merlino and Maybury, 1999), summarization 49 of documents with graphics (e.g., Futrelle, 1998) and multi-lingual summarization (e.g., Cowie, 1998). But systematic user studies on interface support, applicability of proposed summarization features, or on the real-world use of demonstration and prototype systems or even commercial systems have not materialized. 3.0 Overview This paper presents a user study of a _ summarization system and provides insights on a number of technical issues relevant to the summarization R&amp;D community that arise in, the context of u</context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>McKeown, K. and Radev, D. 1995. Generating summaries of multiple news articles. In Proceedings of the 18th Annual International SIGIR Conference on Research and Development in Information Retrieval. pages 74-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jordan McKeown</author>
<author>D</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Generating patient specific summaries on online literature. Intelligent Text Summarization. (Papers from the 1998. AAAI Spring Symposium</title>
<date>1998</date>
<tech>Technical Report SS-98-06),</tech>
<pages>34--43</pages>
<contexts>
<context position="2143" citStr="McKeown et al. (1998)" startWordPosition="315" endWordPosition="318">t al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involving subjects performing short term controlled tasks may yield results of statistical significance, generalizability to the user community is limited. In addition, the level of user support text summarization systems should provide also continues to be speculative. More interest lies in new areas of inquiry like visualization and browsing techniques (e.g., Boguraev et al., 1998), multi-document summarization ( e.g., McKeown and Radev, 1995), multi-media summarization (e.g., Merlino and Maybury, 1999), summarization 49 of documents with graphics (e</context>
<context position="11265" citStr="McKeown et al. (1998)" startWordPosition="1739" endWordPosition="1742">. Mani et at. (1999) report that summaries at a low compression rate reduced decision making time by 40% (categorization) and 50% (adhoc) with relevance asessments almost as accurate as the full text. Although evaluators acknowledge the role of data presentation ( e.g., Firmin and Chrzanowski, 1999; Merlino and Maybury, 1999), most studies use summary system output as the metric for evaluation. The question routinely posed seems to be &amp;quot;Do summaries save the user time without loss in accuracy?&amp;quot; However, we confirmed observations on the integration of summarization and retrieval technologies of McKeown et al. (1998) and learned that users are not likely to consider using summaries as a time saver unless the summaries are efficiently accessed. For our users a tight coupling of retrieval and summarization is pre-requisite. Batches automatically routed to the summary server available for user review were preferred over those requiring the user to upload files for summarization. Users pointed out that the uploading took more time then they, were willing to spend. User needs and their work practices often constrain how technology is applied. For example, McKeown et al. (1998) focused on the needs of physician</context>
</contexts>
<marker>McKeown, D, Hatzivassiloglou, 1998</marker>
<rawString>McKeown, K. Jordan, D. and Hatzivassiloglou, V. 1998. Generating patient specific summaries on online literature. Intelligent Text Summarization. (Papers from the 1998. AAAI Spring Symposium Technical Report SS-98-06), pages 34-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Merlino</author>
<author>M Maybury</author>
</authors>
<title>An empirical study of the optimal presentation of multi-media summaries of broadcast news.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>391--401</pages>
<editor>In Mani, I. and Maybury, M. (eds.),</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="1579" citStr="Merlino and Maybury, 1999" startWordPosition="225" endWordPosition="228"> considerations and . motivate as well as direct development likely to result in higher return on investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation de</context>
<context position="10971" citStr="Merlino and Maybury, 1999" startWordPosition="1694" endWordPosition="1697">chnology • performance must go beyond a good summary. It requires an understanding of the users&apos; work practices. We learned that many factors in the task environment affect technology performance and user acceptance. Underpinning much work in summarization is the view that summaries are time savers. Mani et at. (1999) report that summaries at a low compression rate reduced decision making time by 40% (categorization) and 50% (adhoc) with relevance asessments almost as accurate as the full text. Although evaluators acknowledge the role of data presentation ( e.g., Firmin and Chrzanowski, 1999; Merlino and Maybury, 1999), most studies use summary system output as the metric for evaluation. The question routinely posed seems to be &amp;quot;Do summaries save the user time without loss in accuracy?&amp;quot; However, we confirmed observations on the integration of summarization and retrieval technologies of McKeown et al. (1998) and learned that users are not likely to consider using summaries as a time saver unless the summaries are efficiently accessed. For our users a tight coupling of retrieval and summarization is pre-requisite. Batches automatically routed to the summary server available for user review were preferred over</context>
</contexts>
<marker>Merlino, Maybury, 1999</marker>
<rawString>Merlino, A. and Maybury, M. 1999. An empirical study of the optimal presentation of multi-media summaries of broadcast news. In Mani, I. and Maybury, M. (eds.), Advances in Automatic Text Summarization. pages 391-401, Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Morris</author>
<author>G Kasper</author>
<author>D Adams</author>
</authors>
<title>The effects and limitations of automated text condensing on reading comprehension performance.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>305--323</pages>
<editor>In Mani, I. and Maybury, M. (eds.),</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="1533" citStr="Morris et al., 1999" startWordPosition="217" endWordPosition="220">echnology in-use &apos;can temper theoretical considerations and . motivate as well as direct development likely to result in higher return on investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et </context>
</contexts>
<marker>Morris, Kasper, Adams, 1999</marker>
<rawString>Morris, A., Kasper, G., and Adams, D. 1999. The effects and limitations of automated text condensing on reading comprehension performance. In Mani, I. and Maybury, M. (eds.), Advances in Automatic Text Summarization. pages 305-323, Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Paice</author>
</authors>
<title>Constructing literature abstracts by computer: Techniques and prospects.</title>
<date>1990</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>26</volume>
<issue>1</issue>
<pages>171--186</pages>
<contexts>
<context position="1241" citStr="Paice, 1990" startWordPosition="177" endWordPosition="178">ugh commercial summarization products have appeared in the market place and developers continue to explore new summarization areas, few papers have been user-centered, examining summarization technology in-use. In this paper, we show how applied work and the knowledge gleaned about technology in-use &apos;can temper theoretical considerations and . motivate as well as direct development likely to result in higher return on investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., </context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Paice, C.D., 1990. Constructing literature abstracts by computer: Techniques and prospects. Information Processing and Management, 26(1): 171-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck-Jones</author>
</authors>
<title>What might be in a summary?</title>
<date>1993</date>
<booktitle>In Information Retrieval 93: Von der Modellierung zur Anwendung,</booktitle>
<pages>9--26</pages>
<contexts>
<context position="1262" citStr="Sparck-Jones, 1993" startWordPosition="179" endWordPosition="180">l summarization products have appeared in the market place and developers continue to explore new summarization areas, few papers have been user-centered, examining summarization technology in-use. In this paper, we show how applied work and the knowledge gleaned about technology in-use &apos;can temper theoretical considerations and . motivate as well as direct development likely to result in higher return on investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of fa</context>
</contexts>
<marker>Sparck-Jones, 1993</marker>
<rawString>Sparck-Jones, K. 1993. What might be in a summary? In Information Retrieval 93: Von der Modellierung zur Anwendung, pages 9-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Singhal</author>
<author>M Mitra</author>
<author>C Buckely</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>342--355</pages>
<editor>In Mani, I. and Maybury, M. (eds.),</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="1818" citStr="Salton et al., 1999" startWordPosition="263" endWordPosition="266">ng summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involving subjects performing short term controlled tasks may yield results of statistical significance, generalizability to the user community is limited. In addition, the level of user support text summarization systems should prov</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckely, 1999</marker>
<rawString>Salton, G., Singhal, A., Mitra, M. and Buckely, C. 1999. Automatic text structuring and summarization. In Mani, I. and Maybury, M. (eds.), Advances in Automatic Text Summarization. pages 342-355, Cambridge, Massachusetts: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
<author>J Wang</author>
<author>B Wise</author>
</authors>
<title>A robust practical text summarization.</title>
<date>1998</date>
<booktitle>Intelligent Text „ Summarization. (Papers from the</booktitle>
<tech>Technical Report SS-98-06),</tech>
<pages>26--33</pages>
<contexts>
<context position="1846" citStr="Strzalkowski et al., 1998" startWordPosition="267" endWordPosition="270">unctions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involving subjects performing short term controlled tasks may yield results of statistical significance, generalizability to the user community is limited. In addition, the level of user support text summarization systems should provide also continues to be spe</context>
</contexts>
<marker>Strzalkowski, Wang, Wise, 1998</marker>
<rawString>Strzalkowski, T., Wang, J. and Wise, B., 1998. A robust practical text summarization. Intelligent Text „ Summarization. (Papers from the 1998 AAAI Spring Symposium Technical Report SS-98-06), pages 26-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tombros</author>
<author>M Sanderson</author>
</authors>
<title>Advantages of query-based summaries in information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st ACM SIGIR Conference (SIGIR98).</booktitle>
<pages>2--10</pages>
<marker>Tombros, Sanderson, 1998</marker>
<rawString>Tombros, A., and Sanderson, M. 1998. Advantages of query-based summaries in information retrieval. In Proceedings of the 21st ACM SIGIR Conference (SIGIR98). pages 2-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tuovinen</author>
<author>J Sweller</author>
</authors>
<title>A comparison of cognitive load associated with discovery learning and worked examples.</title>
<date>1999</date>
<journal>Journal of Educational Psychology,</journal>
<pages>9--2</pages>
<contexts>
<context position="22164" citStr="Tuovinen and Sweller, 1999" startWordPosition="3483" endWordPosition="3486">te both procedural training in a &amp;quot;Getting Started&amp;quot; tutorial and conceptual training in &amp;quot;The Lab.&amp;quot; In &amp;quot;Getting Started&amp;quot;, users learn basic system actions (e.g., creating set-ups, submitting batches for summarization, viewing summaries). &amp;quot;The Lab&amp;quot;, on the other hand, supports guided discovery training in which users explore the system through a series of experiments in which they use their own data against various tool options and record their observations. Given our own experience with underutilization and research reporting difficulties with unguided exploratory learning (Hsu et 55 al., 1993; Tuovinen and Sweller, 1999), we built on the work of de Mul and Van Oostendorf (1996) and Van Oostendorf and de Mul (1999) and their finding that task-oriented exploratory support leads to more effective learning of computer systems. We created a series of experiments that the user conducts to discover how the summarization technology can best meet their needs. For example, users are directed to change summary length and to determine for themselves how the variation affects their ability to judge relevance using their data. In February, we conducted a study of, two groups, one with the EPSS and &amp;quot;Getting Starting&amp;quot; Tutori</context>
</contexts>
<marker>Tuovinen, Sweller, 1999</marker>
<rawString>Tuovinen, J. and Sweller, J. 1999. A comparison of cognitive load associated with discovery learning and worked examples. Journal of Educational Psychology, 9(2):334-341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Van Oostendorp</author>
<author>S de Mul</author>
</authors>
<title>Learning by exploring: Thinking aloud while exploring an information system.</title>
<date>1999</date>
<journal>Instruction Science,</journal>
<pages>27--269</pages>
<marker>Van Oostendorp, de Mul, 1999</marker>
<rawString>Van Oostendorp, H. and de Mul, S. 1999. Learning by exploring: Thinking aloud while exploring an information system. Instruction Science, 27:269-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S W Villachica</author>
<author>D Stone</author>
</authors>
<title>Performance support systems.</title>
<date>1999</date>
<booktitle>Handbook of Human Performance Technology.</booktitle>
<editor>In Stolovitch, H.D. and Keeps, K.J., (eds.),</editor>
<location>San Francisco: Jossey-Bass Pfeiffer.</location>
<contexts>
<context position="20682" citStr="Villachica and Stone, 1999" startWordPosition="3273" endWordPosition="3276">ds from three different angles, each of which we discuss below: incorporation of Electronic Performance Support Systems, design and implementation of procedural on-line training and guided discovery training, and user analysis of summary quality. Electronic Performance Support Systems (EPSS) is a widely acknowledged strategy for on the job performance support. Defined as &amp;quot;an optimized body of coordinated on-line methods and resources that enable and maintain a person&apos;s or an organization&apos;s performance,&amp;quot; EPSS interventions range from simple help systems to intelligent wizard-types of support. (Villachica and Stone, 1999; Gery 1991). We elected to incorporate EPSS rather than , classroom instruction. Based on an analysis of tool usage data, user requirements, and user observations, experts in interface design and technology performance support prototyped an EPSS enhanced interface. Active system users reviewed these changes before implementation. The on-line perfomance support available at all times includes system feature procedures, a term glossary, FAQ, and a new interface design. With incorporation of the EPSS, we also addressed the under-utilization of the configurable features. Although simple technolog</context>
</contexts>
<marker>Villachica, Stone, 1999</marker>
<rawString>Villachica, S.W. and Stone, D. 1999. Performance support systems. In Stolovitch, H.D. and Keeps, K.J., (eds.), Handbook of Human Performance Technology. San Francisco: Jossey-Bass Pfeiffer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wasson</author>
</authors>
<title>Using leading text for news summaries: Evaluation results and implications for commercial summarization.</title>
<date>1998</date>
<booktitle>In Coling-ACL 98.</booktitle>
<pages>1364--1368</pages>
<location>Montreal, Quebec,</location>
<contexts>
<context position="1593" citStr="Wasson, 1998" startWordPosition="229" endWordPosition="230">ate as well as direct development likely to result in higher return on investment. 2.0 Background The importance of understanding the function a summary serves for users is widely acknowledged, and seminal works defining summary types by functions (Paice, 1990; Sparck-Jones, 1993) are frequently cited by developers. Task orientation defines extrinsic technology assessments, and the research literature on how to assess performance for machine generated summaries in an experimental task scenario has grown ( Brandow et al., 1994; Morris et al., 1999; Jing et al., 1998; Merlino and Maybury, 1999; Wasson, 1998; Tombros et al., 1998; Firmin and Chrzanowski, 1999; and Mani et al., 1999). An increasing number of research papers on summarization systems now also describe some type of extrinsic evaluative task (e.g. Salton et al., 1999; Strzalkowski et al., 1998). A number of factors (i.e. characteristics of summaries, documents, users, and tasks) have surfaced which have implications for technology use. More research assessing technology (or any aspect of it) in-use on a user&apos;s own data even in a development mode along the lines of McKeown et al. (1998) is needed. While experimentation designs involvin</context>
<context position="11979" citStr="Wasson (1998)" startWordPosition="1854" endWordPosition="1855">es are efficiently accessed. For our users a tight coupling of retrieval and summarization is pre-requisite. Batches automatically routed to the summary server available for user review were preferred over those requiring the user to upload files for summarization. Users pointed out that the uploading took more time then they, were willing to spend. User needs and their work practices often constrain how technology is applied. For example, McKeown et al. (1998) focused on the needs of physicians who want to examine only data for patients with similar characteristics to their own patients, and Wasson (1998) focused on the needs of news information customers who want to retrieve documents likely to be on-topic. We too 52 discovered that the user needs affect their interest in summarization technology, but from a more general perspective. Text REtrieval Conferences (e.g., Harman, 1996) have baselined system performance in terms of two types of tasks--routing or ad-hoc. In our environment the ad-hoc users were less likely .to want a summary. They simply wanted an answer to a question and did not want to review summaries. If too many documents were retrieved, they would simply craft a more effective</context>
</contexts>
<marker>Wasson, 1998</marker>
<rawString>Wasson, M. 1998. Using leading text for news summaries: Evaluation results and implications for commercial summarization. In Coling-ACL 98. Montreal, Quebec, Canada. pages 1364-1368.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>