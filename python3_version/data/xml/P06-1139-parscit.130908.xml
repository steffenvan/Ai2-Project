<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999362">
Stochastic Language Generation Using WIDL-expressions and its
Application in Machine Translation and Summarization
</title>
<author confidence="0.989026">
Radu Soricut
</author>
<affiliation confidence="0.897485333333333">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.622851">
Marina del Rey, CA 90292
</address>
<email confidence="0.998471">
radu@isi.edu
</email>
<author confidence="0.996262">
Daniel Marcu
</author>
<affiliation confidence="0.899258666666667">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.623313">
Marina del Rey, CA 90292
</address>
<email confidence="0.999311">
marcu@isi.edu
</email>
<sectionHeader confidence="0.99565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999848846153846">
We propose WIDL-expressions as a flex-
ible formalism that facilitates the integra-
tion of a generic sentence realization sys-
tem within end-to-end language process-
ing applications. WIDL-expressions rep-
resent compactly probability distributions
over finite sets of candidate realizations,
and have optimal algorithms for realiza-
tion via interpolation with language model
probability distributions. We show the ef-
fectiveness of a WIDL-based NLG system
in two sentence realization tasks: auto-
matic translation and headline generation.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920135593221">
The Natural Language Generation (NLG) com-
munity has produced over the years a consid-
erable number of generic sentence realization
systems: Penman (Matthiessen and Bateman,
1991), FUF (Elhadad, 1991), Nitrogen (Knight
and Hatzivassiloglou, 1995), Fergus (Bangalore
and Rambow, 2000), HALogen (Langkilde-Geary,
2002), Amalgam (Corston-Oliver et al., 2002), etc.
However, when it comes to end-to-end, text-to-
text applications – Machine Translation, Summa-
rization, Question Answering – these generic sys-
tems either cannot be employed, or, in instances
where they can be, the results are significantly
below that of state-of-the-art, application-specific
systems (Hajic et al., 2002; Habash, 2003). We
believe two reasons explain this state of affairs.
First, these generic NLG systems use input rep-
resentation languages with complex syntax and se-
mantics. These languages involve deep, semantic-
based subject-verb or verb-object relations (such
as ACTOR, AGENT, PATIENT, etc., for Penman
and FUF), syntactic relations (such as subject,
object, premod, etc., for HALogen), or lexi-
cal dependencies (Fergus, Amalgam). Such inputs
cannot be accurately produced by state-of-the-art
analysis components from arbitrary textual input
in the context of text-to-text applications.
Second, most of the recent systems (starting
with Nitrogen) have adopted a hybrid approach
to generation, which has increased their robust-
ness. These hybrid systems use, in a first phase,
symbolic knowledge to (over)generate a large set
of candidate realizations, and, in a second phase,
statistical knowledge about the target language
(such as stochastic language models) to rank the
candidate realizations and find the best scoring
one. The disadvantage of the hybrid approach
– from the perspective of integrating these sys-
tems within end-to-end applications – is that the
two generation phases cannot be tightly coupled.
More precisely, input-driven preferences and tar-
get language–driven preferences cannot be inte-
grated in a true probabilistic model that can be
trained and tuned for maximum performance.
In this paper, we propose WIDL-expressions
(WIDL stands for Weighted Interleave, Disjunc-
tion, and Lock, after the names of the main op-
erators) as a representation formalism that facil-
itates the integration of a generic sentence real-
ization system within end-to-end language appli-
cations. The WIDL formalism, an extension of
the IDL-expressions formalism of Nederhof and
Satta (2004), has several crucial properties that
differentiate it from previously-proposed NLG
representation formalisms. First, it has a sim-
ple syntax (expressions are built using four oper-
ators) and a simple, formal semantics (probability
distributions over finite sets of strings). Second,
it is a compact representation that grows linearly
</bodyText>
<page confidence="0.931263">
1105
</page>
<note confidence="0.5208495">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1105–1112,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99602135">
in the number of words available for generation
(see Section 2). (In contrast, representations such
as word lattices (Knight and Hatzivassiloglou,
1995) or non-recursive CFGs (Langkilde-Geary,
2002) require exponential space in the number
of words available for generation (Nederhof and
Satta, 2004).) Third, it has good computational
properties, such as optimal algorithms for inter-
section with -gram language models (Section 3).
Fourth, it is flexible with respect to the amount of
linguistic processing required to produce WIDL-
expressions directly from text (Sections 4 and 5).
Fifth, it allows for a tight integration of input-
specific preferences and target-language prefer-
ences via interpolation of probability distributions
using log-linear models. We show the effec-
tiveness of our proposal by directly employing
a generic WIDL-based generation system in two
end-to-end tasks: machine translation and auto-
matic headline generation.
</bodyText>
<sectionHeader confidence="0.973839" genericHeader="introduction">
2 The WIDL Representation Language
</sectionHeader>
<subsectionHeader confidence="0.995566">
2.1 WIDL-expressions
</subsectionHeader>
<bodyText confidence="0.931933617647059">
In this section, we introduce WIDL-expressions, a
formal language used to compactly represent prob-
ability distributions over finite sets of strings.
Given a finite alphabet of symbols , atomic
WIDL-expressions are of the form , with .
For a WIDL-expression
a probability distribution
, where and
. Complex WIDL-expressions are created from
other WIDL-expressions, by employing the fol-
lowing four operators, as well as operator distri-
bution functions from an alphabet .
Weighted Disjunction. If are
WIDL-expressions, then ,
with ,specified
such that ,is a WIDL-
expression. Its semantics is a probability
distribution , where
, and the probabil-
ity values are induced by and ,
. For example, if ,
, its semantics is a proba-
bility distribution over ,
defined by and
.
Precedence. If are WIDL-expressions,
then is a WIDL-expression. Its
semantics is a probability distribution
, where is the set of all
strings that obey the precedence imposed over
the arguments, and the probability values are in-
duced by and . For example, if
, and
, defined
</bodyText>
<equation confidence="0.893162">
,
,etc.
</equation>
<bodyText confidence="0.9742325">
Weighted Interleave. If are WIDL-
expressions, then , with
, ,
specified such that , is a
WIDL-expression. Its semantics is a probability
distribution , where
consists of all the possible interleavings of
strings from ,, and the proba-
bility values are induced by and . The
distribution function is defined either explicitly,
over (the set of all permutations of
elements), or implicitly, as . Be-
cause the set of argument permutations is a sub-
set of all possible interleavings, also needs to
specify the probability mass for the strings that
are not argument permutations, . For
</bodyText>
<equation confidence="0.613259">
example, if ,
, its
</equation>
<bodyText confidence="0.907868684210526">
semantics is a probability distribution ,
with domain
by
, .
Lock. If is a WIDL-expression, then
is a WIDL-expression. The semantic map-
ping is the same as , except
that contains strings in which no addi-
tional symbol can be interleaved. For exam-
ple, if
, defined by
, .
In Figure 1, we show a more complex WIDL-
expression. The probability distribution associ-
ated with the operator assigns probability 0.2
to the argument order ; from a probability
mass of 0.7, it assigns uniformly, for each of the
remaining argument permutations, a
permutation probability value of . The
</bodyText>
<figure confidence="0.992877">
by
, ,then
represents a probability distribution
over the set
,
,
, defined
,
, its semantics is a proba-
bility distribution , with domain
</figure>
<page confidence="0.843155">
1106
</page>
<figureCaption confidence="0.999253">
Figure 1: An example of a WIDL-expression.
</figureCaption>
<bodyText confidence="0.99202425">
remaining probability mass of 0.1 is left for the
12 shuffles associated with the unlocked expres-
sion , for a shuffle probability of
. The list below enumerates some of the
pairs that belong to the proba-
bility distribution defined by our example:
rebels fighting turkish government in iraq 0.130
in iraq attacked rebels turkish goverment 0.049
in turkish goverment iraq rebels fighting 0.005
The following result characterizes an important
representation property for WIDL-expressions.
Theorem 1 A WIDL-expression over and
using atomic expressions has space complexity
O( ), if the operator distribution functions of
have space complexity at most O( ).
For proofs and more details regarding WIDL-
expressions, we refer the interested reader
to (Soricut, 2006). Theorem 1 ensures that high-
complexity hypothesis spaces can be represented
efficiently by WIDL-expressions (Section 5).
</bodyText>
<subsectionHeader confidence="0.9969095">
2.2 WIDL-graphs and Probabilistic
Finite-State Acceptors
</subsectionHeader>
<bodyText confidence="0.970690298507463">
WIDL-graphs. Equivalent at the representation
level with WIDL-expressions, WIDL-graphs al-
low for formulations of algorithms that process
them. For each WIDL-expression , there exists
an equivalent WIDL-graph . As an example,
we illustrate in Figure 2(a) the WIDL-graph cor-
responding to the WIDL-expression in Figure 1.
WIDL-graphs have an initial vertex and a final
vertex . Vertices ,, and with in-going
edges labeled ,, and , respectively, and
vertices ,, and with out-going edges la-
beled ,, and , respectively, result from
the expansion of the operator. Vertices
and with in-going edges labeled ,, re-
spectively, and vertices and with out-going
edges labeled ,, respectively, result from the
expansion of the operator.
With each WIDL-graph
traversal of , starting from and ending in .
Each path (and its associated string) has a proba-
bility value induced by the probability distribution
functions associated with the edge labels of . A
WIDL-expression and its corresponding WIDL-
graph are said to be equivalent because they
represent the same distribution .
WIDL-graphs and Probabilistic FSA. Proba-
bilistic finite-state acceptors (pFSA) are a well-
known formalism for representing probability dis-
tributions (Mohri et al., 2002). For a WIDL-
expression , we define a mapping, called
UNFOLD, between the WIDL-graph and a
pFSA . A state in is created for each
set of WIDL-graph vertices that can be reached
simultaneously when traversing the graph. State
records, in what we call a -stack (interleave
stack), the order in which ,–bordered sub-
graphs are traversed. Consider Figure 2(b), in
which state (at the bottom) cor-
responds to reaching vertices , and (see
the WIDL-graph in Figure 2(a)), by first reach-
ing vertex (inside the
(Fig-
ure 2(b)) results from unfolding the path
(Figure 2(a)). A tran-
sition labeled between two states and in
exists if there exists a vertex in the descrip-
tion of and vertices in the descrip-
,
(see transition ), or if
there exists vertices in the description
of and vertex in the description of , such
tion of , such that
, . The -transitions
that
, we associate a
probability distribution. The domain of this dis-
tribution is the finite collection of strings that can
be generated from the paths of a WIDL-specific
, –bordered sub-
graph), and then reaching vertex (inside the ,
–bordered sub-graph).
A transition labeled between two states
and in exists if there exists a vertex
in the description of and a vertex in the de-
scription of such that there exists a path in
between and , and is the only -labeled
transitions in this path. For example, transition
</bodyText>
<page confidence="0.995207">
1107
</page>
<figureCaption confidence="0.997782">
Figure 2: The WIDL-graph corresponding to the WIDL-expression in Figure 1 is shown in (a). The
probabilistic finite-state acceptor (pFSA) that corresponds to the WIDL-graph is shown in (b).
</figureCaption>
<figure confidence="0.999585714285714">
[v v v ,&lt; 0 &gt; ]
0 19 23 δ1 δ1
[v v v ,&lt; 321 &gt; ]
0 19 23 δ1 δ1
rebels
a
:
rebels
:0.18
attacked
[v v v ,&lt;3]
0 6 21 δ1
[v v v ,&lt;32] [v v v ,&lt;32]
0 21
9 δ1 0 11 21 δ1
δ1= { 2 1 3 0.2, other perms
δ2 = { 1 0.65, 2 0.35 }
in ε iraq
shuffles
0.7,
0.1 }
(a)
(b)
v v v v
20 21 22 23
attacked rebels
[v v v ,&lt;1 ]
2 6 20 δ1
v s
δ1
1
2
δ1
�z V
0 1
1
ε turkish 1 ε 1 government 1 ε
2 3 4 v
✟✠
ε 1 attacked 1 ε 1 rebels 1
v v v v
8 9 10 11
5
ε
❁ 12 )i,&apos;
2
δ1
δ1
1
v e
[v , ]
ε
s
[v v v ,&lt; &gt; ]
0 6 20 δ1 δ1
[v v v ,&lt;2]
0 1520 δ1
δ1
ε attacked :0.1 rebels :1
[v v v ,&lt;2]
0 9 20 δ1
δ1
attacked
rebels
v6
3
δ1
✼ ✽&apos;ez
✾v
19
v v v v v v
13 14 15 16 17 18
1 rebels 1 1
ε ε fighting 1 ε
✿ ✿fδz
3
δ1
ε
ε [v , ]
ε
e
[v v v ,&lt;3] [v v v ,&lt;32]
0 6 23 δ1 0 9 23 δ1
attacked
:0.18
rebels
:
[v v v ,&lt;0]
0 19 23 δ1
[v v v ,&lt;32]
0 19 23 δ1
</figure>
<bodyText confidence="0.9986935">
are responsible for adding and removing, respec-
tively, the , symbols in the -stack. The prob-
abilities associated with transitions are com-
puted using the vertex set and the -stack of each
state, together with the distribution functions
of the and operators. For a detailed presen-
tation of the UNFOLD relation we refer the reader
to (Soricut, 2006).
</bodyText>
<sectionHeader confidence="0.97886" genericHeader="method">
3 Stochastic Language Generation from
WIDL-expressions
</sectionHeader>
<subsectionHeader confidence="0.9601625">
3.1 Interpolating Probability Distributions in
a Log-linear Framework
</subsectionHeader>
<bodyText confidence="0.896641625">
Let us assume a finite set of strings over a
finite alphabet , representing the set of possi-
ble sentence realizations. In a log-linear frame-
work, we have a vector of feature functions
, and a vector of parameters
. For any , the interpolated
probability can be written under a log-linear
model as in Equation 1:
</bodyText>
<equation confidence="0.824109">
(1)
</equation>
<bodyText confidence="0.9980804">
We can formulate the search problem of finding
the most probable realization under this model
as shown in Equation 2, and therefore we do not
need to be concerned about computing expensive
normalization factors.
For a given WIDL-expression over , the set
is defined by , and feature function
is taken to be . Any language model
we want to employ may be added in Equation 2 as
a feature function ,
</bodyText>
<subsectionHeader confidence="0.619660666666667">
3.2 Algorithms for Intersecting
WIDL-expressions with Language
Models
</subsectionHeader>
<bodyText confidence="0.9802355">
Algorithm WIDL-NGLM-A (Figure 3) solves
the search problem defined by Equation 2 for a
WIDL-expression (which provides feature func-
tion ) and -gram language models (which
provide feature functions . It does
so by incrementally computing UNFOLD for
(i.e., on-demand computation of the correspond-
ing pFSA ), by keeping track of a set of active
states, called . The set of newly UNFOLDed
states is called . Using Equation 1 (unnor-
malized), we EVALUATE the current scores
for the states. Additionally, EVALUATE
uses an admissible heuristic function to compute
future (admissible) scores for the states.
The algorithm PUSHes each state from the cur-
rent into a priority queue , which sorts
the states according to their total score (current
admissible). In the next iteration, is a sin-
gleton set containing the state POPed out from the
top of . The admissible heuristic function we use
is the one defined in (Soricut and Marcu, 2005),
using Equation 1 (unnormalized) for computing
the event costs. Given the existence of the ad-
missible heuristic and the monotonicity property
of the unfolding provided by the priority queue ,
the proof for A optimality (Russell and Norvig,
1995) guarantees that WIDL-NGLM-A finds a
path in that provides an optimal solution.
</bodyText>
<figure confidence="0.898782">
(2)
.
1108
9 return
</figure>
<figureCaption confidence="0.9990115">
Figure 3: A algorithm for interpolating WIDL-
expressions with -gram language models.
</figureCaption>
<bodyText confidence="0.9879991">
An important property of the
WIDL-NGLM-A algorithm is that the UNFOLD
relation (and, implicitly, the acceptor) is
computed only partially, for those states for
which the total cost is less than the cost of the
optimal path. This results in important savings,
both in space and time, over simply running a
single-source shortest-path algorithm for directed
acyclic graphs (Cormen et al., 2001) over the full
acceptor (Soricut and Marcu, 2005).
</bodyText>
<sectionHeader confidence="0.893202" genericHeader="method">
4 Headline Generation using
WIDL-expressions
</sectionHeader>
<bodyText confidence="0.964613192307692">
We employ the WIDL formalism (Section 2) and
the WIDL-NGLM-A algorithm (Section 3) in a
summarization application that aims at producing
both informative and fluent headlines. Our head-
lines are generated in an abstractive, bottom-up
manner, starting from words and phrases. A more
common, extractive approach operates top-down,
by starting from an extracted sentence that is com-
pressed (Dorr et al., 2003) and annotated with ad-
ditional information (Zajic et al., 2004).
Automatic Creation of WIDL-expressions for
Headline Generation. We generate WIDL-
expressions starting from an input document.
First, we extract a weighted list of topic keywords
from the input document using the algorithm of
Zhou and Hovy (2003). This list is enriched
with phrases created from the lexical dependen-
cies the topic keywords have in the input docu-
ment. We associate probability distributions with
these phrases using their frequency (we assume
Keywords iraq 0.32, syria 0.25, rebels 0.22,
kurdish 0.17, turkish 0.14, attack 0.10
Phrases
iraq in iraq 0.4, northern iraq 0.5,iraq and iran 0.1 ,
syria into syria 0.6, and syria 0.4
rebels attacked rebels 0.7,rebels fighting 0.3
</bodyText>
<figure confidence="0.877168">
. . .
WIDL-expression &amp; trigram interpolation
TURKISH GOVERNMENT ATTACKED REBELS IN IRAQ AND SYRIA
</figure>
<figureCaption confidence="0.999388">
Figure 4: Input and output for our automatic head-
line generation system.
</figureCaption>
<bodyText confidence="0.995347131578947">
that higher frequency is indicative of increased im-
portance) and their position in the document (we
assume that proximity to the beginning of the doc-
ument is also indicative of importance). In Fig-
ure 4, we present an example of input keywords
and lexical-dependency phrases automatically ex-
tracted from a document describing incidents at
the Turkey-Iraq border.
The algorithm for producing WIDL-
expressions combines the lexical-dependency
phrases for each keyword using a operator with
the associated probability values for each phrase
multiplied with the probability value of each
topic keyword. It then combines all the -headed
expressions into a single WIDL-expression using
a operator with uniform probability. The WIDL-
expression in Figure 1 is a (scaled-down) example
of the expressions created by this algorithm.
On average, a WIDL-expression created by this
algorithm, using keywords and an average
of lexical-dependency phrases per keyword,
compactly encodes a candidate set of about 3
million possible realizations. As the specification
of the operator takes space for uniform ,
Theorem 1 guarantees that the space complexity
of these expressions is .
Finally, we generate headlines from WIDL-
expressions using the WIDL-NGLM-A algo-
rithm, which interpolates the probability distribu-
tions represented by the WIDL-expressions with
-gram language model distributions. The output
presented in Figure 4 is the most likely headline
realization produced by our system.
Headline Generation Evaluation. To evaluate
the accuracy of our headline generation system,
we use the documents from the DUC 2003 eval-
uation competition. Half of these documents
are used as development set (283 documents),
</bodyText>
<figure confidence="0.994574833333333">
WIDL-NGLM-A
1
2
3 while
4 do UNFOLD
5 EVALUATE
6 if
7 then
8 for each
do PUSH
POP
in
</figure>
<page confidence="0.985091">
1109
</page>
<table confidence="0.997627444444444">
ALG (uni) (bi) Len. Rouge Rouge
Extractive 458 114 9.9 20.8 11.1
Lead10
HedgeTrimmer 399 104 7.4 18.1 9.9
Topiary 576 115 9.9 26.2 12.5
Abstractive
Keywords 585 22 9.9 26.6 5.5
Webcl 311 76 7.3 14.1 7.5
WIDL-A 562 126 10.0 25.5 12.9
</table>
<tableCaption confidence="0.999442">
Table 1: Headline generation evaluation. We com-
</tableCaption>
<bodyText confidence="0.994253128205128">
pare extractive algorithms against abstractive al-
gorithms, including our WIDL-based algorithm.
and the other half is used as test set (273 docu-
ments). We automatically measure performance
by comparing the produced headlines against one
reference headline produced by a human using
ROUGE (Lin, 2004).
For each input document, we train two language
models, using the SRI Language Model Toolkit
(with modified Kneser-Ney smoothing). A gen-
eral trigram language model, trained on 170M
English words from the Wall Street Journal, is
used to model fluency. A document-specific tri-
gram language model, trained on-the-fly for each
input document, accounts for both fluency and
content validity. We also employ a word-count
model (which counts the number of words in a
proposed realization) and a phrase-count model
(which counts the number of phrases in a proposed
realization), which allow us to learn to produce
headlines that have restrictions in the number of
words allowed (10, in our case). The interpolation
weights (Equation 2) are trained using discrimi-
native training (Och, 2003) using ROUGE as the
objective function, on the development set.
The results are presented in Table 1. We com-
pare the performance of several extractive algo-
rithms (which operate on an extracted sentence
to arrive at a headline) against several abstractive
algorithms (which create headlines starting from
scratch). For the extractive algorithms, Lead10
is a baseline which simply proposes as headline
the lead sentence, cut after the first 10 words.
HedgeTrimmer is our implementation of the Hedge
Trimer system (Dorr et al., 2003), and Topiary is
our implementation of the Topiary system (Zajic
et al., 2004). For the abstractive algorithms, Key-
words is a baseline that proposes as headline the
sequence of topic keywords, Webcl is the system
</bodyText>
<note confidence="0.9993704">
THREE GORGES PROJECT IN CHINA HAS WON APPROVAL
WATER IS LINK BETWEEN CLUSTER OF E. COLI CASES
SRI LANKA ’S JOINT VENTURE TO EXPAND EXPORTS
OPPOSITION TO EUROPEAN UNION SINGLE CURRENCY EURO
OF INDIA AND BANGLADESH WATER BARRAGE
</note>
<figureCaption confidence="0.9192825">
Figure 5: Headlines generated automatically using
a WIDL-based sentence realization system.
</figureCaption>
<bodyText confidence="0.989610214285714">
described in (Zhou and Hovy, 2003), and WIDL-
A is the algorithm described in this paper.
This evaluation shows that our WIDL-based
approach to generation is capable of obtaining
headlines that compare favorably, in both content
and fluency, with extractive, state-of-the-art re-
sults (Zajic et al., 2004), while it outperforms a
previously-proposed abstractive system by a wide
margin (Zhou and Hovy, 2003). Also note that our
evaluation makes these results directly compara-
ble, as they use the same parsing and topic identi-
fication algorithms. In Figure 5, we present a sam-
ple of headlines produced by our system, which
includes both good and not-so-good outputs.
</bodyText>
<sectionHeader confidence="0.727555" genericHeader="method">
5 Machine Translation using
WIDL-expressions
</sectionHeader>
<bodyText confidence="0.999663090909091">
We also employ our WIDL-based realization en-
gine in a machine translation application that uses
a two-phase generation approach: in a first phase,
WIDL-expressions representing large sets of pos-
sible translations are created from input foreign-
language sentences. In a second phase, we use
our generic, WIDL-based sentence realization en-
gine to intersect WIDL-expressions with an -
gram language model. In the experiments reported
here, we translate between Chinese (source lan-
guage) and English (target language).
</bodyText>
<subsectionHeader confidence="0.665632">
Automatic Creation of WIDL-expressions for
</subsectionHeader>
<bodyText confidence="0.999484769230769">
MT. We generate WIDL-expressions from Chi-
nese strings by exploiting a phrase-based trans-
lation table (Koehn et al., 2003). We use an al-
gorithm resembling probabilistic bottom-up pars-
ing to build a WIDL-expression for an input Chi-
nese string: each contiguous span over a
Chinese string is considered a possible “con-
stituent”, and the “non-terminals” associated with
each constituent are the English phrase transla-
tions that correspond in the translation ta-
ble to the Chinese string . Multiple-word En-
glish phrases, such as , are represented
as WIDL-expressions using the precedence () and
</bodyText>
<page confidence="0.968149">
1110
</page>
<figure confidence="0.4796425">
WIDL-expression &amp; trigram interpolation
gunman was killed by police.
</figure>
<figureCaption confidence="0.911818">
Figure 6: A Chinese string is converted into a
</figureCaption>
<bodyText confidence="0.991576243902439">
WIDL-expression, which provides a translation as
the best scoring hypothesis under the interpolation
with a trigram language model.
lock ( ) operators, as . To limit
the number of possible translations corre-
sponding to a Chinese span , we use a prob-
abilistic beam and a histogram beam to beam
out low probability translation alternatives. At this
point, each span is “tiled” with likely transla-
tions taken from the translation table.
Tiles that are adjacent are joined together in
a larger tile by a operator, where
. That is, reordering of
the component tiles are permitted by the op-
erators (assigned non-zero probability), but the
longer the movement from the original order of
the tiles, the lower the probability. (This distor-
tion model is similar with the one used in (Koehn,
2004).) When multiple tiles are available for the
same span , they are joined by a opera-
tor, where is specified by the probability distri-
butions specified in the translation table. Usually,
statistical phrase-based translation tables specify
not only one, but multiple distributions that ac-
count for context preferences. In our experi-
ments, we consider four probability distributions:
, and , where
and are Chinese-English phrase translations as
they appear in the translation table. In Figure 6,
we show an example of WIDL-expression created
by this algorithm&apos;.
On average, a WIDL-expression created by this
algorithm, using an average of tiles per
sentence (for an average input sentence length of
30 words) and an average of possible trans-
lations per tile, encodes a candidate set of about
10 possible translations. As the specification
of the operators takes space , Theorem 1
&apos;English reference: the gunman was shot dead by the police.
guarantees that these WIDL-expressions encode
compactly these huge spaces in .
In the second phase, we employ our WIDL-
based realization engine to interpolate the distri-
bution probabilities of WIDL-expressions with a
trigram language model. In the notation of Equa-
tion 2, we use four feature functions for
the WIDL-expression distributions (one for each
probability distribution encoded); a feature func-
tion for a trigram language model; a feature
function for a word-count model, and a feature
function for a phrase-count model.
As acknowledged in the Machine Translation
literature (Germann et al., 2003), full A search is
not usually possible, due to the large size of the
search spaces. We therefore use an approxima-
tion algorithm, called WIDL-NGLM-A , which
considers for unfolding only the nodes extracted
from the priority queue which already unfolded
a path of length greater than or equal to the max-
imum length already unfolded minus (we used
in the experiments reported here).
MT Performance Evaluation. When evaluated
against the state-of-the-art, phrase-based decoder
Pharaoh (Koehn, 2004), using the same experi-
mental conditions – translation table trained on
the FBIS corpus (7.2M Chinese words and 9.2M
English words of parallel text), trigram lan-
guage model trained on 155M words of English
newswire, interpolation weights (Equation 2)
trained using discriminative training (Och, 2003)
(on the 2002 NIST MT evaluation set), probabilis-
tic beam set to 0.01, histogram beam set to 10
– and BLEU (Papineni et al., 2002) as our met-
ric, the WIDL-NGLM-A algorithm produces
translations that have a BLEU score of 0.2570,
while Pharaoh translations have a BLEU score of
0.2635. The difference is not statistically signifi-
cant at 95% confidence level.
These results show that the WIDL-based ap-
proach to machine translation is powerful enough
to achieve translation accuracy comparable with
state-of-the-art systems in machine translation.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999955">
The approach to sentence realization we advocate
in this paper relies on WIDL-expressions, a for-
mal language with convenient theoretical proper-
ties that can accommodate a wide range of gener-
ation scenarios. In the worst case, one can work
with simple bags of words that encode no context
</bodyText>
<page confidence="0.972067">
1111
</page>
<bodyText confidence="0.996666315789474">
preferences (Soricut and Marcu, 2005). One can
also work with bags of words and phrases that en-
code context preferences, a scenario that applies to
current approaches in statistical machine transla-
tion (Section 5). And one can also encode context
and ordering preferences typically used in summa-
rization (Section 4).
The generation engine we describe enables
a tight coupling of content selection with sen-
tence realization preferences. Its algorithm comes
with theoretical guarantees about its optimality.
Because the requirements for producing WIDL-
expressions are minimal, our WIDL-based genera-
tion engine can be employed, with state-of-the-art
results, in a variety of text-to-text applications.
Acknowledgments This work was partially sup-
ported under the GALE program of the Defense
Advanced Research Projects Agency, Contract
No. HR0011-06-C-0022.
</bodyText>
<sectionHeader confidence="0.997795" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997846275">
Srinivas Bangalore and Owen Rambow. 2000. Using
TAG, a tree model, and a language model for gen-
eration. In Proceedings of the Fifth International
Workshop on Tree-Adjoining Grammars (TAG+).
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to
Algorithms. The MIT Press and McGraw-Hill.
Simon Corston-Oliver, Michael Gamon, Eric K. Ring-
ger, and Robert Moore. 2002. An overview of
Amalgam: A machine-learned generation module.
In Proceedings of the INLG.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: a parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL Text Summarization Workshop, pages 1–8.
Michael Elhadad. 1991. FUF User manual — version
5.0. Technical Report CUCS-038-91, Department
of Computer Science, Columbia University.
Ulrich Germann, Mike Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2003. Fast decoding and
optimal decoding for machine translation. Artificial
Intelligence, 154(1–2):127-143.
Nizar Habash. 2003. Matador: A large-scale Spanish-
English GHMT system. In Proceedings ofAMTA.
J. Hajic, M. Cmejrek, B. Dorr, Y. Ding, J. Eisner,
D. Gildea, T. Koo, K. Parton, G. Penn, D. Radev,
and O. Rambow. 2002. Natural language genera-
tion in the context of machine translation. Summer
workshop final report, Johns Hopkins University.
K. Knight and V. Hatzivassiloglou. 1995. Two level,
many-path generation. In Proceedings of the ACL.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase based translation. In Proceedings
of the HLT-NAACL, pages 127–133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine transltion mod-
els. In Proceedings of the AMTA, pages 115–124.
I. Langkilde-Geary. 2002. A foundation for general-
purpose natural language generation: sentence re-
alization using probabilistic models of language.
Ph.D. thesis, University of Southern California.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004).
Christian Matthiessen and John Bateman. 1991.
Text Generation and Systemic-Functional Linguis-
tic. Pinter Publishers, London.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Lan-
guage, 16(1):69–88.
Mark-Jan Nederhof and Giorgio Satta. 2004. IDL-
expressions: a formalism for representing and pars-
ing finite languages in natural language processing.
Journal of Artificial Intelligence Research, pages
287–317.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the ACL, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In In Proceedings
of the ACL, pages 311–318.
Stuart Russell and Peter Norvig. 1995. Artificial Intel-
ligence. A Modern Approach. Prentice Hall.
Radu Soricut and Daniel Marcu. 2005. Towards devel-
oping generation algorithms for text-to-text applica-
tions. In Proceedings of the ACL, pages 66–74.
Radu Soricut. 2006. Natural Language Generation for
Text-to-Text Applications Using an Information-Slim
Representation. Ph.D. thesis, University of South-
ern California.
David Zajic, Bonnie J. Dorr, and Richard Schwartz.
2004. BBN/UMD at DUC-2004: Topiary. In Pro-
ceedings of the NAACL Workshop on Document Un-
derstanding, pages 112–119.
Liang Zhou and Eduard Hovy. 2003. Headline sum-
marization at ISI. In Proceedings of the NAACL
Workshop on Document Understanding.
</reference>
<page confidence="0.996324">
1112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.907755">
<title confidence="0.9975105">Stochastic Language Generation Using WIDL-expressions and its Application in Machine Translation and Summarization</title>
<author confidence="0.958918">Radu Soricut</author>
<affiliation confidence="0.9985915">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.9979195">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.999735">radu@isi.edu</email>
<author confidence="0.99937">Daniel Marcu</author>
<affiliation confidence="0.998635">Information Sciences Institute University of Southern California</affiliation>
<address confidence="0.994875">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.997971">marcu@isi.edu</email>
<abstract confidence="0.998048214285714">We propose WIDL-expressions as a flexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications. WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Using TAG, a tree model, and a language model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth International Workshop on Tree-Adjoining Grammars (TAG+).</booktitle>
<contexts>
<context position="1244" citStr="Bangalore and Rambow, 2000" startWordPosition="164" endWordPosition="167">resent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, </context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Using TAG, a tree model, and a language model for generation. In Proceedings of the Fifth International Workshop on Tree-Adjoining Grammars (TAG+).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>2001</date>
<publisher>The MIT Press</publisher>
<contexts>
<context position="14876" citStr="Cormen et al., 2001" startWordPosition="2467" endWordPosition="2470"> for A optimality (Russell and Norvig, 1995) guarantees that WIDL-NGLM-A finds a path in that provides an optimal solution. (2) . 1108 9 return Figure 3: A algorithm for interpolating WIDLexpressions with -gram language models. An important property of the WIDL-NGLM-A algorithm is that the UNFOLD relation (and, implicitly, the acceptor) is computed only partially, for those states for which the total cost is less than the cost of the optimal path. This results in important savings, both in space and time, over simply running a single-source shortest-path algorithm for directed acyclic graphs (Cormen et al., 2001) over the full acceptor (Soricut and Marcu, 2005). 4 Headline Generation using WIDL-expressions We employ the WIDL formalism (Section 2) and the WIDL-NGLM-A algorithm (Section 3) in a summarization application that aims at producing both informative and fluent headlines. Our headlines are generated in an abstractive, bottom-up manner, starting from words and phrases. A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al., 2003) and annotated with additional information (Zajic et al., 2004). Automatic Creation of WIDL-express</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms. The MIT Press and McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
<author>Eric K Ringger</author>
<author>Robert Moore</author>
</authors>
<title>An overview of Amalgam: A machine-learned generation module.</title>
<date>2002</date>
<booktitle>In Proceedings of the INLG.</booktitle>
<contexts>
<context position="1316" citStr="Corston-Oliver et al., 2002" startWordPosition="172" endWordPosition="175">e realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGEN</context>
</contexts>
<marker>Corston-Oliver, Gamon, Ringger, Moore, 2002</marker>
<rawString>Simon Corston-Oliver, Michael Gamon, Eric K. Ringger, and Robert Moore. 2002. An overview of Amalgam: A machine-learned generation module. In Proceedings of the INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: a parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLTNAACL Text Summarization Workshop,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="15377" citStr="Dorr et al., 2003" startWordPosition="2542" endWordPosition="2545">d time, over simply running a single-source shortest-path algorithm for directed acyclic graphs (Cormen et al., 2001) over the full acceptor (Soricut and Marcu, 2005). 4 Headline Generation using WIDL-expressions We employ the WIDL formalism (Section 2) and the WIDL-NGLM-A algorithm (Section 3) in a summarization application that aims at producing both informative and fluent headlines. Our headlines are generated in an abstractive, bottom-up manner, starting from words and phrases. A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al., 2003) and annotated with additional information (Zajic et al., 2004). Automatic Creation of WIDL-expressions for Headline Generation. We generate WIDLexpressions starting from an input document. First, we extract a weighted list of topic keywords from the input document using the algorithm of Zhou and Hovy (2003). This list is enriched with phrases created from the lexical dependencies the topic keywords have in the input document. We associate probability distributions with these phrases using their frequency (we assume Keywords iraq 0.32, syria 0.25, rebels 0.22, kurdish 0.17, turkish 0.14, attac</context>
<context position="19975" citStr="Dorr et al., 2003" startWordPosition="3263" endWordPosition="3266">interpolation weights (Equation 2) are trained using discriminative training (Och, 2003) using ROUGE as the objective function, on the development set. The results are presented in Table 1. We compare the performance of several extractive algorithms (which operate on an extracted sentence to arrive at a headline) against several abstractive algorithms (which create headlines starting from scratch). For the extractive algorithms, Lead10 is a baseline which simply proposes as headline the lead sentence, cut after the first 10 words. HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al., 2003), and Topiary is our implementation of the Topiary system (Zajic et al., 2004). For the abstractive algorithms, Keywords is a baseline that proposes as headline the sequence of topic keywords, Webcl is the system THREE GORGES PROJECT IN CHINA HAS WON APPROVAL WATER IS LINK BETWEEN CLUSTER OF E. COLI CASES SRI LANKA ’S JOINT VENTURE TO EXPAND EXPORTS OPPOSITION TO EUROPEAN UNION SINGLE CURRENCY EURO OF INDIA AND BANGLADESH WATER BARRAGE Figure 5: Headlines generated automatically using a WIDL-based sentence realization system. described in (Zhou and Hovy, 2003), and WIDLA is the algorithm descr</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Bonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: a parse-and-trim approach to headline generation. In Proceedings of the HLTNAACL Text Summarization Workshop, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>FUF User manual — version 5.0.</title>
<date>1991</date>
<tech>Technical Report CUCS-038-91,</tech>
<institution>Department of Computer Science, Columbia University.</institution>
<contexts>
<context position="1161" citStr="Elhadad, 1991" startWordPosition="156" endWordPosition="157">thin end-to-end language processing applications. WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input represe</context>
</contexts>
<marker>Elhadad, 1991</marker>
<rawString>Michael Elhadad. 1991. FUF User manual — version 5.0. Technical Report CUCS-038-91, Department of Computer Science, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Mike Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2003</date>
<journal>Artificial Intelligence,</journal>
<pages>154--1</pages>
<contexts>
<context position="24808" citStr="Germann et al., 2003" startWordPosition="4023" endWordPosition="4026"> by the police. guarantees that these WIDL-expressions encode compactly these huge spaces in . In the second phase, we employ our WIDLbased realization engine to interpolate the distribution probabilities of WIDL-expressions with a trigram language model. In the notation of Equation 2, we use four feature functions for the WIDL-expression distributions (one for each probability distribution encoded); a feature function for a trigram language model; a feature function for a word-count model, and a feature function for a phrase-count model. As acknowledged in the Machine Translation literature (Germann et al., 2003), full A search is not usually possible, due to the large size of the search spaces. We therefore use an approximation algorithm, called WIDL-NGLM-A , which considers for unfolding only the nodes extracted from the priority queue which already unfolded a path of length greater than or equal to the maximum length already unfolded minus (we used in the experiments reported here). MT Performance Evaluation. When evaluated against the state-of-the-art, phrase-based decoder Pharaoh (Koehn, 2004), using the same experimental conditions – translation table trained on the FBIS corpus (7.2M Chinese wor</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2003</marker>
<rawString>Ulrich Germann, Mike Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2003. Fast decoding and optimal decoding for machine translation. Artificial Intelligence, 154(1–2):127-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Matador: A large-scale SpanishEnglish GHMT system.</title>
<date>2003</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="1655" citStr="Habash, 2003" startWordPosition="223" endWordPosition="224">onsiderable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGENT, PATIENT, etc., for Penman and FUF), syntactic relations (such as subject, object, premod, etc., for HALogen), or lexical dependencies (Fergus, Amalgam). Such inputs cannot be accurately produced by state-of-the-art analysis components from arbitrary textual input in the context of text-to-text applications. Second, most of the recent </context>
</contexts>
<marker>Habash, 2003</marker>
<rawString>Nizar Habash. 2003. Matador: A large-scale SpanishEnglish GHMT system. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajic</author>
<author>M Cmejrek</author>
<author>B Dorr</author>
<author>Y Ding</author>
<author>J Eisner</author>
<author>D Gildea</author>
<author>T Koo</author>
<author>K Parton</author>
<author>G Penn</author>
<author>D Radev</author>
<author>O Rambow</author>
</authors>
<title>Natural language generation in the context of machine translation. Summer workshop final report,</title>
<date>2002</date>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="1640" citStr="Hajic et al., 2002" startWordPosition="219" endWordPosition="222">d over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semanticbased subject-verb or verb-object relations (such as ACTOR, AGENT, PATIENT, etc., for Penman and FUF), syntactic relations (such as subject, object, premod, etc., for HALogen), or lexical dependencies (Fergus, Amalgam). Such inputs cannot be accurately produced by state-of-the-art analysis components from arbitrary textual input in the context of text-to-text applications. Second, most</context>
</contexts>
<marker>Hajic, Cmejrek, Dorr, Ding, Eisner, Gildea, Koo, Parton, Penn, Radev, Rambow, 2002</marker>
<rawString>J. Hajic, M. Cmejrek, B. Dorr, Y. Ding, J. Eisner, D. Gildea, T. Koo, K. Parton, G. Penn, D. Radev, and O. Rambow. 2002. Natural language generation in the context of machine translation. Summer workshop final report, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Two level, many-path generation.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1207" citStr="Knight and Hatzivassiloglou, 1995" startWordPosition="159" endWordPosition="162">rocessing applications. WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and sema</context>
<context position="4110" citStr="Knight and Hatzivassiloglou, 1995" startWordPosition="579" endWordPosition="582">from previously-proposed NLG representation formalisms. First, it has a simple syntax (expressions are built using four operators) and a simple, formal semantics (probability distributions over finite sets of strings). Second, it is a compact representation that grows linearly 1105 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1105–1112, Sydney, July 2006. c�2006 Association for Computational Linguistics in the number of words available for generation (see Section 2). (In contrast, representations such as word lattices (Knight and Hatzivassiloglou, 1995) or non-recursive CFGs (Langkilde-Geary, 2002) require exponential space in the number of words available for generation (Nederhof and Satta, 2004).) Third, it has good computational properties, such as optimal algorithms for intersection with -gram language models (Section 3). Fourth, it is flexible with respect to the amount of linguistic processing required to produce WIDLexpressions directly from text (Sections 4 and 5). Fifth, it allows for a tight integration of inputspecific preferences and target-language preferences via interpolation of probability distributions using log-linear model</context>
</contexts>
<marker>Knight, Hatzivassiloglou, 1995</marker>
<rawString>K. Knight and V. Hatzivassiloglou. 1995. Two level, many-path generation. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="21893" citStr="Koehn et al., 2003" startWordPosition="3557" endWordPosition="3560">ne translation application that uses a two-phase generation approach: in a first phase, WIDL-expressions representing large sets of possible translations are created from input foreignlanguage sentences. In a second phase, we use our generic, WIDL-based sentence realization engine to intersect WIDL-expressions with an - gram language model. In the experiments reported here, we translate between Chinese (source language) and English (target language). Automatic Creation of WIDL-expressions for MT. We generate WIDL-expressions from Chinese strings by exploiting a phrase-based translation table (Koehn et al., 2003). We use an algorithm resembling probabilistic bottom-up parsing to build a WIDL-expression for an input Chinese string: each contiguous span over a Chinese string is considered a possible “constituent”, and the “non-terminals” associated with each constituent are the English phrase translations that correspond in the translation table to the Chinese string . Multiple-word English phrases, such as , are represented as WIDL-expressions using the precedence () and 1110 WIDL-expression &amp; trigram interpolation gunman was killed by police. Figure 6: A Chinese string is converted into a WIDL-express</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase based translation. In Proceedings of the HLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine transltion models.</title>
<date>2004</date>
<booktitle>In Proceedings of the AMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="23267" citStr="Koehn, 2004" startWordPosition="3784" endWordPosition="3785">er of possible translations corresponding to a Chinese span , we use a probabilistic beam and a histogram beam to beam out low probability translation alternatives. At this point, each span is “tiled” with likely translations taken from the translation table. Tiles that are adjacent are joined together in a larger tile by a operator, where . That is, reordering of the component tiles are permitted by the operators (assigned non-zero probability), but the longer the movement from the original order of the tiles, the lower the probability. (This distortion model is similar with the one used in (Koehn, 2004).) When multiple tiles are available for the same span , they are joined by a operator, where is specified by the probability distributions specified in the translation table. Usually, statistical phrase-based translation tables specify not only one, but multiple distributions that account for context preferences. In our experiments, we consider four probability distributions: , and , where and are Chinese-English phrase translations as they appear in the translation table. In Figure 6, we show an example of WIDL-expression created by this algorithm&apos;. On average, a WIDL-expression created by t</context>
<context position="25303" citStr="Koehn, 2004" startWordPosition="4102" endWordPosition="4103">ature function for a phrase-count model. As acknowledged in the Machine Translation literature (Germann et al., 2003), full A search is not usually possible, due to the large size of the search spaces. We therefore use an approximation algorithm, called WIDL-NGLM-A , which considers for unfolding only the nodes extracted from the priority queue which already unfolded a path of length greater than or equal to the maximum length already unfolded minus (we used in the experiments reported here). MT Performance Evaluation. When evaluated against the state-of-the-art, phrase-based decoder Pharaoh (Koehn, 2004), using the same experimental conditions – translation table trained on the FBIS corpus (7.2M Chinese words and 9.2M English words of parallel text), trigram language model trained on 155M words of English newswire, interpolation weights (Equation 2) trained using discriminative training (Och, 2003) (on the 2002 NIST MT evaluation set), probabilistic beam set to 0.01, histogram beam set to 10 – and BLEU (Papineni et al., 2002) as our metric, the WIDL-NGLM-A algorithm produces translations that have a BLEU score of 0.2570, while Pharaoh translations have a BLEU score of 0.2635. The difference i</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine transltion models. In Proceedings of the AMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>A foundation for generalpurpose natural language generation: sentence realization using probabilistic models of language.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<contexts>
<context position="1277" citStr="Langkilde-Geary, 2002" startWordPosition="169" endWordPosition="170">ions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG systems use input representation languages with complex syntax and semantics. These languages involve deep, semanticbased subject-verb or ver</context>
<context position="4156" citStr="Langkilde-Geary, 2002" startWordPosition="586" endWordPosition="587">rst, it has a simple syntax (expressions are built using four operators) and a simple, formal semantics (probability distributions over finite sets of strings). Second, it is a compact representation that grows linearly 1105 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1105–1112, Sydney, July 2006. c�2006 Association for Computational Linguistics in the number of words available for generation (see Section 2). (In contrast, representations such as word lattices (Knight and Hatzivassiloglou, 1995) or non-recursive CFGs (Langkilde-Geary, 2002) require exponential space in the number of words available for generation (Nederhof and Satta, 2004).) Third, it has good computational properties, such as optimal algorithms for intersection with -gram language models (Section 3). Fourth, it is flexible with respect to the amount of linguistic processing required to produce WIDLexpressions directly from text (Sections 4 and 5). Fifth, it allows for a tight integration of inputspecific preferences and target-language preferences via interpolation of probability distributions using log-linear models. We show the effectiveness of our proposal b</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>I. Langkilde-Geary. 2002. A foundation for generalpurpose natural language generation: sentence realization using probabilistic models of language. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out (WAS</booktitle>
<contexts>
<context position="18663" citStr="Lin, 2004" startWordPosition="3059" endWordPosition="3060"> 8 for each do PUSH POP in 1109 ALG (uni) (bi) Len. Rouge Rouge Extractive 458 114 9.9 20.8 11.1 Lead10 HedgeTrimmer 399 104 7.4 18.1 9.9 Topiary 576 115 9.9 26.2 12.5 Abstractive Keywords 585 22 9.9 26.6 5.5 Webcl 311 76 7.3 14.1 7.5 WIDL-A 562 126 10.0 25.5 12.9 Table 1: Headline generation evaluation. We compare extractive algorithms against abstractive algorithms, including our WIDL-based algorithm. and the other half is used as test set (273 documents). We automatically measure performance by comparing the produced headlines against one reference headline produced by a human using ROUGE (Lin, 2004). For each input document, we train two language models, using the SRI Language Model Toolkit (with modified Kneser-Ney smoothing). A general trigram language model, trained on 170M English words from the Wall Street Journal, is used to model fluency. A document-specific trigram language model, trained on-the-fly for each input document, accounts for both fluency and content validity. We also employ a word-count model (which counts the number of words in a proposed realization) and a phrase-count model (which counts the number of phrases in a proposed realization), which allow us to learn to p</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Matthiessen</author>
<author>John Bateman</author>
</authors>
<title>Text Generation and Systemic-Functional Linguistic.</title>
<date>1991</date>
<publisher>Pinter Publishers,</publisher>
<location>London.</location>
<contexts>
<context position="1140" citStr="Matthiessen and Bateman, 1991" startWordPosition="151" endWordPosition="154">eneric sentence realization system within end-to-end language processing applications. WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations, and have optimal algorithms for realization via interpolation with language model probability distributions. We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation. 1 Introduction The Natural Language Generation (NLG) community has produced over the years a considerable number of generic sentence realization systems: Penman (Matthiessen and Bateman, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al., 2002), etc. However, when it comes to end-to-end, text-totext applications – Machine Translation, Summarization, Question Answering – these generic systems either cannot be employed, or, in instances where they can be, the results are significantly below that of state-of-the-art, application-specific systems (Hajic et al., 2002; Habash, 2003). We believe two reasons explain this state of affairs. First, these generic NLG syst</context>
</contexts>
<marker>Matthiessen, Bateman, 1991</marker>
<rawString>Christian Matthiessen and John Bateman. 1991. Text Generation and Systemic-Functional Linguistic. Pinter Publishers, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="9514" citStr="Mohri et al., 2002" startWordPosition="1438" endWordPosition="1441">ively, and vertices and with out-going edges labeled ,, respectively, result from the expansion of the operator. With each WIDL-graph traversal of , starting from and ending in . Each path (and its associated string) has a probability value induced by the probability distribution functions associated with the edge labels of . A WIDL-expression and its corresponding WIDLgraph are said to be equivalent because they represent the same distribution . WIDL-graphs and Probabilistic FSA. Probabilistic finite-state acceptors (pFSA) are a wellknown formalism for representing probability distributions (Mohri et al., 2002). For a WIDLexpression , we define a mapping, called UNFOLD, between the WIDL-graph and a pFSA . A state in is created for each set of WIDL-graph vertices that can be reached simultaneously when traversing the graph. State records, in what we call a -stack (interleave stack), the order in which ,–bordered subgraphs are traversed. Consider Figure 2(b), in which state (at the bottom) corresponds to reaching vertices , and (see the WIDL-graph in Figure 2(a)), by first reaching vertex (inside the (Figure 2(b)) results from unfolding the path (Figure 2(a)). A transition labeled between two states a</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
<author>Giorgio Satta</author>
</authors>
<title>IDLexpressions: a formalism for representing and parsing finite languages in natural language processing.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>287--317</pages>
<contexts>
<context position="3421" citStr="Nederhof and Satta (2004)" startWordPosition="483" endWordPosition="486">hat the two generation phases cannot be tightly coupled. More precisely, input-driven preferences and target language–driven preferences cannot be integrated in a true probabilistic model that can be trained and tuned for maximum performance. In this paper, we propose WIDL-expressions (WIDL stands for Weighted Interleave, Disjunction, and Lock, after the names of the main operators) as a representation formalism that facilitates the integration of a generic sentence realization system within end-to-end language applications. The WIDL formalism, an extension of the IDL-expressions formalism of Nederhof and Satta (2004), has several crucial properties that differentiate it from previously-proposed NLG representation formalisms. First, it has a simple syntax (expressions are built using four operators) and a simple, formal semantics (probability distributions over finite sets of strings). Second, it is a compact representation that grows linearly 1105 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1105–1112, Sydney, July 2006. c�2006 Association for Computational Linguistics in the number of words available for generation (see Section 2)</context>
</contexts>
<marker>Nederhof, Satta, 2004</marker>
<rawString>Mark-Jan Nederhof and Giorgio Satta. 2004. IDLexpressions: a formalism for representing and parsing finite languages in natural language processing. Journal of Artificial Intelligence Research, pages 287–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="19445" citStr="Och, 2003" startWordPosition="3182" endWordPosition="3183">d on 170M English words from the Wall Street Journal, is used to model fluency. A document-specific trigram language model, trained on-the-fly for each input document, accounts for both fluency and content validity. We also employ a word-count model (which counts the number of words in a proposed realization) and a phrase-count model (which counts the number of phrases in a proposed realization), which allow us to learn to produce headlines that have restrictions in the number of words allowed (10, in our case). The interpolation weights (Equation 2) are trained using discriminative training (Och, 2003) using ROUGE as the objective function, on the development set. The results are presented in Table 1. We compare the performance of several extractive algorithms (which operate on an extracted sentence to arrive at a headline) against several abstractive algorithms (which create headlines starting from scratch). For the extractive algorithms, Lead10 is a baseline which simply proposes as headline the lead sentence, cut after the first 10 words. HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al., 2003), and Topiary is our implementation of the Topiary system (Zajic et al</context>
<context position="25603" citStr="Och, 2003" startWordPosition="4147" endWordPosition="4148"> the nodes extracted from the priority queue which already unfolded a path of length greater than or equal to the maximum length already unfolded minus (we used in the experiments reported here). MT Performance Evaluation. When evaluated against the state-of-the-art, phrase-based decoder Pharaoh (Koehn, 2004), using the same experimental conditions – translation table trained on the FBIS corpus (7.2M Chinese words and 9.2M English words of parallel text), trigram language model trained on 155M words of English newswire, interpolation weights (Equation 2) trained using discriminative training (Och, 2003) (on the 2002 NIST MT evaluation set), probabilistic beam set to 0.01, histogram beam set to 10 – and BLEU (Papineni et al., 2002) as our metric, the WIDL-NGLM-A algorithm produces translations that have a BLEU score of 0.2570, while Pharaoh translations have a BLEU score of 0.2635. The difference is not statistically significant at 95% confidence level. These results show that the WIDL-based approach to machine translation is powerful enough to achieve translation accuracy comparable with state-of-the-art systems in machine translation. 6 Conclusions The approach to sentence realization we ad</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation. In</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="25733" citStr="Papineni et al., 2002" startWordPosition="4170" endWordPosition="4173">mum length already unfolded minus (we used in the experiments reported here). MT Performance Evaluation. When evaluated against the state-of-the-art, phrase-based decoder Pharaoh (Koehn, 2004), using the same experimental conditions – translation table trained on the FBIS corpus (7.2M Chinese words and 9.2M English words of parallel text), trigram language model trained on 155M words of English newswire, interpolation weights (Equation 2) trained using discriminative training (Och, 2003) (on the 2002 NIST MT evaluation set), probabilistic beam set to 0.01, histogram beam set to 10 – and BLEU (Papineni et al., 2002) as our metric, the WIDL-NGLM-A algorithm produces translations that have a BLEU score of 0.2570, while Pharaoh translations have a BLEU score of 0.2635. The difference is not statistically significant at 95% confidence level. These results show that the WIDL-based approach to machine translation is powerful enough to achieve translation accuracy comparable with state-of-the-art systems in machine translation. 6 Conclusions The approach to sentence realization we advocate in this paper relies on WIDL-expressions, a formal language with convenient theoretical properties that can accommodate a w</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In In Proceedings of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence. A Modern Approach.</title>
<date>1995</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="14300" citStr="Russell and Norvig, 1995" startWordPosition="2376" endWordPosition="2379">compute future (admissible) scores for the states. The algorithm PUSHes each state from the current into a priority queue , which sorts the states according to their total score (current admissible). In the next iteration, is a singleton set containing the state POPed out from the top of . The admissible heuristic function we use is the one defined in (Soricut and Marcu, 2005), using Equation 1 (unnormalized) for computing the event costs. Given the existence of the admissible heuristic and the monotonicity property of the unfolding provided by the priority queue , the proof for A optimality (Russell and Norvig, 1995) guarantees that WIDL-NGLM-A finds a path in that provides an optimal solution. (2) . 1108 9 return Figure 3: A algorithm for interpolating WIDLexpressions with -gram language models. An important property of the WIDL-NGLM-A algorithm is that the UNFOLD relation (and, implicitly, the acceptor) is computed only partially, for those states for which the total cost is less than the cost of the optimal path. This results in important savings, both in space and time, over simply running a single-source shortest-path algorithm for directed acyclic graphs (Cormen et al., 2001) over the full acceptor </context>
</contexts>
<marker>Russell, Norvig, 1995</marker>
<rawString>Stuart Russell and Peter Norvig. 1995. Artificial Intelligence. A Modern Approach. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Towards developing generation algorithms for text-to-text applications.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>66--74</pages>
<contexts>
<context position="14054" citStr="Soricut and Marcu, 2005" startWordPosition="2337" endWordPosition="2340">), by keeping track of a set of active states, called . The set of newly UNFOLDed states is called . Using Equation 1 (unnormalized), we EVALUATE the current scores for the states. Additionally, EVALUATE uses an admissible heuristic function to compute future (admissible) scores for the states. The algorithm PUSHes each state from the current into a priority queue , which sorts the states according to their total score (current admissible). In the next iteration, is a singleton set containing the state POPed out from the top of . The admissible heuristic function we use is the one defined in (Soricut and Marcu, 2005), using Equation 1 (unnormalized) for computing the event costs. Given the existence of the admissible heuristic and the monotonicity property of the unfolding provided by the priority queue , the proof for A optimality (Russell and Norvig, 1995) guarantees that WIDL-NGLM-A finds a path in that provides an optimal solution. (2) . 1108 9 return Figure 3: A algorithm for interpolating WIDLexpressions with -gram language models. An important property of the WIDL-NGLM-A algorithm is that the UNFOLD relation (and, implicitly, the acceptor) is computed only partially, for those states for which the </context>
<context position="26491" citStr="Soricut and Marcu, 2005" startWordPosition="4287" endWordPosition="4290">EU score of 0.2635. The difference is not statistically significant at 95% confidence level. These results show that the WIDL-based approach to machine translation is powerful enough to achieve translation accuracy comparable with state-of-the-art systems in machine translation. 6 Conclusions The approach to sentence realization we advocate in this paper relies on WIDL-expressions, a formal language with convenient theoretical properties that can accommodate a wide range of generation scenarios. In the worst case, one can work with simple bags of words that encode no context 1111 preferences (Soricut and Marcu, 2005). One can also work with bags of words and phrases that encode context preferences, a scenario that applies to current approaches in statistical machine translation (Section 5). And one can also encode context and ordering preferences typically used in summarization (Section 4). The generation engine we describe enables a tight coupling of content selection with sentence realization preferences. Its algorithm comes with theoretical guarantees about its optimality. Because the requirements for producing WIDLexpressions are minimal, our WIDL-based generation engine can be employed, with state-of</context>
</contexts>
<marker>Soricut, Marcu, 2005</marker>
<rawString>Radu Soricut and Daniel Marcu. 2005. Towards developing generation algorithms for text-to-text applications. In Proceedings of the ACL, pages 66–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
</authors>
<title>Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<contexts>
<context position="8108" citStr="Soricut, 2006" startWordPosition="1228" endWordPosition="1229"> below enumerates some of the pairs that belong to the probability distribution defined by our example: rebels fighting turkish government in iraq 0.130 in iraq attacked rebels turkish goverment 0.049 in turkish goverment iraq rebels fighting 0.005 The following result characterizes an important representation property for WIDL-expressions. Theorem 1 A WIDL-expression over and using atomic expressions has space complexity O( ), if the operator distribution functions of have space complexity at most O( ). For proofs and more details regarding WIDLexpressions, we refer the interested reader to (Soricut, 2006). Theorem 1 ensures that highcomplexity hypothesis spaces can be represented efficiently by WIDL-expressions (Section 5). 2.2 WIDL-graphs and Probabilistic Finite-State Acceptors WIDL-graphs. Equivalent at the representation level with WIDL-expressions, WIDL-graphs allow for formulations of algorithms that process them. For each WIDL-expression , there exists an equivalent WIDL-graph . As an example, we illustrate in Figure 2(a) the WIDL-graph corresponding to the WIDL-expression in Figure 1. WIDL-graphs have an initial vertex and a final vertex . Vertices ,, and with in-going edges labeled ,,</context>
<context position="12217" citStr="Soricut, 2006" startWordPosition="2033" endWordPosition="2034">:1 [v v v ,&lt;2] 0 9 20 δ1 δ1 attacked rebels v6 3 δ1 ✼ ✽&apos;ez ✾v 19 v v v v v v 13 14 15 16 17 18 1 rebels 1 1 ε ε fighting 1 ε ✿ ✿fδz 3 δ1 ε ε [v , ] ε e [v v v ,&lt;3] [v v v ,&lt;32] 0 6 23 δ1 0 9 23 δ1 attacked :0.18 rebels : [v v v ,&lt;0] 0 19 23 δ1 [v v v ,&lt;32] 0 19 23 δ1 are responsible for adding and removing, respectively, the , symbols in the -stack. The probabilities associated with transitions are computed using the vertex set and the -stack of each state, together with the distribution functions of the and operators. For a detailed presentation of the UNFOLD relation we refer the reader to (Soricut, 2006). 3 Stochastic Language Generation from WIDL-expressions 3.1 Interpolating Probability Distributions in a Log-linear Framework Let us assume a finite set of strings over a finite alphabet , representing the set of possible sentence realizations. In a log-linear framework, we have a vector of feature functions , and a vector of parameters . For any , the interpolated probability can be written under a log-linear model as in Equation 1: (1) We can formulate the search problem of finding the most probable realization under this model as shown in Equation 2, and therefore we do not need to be conc</context>
</contexts>
<marker>Soricut, 2006</marker>
<rawString>Radu Soricut. 2006. Natural Language Generation for Text-to-Text Applications Using an Information-Slim Representation. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>BBN/UMD at DUC-2004: Topiary.</title>
<date>2004</date>
<booktitle>In Proceedings of the NAACL Workshop on Document Understanding,</booktitle>
<pages>112--119</pages>
<contexts>
<context position="15440" citStr="Zajic et al., 2004" startWordPosition="2552" endWordPosition="2555">rithm for directed acyclic graphs (Cormen et al., 2001) over the full acceptor (Soricut and Marcu, 2005). 4 Headline Generation using WIDL-expressions We employ the WIDL formalism (Section 2) and the WIDL-NGLM-A algorithm (Section 3) in a summarization application that aims at producing both informative and fluent headlines. Our headlines are generated in an abstractive, bottom-up manner, starting from words and phrases. A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al., 2003) and annotated with additional information (Zajic et al., 2004). Automatic Creation of WIDL-expressions for Headline Generation. We generate WIDLexpressions starting from an input document. First, we extract a weighted list of topic keywords from the input document using the algorithm of Zhou and Hovy (2003). This list is enriched with phrases created from the lexical dependencies the topic keywords have in the input document. We associate probability distributions with these phrases using their frequency (we assume Keywords iraq 0.32, syria 0.25, rebels 0.22, kurdish 0.17, turkish 0.14, attack 0.10 Phrases iraq in iraq 0.4, northern iraq 0.5,iraq and ira</context>
<context position="20053" citStr="Zajic et al., 2004" startWordPosition="3276" endWordPosition="3279">(Och, 2003) using ROUGE as the objective function, on the development set. The results are presented in Table 1. We compare the performance of several extractive algorithms (which operate on an extracted sentence to arrive at a headline) against several abstractive algorithms (which create headlines starting from scratch). For the extractive algorithms, Lead10 is a baseline which simply proposes as headline the lead sentence, cut after the first 10 words. HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al., 2003), and Topiary is our implementation of the Topiary system (Zajic et al., 2004). For the abstractive algorithms, Keywords is a baseline that proposes as headline the sequence of topic keywords, Webcl is the system THREE GORGES PROJECT IN CHINA HAS WON APPROVAL WATER IS LINK BETWEEN CLUSTER OF E. COLI CASES SRI LANKA ’S JOINT VENTURE TO EXPAND EXPORTS OPPOSITION TO EUROPEAN UNION SINGLE CURRENCY EURO OF INDIA AND BANGLADESH WATER BARRAGE Figure 5: Headlines generated automatically using a WIDL-based sentence realization system. described in (Zhou and Hovy, 2003), and WIDLA is the algorithm described in this paper. This evaluation shows that our WIDL-based approach to gene</context>
</contexts>
<marker>Zajic, Dorr, Schwartz, 2004</marker>
<rawString>David Zajic, Bonnie J. Dorr, and Richard Schwartz. 2004. BBN/UMD at DUC-2004: Topiary. In Proceedings of the NAACL Workshop on Document Understanding, pages 112–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Eduard Hovy</author>
</authors>
<title>Headline summarization at ISI.</title>
<date>2003</date>
<booktitle>In Proceedings of the NAACL Workshop on Document Understanding.</booktitle>
<contexts>
<context position="15686" citStr="Zhou and Hovy (2003)" startWordPosition="2589" endWordPosition="2592">ization application that aims at producing both informative and fluent headlines. Our headlines are generated in an abstractive, bottom-up manner, starting from words and phrases. A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al., 2003) and annotated with additional information (Zajic et al., 2004). Automatic Creation of WIDL-expressions for Headline Generation. We generate WIDLexpressions starting from an input document. First, we extract a weighted list of topic keywords from the input document using the algorithm of Zhou and Hovy (2003). This list is enriched with phrases created from the lexical dependencies the topic keywords have in the input document. We associate probability distributions with these phrases using their frequency (we assume Keywords iraq 0.32, syria 0.25, rebels 0.22, kurdish 0.17, turkish 0.14, attack 0.10 Phrases iraq in iraq 0.4, northern iraq 0.5,iraq and iran 0.1 , syria into syria 0.6, and syria 0.4 rebels attacked rebels 0.7,rebels fighting 0.3 . . . WIDL-expression &amp; trigram interpolation TURKISH GOVERNMENT ATTACKED REBELS IN IRAQ AND SYRIA Figure 4: Input and output for our automatic headline ge</context>
<context position="20541" citStr="Zhou and Hovy, 2003" startWordPosition="3353" endWordPosition="3356">ntation of the Hedge Trimer system (Dorr et al., 2003), and Topiary is our implementation of the Topiary system (Zajic et al., 2004). For the abstractive algorithms, Keywords is a baseline that proposes as headline the sequence of topic keywords, Webcl is the system THREE GORGES PROJECT IN CHINA HAS WON APPROVAL WATER IS LINK BETWEEN CLUSTER OF E. COLI CASES SRI LANKA ’S JOINT VENTURE TO EXPAND EXPORTS OPPOSITION TO EUROPEAN UNION SINGLE CURRENCY EURO OF INDIA AND BANGLADESH WATER BARRAGE Figure 5: Headlines generated automatically using a WIDL-based sentence realization system. described in (Zhou and Hovy, 2003), and WIDLA is the algorithm described in this paper. This evaluation shows that our WIDL-based approach to generation is capable of obtaining headlines that compare favorably, in both content and fluency, with extractive, state-of-the-art results (Zajic et al., 2004), while it outperforms a previously-proposed abstractive system by a wide margin (Zhou and Hovy, 2003). Also note that our evaluation makes these results directly comparable, as they use the same parsing and topic identification algorithms. In Figure 5, we present a sample of headlines produced by our system, which includes both g</context>
</contexts>
<marker>Zhou, Hovy, 2003</marker>
<rawString>Liang Zhou and Eduard Hovy. 2003. Headline summarization at ISI. In Proceedings of the NAACL Workshop on Document Understanding.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>