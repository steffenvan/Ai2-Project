<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.216255">
<title confidence="0.868111">
NLP for Indexing and Retrieval of Captioned Photographs
</title>
<author confidence="0.998019">
Katerina Pastra, Horacio Saggion, Yorick Wilks
</author>
<affiliation confidence="0.997636">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.916057666666667">
England - UK
Tel: +44-114-222-1800
Fax: +44-114-222-1810
</address>
<email confidence="0.856229">
fkaterina,saggion,yorickl@dcs shef.ac.uk
</email>
<sectionHeader confidence="0.997123" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982809523809">
We present a text-based approach for the
automatic indexing and retrieval of dig-
ital photographs taken at crime scenes.
Our research prototype, SOCIS, goes
beyond keyword-based approaches and
methods that extract syntactic relations
from captions; it relies on advanced Nat-
ural Language Processing techniques in
order to extract relational facts. These
relational facts consist of a &amp;quot;pragmatic
relation&amp;quot; and the entities this relation
connects (triples of the form: ARG1-
REL- ARG2). In SOCIS, the triples are
used as complex image indexing terms;
however, the extraction mechanism is
used not only for indexing purposes but
also for image retrieval using free text
queries. The retrieval mechanism com-
putes similarity scores between query-
triples and indexing-triples making use
of a domain-specific ontology.
</bodyText>
<sectionHeader confidence="0.975599" genericHeader="categories and subject descriptors">
1 Indexing and Retrieval of Photographs
</sectionHeader>
<bodyText confidence="0.999956186046511">
The normal practice in human indexing or cata-
loguing of photographs is to use a text-based rep-
resentation of the pictorial record having recourse
to a controlled vocabulary or to &amp;quot;free-text&amp;quot;. On
the one hand, an index using authoritative sources
(e.g., thesauri) ensures consistency across human
indexers, but at the same time it renders the in-
dexing task difficult due to the size of the key-
word list that is used - not to mention the cum-
bersome and unintuitive requirement impose to
the user, to become familiar with using specific
wording for the subsequent retrieval of the images.
On the other hand, the use of free-text associa-
tion, while natural, makes the index representation
subjective and error prone. Content-based Image
Processing methods are used as an alternative to
the manual-annotation bottleneck (Veltkamp and
Tanase, 2000). Content-based indexing and re-
trieval of images is based on features such as
colour, texture, and shape. Yet, image understand-
ing is not well advanced and is very difficult even
in closed domains. When linguistic descriptions
of the photographs are available (i.e., captions or
collateral texts), they can be used as the starting
point for indexing. We have focused on the devel-
opment and implementation of automatic caption-
based techniques for indexing and retrieval of pho-
tographs taken at scenes of crime (SOC).
Researchers in information retrieval argue that
detailed linguistic analysis is usually unnecessary
to improve accuracy for text indexing and re-
trieval; however, in the case of captioned pho-
tographs, natural language processing (NLP) tech-
niques have proved to be particularly effective for
the very same tasks (Rose et al., 2000; Guglielmo
and Rowe, 1996).
Current approaches in automatic text-based im-
age indexing fail in capturing semantic informa-
tion expressed in the captions, that is important
for the subsequent retrieval of the images (Pastra
et al., 2002). Unlike traditional &amp;quot;bag of words&amp;quot;
techniques and other methods for extracting syn-
tactic relations from captions for indexing pur-
</bodyText>
<page confidence="0.998516">
143
</page>
<bodyText confidence="0.999981658536585">
poses, our prototype extracts meaning representa-
tions that capture pragmatic relations between ob-
jects depicted in the photographs. Therefore, most
of the complexity of the written text is eliminated,
while its meaning is retained in an elegant and
simple way. The relational facts that are extracted
are of the form: ARG1-RELATION-ARG2 and
they are used as indexing terms for the crime scene
visual records. In these triples, the arguments
may be simple or complex noun phrases, whereas
the relations express locative arrangements, part-
of associations and other relations, all coming up
to 17 different relations as indicated through the
analysis of a corpus of 1000 captions. The no-
tion of extracting structres that capture semantic
relations among entities originates from early the-
ories on text representation. Our approach bears
a loose connection to the &amp;quot;Preference Semantics&amp;quot;
theory (Wilks, 1975; Wilks, 1978); however, in
the latter, the RELATIONs captured in seman-
tic templates were a mixture of CASE and ACT
denoting relations, whereas SOCIS focuses on
&amp;quot;static&amp;quot;, pragmatic relations between tangible ob-
jects. The binary relational templates extracted
by SOCIS allow for the indexing terms to cap-
ture semantic equivalences and differences that go
beyond syntactic dependencies, bindings to spe-
cific wording or implied information such as the
absence/presence of objects : &amp;quot;red substance on
yellow table&amp;quot; vs. &amp;quot;yellow substance on red ta-
ble&amp;quot;, &amp;quot;knife on table&amp;quot; vs. &amp;quot;blade on bar counter&amp;quot;,
and &amp;quot;cable around neck&amp;quot; vs. &amp;quot;neck with cable re-
moved&amp;quot; respectively.
SOCIS consists of a pipeline of processing
resources that perform the following tasks: (i)
pre-processing (e.g., tokenisation, POS tagging,
named entity recognition and classification, etc.);
(ii) parsing and naive semantic interpretation; (iii)
inference; (iv) triple extraction.
The rest of this paper describes our method for
indexing and retrieval using relational facts.
</bodyText>
<sectionHeader confidence="0.626811" genericHeader="general terms">
2 Ontology and Indexing Terms
</sectionHeader>
<bodyText confidence="0.999580153846154">
We have made use of the British Police Infor-
mation Technology Organisation Common Data
Model and a collection of formal reports produced
by scene of crime officers (SOCO) to develop On-
toCrime, a concept hierarchy that structures con-
cepts relevant to SOC investigation (e.g., physi-
cal evidence, trace evidence, weapon, cutting in-
strument, criminal event etc.). The ontology is
used during indexing-term computations. Two
types of indexing terms are obtained for each cap-
tion: (i) &amp;quot;lexical&amp;quot; terms, which are canonical rep-
resentation of objects mentioned in the caption;
and (ii) triples of the form (Argument&apos;, Relation,
Argument2), where Relation is the name of the
relation and Argument, are its arguments. The
arguments have the form Class : String, where
Class is the immediate hypernym the entity be-
longs to (according to OntoCrime), and String is
of the form (AdjlQual) * Head, where Head is
the head of the noun phase and Adj and Qual are
adjectives and nominal qualifiers syntactically at-
tached to the head. For example, the noun phrase
&amp;quot;the left rear bedroom&amp;quot; is represented as premises
: left rear bedroom and the full caption &amp;quot;neck
with cable removed&amp;quot; is represented as (body part:
neck, Without, physical object: cable).
</bodyText>
<sectionHeader confidence="0.997858" genericHeader="keywords">
3 NLP Processes
</sectionHeader>
<bodyText confidence="0.99993225">
We have used some resources available within
GATE (Cunningham et al., 2002) and have
integrated a robust parser and inference mecha-
nism implemented in Prolog. The preprocessing
consists of a simple tokeniser that identifies words
and spaces, a sentence segmenter, a named entity
recogniser specially developed for the SOC, a
POS tagger, and a morphological analyser. The
NE recogniser identifies all the types of named
entities that may be mentioned in the captions
such as: address, age, conveyance-make, date,
drug, gun-type, identifier, location, measurement,
money, offence, organisation, person, time. It is
a rule-based module developed through intensive
corpus analysis and implemented in JAPE (Cun-
ningham et al., 2002), a regular pattern matching
formalism within GATE. Part of speech tagging is
done with a transformation-based learning tagger
whose lexicon has been adapted to the SOC,
and lemmatisation is performed with a robust
rule-based system. The lexicon of the domain was
obtained from the corpus and appropriate part of
speech tags were produced semi-automatically
(this lexicon is used during POS tagging).
</bodyText>
<page confidence="0.994504">
144
</page>
<bodyText confidence="0.994379782608696">
Logical forms for each caption are obtained
through a bottom-up parsing component that uses
a context-free syntactic-semantic grammar. Log-
ical forms are mapped into the ontology using
a lexicon attached to the ontology (implemented
in XI (Gaizauskas and Humphreys, 1996)) and a
number of rules. After the &amp;quot;explicit&amp;quot; semantics
is mapped into the ontology, the following pro-
cedure is applied: each triple mapped onto the
model is examined in the order it is asserted. For
each triple X-Rel-Y, the system checks whether X
and Y occur as arguments in other relations and in
that case rules that account for transitive and dis-
tributive properties of the semantic relations such
as AND-distribution, WITH-transitivity, WITH-
distribution, etc. are fired to infer new triples (Pas-
tra et al., 2003). Our AND-distribution rule over
&amp;quot;On&amp;quot; is stated with the following rule:
If X-And-Y &amp; Y-On-Z Then X-On-Z
The WITH-distribution rule is stated as follows:
If X-With-Y &amp; Y-REL-Z Then X-REL-Z
So a caption such as &amp;quot;knife together with
revolver in kitchen&amp;quot; is represented with the triples:
</bodyText>
<listItem confidence="0.999530333333333">
• (i) (cutting instrument: knife, With, firearm:
revolver)
• (ii) (firearm : revolver, In, part of dwelling :
kitchen)
• (iii) (cutting instrument : knife, In, part of
dwelling: kitchen)
</listItem>
<bodyText confidence="0.999904285714286">
where triple (iii) was inferred using the rule.
We have evaluated the triple extraction and in-
ference mechanism using a test corpus of 500 cap-
tions and obtained accuracy of 80%. This glass-
box evaluation has indicated refinements to the ex-
traction rules and has also enhanced the set of in-
ferences that the system should be able to make.
</bodyText>
<sectionHeader confidence="0.988363" genericHeader="introduction">
4 Querying and Retrieval
</sectionHeader>
<bodyText confidence="0.975386714285714">
The same semantic representation mechanism is
also used for retrieval; SOCIS allows for free text
querying. The system&apos;s interface prompts the user
to think as if completing a sentence of the form
&amp;quot;show me all the photographs in the database that
depict...&amp;quot;. This query is then processed exactly as
if it was a caption (as described in the previous
section 3). Relational facts are extracted from the
query, if possible. These relational facts are then
matched against each photograph&apos;s indexing terms
and similarity scores are computed. For triples to
match, their RELATION slot has to be identical.
Then, a score is computed that takes into account
class and argument similarity. OntoCrime is used
to compute the semantic distance of the nodes
needed to be transversed in order to find a class
match. The formula we implement for computing
the similarity between query term T1 = (Class&apos; :
Argi, Rel,Class2 : Ar g2) and indexing term
T2 — (C 1(1883 : Ar g3, Rel,Class4 : Ar g4) is as
follows:
</bodyText>
<equation confidence="0.9977688">
Sim(Ti, T2) =
al * OntoSim(Classi,Class3)+
a2 * OntoSim(Class2,Class4)+
a3 * ArgSim(Argl, Arg3)±
* ArgSim(Arg2, Arg4)
</equation>
<bodyText confidence="0.982507">
where OntoSim(X,Y) is the inverse of the
length between X and Y in OntoCrime, and
ArgSim(A, B) is computed using the formula:
</bodyText>
<equation confidence="0.99132725">
ArgSim(A, B) =
* M atch(A Head, B Head)+
02 * M atCh(AQualIBQual)+
03 * M atch(AAdj, B Adj)
</equation>
<bodyText confidence="0.999539692307692">
where M atch(X ,Y) is 1 when X = Y and
0 when X X. The weighs a, and 0, have to
be experimentally identified. When more than one
relational fact is extracted from the query, the sys-
tem attempts to match each query triple with each
indexing term of each photograph and a sum of the
scores that each photograph receives is calculated
and used for the final selection of the most appro-
priate images to be returned to the user. In cases
when no relational facts can be extracted from the
query, simple keyword extraction (following the
rules for argument extraction for the triples) and
matching takes place, using the ontology for se-
</bodyText>
<page confidence="0.997604">
145
</page>
<bodyText confidence="0.884752">
mantic expansion.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99997812">
The use of conceptual structures as a means to cap-
ture the essential content of a text has a long his-
tory in Artificial Intelligence. For SOCIS, we have
attempted a pragmatic, corpus-based approach,
where the set of primitives emerge from the data.
MARIE (Guglielmo and Rowe, 1996) is a system
that uses a domain lexicon and a type hierarchy
to represent both queries and captions in a logical
form and then matches these representations in-
stead of mere keywords; the logical forms are case
grammar constructs structured in a slot-assertion
notation. Our approach is similar in the use of an
ontology for the domain and in the fact that trans-
formations are applied to the &amp;quot;superficial&amp;quot; forms
produced by the parser to obtain a semantic repre-
sentation, but we differ in that our method does not
extract full logical forms from the semantic rep-
resentation, but a finite set of possible relations.
Also related to SOCIS is the ANVIL system (Rose
et al., 2000) that parses captions in order to extract
dependency relations (e.g., head-modifier) that are
recursively compared with dependency relations
produced from user queries. Unlike SOCIS, in
ANVIL no logical form is produced nor any in-
ference to enrich the indexes.
</bodyText>
<sectionHeader confidence="0.995163" genericHeader="conclusions">
6 Work in Progress
</sectionHeader>
<bodyText confidence="0.99997852173913">
The SOCIS prototype is a web-based applica-
tion that allows SOC officers to upload digital
photographs and their descriptions in a central
database, index the photographs automatically ac-
cording to these textual descriptions and retrieve
them using free text queries. The retrieval mech-
anism is currently being implemented. Once the
retrieval will have been fully implemented, proper
usability testing of the whole system by real users
will take place and a comparison of our free-text
retrieval approach to other approaches that allow
for unrestricted natural language queries will be
undertaken. During the system&apos;s development cy-
cle usability evaluation through constant user as-
sessment has been carried out with the help of
the project&apos;s advisory board consisting of scene
of crime officers and investigators. This prelim-
inary feedback has indicated that making use of
relational facts in order to make a digital image
collection accessible with high precision and re-
call, since expressing such relations in both cap-
tions and queries is intuitive for the target users of
SOCIS.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863051282051">
H. Cunningham, D. Maynard, K. Bontcheva, and
V. TabIan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the Association for Computational
Linguistics, Philadelphia, PA.
R. Gaizauskas and K. Humphreys. 1996. XI: A Simple
Prolog-based Language for Cross-Classification and
Inhetotance. In Proceedings of the 7th International
Conference in Artificial Intelligence: Methodology,
Systems, Applications, pages 86-95, Sozopol, Bul-
garia.
E. Guglielmo and N. Rowe. 1996. Natural lan-
guage retrieval of images based on descriptive cap-
tions. ACM Transactions on Information Systems,
14(3):237-267.
K. Pastra, H. Saggion, and Y. Wilks. 2002. Extract-
ing Relational Facts for Indexing and Retrieval of
Crime-Scene Photographhs. In A. Macintosh, R. El-
lis, and F. Coenen, editors, Applications and Inno-
vations in Intelligent Systems X, British Computer
Society Conference Series, pages 121-134. Springer
Verlag.
K. Pastra, H. Saggion, and Y. Wilks. 2003. Intelligent
Indexing of Crime-Scene Photographs. IEEE Intel-
ligent Systems, Special Issue in Advances in Natural
Language Processing, 18(1):55-61.
T. Rose, D. Elworthy, A. Kotcheff, and A. Clare. 2000.
ANVIL: a System for Retrieval of Captioned Images
using NLP Techniques. In Proceedings of Chal-
lenge of Image Retrieval, Brighton, UK.
R. Veltkamp and M. Tanase. 2000. Content-based im-
age retrieval systems: a survey. Technical Report
UU-CS-2000-34, Utrecht University.
Y. Wilks. 1975. A Preferential, Pattern-Seeking, Se-
mantics for Natural Language Inference. Artificial
Intelligence, 6:53-74.
Y. Wilks. 1978. Making Preferences More Active .
Artificial Intelligence, 11:197-223.
</reference>
<page confidence="0.998814">
146
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.031258">
<title confidence="0.998413">NLP for Indexing and Retrieval of Captioned Photographs</title>
<author confidence="0.999122">Katerina Pastra</author>
<author confidence="0.999122">Horacio Saggion</author>
<author confidence="0.999122">Yorick Wilks</author>
<affiliation confidence="0.999934">Department of Computer Science University of Sheffield</affiliation>
<address confidence="0.807778">England - UK</address>
<phone confidence="0.999047">Tel: +44-114-222-1800 Fax: +44-114-222-1810</phone>
<email confidence="0.983083">fkaterina,saggion,yorickl@dcsshef.ac.uk</email>
<abstract confidence="0.99822710472973">We present a text-based approach for the indexing and of digital photographs taken at crime scenes. Our research prototype, SOCIS, goes beyond keyword-based approaches and methods that extract syntactic relations from captions; it relies on advanced Natural Language Processing techniques in order to extract relational facts. These relational facts consist of a &amp;quot;pragmatic relation&amp;quot; and the entities this relation connects (triples of the form: ARG1- REL- ARG2). In SOCIS, the triples are used as complex image indexing terms; however, the extraction mechanism is used not only for indexing purposes but also for image retrieval using free text queries. The retrieval mechanism computes similarity scores between querytriples and indexing-triples making use of a domain-specific ontology. 1 Indexing and Retrieval of Photographs The normal practice in human indexing or cataloguing of photographs is to use a text-based representation of the pictorial record having recourse to a controlled vocabulary or to &amp;quot;free-text&amp;quot;. On the one hand, an index using authoritative sources (e.g., thesauri) ensures consistency across human indexers, but at the same time it renders the indexing task difficult due to the size of the keylist that is used not to mention the cumbersome and unintuitive requirement impose to the user, to become familiar with using specific wording for the subsequent retrieval of the images. On the other hand, the use of free-text association, while natural, makes the index representation subjective and error prone. Content-based Image Processing methods are used as an alternative to the manual-annotation bottleneck (Veltkamp and Tanase, 2000). Content-based indexing and retrieval of images is based on features such as colour, texture, and shape. Yet, image understanding is not well advanced and is very difficult even in closed domains. When linguistic descriptions of the photographs are available (i.e., captions or collateral texts), they can be used as the starting point for indexing. We have focused on the development and implementation of automatic captionbased techniques for indexing and retrieval of photographs taken at scenes of crime (SOC). Researchers in information retrieval argue that detailed linguistic analysis is usually unnecessary to improve accuracy for text indexing and retrieval; however, in the case of captioned photographs, natural language processing (NLP) techniques have proved to be particularly effective for the very same tasks (Rose et al., 2000; Guglielmo and Rowe, 1996). Current approaches in automatic text-based image indexing fail in capturing semantic information expressed in the captions, that is important for the subsequent retrieval of the images (Pastra et al., 2002). Unlike traditional &amp;quot;bag of words&amp;quot; techniques and other methods for extracting synrelations from captions for indexing pur- 143 poses, our prototype extracts meaning representations that capture pragmatic relations between objects depicted in the photographs. Therefore, most of the complexity of the written text is eliminated, while its meaning is retained in an elegant and simple way. The relational facts that are extracted are of the form: ARG1-RELATION-ARG2 and they are used as indexing terms for the crime scene visual records. In these triples, the arguments may be simple or complex noun phrases, whereas the relations express locative arrangements, partof associations and other relations, all coming up to 17 different relations as indicated through the analysis of a corpus of 1000 captions. The notion of extracting structres that capture semantic relations among entities originates from early theories on text representation. Our approach bears a loose connection to the &amp;quot;Preference Semantics&amp;quot; theory (Wilks, 1975; Wilks, 1978); however, in the latter, the RELATIONs captured in semantic templates were a mixture of CASE and ACT denoting relations, whereas SOCIS focuses on &amp;quot;static&amp;quot;, pragmatic relations between tangible objects. The binary relational templates extracted by SOCIS allow for the indexing terms to capture semantic equivalences and differences that go beyond syntactic dependencies, bindings to specific wording or implied information such as the absence/presence of objects : &amp;quot;red substance on yellow table&amp;quot; vs. &amp;quot;yellow substance on red table&amp;quot;, &amp;quot;knife on table&amp;quot; vs. &amp;quot;blade on bar counter&amp;quot;, and &amp;quot;cable around neck&amp;quot; vs. &amp;quot;neck with cable removed&amp;quot; respectively. SOCIS consists of a pipeline of processing resources that perform the following tasks: (i) pre-processing (e.g., tokenisation, POS tagging, named entity recognition and classification, etc.); (ii) parsing and naive semantic interpretation; (iii) inference; (iv) triple extraction. The rest of this paper describes our method for indexing and retrieval using relational facts. 2 Ontology and Indexing Terms We have made use of the British Police Information Technology Organisation Common Data Model and a collection of formal reports produced by scene of crime officers (SOCO) to develop Ona concept hierarchy that structures concepts relevant to SOC investigation (e.g., physical evidence, trace evidence, weapon, cutting instrument, criminal event etc.). The ontology is used during indexing-term computations. Two types of indexing terms are obtained for each caption: (i) &amp;quot;lexical&amp;quot; terms, which are canonical representation of objects mentioned in the caption; (ii) triples of the form Relation, the name of the and its arguments. The have the form : String, the immediate hypernym the entity beto (according to OntoCrime), and the form * Head, head of the noun phase and adjectives and nominal qualifiers syntactically attached to the head. For example, the noun phrase left rear bedroom&amp;quot; is represented as left rear bedroom the full caption &amp;quot;neck cable removed&amp;quot; is represented as part: neck, Without, physical object: cable). 3 NLP Processes We have used some resources available within GATE (Cunningham et al., 2002) and have integrated a robust parser and inference mechanism implemented in Prolog. The preprocessing consists of a simple tokeniser that identifies words and spaces, a sentence segmenter, a named entity recogniser specially developed for the SOC, a POS tagger, and a morphological analyser. The NE recogniser identifies all the types of named entities that may be mentioned in the captions as: age, conveyance-make, date, drug, gun-type, identifier, location, measurement, offence, organisation, person, time. is a rule-based module developed through intensive corpus analysis and implemented in JAPE (Cunningham et al., 2002), a regular pattern matching formalism within GATE. Part of speech tagging is done with a transformation-based learning tagger whose lexicon has been adapted to the SOC, and lemmatisation is performed with a robust rule-based system. The lexicon of the domain was obtained from the corpus and appropriate part of speech tags were produced semi-automatically (this lexicon is used during POS tagging). 144 Logical forms for each caption are obtained through a bottom-up parsing component that uses a context-free syntactic-semantic grammar. Logical forms are mapped into the ontology using a lexicon attached to the ontology (implemented in XI (Gaizauskas and Humphreys, 1996)) and a number of rules. After the &amp;quot;explicit&amp;quot; semantics is mapped into the ontology, the following procedure is applied: each triple mapped onto the model is examined in the order it is asserted. For each triple X-Rel-Y, the system checks whether X and Y occur as arguments in other relations and in that case rules that account for transitive and distributive properties of the semantic relations such as AND-distribution, WITH-transitivity, WITHdistribution, etc. are fired to infer new triples (Pastra et al., 2003). Our AND-distribution rule over &amp;quot;On&amp;quot; is stated with the following rule: &amp; Y-On-Z The WITH-distribution rule is stated as follows: &amp; Y-REL-Z So a caption such as &amp;quot;knife together with revolver in kitchen&amp;quot; is represented with the triples: • (i) (cutting instrument: knife, With, firearm: revolver) (ii) : revolver, In, part of dwelling : kitchen) (iii) instrument : knife, In, part of dwelling: kitchen) where triple (iii) was inferred using the rule. We have evaluated the triple extraction and inference mechanism using a test corpus of 500 captions and obtained accuracy of 80%. This glassbox evaluation has indicated refinements to the extraction rules and has also enhanced the set of inferences that the system should be able to make. 4 Querying and Retrieval The same semantic representation mechanism is also used for retrieval; SOCIS allows for free text querying. The system&apos;s interface prompts the user to think as if completing a sentence of the form &amp;quot;show me all the photographs in the database that depict...&amp;quot;. This query is then processed exactly as if it was a caption (as described in the previous section 3). Relational facts are extracted from the query, if possible. These relational facts are then matched against each photograph&apos;s indexing terms and similarity scores are computed. For triples to match, their RELATION slot has to be identical. Then, a score is computed that takes into account class and argument similarity. OntoCrime is used to compute the semantic distance of the nodes needed to be transversed in order to find a class match. The formula we implement for computing similarity between query term = : Rel,Class2 : Ar g2) indexing term — (C 1(1883 : g3, Rel,Class4 : Ar g4) follows: Sim(Ti, T2) = * * * Arg4) the inverse of the between Y in OntoCrime, and B) computed using the formula: ArgSim(A, B) = atch(A Head, B Head)+ 02 * M atCh(AQualIBQual)+ * atch(AAdj, B Adj) atch(X ,Y) 1 when = and when X. weighs a, and 0, have to be experimentally identified. When more than one fact is extracted from the query, the sysattempts match each query triple with each indexing term of each photograph and a sum of the scores that each photograph receives is calculated and used for the final selection of the most appropriate images to be returned to the user. In cases when no relational facts can be extracted from the query, simple keyword extraction (following the rules for argument extraction for the triples) and takes place, using the ontology for se- 145 mantic expansion. 5 Related Work The use of conceptual structures as a means to capture the essential content of a text has a long history in Artificial Intelligence. For SOCIS, we have attempted a pragmatic, corpus-based approach, where the set of primitives emerge from the data. MARIE (Guglielmo and Rowe, 1996) is a system that uses a domain lexicon and a type hierarchy to represent both queries and captions in a logical form and then matches these representations instead of mere keywords; the logical forms are case grammar constructs structured in a slot-assertion notation. Our approach is similar in the use of an ontology for the domain and in the fact that transformations are applied to the &amp;quot;superficial&amp;quot; forms produced by the parser to obtain a semantic representation, but we differ in that our method does not extract full logical forms from the semantic representation, but a finite set of possible relations. Also related to SOCIS is the ANVIL system (Rose et al., 2000) that parses captions in order to extract dependency relations (e.g., head-modifier) that are recursively compared with dependency relations produced from user queries. Unlike SOCIS, in ANVIL no logical form is produced nor any inference to enrich the indexes. 6 Work in Progress The SOCIS prototype is a web-based application that allows SOC officers to upload digital photographs and their descriptions in a central database, index the photographs automatically according to these textual descriptions and retrieve them using free text queries. The retrieval mechanism is currently being implemented. Once the retrieval will have been fully implemented, proper usability testing of the whole system by real users will take place and a comparison of our free-text retrieval approach to other approaches that allow for unrestricted natural language queries will be undertaken. During the system&apos;s development cycle usability evaluation through constant user assessment has been carried out with the help of the project&apos;s advisory board consisting of scene of crime officers and investigators. This preliminary feedback has indicated that making use of relational facts in order to make a digital image collection accessible with high precision and recall, since expressing such relations in both captions and queries is intuitive for the target users of SOCIS.</abstract>
<note confidence="0.645059833333333">References D. Maynard, K. Bontcheva, and V. TabIan. 2002. GATE: A framework and graphical development environment for robust NLP tools and In of the 40th Anniversary Meeting of the Association for Computational PA. R. Gaizauskas and K. Humphreys. 1996. XI: A Simple Prolog-based Language for Cross-Classification and In of the 7th International Conference in Artificial Intelligence: Methodology, Applications, 86-95, Sozopol, Bulgaria. E. Guglielmo and N. Rowe. 1996. Natural language retrieval of images based on descriptive cap- Transactions on Information Systems, 14(3):237-267. Pastra, and Y. Wilks. 2002. Extract-</note>
<title confidence="0.918064">ing Relational Facts for Indexing and Retrieval of</title>
<author confidence="0.963636">In A Macintosh</author>
<author confidence="0.963636">R El-</author>
<affiliation confidence="0.872208">and F. Coenen, editors, and Inno-</affiliation>
<note confidence="0.920701368421053">in Intelligent Systems X, Computer Society Conference Series, pages 121-134. Springer Verlag. K. Pastra, H. Saggion, and Y. Wilks. 2003. Intelligent of Crime-Scene Photographs. Intelligent Systems, Special Issue in Advances in Natural Processing, T. Rose, D. Elworthy, A. Kotcheff, and A. Clare. 2000. ANVIL: a System for Retrieval of Captioned Images NLP Techniques. In of Chalof Image Retrieval, UK. R. Veltkamp and M. Tanase. 2000. Content-based image retrieval systems: a survey. Technical Report UU-CS-2000-34, Utrecht University. Y. Wilks. 1975. A Preferential, Pattern-Seeking, Sefor Natural Language Inference. Y. Wilks. 1978. Making Preferences More Active . Artificial Intelligence, 11:197-223. 146</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V TabIan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6461" citStr="Cunningham et al., 2002" startWordPosition="991" endWordPosition="994">guments. The arguments have the form Class : String, where Class is the immediate hypernym the entity belongs to (according to OntoCrime), and String is of the form (AdjlQual) * Head, where Head is the head of the noun phase and Adj and Qual are adjectives and nominal qualifiers syntactically attached to the head. For example, the noun phrase &amp;quot;the left rear bedroom&amp;quot; is represented as premises : left rear bedroom and the full caption &amp;quot;neck with cable removed&amp;quot; is represented as (body part: neck, Without, physical object: cable). 3 NLP Processes We have used some resources available within GATE (Cunningham et al., 2002) and have integrated a robust parser and inference mechanism implemented in Prolog. The preprocessing consists of a simple tokeniser that identifies words and spaces, a sentence segmenter, a named entity recogniser specially developed for the SOC, a POS tagger, and a morphological analyser. The NE recogniser identifies all the types of named entities that may be mentioned in the captions such as: address, age, conveyance-make, date, drug, gun-type, identifier, location, measurement, money, offence, organisation, person, time. It is a rule-based module developed through intensive corpus analysi</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, TabIan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. TabIan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
<author>K Humphreys</author>
</authors>
<title>XI: A Simple Prolog-based Language for Cross-Classification and Inhetotance.</title>
<date>1996</date>
<booktitle>In Proceedings of the 7th International Conference in Artificial Intelligence: Methodology, Systems, Applications,</booktitle>
<pages>86--95</pages>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="7787" citStr="Gaizauskas and Humphreys, 1996" startWordPosition="1189" endWordPosition="1192">n GATE. Part of speech tagging is done with a transformation-based learning tagger whose lexicon has been adapted to the SOC, and lemmatisation is performed with a robust rule-based system. The lexicon of the domain was obtained from the corpus and appropriate part of speech tags were produced semi-automatically (this lexicon is used during POS tagging). 144 Logical forms for each caption are obtained through a bottom-up parsing component that uses a context-free syntactic-semantic grammar. Logical forms are mapped into the ontology using a lexicon attached to the ontology (implemented in XI (Gaizauskas and Humphreys, 1996)) and a number of rules. After the &amp;quot;explicit&amp;quot; semantics is mapped into the ontology, the following procedure is applied: each triple mapped onto the model is examined in the order it is asserted. For each triple X-Rel-Y, the system checks whether X and Y occur as arguments in other relations and in that case rules that account for transitive and distributive properties of the semantic relations such as AND-distribution, WITH-transitivity, WITHdistribution, etc. are fired to infer new triples (Pastra et al., 2003). Our AND-distribution rule over &amp;quot;On&amp;quot; is stated with the following rule: If X-And-</context>
</contexts>
<marker>Gaizauskas, Humphreys, 1996</marker>
<rawString>R. Gaizauskas and K. Humphreys. 1996. XI: A Simple Prolog-based Language for Cross-Classification and Inhetotance. In Proceedings of the 7th International Conference in Artificial Intelligence: Methodology, Systems, Applications, pages 86-95, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Guglielmo</author>
<author>N Rowe</author>
</authors>
<title>Natural language retrieval of images based on descriptive captions.</title>
<date>1996</date>
<journal>ACM Transactions on Information Systems,</journal>
<pages>14--3</pages>
<contexts>
<context position="2828" citStr="Guglielmo and Rowe, 1996" startWordPosition="426" endWordPosition="429">are available (i.e., captions or collateral texts), they can be used as the starting point for indexing. We have focused on the development and implementation of automatic captionbased techniques for indexing and retrieval of photographs taken at scenes of crime (SOC). Researchers in information retrieval argue that detailed linguistic analysis is usually unnecessary to improve accuracy for text indexing and retrieval; however, in the case of captioned photographs, natural language processing (NLP) techniques have proved to be particularly effective for the very same tasks (Rose et al., 2000; Guglielmo and Rowe, 1996). Current approaches in automatic text-based image indexing fail in capturing semantic information expressed in the captions, that is important for the subsequent retrieval of the images (Pastra et al., 2002). Unlike traditional &amp;quot;bag of words&amp;quot; techniques and other methods for extracting syntactic relations from captions for indexing pur143 poses, our prototype extracts meaning representations that capture pragmatic relations between objects depicted in the photographs. Therefore, most of the complexity of the written text is eliminated, while its meaning is retained in an elegant and simple wa</context>
<context position="11417" citStr="Guglielmo and Rowe, 1996" startWordPosition="1810" endWordPosition="1813">lculated and used for the final selection of the most appropriate images to be returned to the user. In cases when no relational facts can be extracted from the query, simple keyword extraction (following the rules for argument extraction for the triples) and matching takes place, using the ontology for se145 mantic expansion. 5 Related Work The use of conceptual structures as a means to capture the essential content of a text has a long history in Artificial Intelligence. For SOCIS, we have attempted a pragmatic, corpus-based approach, where the set of primitives emerge from the data. MARIE (Guglielmo and Rowe, 1996) is a system that uses a domain lexicon and a type hierarchy to represent both queries and captions in a logical form and then matches these representations instead of mere keywords; the logical forms are case grammar constructs structured in a slot-assertion notation. Our approach is similar in the use of an ontology for the domain and in the fact that transformations are applied to the &amp;quot;superficial&amp;quot; forms produced by the parser to obtain a semantic representation, but we differ in that our method does not extract full logical forms from the semantic representation, but a finite set of possib</context>
</contexts>
<marker>Guglielmo, Rowe, 1996</marker>
<rawString>E. Guglielmo and N. Rowe. 1996. Natural language retrieval of images based on descriptive captions. ACM Transactions on Information Systems, 14(3):237-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Pastra</author>
<author>H Saggion</author>
<author>Y Wilks</author>
</authors>
<title>Extracting Relational Facts for Indexing and Retrieval of Crime-Scene Photographhs.</title>
<date>2002</date>
<booktitle>Applications and Innovations in Intelligent Systems X, British Computer Society Conference Series,</booktitle>
<pages>121--134</pages>
<editor>In A. Macintosh, R. Ellis, and F. Coenen, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="3036" citStr="Pastra et al., 2002" startWordPosition="458" endWordPosition="461">retrieval of photographs taken at scenes of crime (SOC). Researchers in information retrieval argue that detailed linguistic analysis is usually unnecessary to improve accuracy for text indexing and retrieval; however, in the case of captioned photographs, natural language processing (NLP) techniques have proved to be particularly effective for the very same tasks (Rose et al., 2000; Guglielmo and Rowe, 1996). Current approaches in automatic text-based image indexing fail in capturing semantic information expressed in the captions, that is important for the subsequent retrieval of the images (Pastra et al., 2002). Unlike traditional &amp;quot;bag of words&amp;quot; techniques and other methods for extracting syntactic relations from captions for indexing pur143 poses, our prototype extracts meaning representations that capture pragmatic relations between objects depicted in the photographs. Therefore, most of the complexity of the written text is eliminated, while its meaning is retained in an elegant and simple way. The relational facts that are extracted are of the form: ARG1-RELATION-ARG2 and they are used as indexing terms for the crime scene visual records. In these triples, the arguments may be simple or complex </context>
</contexts>
<marker>Pastra, Saggion, Wilks, 2002</marker>
<rawString>K. Pastra, H. Saggion, and Y. Wilks. 2002. Extracting Relational Facts for Indexing and Retrieval of Crime-Scene Photographhs. In A. Macintosh, R. Ellis, and F. Coenen, editors, Applications and Innovations in Intelligent Systems X, British Computer Society Conference Series, pages 121-134. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Pastra</author>
<author>H Saggion</author>
<author>Y Wilks</author>
</authors>
<title>Intelligent Indexing of Crime-Scene Photographs.</title>
<date>2003</date>
<journal>IEEE Intelligent Systems, Special Issue</journal>
<booktitle>in Advances in Natural Language Processing,</booktitle>
<pages>18--1</pages>
<contexts>
<context position="8305" citStr="Pastra et al., 2003" startWordPosition="1274" endWordPosition="1278"> ontology using a lexicon attached to the ontology (implemented in XI (Gaizauskas and Humphreys, 1996)) and a number of rules. After the &amp;quot;explicit&amp;quot; semantics is mapped into the ontology, the following procedure is applied: each triple mapped onto the model is examined in the order it is asserted. For each triple X-Rel-Y, the system checks whether X and Y occur as arguments in other relations and in that case rules that account for transitive and distributive properties of the semantic relations such as AND-distribution, WITH-transitivity, WITHdistribution, etc. are fired to infer new triples (Pastra et al., 2003). Our AND-distribution rule over &amp;quot;On&amp;quot; is stated with the following rule: If X-And-Y &amp; Y-On-Z Then X-On-Z The WITH-distribution rule is stated as follows: If X-With-Y &amp; Y-REL-Z Then X-REL-Z So a caption such as &amp;quot;knife together with revolver in kitchen&amp;quot; is represented with the triples: • (i) (cutting instrument: knife, With, firearm: revolver) • (ii) (firearm : revolver, In, part of dwelling : kitchen) • (iii) (cutting instrument : knife, In, part of dwelling: kitchen) where triple (iii) was inferred using the rule. We have evaluated the triple extraction and inference mechanism using a test cor</context>
</contexts>
<marker>Pastra, Saggion, Wilks, 2003</marker>
<rawString>K. Pastra, H. Saggion, and Y. Wilks. 2003. Intelligent Indexing of Crime-Scene Photographs. IEEE Intelligent Systems, Special Issue in Advances in Natural Language Processing, 18(1):55-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rose</author>
<author>D Elworthy</author>
<author>A Kotcheff</author>
<author>A Clare</author>
</authors>
<title>ANVIL: a System for Retrieval of Captioned Images using NLP Techniques.</title>
<date>2000</date>
<booktitle>In Proceedings of Challenge of Image Retrieval,</booktitle>
<location>Brighton, UK.</location>
<contexts>
<context position="2801" citStr="Rose et al., 2000" startWordPosition="422" endWordPosition="425">of the photographs are available (i.e., captions or collateral texts), they can be used as the starting point for indexing. We have focused on the development and implementation of automatic captionbased techniques for indexing and retrieval of photographs taken at scenes of crime (SOC). Researchers in information retrieval argue that detailed linguistic analysis is usually unnecessary to improve accuracy for text indexing and retrieval; however, in the case of captioned photographs, natural language processing (NLP) techniques have proved to be particularly effective for the very same tasks (Rose et al., 2000; Guglielmo and Rowe, 1996). Current approaches in automatic text-based image indexing fail in capturing semantic information expressed in the captions, that is important for the subsequent retrieval of the images (Pastra et al., 2002). Unlike traditional &amp;quot;bag of words&amp;quot; techniques and other methods for extracting syntactic relations from captions for indexing pur143 poses, our prototype extracts meaning representations that capture pragmatic relations between objects depicted in the photographs. Therefore, most of the complexity of the written text is eliminated, while its meaning is retained </context>
<context position="12092" citStr="Rose et al., 2000" startWordPosition="1927" endWordPosition="1930">chy to represent both queries and captions in a logical form and then matches these representations instead of mere keywords; the logical forms are case grammar constructs structured in a slot-assertion notation. Our approach is similar in the use of an ontology for the domain and in the fact that transformations are applied to the &amp;quot;superficial&amp;quot; forms produced by the parser to obtain a semantic representation, but we differ in that our method does not extract full logical forms from the semantic representation, but a finite set of possible relations. Also related to SOCIS is the ANVIL system (Rose et al., 2000) that parses captions in order to extract dependency relations (e.g., head-modifier) that are recursively compared with dependency relations produced from user queries. Unlike SOCIS, in ANVIL no logical form is produced nor any inference to enrich the indexes. 6 Work in Progress The SOCIS prototype is a web-based application that allows SOC officers to upload digital photographs and their descriptions in a central database, index the photographs automatically according to these textual descriptions and retrieve them using free text queries. The retrieval mechanism is currently being implemente</context>
</contexts>
<marker>Rose, Elworthy, Kotcheff, Clare, 2000</marker>
<rawString>T. Rose, D. Elworthy, A. Kotcheff, and A. Clare. 2000. ANVIL: a System for Retrieval of Captioned Images using NLP Techniques. In Proceedings of Challenge of Image Retrieval, Brighton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Veltkamp</author>
<author>M Tanase</author>
</authors>
<title>Content-based image retrieval systems: a survey.</title>
<date>2000</date>
<tech>Technical Report UU-CS-2000-34,</tech>
<institution>Utrecht University.</institution>
<contexts>
<context position="1957" citStr="Veltkamp and Tanase, 2000" startWordPosition="289" endWordPosition="292">x using authoritative sources (e.g., thesauri) ensures consistency across human indexers, but at the same time it renders the indexing task difficult due to the size of the keyword list that is used - not to mention the cumbersome and unintuitive requirement impose to the user, to become familiar with using specific wording for the subsequent retrieval of the images. On the other hand, the use of free-text association, while natural, makes the index representation subjective and error prone. Content-based Image Processing methods are used as an alternative to the manual-annotation bottleneck (Veltkamp and Tanase, 2000). Content-based indexing and retrieval of images is based on features such as colour, texture, and shape. Yet, image understanding is not well advanced and is very difficult even in closed domains. When linguistic descriptions of the photographs are available (i.e., captions or collateral texts), they can be used as the starting point for indexing. We have focused on the development and implementation of automatic captionbased techniques for indexing and retrieval of photographs taken at scenes of crime (SOC). Researchers in information retrieval argue that detailed linguistic analysis is usua</context>
</contexts>
<marker>Veltkamp, Tanase, 2000</marker>
<rawString>R. Veltkamp and M. Tanase. 2000. Content-based image retrieval systems: a survey. Technical Report UU-CS-2000-34, Utrecht University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>A Preferential, Pattern-Seeking, Semantics for Natural Language Inference.</title>
<date>1975</date>
<journal>Artificial Intelligence,</journal>
<pages>6--53</pages>
<contexts>
<context position="4072" citStr="Wilks, 1975" startWordPosition="619" endWordPosition="620">extracted are of the form: ARG1-RELATION-ARG2 and they are used as indexing terms for the crime scene visual records. In these triples, the arguments may be simple or complex noun phrases, whereas the relations express locative arrangements, partof associations and other relations, all coming up to 17 different relations as indicated through the analysis of a corpus of 1000 captions. The notion of extracting structres that capture semantic relations among entities originates from early theories on text representation. Our approach bears a loose connection to the &amp;quot;Preference Semantics&amp;quot; theory (Wilks, 1975; Wilks, 1978); however, in the latter, the RELATIONs captured in semantic templates were a mixture of CASE and ACT denoting relations, whereas SOCIS focuses on &amp;quot;static&amp;quot;, pragmatic relations between tangible objects. The binary relational templates extracted by SOCIS allow for the indexing terms to capture semantic equivalences and differences that go beyond syntactic dependencies, bindings to specific wording or implied information such as the absence/presence of objects : &amp;quot;red substance on yellow table&amp;quot; vs. &amp;quot;yellow substance on red table&amp;quot;, &amp;quot;knife on table&amp;quot; vs. &amp;quot;blade on bar counter&amp;quot;, and &amp;quot;ca</context>
</contexts>
<marker>Wilks, 1975</marker>
<rawString>Y. Wilks. 1975. A Preferential, Pattern-Seeking, Semantics for Natural Language Inference. Artificial Intelligence, 6:53-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Making Preferences More Active .</title>
<date>1978</date>
<journal>Artificial Intelligence,</journal>
<pages>11--197</pages>
<contexts>
<context position="4086" citStr="Wilks, 1978" startWordPosition="621" endWordPosition="622"> of the form: ARG1-RELATION-ARG2 and they are used as indexing terms for the crime scene visual records. In these triples, the arguments may be simple or complex noun phrases, whereas the relations express locative arrangements, partof associations and other relations, all coming up to 17 different relations as indicated through the analysis of a corpus of 1000 captions. The notion of extracting structres that capture semantic relations among entities originates from early theories on text representation. Our approach bears a loose connection to the &amp;quot;Preference Semantics&amp;quot; theory (Wilks, 1975; Wilks, 1978); however, in the latter, the RELATIONs captured in semantic templates were a mixture of CASE and ACT denoting relations, whereas SOCIS focuses on &amp;quot;static&amp;quot;, pragmatic relations between tangible objects. The binary relational templates extracted by SOCIS allow for the indexing terms to capture semantic equivalences and differences that go beyond syntactic dependencies, bindings to specific wording or implied information such as the absence/presence of objects : &amp;quot;red substance on yellow table&amp;quot; vs. &amp;quot;yellow substance on red table&amp;quot;, &amp;quot;knife on table&amp;quot; vs. &amp;quot;blade on bar counter&amp;quot;, and &amp;quot;cable around nec</context>
</contexts>
<marker>Wilks, 1978</marker>
<rawString>Y. Wilks. 1978. Making Preferences More Active . Artificial Intelligence, 11:197-223.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>