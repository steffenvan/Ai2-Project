<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013155">
<title confidence="0.9993955">
Intra-Speaker Topic Modeling for Improved Multi-Party Meeting
Summarization with Integrated Random Walk
</title>
<author confidence="0.998153">
Yun-Nung Chen and Florian Metze
</author>
<affiliation confidence="0.999545">
School of Computer Science, Carnegie Mellon University
</affiliation>
<address confidence="0.546397">
5000 Forbes Avenue, Pittsburgh, PA 15213-3891, USA
</address>
<email confidence="0.999151">
{yvchen,fmetze}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997106875">
This paper proposes an improved approach to extrac-
tive summarization of spoken multi-party interac-
tion, in which integrated random walk is performed
on a graph constructed on topical/ lexical relations.
Each utterance is represented as a node of the graph,
and the edges’ weights are computed from the topi-
cal similarity between the utterances, evaluated us-
ing probabilistic latent semantic analysis (PLSA),
and from word overlap. We model intra-speaker
topics by partially sharing the topics from the same
speaker in the graph. In this paper, we perform ex-
periments on automatically and manually generated
transcripts. For automatic transcripts, our results
show that intra-speaker topic sharing and integrating
topical/ lexical relations can help include the impor-
tant utterances.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979684875">
Speech summarization is an active and important topic of
research (Lee and Chen, 2005), because multimedia/ spo-
ken documents are more difficult to browse than text or
image content. While earlier work was focused primarily
on broadcast news content, recent effort has been increas-
ingly directed to new domains such as lectures (Glass
et al., 2007; Chen et al., 2011) and multi-party interac-
tion (Banerjee and Rudnicky, 2008; Liu and Liu, 2010).
We describe experiments on multi-party interaction found
in meeting recordings, performing extractive summariza-
tion (Liu et al., 2010) on transcripts generated by auto-
matic speech recognition (ASR) and human annotators.
Graph-based methods for computing lexical centrality
as importance to extract summaries (Erkan and Radev,
2004) have been investigated in the context of text sum-
marization. Some works focus on maximizing cover-
age of summaries using the objective function (Gillick,
2011). Speech summarization carries intrinsic difficul-
ties due to the presence of recognition errors, sponta-
neous speech effect, and lack of segmentation. A gen-
eral approach has been found very successful (Furui et
al., 2004), in which each utterance in the document d,
U = t1t2...ti...tn, represented as a sequence of terms ti,
is given an importance score:
</bodyText>
<equation confidence="0.999702">
[λ1s(ti, d) + λ2l(ti) (1)
+ λ3c(ti) + λ4g(ti)] + λ5b(U),
</equation>
<bodyText confidence="0.999964583333333">
where s(ti, d), l(ti), c(ti), g(ti) are respectively some
statistical measure (such as TF-IDF), linguistic measure
(e.g., different part-of-speech tags are given different
weights), confidence score and N-gram score for the term
ti, and b(U) is calculated from the grammatical structure
of the utterance U, and λ1, λ2, λ3, λ4 and λ5 are weight-
ing parameters. For each document, the utterances to be
used in the summary are then selected based on this score.
In recent work, Chen (2011) proposed a graphical
structure to rescore I(U, d), which can model the topical
coherence between utterances using random walk within
documents. Similarly, we now use a graph-based ap-
proach to consider the importance of terms and the simi-
larity between utterances, where topical and lexical simi-
larity are integrated in the graph, so that utterances topi-
cally or lexically similar to more important utterances are
given higher scores. Using topical similarity can com-
pensate the negative effects of recognition errors on sim-
ilarity evaluated on word overlap to some extent. In addi-
tion, this paper proposes an approach of modeling intra-
speaker topics in the graph to improve meeting summa-
rization (Garg et al., 2009) using information from multi-
party interaction, which is not available in lectures or
broadcast news.
</bodyText>
<sectionHeader confidence="0.945792" genericHeader="method">
2 Proposed Approach
</sectionHeader>
<bodyText confidence="0.998482333333333">
We apply word stemming and noise utterance filtering for
utterances in all meetings. Then we construct a graph to
compute the importance of all utterances.
</bodyText>
<equation confidence="0.9975044">
1
I(U, d) =
n
�n
i=1
</equation>
<page confidence="0.934219">
377
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 377–381,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<figureCaption confidence="0.999775">
Figure 1: A simplified example of the graph considered.
</figureCaption>
<bodyText confidence="0.999990266666667">
We formulate the utterance selection problem as ran-
dom walk on a directed graph, in which each utterance
is a node and the edges between them are weighted by
topical and lexical similarity. The basic idea is that an
utterance similar to more important utterances should be
more important (Chen et al., 2011). We formulate two
types of directed edge, topical edges and lexical edges,
which are weighted by topical and lexical similarity re-
spectively. We then keep only the top N outgoing edges
with the highest weights from each node, while consider
incoming edges to each node for importance propagation
in the graph. A simplified example for such a graph with
topical edges is in Figure 1, in which Ati and Bti are the
sets of neighbors of the node Ui connected respectively
by outgoing and incoming topical edges.
</bodyText>
<subsectionHeader confidence="0.997218">
2.1 Parameters from PLSA
</subsectionHeader>
<bodyText confidence="0.999603071428571">
Probabilistic latent semantic analysis (PLSA) (Hofmann,
1999) has been widely used to analyze the semantics
of documents based on a set of latent topics. Given
a set of documents {dj, j = 1, 2, ..., J} and all terms
{ti, i = 1, 2,..., M} they include, PLSA uses a set of
latent topic variables, {Tk, k = 1, 2,..., K}, to charac-
terize the “term-document” co-occurrence relationships.
The PLSA model can be optimized with EM algorithm
by maximizing a likelihood function. We utilize two pa-
rameters from PLSA, latent topic significance (LTS) and
latent topic entropy (LTE) (Kong and Lee, 2011) in the
paper.
Latent Topic Significance (LTS) for a given term ti
with respect to a topic Tk can be defined as
</bodyText>
<equation confidence="0.996226">
LTSti(Tk) = Pdj ED n(ti, dj)P(Tk  |dj) , (2)
PdjED n(ti,dj)[1 − P(Tk  |dj)]
</equation>
<bodyText confidence="0.9994418">
where n(ti, dj) is the occurrence count of term ti in a
document dj. Thus, a higher LTSti(Tk) indicates the
term ti is more significant for the latent topic Tk.
Latent Topic Entropy (LTE), for a given term ti can be
calculated from the topic distribution P(Tk  |ti):
</bodyText>
<equation confidence="0.995447333333333">
K
LTE(ti) = − X P(Tk  |ti) log P(Tk  |ti), (3)
k=1
</equation>
<bodyText confidence="0.9999935">
where the topic distribution P(Tk  |ti) can be estimated
from PLSA. LTE(ti) is a measure of how the term ti is
focused on a few topics, so a lower latent topic entropy
implies the term carries more topical information.
</bodyText>
<subsectionHeader confidence="0.998669">
2.2 Statistical Measures of a Term
</subsectionHeader>
<bodyText confidence="0.999948">
The statistical measure of a term ti, s(ti, d) in (1) can be
defined in terms of LTE(ti) in (3) as
</bodyText>
<equation confidence="0.9735925">
s(ti, d) = y n(ti, d), (4)
LTE(ti)
</equation>
<bodyText confidence="0.999663285714286">
where y is a scaling factor such that 0 &lt; s(ti, d) &lt; 1; the
score s(ti, d) is inversely proportion to the latent topic
entropy LTE(ti). Some works (Kong and Lee, 2011)
showed that the use in (1) of s(ti, d) as defined in (4) out-
performed the very successful “significance score” (Furui
et al., 2004) in speech summarization; then, we use it as
the baseline.
</bodyText>
<subsectionHeader confidence="0.99922">
2.3 Similarity between Utterances
</subsectionHeader>
<bodyText confidence="0.999993">
Within a document d, we can first compute the probabil-
ity that the topic Tk is addressed by an utterance Ui:
</bodyText>
<equation confidence="0.996987">
P(Tk  |Ui) = PtEUi n(t, Ui)P(Tk  |t)
PtEUi n(t, Ui)
</equation>
<bodyText confidence="0.97746475">
Then an asymmetric topical similarity TopicSim(Ui, Uj)
for utterances Ui to Uj (with direction Ui → Uj) can
be defined by accumulating LTSt(Tk) in (2) weighted by
P(Tk  |Ui) for all terms t in Uj over all latent topics:
</bodyText>
<equation confidence="0.616812">
X
TopicSim(Ui, Uj) =
</equation>
<bodyText confidence="0.987061">
where the idea is very similar to the generative probability
in IR. We call it generative significance of Ui given Uj.
Within a document d, the lexical similarity is the mea-
sure of word overlap between the utterance Ui and Uj.
We compute LexSim(Ui, Uj) as the cosine similarity be-
tween two TF-IDF vectors from Ui and Uj like well-
known LexRank (Erkan and Radev, 2004). Note that
LexSim(Ui, Uj) = LexSim(Uj, Ui)
</bodyText>
<subsectionHeader confidence="0.947098">
2.4 Intra-Speaker Topic Modeling
</subsectionHeader>
<bodyText confidence="0.9998866">
We assume a single speaker usually focuses on similar
topics, so if an utterance is important, the scores of the
utterances from the same speaker should be increased.
Then we increase the similarity between the utterances
from the same speaker to share the topics:
</bodyText>
<equation confidence="0.933131434782609">
TopicSim(Ui, Uj)1+w
, if Ui E Sk and Uj E Sk
TopicSim(Ui, Uj)1−w
, otherwise
U1
U2
U3
A4t U3, U5, U6
B4t U1, U3, U6
pt 3, 4
pt 4, 3
U4
U5
U6
. (5)
K
X
tEUj k=1
LTSt(Tk)P(Tk  |Ui),
TopicSimk(Ui, Uj) =
⎧
⎨⎪⎪
⎪⎪⎩
</equation>
<page confidence="0.983571">
378
</page>
<bodyText confidence="0.9999866">
where Sk is the set including all utterances from speaker
k, and w is a weighting parameter for modeling the
speaker relation, which means the level of coherence of
topics within a single speaker. Here the topics from the
same speaker can partially shared.
</bodyText>
<subsectionHeader confidence="0.853809">
2.5 Integrated Random Walk
</subsectionHeader>
<bodyText confidence="0.993986142857143">
We modify random walk (Hsu and Kennedy, 2007; Chen
et al., 2011) to integrate two types of similarity over the
graph obtained above. v(i) is the new score for node Ui,
which is the interpolation of three scores, the normalized
initial importance r(i) for node Ui and the score con-
tributed by all neighboring nodes Uj of node Ui weighted
by pt(j, i) and pl(j, i),
</bodyText>
<equation confidence="0.985953666666667">
v(i) = (1 − α − β)r(i) (8)
�+ α pt(j, i)v(j) + β � pl(j, i)v(j),
UjEBS UjEBS
</equation>
<bodyText confidence="0.99987825">
where α and β are the interpolation weights, Bti is the set
of neighbors connected to node Ui via topical incoming
edges, Bli is the set of neighbors connected to node Ui via
lexical incoming edges, and
</bodyText>
<equation confidence="0.997995">
r(i) = I (Ui, d) (9)
( EUj I(Uj, d)
</equation>
<bodyText confidence="0.999352166666667">
is normalized importance scores of utterance Ui, I(Ui, d)
in (1). We normalize topical similarity by the total sim-
ilarity summed over the set of outgoing edges, to pro-
duce the weight pt(j, i) for the edge from Uj to Ui on the
graph. Similarly, pl(j, i) is normalized in lexical edges.
(8) can be iteratively solved with the approach very
similar to that for the PageRank problem (Page et al.,
1998). Let v = [v(i), i = 1, 2,..., L]T and r = [r(i), i =
1, 2,..., L]T be the column vectors for v(i) and r(i) for all
utterances in the document, where L is the total number
of utterances in the document d and T represents trans-
pose. (8) then has a vector form below,
</bodyText>
<equation confidence="0.99298">
v = (1 − α − β)r + αPtv + βPlv (10)
= ((1 − α − β)reT + αPt + βPl) v = P&apos;v,
</equation>
<bodyText confidence="0.999957">
where Pt and Pl are LxL matrices of pt(j, i) and pl(j, i)
respectively, and e = [1, 1, ...,1]T. It has been shown
that the solution v of (10) is the dominant eigenvector
of P&apos; (Langville and Meyer, 2006), or the eigenvector
corresponding to the largest absolute eigenvalue of P&apos;.
The solution v(i) can then be obtained.
</bodyText>
<sectionHeader confidence="0.999603" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.987182">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999722173913044">
The corpus used in this research consists of a sequence of
naturally occuring meetings, which featured largely over-
lapping participant sets and topics of discussion. For each
meeting, SmartNotes (Banerjee and Rudnicky, 2008) was
used to record both the audio from each participant as
well as his notes. The meetings were transcribed both
manually and using a speech recognizer; the word error
rate is around 44%. In this paper we use 10 meetings held
from April to June of 2006. On average each meeting had
about 28 minutes of speech. Across these 10 meetings
there were 6 unique participants; each meeting featured
between 2 and 4 of these participants (average: 3.7). The
total number of utterances is 9837 across 10 meetings. In
this paper, we separate dev set (2 meetings) and test set
(8 meetings). Dev set is used to tune the parameters such
as α, β, w.
The reference summaries are given by the set of “note-
worthy utterances”: two annotators manually labelled the
degree (three levels) of “noteworthiness” for each utter-
ance, and we extract the utterances with the top level of
“noteworthiness” to form the summary of each meeting.
In the following experiments, for each meeting, we ex-
tract the top 30% number of terms as the summary.
</bodyText>
<subsectionHeader confidence="0.993877">
3.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999972375">
Automated evaluation utilizes the standard DUC eval-
uation metric ROUGE (Lin, 2004) which represents
recall over various n-grams statistics from a system-
generated summary against a set of human generated peer
summaries. F-measures for ROUGE-1 (unigram) and
ROUGE-L (longest common subsequence) can be eval-
uated in exactly the same way, which are used in the fol-
lowing results.
</bodyText>
<subsectionHeader confidence="0.900009">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.998738">
Table 1 shows the performance achieved by all proposed
approaches. In these experiments, the damping factor,
(1 − α − β) in (8), is empirically set to 0.1. Row (a)
is the baseline, which use LTE-based statistical measure
to compute the importance of utterances I(U,d). Row
</bodyText>
<listItem confidence="0.568092">
(b) is the result only considering lexical similarity; row
(c) only uses topical similarity. Row (d) are the re-
sults additionally including speaker information such as
TopicSim&apos;(Ui, Uj). Row (e) is the result performed by
integrated random walk (with α =� 0 and β =� 0) using
parameters that have been optimized on the dev set.
</listItem>
<subsectionHeader confidence="0.851549">
3.3.1 Graph-Based Approach
</subsectionHeader>
<bodyText confidence="0.999951444444445">
We can see the performance after graph-based re-
computation, shown in rows (b) and (c), is significantly
better than the baseline, shown in row (a), for both ASR
and manual transcripts. For ASR transcripts, topical sim-
ilarity and lexical similarity give similar results. For man-
ual transcripts, topical similarity performs slightly worse
than lexical similarity, because manual transcripts don’t
contain the recognition errors, and therefore word overlap
can accurately measure the similarity between two utter-
</bodyText>
<page confidence="0.99665">
379
</page>
<table confidence="0.9997925">
F-measure ASR Transcripts Manual Transcripts
ROUGE-1 ROUGE-L ROUGE-1 ROUGE-L
Baseline: LTE 46.816 46.256 44.987 44.162
LexSim (α = 0, 0 = 0.9) 48.940 48.504 46.540 45.858
TopicSim (α = 0.9, 0 = 0) 49.058 48.436 46.199 45.392
Intra-Speaker TopicSim 49.212 48.351 47.104 46.299
Integrated Random Walk 49.792 49.156 46.714 46.064
MAX RI +6.357 +6.269 +4.706 +4.839
</table>
<tableCaption confidence="0.991162">
Table 1: Maximum relative improvement (RI) with respect to the baseline for all proposed approaches (%).
</tableCaption>
<table confidence="0.7737415">
α 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0
β 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
</table>
<figureCaption confidence="0.977946333333333">
Figure 2: The performance from integrated random walk with
different combination weights, α and ,3 (α + ,3 = 0.9 in all
cases) for ASR transcripts.
</figureCaption>
<bodyText confidence="0.9997744">
ances. However, for ASR transcripts, although topical
similarity is not as accurate as lexical similarity, it can
compensate for recognition errors, so that the approaches
have similar performance. Thus, graph-based approaches
can significantly improve the baseline results.
</bodyText>
<subsectionHeader confidence="0.996708">
3.3.2 Effectiveness of Intra-Speaker Modeling
</subsectionHeader>
<bodyText confidence="0.999745166666667">
We find that modeling intra-speaker topics can improve
the performance (row (c) and row (d)), which means
speaker information is useful to model the topical simi-
larity. The experiment shows intra-speaker modeling can
help us include the important utterances for both ASR
and manual transcripts.
</bodyText>
<subsectionHeader confidence="0.988767">
3.3.3 Integration of Topical and Lexical Similarity
</subsectionHeader>
<bodyText confidence="0.999397727272727">
Row (e) shows the result of the proposed approach,
which integrates topical and lexical similarity into a sin-
gle graph, considering two types of relations together.
For ASR transcripts, row (e) is better than row (b) and
row (d), which means topical similarity and lexical sim-
ilarity can model different types of relations, because of
recognition errors. Figure 2 shows the sensitivity of the
combination weights for integrated random walk. We can
see topical similarity and lexical similarity are additive,
i.e. they can compensate each other, improving the per-
formance by integrating two types of edges in a single
graph. Note that the exact values of α and 0 do not mat-
ter so much for the performance.
For manual transcripts, row (e) cannot perform better
by combing two types of similarity, which means topical
similarity can dominate lexical similarity, since without
recognition errors topical similarity can model the rela-
tions accurately and additionally modeling intra-speaker
topics can effectively improve the performance.
In addition, Banerjee and Rudnicky (2008) used su-
pervised learning to detect noteworthy utterances on the
same corpus, and achieved ROGURE-1 scores of around
43% for ASR, and 47% for manual transcriptions. Our
unsupervised approach performs better, especially for
ASR transcripts.
Note that the performance on ASR is better than on
manual transcripts. Because a higher percentage of
recognition errors occurs on “unimportant” words, these
words tend to receive lower scores; we can then exclude
the utterances with more errors, and achieve better sum-
marization results. Other recent work has also demon-
strated better performance for ASR than manual tran-
scripts (Chen et al., 2011; Kong and Lee, 2011).
</bodyText>
<sectionHeader confidence="0.995609" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999963714285714">
Extensive experiments and evaluation with ROUGE met-
rics showed that intra-speaker topics can be modeled
in topical similarity and that integrated random walk
can combine the advantages from two types of edges
for imperfect ASR transcripts, where we achieved more
than 6% relative improvement. We plan to model inter-
speaker topics in the graph-based approach in the future.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998110125">
The first author was supported by the Institute of Edu-
cation Science, U.S. Department of Education, through
Grants R305A080628 to Carnegie Mellon University.
Any opinions, findings, and conclusions or recommen-
dations expressed in this publication are those of the au-
thors and do not necessarily reflect the views or official
policies, either expressed or implied of the Institute or
the U.S. Department of Education.
</bodyText>
<figure confidence="0.992178">
F-measure
50
48
49.5
49
48.5
ROUGE-1
ROUGE-L
</figure>
<page confidence="0.985535">
380
</page>
<sectionHeader confidence="0.994623" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975729787234">
Banerjee, S. and Rudnicky, A. I. 2008. An extractive-
summarizaion baseline for the automatic detection of note-
worthy utterances in multi-party human-human dialog. Proc.
of SLT.
Chen, Y.-N., Huang, Y., Yeh, C.-F., and Lee, L.-S. 2011. Spo-
ken lecture summarization by random walk over a graph con-
structed with automatically extracted key terms. Proc. of In-
terSpeech.
Erkan, G. and D. R. Radev., D. R. 2004. LexRank: Graph-
based lexical centrality as salience in text summarization.
Journal of Artificial Intelligence Research, Vol. 22.
Furui, S., Kikuchi, T., Shinnaka, Y., and Hori, C. 2004.
Speech-to-text and speech-to-speech summarization of spon-
taneous speech. IEEE Trans. on Speech and Audio Process-
ing.
Garg, N., Favre, B., Reidhammer, K., and Hakkani-T¨ur 2009.
ClusterRank: A graph based method for meeting summariza-
tion. Proc. of InterSpeech.
Gillick, D. J. 2011. The elements of automatic summarization.
PhD thesis, EECS, UC Berkeley.
Glass J., Hazen, T. J., Cyphers, S., Malioutov, I., Huynh, D.,
and Barzilay, R. 2007. Recent progress in the MIT spoken
lecture processing project. Proc. of InterSpeech.
Hofmann, T. 1999. Probabilistic latent semantic indexing.
Proc. of SIGIR.
Hsu, W. and Kennedy, L. 2007. Video search reranking through
random walk over document-level context graph. Proc. of
MM.
Kong, S.-Y. and Lee, L.-S. 2011. Semantic analysis and orga-
nization of spoken documents based on parameters derived
from latent topics. IEEE Trans. on Audio, Speech and Lan-
guage Processing, 19(7): 1875-1889.
Langville, A. and Meyer, C. 2005. A survey of eigenvector
methods for web information retrieval. SIAM Review.
Lee, L.-S. and Chen, B. 2005. Spoken document understanding
and organization. IEEE Signal Processing Magazine.
Lin, C. 2004. Rouge: A package for automatic evaluation
of summaries. Proc. of Workshop on Text Summarization
Branches Out.
Liu, F. and Liu, Y. 2010. Using spoken utterance compression
for meeting summarization: A pilot study. Proc. of SLT.
Liu Y., Xie, S., and Liu, F. 2010. Using N-best recognition
output for extractive summarization and keyword extraction
in meeting speech. Proc. of ICASSP.
Page, L., Brin, S., Motwani, R., Winograd, T. 1998. The pager-
ank citation ranking: bringing order to the web. Technical
Report, Stanford Digital Library Technologies Project.
</reference>
<page confidence="0.998704">
381
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.802786">
<title confidence="0.917156">Intra-Speaker Topic Modeling for Improved Multi-Party Meeting Summarization with Integrated Random Walk</title>
<author confidence="0.99505">Yun-Nung Chen</author>
<author confidence="0.99505">Florian</author>
<affiliation confidence="0.999522">School of Computer Science, Carnegie Mellon</affiliation>
<address confidence="0.999631">5000 Forbes Avenue, Pittsburgh, PA 15213-3891,</address>
<abstract confidence="0.997950882352941">This paper proposes an improved approach to extractive summarization of spoken multi-party interaction, in which integrated random walk is performed on a graph constructed on topical/ lexical relations. Each utterance is represented as a node of the graph, and the edges’ weights are computed from the topical similarity between the utterances, evaluated using probabilistic latent semantic analysis (PLSA), and from word overlap. We model intra-speaker topics by partially sharing the topics from the same speaker in the graph. In this paper, we perform experiments on automatically and manually generated transcripts. For automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A I Rudnicky</author>
</authors>
<title>An extractivesummarizaion baseline for the automatic detection of noteworthy utterances in multi-party human-human dialog.</title>
<date>2008</date>
<booktitle>Proc. of SLT.</booktitle>
<contexts>
<context position="1499" citStr="Banerjee and Rudnicky, 2008" startWordPosition="216" endWordPosition="219">ly generated transcripts. For automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances. 1 Introduction Speech summarization is an active and important topic of research (Lee and Chen, 2005), because multimedia/ spoken documents are more difficult to browse than text or image content. While earlier work was focused primarily on broadcast news content, recent effort has been increasingly directed to new domains such as lectures (Glass et al., 2007; Chen et al., 2011) and multi-party interaction (Banerjee and Rudnicky, 2008; Liu and Liu, 2010). We describe experiments on multi-party interaction found in meeting recordings, performing extractive summarization (Liu et al., 2010) on transcripts generated by automatic speech recognition (ASR) and human annotators. Graph-based methods for computing lexical centrality as importance to extract summaries (Erkan and Radev, 2004) have been investigated in the context of text summarization. Some works focus on maximizing coverage of summaries using the objective function (Gillick, 2011). Speech summarization carries intrinsic difficulties due to the presence of recognition</context>
<context position="10509" citStr="Banerjee and Rudnicky, 2008" startWordPosition="1800" endWordPosition="1803">+ αPtv + βPlv (10) = ((1 − α − β)reT + αPt + βPl) v = P&apos;v, where Pt and Pl are LxL matrices of pt(j, i) and pl(j, i) respectively, and e = [1, 1, ...,1]T. It has been shown that the solution v of (10) is the dominant eigenvector of P&apos; (Langville and Meyer, 2006), or the eigenvector corresponding to the largest absolute eigenvalue of P&apos;. The solution v(i) can then be obtained. 3 Experiments 3.1 Corpus The corpus used in this research consists of a sequence of naturally occuring meetings, which featured largely overlapping participant sets and topics of discussion. For each meeting, SmartNotes (Banerjee and Rudnicky, 2008) was used to record both the audio from each participant as well as his notes. The meetings were transcribed both manually and using a speech recognizer; the word error rate is around 44%. In this paper we use 10 meetings held from April to June of 2006. On average each meeting had about 28 minutes of speech. Across these 10 meetings there were 6 unique participants; each meeting featured between 2 and 4 of these participants (average: 3.7). The total number of utterances is 9837 across 10 meetings. In this paper, we separate dev set (2 meetings) and test set (8 meetings). Dev set is used to t</context>
<context position="15520" citStr="Banerjee and Rudnicky (2008)" startWordPosition="2604" endWordPosition="2607"> see topical similarity and lexical similarity are additive, i.e. they can compensate each other, improving the performance by integrating two types of edges in a single graph. Note that the exact values of α and 0 do not matter so much for the performance. For manual transcripts, row (e) cannot perform better by combing two types of similarity, which means topical similarity can dominate lexical similarity, since without recognition errors topical similarity can model the relations accurately and additionally modeling intra-speaker topics can effectively improve the performance. In addition, Banerjee and Rudnicky (2008) used supervised learning to detect noteworthy utterances on the same corpus, and achieved ROGURE-1 scores of around 43% for ASR, and 47% for manual transcriptions. Our unsupervised approach performs better, especially for ASR transcripts. Note that the performance on ASR is better than on manual transcripts. Because a higher percentage of recognition errors occurs on “unimportant” words, these words tend to receive lower scores; we can then exclude the utterances with more errors, and achieve better summarization results. Other recent work has also demonstrated better performance for ASR than</context>
</contexts>
<marker>Banerjee, Rudnicky, 2008</marker>
<rawString>Banerjee, S. and Rudnicky, A. I. 2008. An extractivesummarizaion baseline for the automatic detection of noteworthy utterances in multi-party human-human dialog. Proc. of SLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-N Chen</author>
<author>Y Huang</author>
<author>C-F Yeh</author>
<author>L-S Lee</author>
</authors>
<title>Spoken lecture summarization by random walk over a graph constructed with automatically extracted key terms.</title>
<date>2011</date>
<booktitle>Proc. of InterSpeech.</booktitle>
<contexts>
<context position="1442" citStr="Chen et al., 2011" startWordPosition="208" endWordPosition="211"> perform experiments on automatically and manually generated transcripts. For automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances. 1 Introduction Speech summarization is an active and important topic of research (Lee and Chen, 2005), because multimedia/ spoken documents are more difficult to browse than text or image content. While earlier work was focused primarily on broadcast news content, recent effort has been increasingly directed to new domains such as lectures (Glass et al., 2007; Chen et al., 2011) and multi-party interaction (Banerjee and Rudnicky, 2008; Liu and Liu, 2010). We describe experiments on multi-party interaction found in meeting recordings, performing extractive summarization (Liu et al., 2010) on transcripts generated by automatic speech recognition (ASR) and human annotators. Graph-based methods for computing lexical centrality as importance to extract summaries (Erkan and Radev, 2004) have been investigated in the context of text summarization. Some works focus on maximizing coverage of summaries using the objective function (Gillick, 2011). Speech summarization carries </context>
<context position="4513" citStr="Chen et al., 2011" startWordPosition="693" endWordPosition="696">nces. 1 I(U, d) = n �n i=1 377 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 377–381, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics Figure 1: A simplified example of the graph considered. We formulate the utterance selection problem as random walk on a directed graph, in which each utterance is a node and the edges between them are weighted by topical and lexical similarity. The basic idea is that an utterance similar to more important utterances should be more important (Chen et al., 2011). We formulate two types of directed edge, topical edges and lexical edges, which are weighted by topical and lexical similarity respectively. We then keep only the top N outgoing edges with the highest weights from each node, while consider incoming edges to each node for importance propagation in the graph. A simplified example for such a graph with topical edges is in Figure 1, in which Ati and Bti are the sets of neighbors of the node Ui connected respectively by outgoing and incoming topical edges. 2.1 Parameters from PLSA Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) has </context>
<context position="8585" citStr="Chen et al., 2011" startWordPosition="1432" endWordPosition="1435">he utterances from the same speaker to share the topics: TopicSim(Ui, Uj)1+w , if Ui E Sk and Uj E Sk TopicSim(Ui, Uj)1−w , otherwise U1 U2 U3 A4t U3, U5, U6 B4t U1, U3, U6 pt 3, 4 pt 4, 3 U4 U5 U6 . (5) K X tEUj k=1 LTSt(Tk)P(Tk |Ui), TopicSimk(Ui, Uj) = ⎧ ⎨⎪⎪ ⎪⎪⎩ 378 where Sk is the set including all utterances from speaker k, and w is a weighting parameter for modeling the speaker relation, which means the level of coherence of topics within a single speaker. Here the topics from the same speaker can partially shared. 2.5 Integrated Random Walk We modify random walk (Hsu and Kennedy, 2007; Chen et al., 2011) to integrate two types of similarity over the graph obtained above. v(i) is the new score for node Ui, which is the interpolation of three scores, the normalized initial importance r(i) for node Ui and the score contributed by all neighboring nodes Uj of node Ui weighted by pt(j, i) and pl(j, i), v(i) = (1 − α − β)r(i) (8) �+ α pt(j, i)v(j) + β � pl(j, i)v(j), UjEBS UjEBS where α and β are the interpolation weights, Bti is the set of neighbors connected to node Ui via topical incoming edges, Bli is the set of neighbors connected to node Ui via lexical incoming edges, and r(i) = I (Ui, d) (9) </context>
<context position="16158" citStr="Chen et al., 2011" startWordPosition="2703" endWordPosition="2706">ning to detect noteworthy utterances on the same corpus, and achieved ROGURE-1 scores of around 43% for ASR, and 47% for manual transcriptions. Our unsupervised approach performs better, especially for ASR transcripts. Note that the performance on ASR is better than on manual transcripts. Because a higher percentage of recognition errors occurs on “unimportant” words, these words tend to receive lower scores; we can then exclude the utterances with more errors, and achieve better summarization results. Other recent work has also demonstrated better performance for ASR than manual transcripts (Chen et al., 2011; Kong and Lee, 2011). 4 Conclusion and Future Work Extensive experiments and evaluation with ROUGE metrics showed that intra-speaker topics can be modeled in topical similarity and that integrated random walk can combine the advantages from two types of edges for imperfect ASR transcripts, where we achieved more than 6% relative improvement. We plan to model interspeaker topics in the graph-based approach in the future. Acknowledgements The first author was supported by the Institute of Education Science, U.S. Department of Education, through Grants R305A080628 to Carnegie Mellon University. </context>
</contexts>
<marker>Chen, Huang, Yeh, Lee, 2011</marker>
<rawString>Chen, Y.-N., Huang, Y., Yeh, C.-F., and Lee, L.-S. 2011. Spoken lecture summarization by random walk over a graph constructed with automatically extracted key terms. Proc. of InterSpeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
<author>D R</author>
</authors>
<title>LexRank: Graphbased lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<marker>Erkan, Radev, R, 2004</marker>
<rawString>Erkan, G. and D. R. Radev., D. R. 2004. LexRank: Graphbased lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, Vol. 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
<author>T Kikuchi</author>
<author>Y Shinnaka</author>
<author>C Hori</author>
</authors>
<title>Speech-to-text and speech-to-speech summarization of spontaneous speech.</title>
<date>2004</date>
<journal>IEEE Trans. on Speech and Audio Processing.</journal>
<contexts>
<context position="2231" citStr="Furui et al., 2004" startWordPosition="324" endWordPosition="327">extractive summarization (Liu et al., 2010) on transcripts generated by automatic speech recognition (ASR) and human annotators. Graph-based methods for computing lexical centrality as importance to extract summaries (Erkan and Radev, 2004) have been investigated in the context of text summarization. Some works focus on maximizing coverage of summaries using the objective function (Gillick, 2011). Speech summarization carries intrinsic difficulties due to the presence of recognition errors, spontaneous speech effect, and lack of segmentation. A general approach has been found very successful (Furui et al., 2004), in which each utterance in the document d, U = t1t2...ti...tn, represented as a sequence of terms ti, is given an importance score: [λ1s(ti, d) + λ2l(ti) (1) + λ3c(ti) + λ4g(ti)] + λ5b(U), where s(ti, d), l(ti), c(ti), g(ti) are respectively some statistical measure (such as TF-IDF), linguistic measure (e.g., different part-of-speech tags are given different weights), confidence score and N-gram score for the term ti, and b(U) is calculated from the grammatical structure of the utterance U, and λ1, λ2, λ3, λ4 and λ5 are weighting parameters. For each document, the utterances to be used in th</context>
<context position="6824" citStr="Furui et al., 2004" startWordPosition="1113" endWordPosition="1116">LTE(ti) is a measure of how the term ti is focused on a few topics, so a lower latent topic entropy implies the term carries more topical information. 2.2 Statistical Measures of a Term The statistical measure of a term ti, s(ti, d) in (1) can be defined in terms of LTE(ti) in (3) as s(ti, d) = y n(ti, d), (4) LTE(ti) where y is a scaling factor such that 0 &lt; s(ti, d) &lt; 1; the score s(ti, d) is inversely proportion to the latent topic entropy LTE(ti). Some works (Kong and Lee, 2011) showed that the use in (1) of s(ti, d) as defined in (4) outperformed the very successful “significance score” (Furui et al., 2004) in speech summarization; then, we use it as the baseline. 2.3 Similarity between Utterances Within a document d, we can first compute the probability that the topic Tk is addressed by an utterance Ui: P(Tk |Ui) = PtEUi n(t, Ui)P(Tk |t) PtEUi n(t, Ui) Then an asymmetric topical similarity TopicSim(Ui, Uj) for utterances Ui to Uj (with direction Ui → Uj) can be defined by accumulating LTSt(Tk) in (2) weighted by P(Tk |Ui) for all terms t in Uj over all latent topics: X TopicSim(Ui, Uj) = where the idea is very similar to the generative probability in IR. We call it generative significance of Ui</context>
</contexts>
<marker>Furui, Kikuchi, Shinnaka, Hori, 2004</marker>
<rawString>Furui, S., Kikuchi, T., Shinnaka, Y., and Hori, C. 2004. Speech-to-text and speech-to-speech summarization of spontaneous speech. IEEE Trans. on Speech and Audio Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Garg</author>
<author>B Favre</author>
<author>K Reidhammer</author>
<author>Hakkani-T¨ur</author>
</authors>
<title>ClusterRank: A graph based method for meeting summarization.</title>
<date>2009</date>
<booktitle>Proc. of InterSpeech.</booktitle>
<marker>Garg, Favre, Reidhammer, Hakkani-T¨ur, 2009</marker>
<rawString>Garg, N., Favre, B., Reidhammer, K., and Hakkani-T¨ur 2009. ClusterRank: A graph based method for meeting summarization. Proc. of InterSpeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Gillick</author>
</authors>
<title>The elements of automatic summarization.</title>
<date>2011</date>
<tech>PhD thesis,</tech>
<institution>EECS, UC Berkeley.</institution>
<contexts>
<context position="2011" citStr="Gillick, 2011" startWordPosition="292" endWordPosition="293">tures (Glass et al., 2007; Chen et al., 2011) and multi-party interaction (Banerjee and Rudnicky, 2008; Liu and Liu, 2010). We describe experiments on multi-party interaction found in meeting recordings, performing extractive summarization (Liu et al., 2010) on transcripts generated by automatic speech recognition (ASR) and human annotators. Graph-based methods for computing lexical centrality as importance to extract summaries (Erkan and Radev, 2004) have been investigated in the context of text summarization. Some works focus on maximizing coverage of summaries using the objective function (Gillick, 2011). Speech summarization carries intrinsic difficulties due to the presence of recognition errors, spontaneous speech effect, and lack of segmentation. A general approach has been found very successful (Furui et al., 2004), in which each utterance in the document d, U = t1t2...ti...tn, represented as a sequence of terms ti, is given an importance score: [λ1s(ti, d) + λ2l(ti) (1) + λ3c(ti) + λ4g(ti)] + λ5b(U), where s(ti, d), l(ti), c(ti), g(ti) are respectively some statistical measure (such as TF-IDF), linguistic measure (e.g., different part-of-speech tags are given different weights), confide</context>
</contexts>
<marker>Gillick, 2011</marker>
<rawString>Gillick, D. J. 2011. The elements of automatic summarization. PhD thesis, EECS, UC Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
<author>T J Hazen</author>
<author>S Cyphers</author>
<author>I Malioutov</author>
<author>D Huynh</author>
<author>R Barzilay</author>
</authors>
<title>Recent progress in the MIT spoken lecture processing project.</title>
<date>2007</date>
<booktitle>Proc. of InterSpeech.</booktitle>
<contexts>
<context position="1422" citStr="Glass et al., 2007" startWordPosition="204" endWordPosition="207">h. In this paper, we perform experiments on automatically and manually generated transcripts. For automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances. 1 Introduction Speech summarization is an active and important topic of research (Lee and Chen, 2005), because multimedia/ spoken documents are more difficult to browse than text or image content. While earlier work was focused primarily on broadcast news content, recent effort has been increasingly directed to new domains such as lectures (Glass et al., 2007; Chen et al., 2011) and multi-party interaction (Banerjee and Rudnicky, 2008; Liu and Liu, 2010). We describe experiments on multi-party interaction found in meeting recordings, performing extractive summarization (Liu et al., 2010) on transcripts generated by automatic speech recognition (ASR) and human annotators. Graph-based methods for computing lexical centrality as importance to extract summaries (Erkan and Radev, 2004) have been investigated in the context of text summarization. Some works focus on maximizing coverage of summaries using the objective function (Gillick, 2011). Speech su</context>
</contexts>
<marker>Glass, Hazen, Cyphers, Malioutov, Huynh, Barzilay, 2007</marker>
<rawString>Glass J., Hazen, T. J., Cyphers, S., Malioutov, I., Huynh, D., and Barzilay, R. 2007. Recent progress in the MIT spoken lecture processing project. Proc. of InterSpeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>Proc. of SIGIR.</booktitle>
<contexts>
<context position="5108" citStr="Hofmann, 1999" startWordPosition="793" endWordPosition="794">(Chen et al., 2011). We formulate two types of directed edge, topical edges and lexical edges, which are weighted by topical and lexical similarity respectively. We then keep only the top N outgoing edges with the highest weights from each node, while consider incoming edges to each node for importance propagation in the graph. A simplified example for such a graph with topical edges is in Figure 1, in which Ati and Bti are the sets of neighbors of the node Ui connected respectively by outgoing and incoming topical edges. 2.1 Parameters from PLSA Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) has been widely used to analyze the semantics of documents based on a set of latent topics. Given a set of documents {dj, j = 1, 2, ..., J} and all terms {ti, i = 1, 2,..., M} they include, PLSA uses a set of latent topic variables, {Tk, k = 1, 2,..., K}, to characterize the “term-document” co-occurrence relationships. The PLSA model can be optimized with EM algorithm by maximizing a likelihood function. We utilize two parameters from PLSA, latent topic significance (LTS) and latent topic entropy (LTE) (Kong and Lee, 2011) in the paper. Latent Topic Significance (LTS) for a given term ti with</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, T. 1999. Probabilistic latent semantic indexing. Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hsu</author>
<author>L Kennedy</author>
</authors>
<title>Video search reranking through random walk over document-level context graph.</title>
<date>2007</date>
<booktitle>Proc. of MM.</booktitle>
<contexts>
<context position="8565" citStr="Hsu and Kennedy, 2007" startWordPosition="1428" endWordPosition="1431">he similarity between the utterances from the same speaker to share the topics: TopicSim(Ui, Uj)1+w , if Ui E Sk and Uj E Sk TopicSim(Ui, Uj)1−w , otherwise U1 U2 U3 A4t U3, U5, U6 B4t U1, U3, U6 pt 3, 4 pt 4, 3 U4 U5 U6 . (5) K X tEUj k=1 LTSt(Tk)P(Tk |Ui), TopicSimk(Ui, Uj) = ⎧ ⎨⎪⎪ ⎪⎪⎩ 378 where Sk is the set including all utterances from speaker k, and w is a weighting parameter for modeling the speaker relation, which means the level of coherence of topics within a single speaker. Here the topics from the same speaker can partially shared. 2.5 Integrated Random Walk We modify random walk (Hsu and Kennedy, 2007; Chen et al., 2011) to integrate two types of similarity over the graph obtained above. v(i) is the new score for node Ui, which is the interpolation of three scores, the normalized initial importance r(i) for node Ui and the score contributed by all neighboring nodes Uj of node Ui weighted by pt(j, i) and pl(j, i), v(i) = (1 − α − β)r(i) (8) �+ α pt(j, i)v(j) + β � pl(j, i)v(j), UjEBS UjEBS where α and β are the interpolation weights, Bti is the set of neighbors connected to node Ui via topical incoming edges, Bli is the set of neighbors connected to node Ui via lexical incoming edges, and r</context>
</contexts>
<marker>Hsu, Kennedy, 2007</marker>
<rawString>Hsu, W. and Kennedy, L. 2007. Video search reranking through random walk over document-level context graph. Proc. of MM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-Y Kong</author>
<author>L-S Lee</author>
</authors>
<title>Semantic analysis and organization of spoken documents based on parameters derived from latent topics.</title>
<date>2011</date>
<journal>IEEE Trans. on Audio, Speech and Language Processing,</journal>
<volume>19</volume>
<issue>7</issue>
<pages>1875--1889</pages>
<contexts>
<context position="5637" citStr="Kong and Lee, 2011" startWordPosition="886" endWordPosition="889">es. 2.1 Parameters from PLSA Probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) has been widely used to analyze the semantics of documents based on a set of latent topics. Given a set of documents {dj, j = 1, 2, ..., J} and all terms {ti, i = 1, 2,..., M} they include, PLSA uses a set of latent topic variables, {Tk, k = 1, 2,..., K}, to characterize the “term-document” co-occurrence relationships. The PLSA model can be optimized with EM algorithm by maximizing a likelihood function. We utilize two parameters from PLSA, latent topic significance (LTS) and latent topic entropy (LTE) (Kong and Lee, 2011) in the paper. Latent Topic Significance (LTS) for a given term ti with respect to a topic Tk can be defined as LTSti(Tk) = Pdj ED n(ti, dj)P(Tk |dj) , (2) PdjED n(ti,dj)[1 − P(Tk |dj)] where n(ti, dj) is the occurrence count of term ti in a document dj. Thus, a higher LTSti(Tk) indicates the term ti is more significant for the latent topic Tk. Latent Topic Entropy (LTE), for a given term ti can be calculated from the topic distribution P(Tk |ti): K LTE(ti) = − X P(Tk |ti) log P(Tk |ti), (3) k=1 where the topic distribution P(Tk |ti) can be estimated from PLSA. LTE(ti) is a measure of how the </context>
<context position="16179" citStr="Kong and Lee, 2011" startWordPosition="2707" endWordPosition="2710">worthy utterances on the same corpus, and achieved ROGURE-1 scores of around 43% for ASR, and 47% for manual transcriptions. Our unsupervised approach performs better, especially for ASR transcripts. Note that the performance on ASR is better than on manual transcripts. Because a higher percentage of recognition errors occurs on “unimportant” words, these words tend to receive lower scores; we can then exclude the utterances with more errors, and achieve better summarization results. Other recent work has also demonstrated better performance for ASR than manual transcripts (Chen et al., 2011; Kong and Lee, 2011). 4 Conclusion and Future Work Extensive experiments and evaluation with ROUGE metrics showed that intra-speaker topics can be modeled in topical similarity and that integrated random walk can combine the advantages from two types of edges for imperfect ASR transcripts, where we achieved more than 6% relative improvement. We plan to model interspeaker topics in the graph-based approach in the future. Acknowledgements The first author was supported by the Institute of Education Science, U.S. Department of Education, through Grants R305A080628 to Carnegie Mellon University. Any opinions, finding</context>
</contexts>
<marker>Kong, Lee, 2011</marker>
<rawString>Kong, S.-Y. and Lee, L.-S. 2011. Semantic analysis and organization of spoken documents based on parameters derived from latent topics. IEEE Trans. on Audio, Speech and Language Processing, 19(7): 1875-1889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Langville</author>
<author>C Meyer</author>
</authors>
<title>A survey of eigenvector methods for web information retrieval.</title>
<date>2005</date>
<journal>SIAM Review.</journal>
<marker>Langville, Meyer, 2005</marker>
<rawString>Langville, A. and Meyer, C. 2005. A survey of eigenvector methods for web information retrieval. SIAM Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L-S Lee</author>
<author>B Chen</author>
</authors>
<title>Spoken document understanding and organization.</title>
<date>2005</date>
<journal>IEEE Signal Processing Magazine.</journal>
<contexts>
<context position="1162" citStr="Lee and Chen, 2005" startWordPosition="162" endWordPosition="165">edges’ weights are computed from the topical similarity between the utterances, evaluated using probabilistic latent semantic analysis (PLSA), and from word overlap. We model intra-speaker topics by partially sharing the topics from the same speaker in the graph. In this paper, we perform experiments on automatically and manually generated transcripts. For automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances. 1 Introduction Speech summarization is an active and important topic of research (Lee and Chen, 2005), because multimedia/ spoken documents are more difficult to browse than text or image content. While earlier work was focused primarily on broadcast news content, recent effort has been increasingly directed to new domains such as lectures (Glass et al., 2007; Chen et al., 2011) and multi-party interaction (Banerjee and Rudnicky, 2008; Liu and Liu, 2010). We describe experiments on multi-party interaction found in meeting recordings, performing extractive summarization (Liu et al., 2010) on transcripts generated by automatic speech recognition (ASR) and human annotators. Graph-based methods f</context>
</contexts>
<marker>Lee, Chen, 2005</marker>
<rawString>Lee, L.-S. and Chen, B. 2005. Spoken document understanding and organization. IEEE Signal Processing Magazine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>Proc. of Workshop on Text Summarization Branches Out.</booktitle>
<contexts>
<context position="11631" citStr="Lin, 2004" startWordPosition="1995" endWordPosition="1996"> paper, we separate dev set (2 meetings) and test set (8 meetings). Dev set is used to tune the parameters such as α, β, w. The reference summaries are given by the set of “noteworthy utterances”: two annotators manually labelled the degree (three levels) of “noteworthiness” for each utterance, and we extract the utterances with the top level of “noteworthiness” to form the summary of each meeting. In the following experiments, for each meeting, we extract the top 30% number of terms as the summary. 3.2 Evaluation Metrics Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a systemgenerated summary against a set of human generated peer summaries. F-measures for ROUGE-1 (unigram) and ROUGE-L (longest common subsequence) can be evaluated in exactly the same way, which are used in the following results. 3.3 Results Table 1 shows the performance achieved by all proposed approaches. In these experiments, the damping factor, (1 − α − β) in (8), is empirically set to 0.1. Row (a) is the baseline, which use LTE-based statistical measure to compute the importance of utterances I(U,d). Row (b) is the result onl</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Lin, C. 2004. Rouge: A package for automatic evaluation of summaries. Proc. of Workshop on Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>Y Liu</author>
</authors>
<title>Using spoken utterance compression for meeting summarization: A pilot study.</title>
<date>2010</date>
<booktitle>Proc. of SLT.</booktitle>
<contexts>
<context position="1519" citStr="Liu and Liu, 2010" startWordPosition="220" endWordPosition="223"> automatic transcripts, our results show that intra-speaker topic sharing and integrating topical/ lexical relations can help include the important utterances. 1 Introduction Speech summarization is an active and important topic of research (Lee and Chen, 2005), because multimedia/ spoken documents are more difficult to browse than text or image content. While earlier work was focused primarily on broadcast news content, recent effort has been increasingly directed to new domains such as lectures (Glass et al., 2007; Chen et al., 2011) and multi-party interaction (Banerjee and Rudnicky, 2008; Liu and Liu, 2010). We describe experiments on multi-party interaction found in meeting recordings, performing extractive summarization (Liu et al., 2010) on transcripts generated by automatic speech recognition (ASR) and human annotators. Graph-based methods for computing lexical centrality as importance to extract summaries (Erkan and Radev, 2004) have been investigated in the context of text summarization. Some works focus on maximizing coverage of summaries using the objective function (Gillick, 2011). Speech summarization carries intrinsic difficulties due to the presence of recognition errors, spontaneous</context>
</contexts>
<marker>Liu, Liu, 2010</marker>
<rawString>Liu, F. and Liu, Y. 2010. Using spoken utterance compression for meeting summarization: A pilot study. Proc. of SLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>S Xie</author>
<author>F Liu</author>
</authors>
<title>Using N-best recognition output for extractive summarization and keyword extraction in meeting speech.</title>
<date>2010</date>
<booktitle>Proc. of ICASSP.</booktitle>
<contexts>
<context position="1655" citStr="Liu et al., 2010" startWordPosition="238" endWordPosition="241"> important utterances. 1 Introduction Speech summarization is an active and important topic of research (Lee and Chen, 2005), because multimedia/ spoken documents are more difficult to browse than text or image content. While earlier work was focused primarily on broadcast news content, recent effort has been increasingly directed to new domains such as lectures (Glass et al., 2007; Chen et al., 2011) and multi-party interaction (Banerjee and Rudnicky, 2008; Liu and Liu, 2010). We describe experiments on multi-party interaction found in meeting recordings, performing extractive summarization (Liu et al., 2010) on transcripts generated by automatic speech recognition (ASR) and human annotators. Graph-based methods for computing lexical centrality as importance to extract summaries (Erkan and Radev, 2004) have been investigated in the context of text summarization. Some works focus on maximizing coverage of summaries using the objective function (Gillick, 2011). Speech summarization carries intrinsic difficulties due to the presence of recognition errors, spontaneous speech effect, and lack of segmentation. A general approach has been found very successful (Furui et al., 2004), in which each utteranc</context>
</contexts>
<marker>Liu, Xie, Liu, 2010</marker>
<rawString>Liu Y., Xie, S., and Liu, F. 2010. Using N-best recognition output for extractive summarization and keyword extraction in meeting speech. Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Page</author>
<author>S Brin</author>
<author>R Motwani</author>
<author>T Winograd</author>
</authors>
<title>The pagerank citation ranking: bringing order to the web.</title>
<date>1998</date>
<tech>Technical Report,</tech>
<institution>Stanford Digital Library Technologies Project.</institution>
<contexts>
<context position="9598" citStr="Page et al., 1998" startWordPosition="1624" endWordPosition="1627">he interpolation weights, Bti is the set of neighbors connected to node Ui via topical incoming edges, Bli is the set of neighbors connected to node Ui via lexical incoming edges, and r(i) = I (Ui, d) (9) ( EUj I(Uj, d) is normalized importance scores of utterance Ui, I(Ui, d) in (1). We normalize topical similarity by the total similarity summed over the set of outgoing edges, to produce the weight pt(j, i) for the edge from Uj to Ui on the graph. Similarly, pl(j, i) is normalized in lexical edges. (8) can be iteratively solved with the approach very similar to that for the PageRank problem (Page et al., 1998). Let v = [v(i), i = 1, 2,..., L]T and r = [r(i), i = 1, 2,..., L]T be the column vectors for v(i) and r(i) for all utterances in the document, where L is the total number of utterances in the document d and T represents transpose. (8) then has a vector form below, v = (1 − α − β)r + αPtv + βPlv (10) = ((1 − α − β)reT + αPt + βPl) v = P&apos;v, where Pt and Pl are LxL matrices of pt(j, i) and pl(j, i) respectively, and e = [1, 1, ...,1]T. It has been shown that the solution v of (10) is the dominant eigenvector of P&apos; (Langville and Meyer, 2006), or the eigenvector corresponding to the largest absol</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Page, L., Brin, S., Motwani, R., Winograd, T. 1998. The pagerank citation ranking: bringing order to the web. Technical Report, Stanford Digital Library Technologies Project.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>