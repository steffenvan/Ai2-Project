<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014490">
<title confidence="0.915929">
ILR-Based MT Comprehension Test with Multi-Level Questions
</title>
<author confidence="0.9460235">
Douglas Jones, Martha Herzog, Hussny Ibrahim, Arvind Jairam, Wade Shen,
Edward Gibson and Michael Emonts
</author>
<affiliation confidence="0.910837">
MIT Lincoln Laboratory DLI Foreign Language Center MIT Brain and Cognitive
</affiliation>
<address confidence="0.5394055">
Lexington, MA 02420 Monterey, CA 93944 Sciences Department
{DAJ,Arvind,SWade}@LL.MIT.EDU {Hussny.Ibrahim,Michael.Emonts} Cambridge MA, 02139
</address>
<email confidence="0.987371">
MHerzog2005@comcast.net @monterey.army.mil EGibson@MIT.EDU
</email>
<sectionHeader confidence="0.997118" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99118585">
We present results from a new Interagency
Language Roundtable (ILR) based compre-
hension test. This new test design presents
questions at multiple ILR difficulty levels
within each document. We incorporated
Arabic machine translation (MT) output
from three independent research sites, arbi-
trarily merging these materials into one MT
condition. We contrast the MT condition,
for both text and audio data types, with high
quality human reference Gold Standard
(GS) translations. Overall, subjects
achieved 95% comprehension for GS and
74% for MT, across 4 genres and 3 diffi-
culty levels. Surprisingly, comprehension
rates do not correlate highly with translation
error rates, suggesting that we are measur-
ing an additional dimension of MT quality.
We observed that it takes 15% more time
overall to read MT than GS.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996424375">
The official Defense Language Proficiency Test
(DLPT) is constructed according to rigorous and
well-established principles that have been devel-
oped to measure the foreign language proficiency
of human language learners in U.S. Department of
Defense settings. In 2004, a variant of that test
type was constructed, following the general DLPT
design principles, but modified to measure the
quality of machine translation. This test, known as
the DLPTstar (Jones et al, 2005), was based on
authentic Arabic materials at ILR text difficulty
levels 1, 2, and 3, accompanied by constructed-
response questions at matching levels. The ILR
level descriptors, used throughout the U.S. gov-
ernment, can be found at the website cited in the
list of references. The text documents were pre-
</bodyText>
<page confidence="0.986823">
77
</page>
<bodyText confidence="0.975703777777778">
sented in two conditions in English translation: (1)
professionally translated into English, and (2) ma-
chine translated with state-of-the art MT systems,
often quite garbled. Results showed that native
readers of English could generally pass the Levels
1 and 2 questions on the test, but not those at Level
3. Also, Level 1 comprehension was less than ex-
pected, given the low level of the original material.
It was not known whether the weak Level 1 per-
formance was due to systematic deficits in MT
performance at Level 1, or whether the materials
were simply mismatched to the MT capabilities.
In this paper, we present a new variant of the
test, using materials specifically created to test the
capabilities of the MT systems. To guarantee that
the MT systems were up to the task of processing
the documents, we used the DARPA GALE 2006
evaluation data sets, against which several research
sites were testing MT algorithms. We arbitrarily
merged the MT output from three sites. The ILR
difficulty of the documents ranged from Level 2 to
Level 3, but the test did not contain any true Level
1 documents. To compensate for this lack, we
constructed questions about Level 1 elements (e.g.,
personal and place names) in Level 2 and 3 docu-
ments. A standard DLPT would have more varia-
tion at Level 1.
</bodyText>
<sectionHeader confidence="0.995683" genericHeader="introduction">
2 Related and Previous Work
</sectionHeader>
<bodyText confidence="0.999905818181818">
Earlier work in MT evaluation incorporated an in-
formativeness measure, based on comprehension
test answers, in addition to fluency, a measure of
output readability without reference to a gold stan-
dard, and adequacy, a measure of accuracy with
reference to a gold standard translation (White and
O&apos;Connell, 1994). Later MT evaluation found flu-
ency and adequacy to correlate well enough with
automatic measures (BLEU), and since compre-
hension tests are relatively more expensive to cre-
ate, the informativeness test was not used in later
</bodyText>
<note confidence="0.394282">
Proceedings of NAACL HLT 2007, Companion Volume, pages 77–80,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998992333333333">
MT evaluations, such as the ones performed by
NIST from 2001-2006. In other work, task-based
evaluation has been used for MT evaluation (Voss
and Tate, 2006), which measures human perform-
ance on exhaustively extracting ‘who’, ‘when’, and
‘where’ type elements in MT output. The DLPT-
star also uses this type of factual question, particu-
larly for Level 2 documents, but not exhaustively.
Instead, the test focuses on text elements most
characteristic of the levels as defined in the ILR
scale. At Level 3, for example, questions may
concern abstract concepts or hypotheses found in
the documents. Applying the ILR construct pro-
vides Defense Department decision makers with
test scores that are readily interpretable.
</bodyText>
<sectionHeader confidence="0.691554" genericHeader="method">
3 Test Construction and Administration
</sectionHeader>
<bodyText confidence="0.99998462962963">
In this paper, we present a new test, based entirely
on the DARPA GALE 2006 evaluation data, se-
lecting approximately half of the material for our
test. We selected twenty-four test documents, with
balanced coverage across four genres: newswire,
newsgroups, broadcast news and talk radio. Our
target was to have at least 2500 words for each
genre, which we exceeded slightly with approxi-
mately 12,200 words in total for the test. We be-
gan with a random selection of documents and
adjusted it for better topic coverage. We con-
structed an exhaustive set of questions for each
document, approximately 200 questions in total.
The questions ranged in ILR difficulty, from &amp;quot;0+,
1,1+, 2, 2+ and 3, with Levels 0+, 1 and 1+ com-
bined to a pseudo-level we called L1~, providing
four levels of difficulty to be measured. We di-
vided the questions into two sets, and each indi-
vidual subject answered questions for one of the
sets. The test itself was constructed by a DLPT
testing expert and a senior native-speaking Arabic
language instructor, using only the original Arabic
documents and the Gold Standard translations.
They had no access to any machine translation
output during the test construction or scoring.
In August 2006, we administered the test at MIT
to 49 test subjects who responded to announce-
ments for paid experimental subjects. The subjects
read the documents in a Latin square design, mean-
ing that each subject saw each document, but only
in one of the two conditions, randomly assigned.
Subjects were allowed 5 hours to complete the test.
Since the questions were divided into two sets for
each document, the actual set of 49 subjects
yielded approximately 25 “virtual subjects” read-
ing the full list of 228 questions. The mean time
spent on testing, not counting breaks or subject
orientation, was 2.5 hours; fastest was 1.1 hours,
slowest was 3.4 hours.
The subject responses were hand-graded by the
two testing experts, following the pre-established
answers in the test protocol. There was no pre-
assessment of whether information was preserved
or garbled in the MT when designing questions or
responses in the test protocol. The testing experts
were provided the reference translations and the
original Arabic documents, but not the MT during
scoring. Moreover, test conditions were masked in
order to provide a blind assessment. The two test-
ing experts provided both preliminary and final
scores; multiple passes provided an opportunity to
clarify the correct answers and to normalize scor-
ing. The scoring agreement rate was 96% for the
final scores.
</bodyText>
<sectionHeader confidence="0.995485" genericHeader="method">
4 Overall Results
</sectionHeader>
<bodyText confidence="0.999764428571429">
The overall result for comprehension accuracy was
95% for subjects reading the Gold Standard trans-
lation and 74% for reading Machine Translation,
across each of the genres and difficulty levels. The
comprehension accuracy for each genre is shown
in Figure 1. The two text genres score better than
the audio genres, which is to be expected because
the audio MT condition has more opportunities for
error. Within each modality, the more standard,
more structured genre fares better: newswire re-
sults are better than newsgroup results, and the
more structured genre of broadcast news scores
better than the less constrained, less structured
conversations present in the talk radio shows.
</bodyText>
<figureCaption confidence="0.999514">
Figure 1. Comprehension Accuracy per Genre
</figureCaption>
<figure confidence="0.986917928571429">
100%
80%
Overall Comprehension Accuracy
97% 93% 94% 94%
80% 77%
GS
MT
72%
66%
60%
40%
20%
0%
Newswire Newsgroups Broadcast News Talk Radio
</figure>
<page confidence="0.98845">
78
</page>
<bodyText confidence="0.999982153846154">
The break-down by ILR level of difficulty for each
question is shown in Figure 2. The general trend is
consistent with what has been observed previously
(Jones et al. 2005). The best results are at Level 2;
Level 1 does well but not as well as expected.
Thus the test has provided a key finding, which is
that MT systems perform more poorly on Level 1,
even when the data is matched to their capabilities.
Level 3 is very challenging for the MT condition,
and also more difficult in the GS condition. Using
a standard 70 percent passing threshold, responses
to questions on all MT documents, except for
Level 3, received a passing grade.
</bodyText>
<figureCaption confidence="0.999071">
Figure 2. Comprehension Accuracy per Level.
</figureCaption>
<bodyText confidence="0.999891875">
To provide a snapshot of the ILR levels: L1 in-
dicates sentence-level comprehensibility, and may
include factual local announcements, etc.; L2 indi-
cates paragraph-level comprehensibility; factual/
concrete, covering a wide spectrum of topics (poli-
tics, economy, society, culture, security, science);
L3 involves extended discourse comprehensibility;
the ability to understand hypotheses, supported
opinion, implications, and abstract linguistic for-
mulations, etc.
It was not possible to balance Level 3 documents
across genres within the GALE evaluation data;
except for those taken from Talk Radio, most
documents did not reach that level of complexity.
Hence, genre and difficulty level were not com-
pletely independent in this test.
</bodyText>
<sectionHeader confidence="0.993016" genericHeader="method">
5 Comprehension and Translation Error
</sectionHeader>
<bodyText confidence="0.9999656">
We expect to see a relationship between compre-
hension rates and translation error. In an idealized
case, we may expect a precise inverse correlation.
We then compared comprehension rates with Hu-
man Translation Error Rate (HTER), an error
measure for machine translation that counts the
number of human edits required to change system
MT output so that it contains all and only the in-
formation present in a Gold Standard reference
(NIST, 2006). The linear regression line in Figure
3 shows the kind of inverse correlation we might
expect. Subjects lose about 12% in comprehension
for every 10% of translation error. The R2 value is
33%. The low correlation suggests that the com-
prehension results are measuring a somewhat inde-
pendent aspect of MT quality, which we feel is
important. HTER does not directly address the
facts that not all MT errors are equally important
and that the texts contain inherent redundancy that
the readers use to answer the questions. For ex-
ploratory purposes, we divide the graph of Figure 3
into four quadrants. Quadrant I and IV contain
expected behavior: 122 data points of good transla-
tions and good comprehension results versus 43
points of bad translations and poor comprehension.
Q-II has 24 robust points: the translations have
high error, but somehow managed to contain
enough well-translated words that people can an-
swer the questions. Q-III has 28 fragile points: the
few translation errors impaired comprehension.
</bodyText>
<figure confidence="0.560715083333333">
y = Comprehension (DLPT*) (All Levels and Genres)
100% 122 (57 %) 24 points Q-II (Robust)
80% Q-I (Good) (10%)
points
60%
40%
20%
0%
Q-III (Fragile) Q-IV (Bad)
28 points (13%) 43 points (20%)
0% 20% 40% 60% 80% 100%
x = Translation Error (HTER)
</figure>
<figureCaption confidence="0.99918">
Figure 3. Comprehension vs. Translation Error.
</figureCaption>
<bodyText confidence="0.999902416666667">
We point out that there is a 1-to-1 mapping be-
tween comprehension questions and individual
sub-passages of the documents in the data. Each
point in Figure 3 plots the HTER of a single seg-
ment versus the average comprehension score on
the corresponding question. The good and bad
items are essentially a sanity-check on the experi-
mental design. We expect to see good comprehen-
sion when translations are good, and we expect to
see poor comprehension when translations are bad.
Next we will examine the two other types: fragile
and robust translations.
</bodyText>
<figure confidence="0.997378933333333">
L1~ L2 L2+ L3
100%
80%
60%
40%
20%
0%
76%
Overall Comprehension Accuracy
82%
51%
97% 96% 91% 88%
77%
GS
MT
</figure>
<page confidence="0.995041">
79
</page>
<bodyText confidence="0.999977666666667">
A fragile translation is one that has a good
HTER score but a bad comprehension score. A
sample fragile translation is one from a broadcast
news which asks for a particular name: the HTER
was a respectable 24%, but the MT comprehension
accuracy was a flat 0%, since the name was miss-
ing. Everyone reading GS answered correctly.
A robust translation is one that has a bad HTER
score but still manages to get a good comprehen-
sion score. A sample robust translation is one
drawn from a posting providing instructions for
foot massage. The text was quite garbled, with an
HTER score of 48%, but the MT comprehension
accuracy was a perfect 100%. Everyone reading
the GS condition also answered the question cor-
rectly, which was that one should start a foot mas-
sage with oil. We note in passing that the highest
error rate for a question with 100% comprehension
is about 50%, shown with the up-arrow in Figure
3. We should be surprised to see any items with
100% comprehension for HTER rates above 50%,
considering Shannon’s estimate that written Eng-
lish is about 50% redundant. We expect that MT
readers are making use of their general world
knowledge to interpret the garbled MT output. A
challenge is to identify robust translations, which
are useful despite their high translation error rate.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="method">
6 Detailed Discussion
</sectionHeader>
<bodyText confidence="0.99998253125">
In this section we will discuss several aspects of
the test in more detail: the scoring methodology,
including a discussion of partial credit and inter-
rater agreement; timing information; questions
about personal names.
Each correct answer was assigned a score of 1,
and each incorrect answer was assigned a score of
0. Partial credit was assigned on an ad-hoc basis,
but normalized for scoring by assigning all non-
integer scores to 0.5. This method yielded scores
that were generally at the midpoint between binary
scoring, in which non-integer scored were uni-
formly mapped either harshly to 0 or leniently to 1,
the average difference between harsh and lenient
scoring being approximately 11%. Inter-rater
agreement was 96%.
The testing infrastructure we used recorded the
amount of time spent on each document. The gen-
eral trend is that people spend longer on MT than
on GS. The mean percentage of time spent on MT
compared with GS is 115% per item, meaning that
it takes 15% more time to read MT than GS. The
standard error was 4%. The median is 111%;
minimum is 89% and maximum is 159%. In future
analysis and experimentation we will conduct more
fine-grained temporal estimates.
As we have seen in previous experiments, the
performance for personal names is lower than for
non-names. We observed that the name questions
have 71% comprehension accuracy, compared with
the 83% for questions about things other than per-
sonal names.
</bodyText>
<sectionHeader confidence="0.998693" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999992227272727">
We have long felt that Level 2 is the natural and
successful level for machine translation. The abil-
ity to present concrete factual information that can
be retrieved by the reader, without requirements
for understanding the style, tone, or organizational
pattern used by the writer seemed to be present in
the previous work. It is worth pointing out that
though we have many Level 1 questions, we are
still not really testing Level 1 because the test does
not contain true Level 1 documents. In future tests
we wish to include Level 1 documents and ques-
tions.
Continuing along these lines, we are currently
creating two new tests. We are constructing a new
Arabic DLPT-star test, tailoring the document se-
lection more specifically for comprehension testing
and ensuring texts and tasks are at the intended
ILR levels. We are also constructing a Mandarin
Chinese test with similar design specifications.
We intend for both of these tests to be available for
a public machine translation evaluation to be con-
ducted in 2007.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998812875">
Doddington, G. 2002. Automatic Evaluation of Machine
Translation Quality Using N-gram Co-Occurrence
Statistics. Proceedings of HLT 2002.
NIST 2006. GALE Go/No-Go Eval Plan; www.nist.gov/
speech/tests/gale/2006/doc/GALE06_evalplan.v2.pdf
Jones, D. A., W. Shen, et al. 2005a. Measuring Transla-
tion Quality by Testing English Speakers with a New
DLPT for Arabic. Int’l Conf. on Intel. Analysis.
Interagency Language Roundtable Website. 2005. ILR
Skill Level Descriptions: http://www.govtilr.org
Voss, Clare and Calandra Tate. 2006. Task-based
Evaluation of MT Engines. European Association for
Machine Translation conference.
White, JS and TA O&apos;Connell. 1994. Evaluation in the
ARPA machine translation program: 1993 method-
ology. Proceedings of the HLT workshop.
</reference>
<page confidence="0.998228">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.480246">
<title confidence="0.998785">ILR-Based MT Comprehension Test with Multi-Level Questions</title>
<author confidence="0.9928275">Douglas Jones</author>
<author confidence="0.9928275">Martha Herzog</author>
<author confidence="0.9928275">Hussny Ibrahim</author>
<author confidence="0.9928275">Arvind Jairam</author>
<author confidence="0.9928275">Wade Edward Gibson</author>
<author confidence="0.9928275">Michael Emonts</author>
<affiliation confidence="0.755766">MIT Lincoln Laboratory DLI Foreign Language Center MIT Brain and Cognitive Lexington, MA 02420 Monterey, CA 93944 Sciences Department</affiliation>
<address confidence="0.995554">{Hussny.Ibrahim,Michael.Emonts} MA, 02139</address>
<email confidence="0.978302">MHerzog2005@comcast.net@monterey.army.milEGibson@MIT.EDU</email>
<abstract confidence="0.997713666666666">We present results from a new Interagency Language Roundtable (ILR) based comprehension test. This new test design presents questions at multiple ILR difficulty levels within each document. We incorporated Arabic machine translation (MT) output from three independent research sites, arbitrarily merging these materials into one MT condition. We contrast the MT condition, for both text and audio data types, with high quality human reference Gold Standard (GS) translations. Overall, achieved 95% comprehension for GS and 74% for MT, across 4 genres and 3 difficulty levels. Surprisingly, comprehension rates do not correlate highly with translation error rates, suggesting that we are measuring an additional dimension of MT quality. We observed that it takes 15% more time overall to read MT than GS.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics.</title>
<date>2002</date>
<booktitle>Proceedings of HLT</booktitle>
<marker>Doddington, 2002</marker>
<rawString>Doddington, G. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics. Proceedings of HLT 2002.</rawString>
</citation>
<citation valid="false">
<authors>
<author>NIST</author>
</authors>
<title>GALE Go/No-Go Eval Plan;</title>
<note>www.nist.gov/ speech/tests/gale/2006/doc/GALE06_evalplan.v2.pdf</note>
<marker>NIST, </marker>
<rawString>NIST 2006. GALE Go/No-Go Eval Plan; www.nist.gov/ speech/tests/gale/2006/doc/GALE06_evalplan.v2.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Jones</author>
<author>W Shen</author>
</authors>
<title>Measuring Translation Quality by Testing English Speakers with a New DLPT for Arabic. Int’l Conf. on Intel.</title>
<date>2005</date>
<publisher>Analysis.</publisher>
<marker>Jones, Shen, 2005</marker>
<rawString>Jones, D. A., W. Shen, et al. 2005a. Measuring Translation Quality by Testing English Speakers with a New DLPT for Arabic. Int’l Conf. on Intel. Analysis.</rawString>
</citation>
<citation valid="true">
<title>Interagency Language Roundtable Website.</title>
<date>2005</date>
<note>ILR Skill Level Descriptions: http://www.govtilr.org</note>
<marker>2005</marker>
<rawString>Interagency Language Roundtable Website. 2005. ILR Skill Level Descriptions: http://www.govtilr.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clare Voss</author>
<author>Calandra Tate</author>
</authors>
<title>Task-based Evaluation of MT Engines. European Association for Machine Translation conference.</title>
<date>2006</date>
<contexts>
<context position="4207" citStr="Voss and Tate, 2006" startWordPosition="653" endWordPosition="656">uacy, a measure of accuracy with reference to a gold standard translation (White and O&apos;Connell, 1994). Later MT evaluation found fluency and adequacy to correlate well enough with automatic measures (BLEU), and since comprehension tests are relatively more expensive to create, the informativeness test was not used in later Proceedings of NAACL HLT 2007, Companion Volume, pages 77–80, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics MT evaluations, such as the ones performed by NIST from 2001-2006. In other work, task-based evaluation has been used for MT evaluation (Voss and Tate, 2006), which measures human performance on exhaustively extracting ‘who’, ‘when’, and ‘where’ type elements in MT output. The DLPTstar also uses this type of factual question, particularly for Level 2 documents, but not exhaustively. Instead, the test focuses on text elements most characteristic of the levels as defined in the ILR scale. At Level 3, for example, questions may concern abstract concepts or hypotheses found in the documents. Applying the ILR construct provides Defense Department decision makers with test scores that are readily interpretable. 3 Test Construction and Administration In </context>
</contexts>
<marker>Voss, Tate, 2006</marker>
<rawString>Voss, Clare and Calandra Tate. 2006. Task-based Evaluation of MT Engines. European Association for Machine Translation conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JS White</author>
<author>TA O&apos;Connell</author>
</authors>
<title>Evaluation in the ARPA machine translation program:</title>
<date>1994</date>
<booktitle>Proceedings of the HLT workshop.</booktitle>
<contexts>
<context position="3688" citStr="White and O&apos;Connell, 1994" startWordPosition="573" endWordPosition="576">ments ranged from Level 2 to Level 3, but the test did not contain any true Level 1 documents. To compensate for this lack, we constructed questions about Level 1 elements (e.g., personal and place names) in Level 2 and 3 documents. A standard DLPT would have more variation at Level 1. 2 Related and Previous Work Earlier work in MT evaluation incorporated an informativeness measure, based on comprehension test answers, in addition to fluency, a measure of output readability without reference to a gold standard, and adequacy, a measure of accuracy with reference to a gold standard translation (White and O&apos;Connell, 1994). Later MT evaluation found fluency and adequacy to correlate well enough with automatic measures (BLEU), and since comprehension tests are relatively more expensive to create, the informativeness test was not used in later Proceedings of NAACL HLT 2007, Companion Volume, pages 77–80, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics MT evaluations, such as the ones performed by NIST from 2001-2006. In other work, task-based evaluation has been used for MT evaluation (Voss and Tate, 2006), which measures human performance on exhaustively extracting ‘who’, ‘when’, and </context>
</contexts>
<marker>White, O&apos;Connell, 1994</marker>
<rawString>White, JS and TA O&apos;Connell. 1994. Evaluation in the ARPA machine translation program: 1993 methodology. Proceedings of the HLT workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>