<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001089">
<title confidence="0.9785505">
Generalizing Syntactic Structures for Product Attribute Candidate
Extraction
</title>
<author confidence="0.99907">
Yanyan Zhao, Bing Qin, Shen Hu, Ting Liu
</author>
<affiliation confidence="0.99745">
Harbin Institute of Technology, Harbin, China
</affiliation>
<email confidence="0.997103">
{yyzhao,bqin,shu,tliu}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.994713" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99876">
Noun phrases (NP) in a product review are
always considered as the product attribute
candidates in previous work. However, this
method limits the recall of the product at-
tribute extraction. We therefore propose
a novel approach by generalizing syntactic
structures of the product attributes with two
strategies: intuitive heuristics and syntactic
structure similarity. Experiments show that
the proposed approach is effective.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997828972972973">
Product attribute extraction is a fundamental task of
sentiment analysis. It aims to extract the product at-
tributes from a product review, such as “picture qual-
ity” in the sentence “The picture quality of Canon is
perfect.” This task is usually performed in two steps:
product attribute candidate extraction and candidate
classification.
Almost all the previous work pays more attention
to the second step, fewer researchers make in-depth
research on the first step. They simply choose the
NPs in a product review as the product attribute can-
didates (Hu and Liu, 2004; Popescu and Etzioni,
2005; Yi et al., 2003). However, this method lim-
its the recall of the product attribute extraction for
two reasons. First, there exist other structures of the
product attributes except NPs. Second, the syntactic
parsing is not perfect, especially for the Non-English
languages, such as Chinese. Experiments on three
Chinese datasetsl show that nearly 15% product at-
tributes are lost, when only using NPs as the can-
didates. Obviously, if using the candidate classifi-
cation techniques on these NP candidates, it would
&apos;It refers to the training data in Section 3.1.
lead to poor performance (especially for recall) for
the final product attribute extraction.
Based on the above discussion, it can be observed
that product attribute candidate extraction is well
worth studying. In this paper, we propose an ap-
proach by generalizing the syntactic structures of the
product attributes to solve this problem. Figure 1
lists some syntactic structure samples from an an-
notated corpus, including the special forms of NPs
in Figure 1(a) and other syntactic structures, such as
VP or IP in Figure 1(b). We can find that the syntac-
tic structures can not only cover more phrase types
besides NP, but also describe the detailed forms of
the product attributes.
</bodyText>
<figure confidence="0.996404428571429">
(a) syntactic structure samples of NP
VP
NP
NN
y}JaI
(photographing function)
(b) syntactic structure samples of other phrases
</figure>
<figureCaption confidence="0.9759895">
Figure 1: Syntactic structure samples of the product at-
tributes (acquired by an automatic phrase parser).
</figureCaption>
<bodyText confidence="0.999880375">
In order to exploit more and useful syntactic struc-
tures, two generalization strategies: intuitive heuris-
tics and syntactic structure similarity are used. Ex-
periments on three Chinese domain-specific datasets
show that our approach can significantly improve the
recall of the product attribute candidate extraction,
and furthermore, improve the performance of the fi-
nal product attribute extraction.
</bodyText>
<figure confidence="0.998529657894737">
NP
NP
NP
NP
NN
5}
(screen resolution)
NP
NN
yifl
(single track)
NP
NN
94
(front seats)
NN
X
(screen)
NP
ADJP
JJ
9
NP
NN
X
QP
CD
�
VB
MPH
IP
VP
VB
8
(screen display)
NP
NN
X
</figure>
<page confidence="0.857887">
377
</page>
<subsubsectionHeader confidence="0.669983">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 377–380,
</subsubsectionHeader>
<page confidence="0.242826">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</page>
<sectionHeader confidence="0.966047" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999987043478261">
The standard syntactic structures of the product at-
tributes can be collected from a training sett. Then
a simple method of exact matching can be used to
select the product attribute candidates from the test
set. In particular, for a syntactic structure&apos; T in
the test set, if T exactly matches with one of the
standard syntactic structures, then its corresponding
string can be treated as a product attribute candidate.
However, this method fails to handle similar syn-
tactic structures, such as the two structures in Fig-
ure 2. Besides, this method treats the syntactic struc-
ture as a whole during exact matching, without con-
sidering any structural information. Therefore, it is
difficult to describe the syntactic structure informa-
tion explicitly. All of these prevent this method from
generalizing unseen data well.
To overcome the above problems, two generaliza-
tion strategies are proposed in this paper. One is to
generalize the syntactic structures with two intuitive
heuristics. The other is to deeply mine the syntactic
structure by decomposing it into several substruc-
tures. Both strategies will be introduced in the fol-
lowing subsections.
</bodyText>
<subsectionHeader confidence="0.922028">
2.1 Intuitive Heuristics
</subsectionHeader>
<bodyText confidence="0.99992025">
Two intuitive heuristics are adopted to generalize the
syntactic structures.
Heu1: For the near-synonymic grammar tags in
syntactic structures, we can generalize them by a
normalized one. Such as the red boxes in Figure 2,
the POSs “NNS” and “NN” show the same syntactic
meaning, we can generalize “NNS” with “NN”. The
near-synonymic grammar tags are listed in Table 1.
</bodyText>
<figureCaption confidence="0.930171">
Figure 2: Generalizing a syntactic structure with two in-
tuitive heuristics.
</figureCaption>
<bodyText confidence="0.855729">
Heu2: For the sequence of identical grammar tags
in syntactic structures, we can replace them with
</bodyText>
<footnote confidence="0.998693333333333">
2We use Dan Bikel’s phrase parser for syntactic parsing.
3We simply select the syntactic structures of the strings un-
der three words or four words with “的”(“of” in English).
</footnote>
<table confidence="0.999161166666667">
Replaced by Near-synonymic grammar tags
JJ JJR, JJS
NN NNS, NNP, NNPS, CD, NR
RB RBR, RBS
VB VBD, VBG, VBN, VBP, VBZ, VV
S SBAR, SBARQ, SINU, SQ
</table>
<tableCaption confidence="0.9999">
Table 1: The near-synonymic grammar tags.
</tableCaption>
<bodyText confidence="0.998859333333333">
one. The reason is that the sequential grammar tags
always describe the same syntactic function as one
grammar tag. Such as the blue circles in Figure 2.
</bodyText>
<subsectionHeader confidence="0.99921">
2.2 Syntactic Structure Similarity
</subsectionHeader>
<bodyText confidence="0.999880266666667">
The heuristic generalization strategy is too restric-
tive to give a good coverage. Moreover, after this
kind of generalization, the syntactic structure is used
as a whole in exact matching all the same. Thus,
as an alternative to the exact matching, tree kernel
based methods can be used to implicitly explore the
substructures of the syntactic structure in a high-
dimensional space. This kind of methods can di-
rectly calculate the similarity between two substruc-
ture vectors using a kernel function. Tree kernel
based methods are effective in modeling structured
features, which are widely used in many natural
language processing tasks, such as syntactic pars-
ing (Collins and Duffy, 2001) and semantic role la-
beling (Che et al., 2008) and so on.
</bodyText>
<equation confidence="0.493580285714286">
IP IP IP
NP VP
VB
IP IP IP
VP
VB NP
NN
</equation>
<figureCaption confidence="0.995009">
Figure 3: Substructures from a syntactic structure.
</figureCaption>
<bodyText confidence="0.9998516">
In this paper, the syntactic structure for a product
attribute can be decomposed into several substruc-
tures, such as in Figure 3. Correspondingly, the syn-
tactic structure T can be represented by a vector of
integer counts of each substructure type:
</bodyText>
<figure confidence="0.924268323529412">
,b(T) _ (01(T ), 02(T), ..., 0.(T))
_ (# of substructures of type 1,
_ # of substructures of type 2,
...,
_ # of substructures of type n)
NP
VP NP
Heu2
NP
VP NP
VB
NNS
NN
NN
NP
VB
Heul
NN
IP
NP
NN
VP
VB
NP
NN
VP
VB
NP
NN
NP
NN
VP NP VP
VP
VB
</figure>
<page confidence="0.995064">
378
</page>
<bodyText confidence="0.999716">
After syntactic structure decomposition, we can
count the number of the common substructures as
the similarity between two syntactic structures. The
commonly used convolution tree kernel is applied in
this paper. Its kernel function is defined as follows:
</bodyText>
<equation confidence="0.9992305">
K(T1,T2) _ ⟨,b(T1), `b(T2)⟩
_ ∑i(Oi(T1) · Oi(T2))
</equation>
<bodyText confidence="0.997524833333333">
Based on these, for a syntactic structure T in the
test set, we can compute the similarity between T
and all the standard syntactic structures by the above
kernel function. A similarity threshold thsim4 is set
to determine whether the string from T is a correct
product attribute candidate.
</bodyText>
<sectionHeader confidence="0.999898" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998525">
3.1 Datasets and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.998568166666667">
Three domain-specific datasets are used in the ex-
periments, which is from an official Chinese Opin-
ion Analysis Evaluation 2008 (COAE2008) (Zhao et
al., 2008). Table 2 shows the statistics of the three
datasets, each of which is divided into training, de-
velopment and test data in a proportion of 2:1:1.
</bodyText>
<table confidence="0.9717402">
Domain # of sentences # of standard
product attributes
Camera 1,780 1,894
Car 2,166 2,504
Phone 2,196 2,293
</table>
<tableCaption confidence="0.998985">
Table 2: The datasets for three product domains.
</tableCaption>
<bodyText confidence="0.999692857142857">
Two evaluation metrics, recall and noise ratio, are
designed to evaluate the performance of the prod-
uct attribute candidate extraction. Recall refers to
the proportion of correctly identified attribute candi-
dates in all standard product attributes. Noise ratio
refers to the proportion of incorrectly identified at-
tribute candidates in all candidates.
</bodyText>
<subsectionHeader confidence="0.937763">
3.2 Comparative methods
</subsectionHeader>
<bodyText confidence="0.999366">
We choose the method, which considers NPs as the
product attribute candidates, as the baseline (shown
as NPs based).
Besides, in order to assess the two generaliza-
tion strategies’ effectiveness, four experiments are
designed as follows:
</bodyText>
<footnote confidence="0.723747">
4In the experiments, th3j, is set to 0.7, which is tuned on
the development set.
</footnote>
<bodyText confidence="0.9976499">
SynStru based: It refers to the syntactic struc-
ture exact matching method, which is implemented
without the two proposed generation strategies.
SynStru h: It refers to the strategy only using the
first generalization.
SynStru kernel: It refers to the strategy only us-
ing the second generalization.
SynStru h+kernel: It refers to the strategy us-
ing both two generalizations, i.e., it refers to our ap-
proach in this paper.
</bodyText>
<subsectionHeader confidence="0.880815">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.809333666666667">
Table 3 lists the comparative performances on the
test data between our approach and the comparative
methods for product attribute candidate extraction.
</bodyText>
<table confidence="0.99985825">
Domain Method Recall Noise ratio
Camera NPs based 81.20% 63.64%
SynStru based 84.80% 67.67%
SynStru h 92.08% 74.74%
SynStru kernel 92.51% 75.92%
SynStru h+kernel 92.72% 76.25%
Car NPs based 85.25% 69.35%
SynStru based 86.31% 72.66%
SynStru h 93.78% 78.01%
SynStru kernel 94.56% 79.50%
SynStru h+kernel 94.71% 80.44%
Phone NPs based 84.11% 63.76%
SynStru based 86.26% 67.09%
SynStru h 93.13% 73.62%
SynStru kernel 93.47% 75.11%
SynStru h+kernel 93.63% 75.35%
</table>
<tableCaption confidence="0.991542">
Table 3: Comparisons between our approach and the
comparative methods for product attribute candidate ex-
traction.
</tableCaption>
<bodyText confidence="0.900354090909091">
Analyzing the recalls in Table 3, we can find that:
1. The performance of SynStru based method
is better than NPs based method for each domain.
This can illustrate that syntactic structures can cover
more forms of the product attributes. However, the
recall of SynStru based method is not high, either.
2. The two generalization strategies, SynStru h
and SynStru kernel can both significantly improve
the performance for each domain, comparing to the
SynStru based method. This can illustrate that our
two generalization strategies are helpful.
</bodyText>
<listItem confidence="0.646484">
3. Our approach SynStru h+kernel achieves the
best performance. This can illustrate that the two
generalization strategies are complementary to each
</listItem>
<page confidence="0.997771">
379
</page>
<bodyText confidence="0.999734">
other. And further, mining and generalizing the syn-
tactic structures is effective for candidate extraction.
However, the noise ratio for each domain is in-
creasing when employing our approach. That’s be-
cause, more kinds of syntactic structures are consid-
ered, more noise is added. However, we can easily
remove the noise in the candidate classification step.
Thus in the next section, we will assess our candi-
date extraction approach by applying it to the prod-
uct attribute extraction task.
</bodyText>
<sectionHeader confidence="0.9906645" genericHeader="method">
4 Application in Product Attribute
Extraction
</sectionHeader>
<bodyText confidence="0.9982582">
For the extracted product attribute candidates, we
train a maximum entropy (ME) based binary clas-
sifier to find the correct product attributes. Several
commonly used features are listed in Table 4.
Feature Description
lexical the words of the product attribute(PA)
the POS for each word of the PA
three words before the PA
three words after the PA
the words’ number of the PA
syntactic the syntactic structure of the PA
binary Is there a stop word in the PA?
(Y/N)
Is there a polarity word in the PA?
Is there an English word or number in the PA?
</bodyText>
<tableCaption confidence="0.988581">
Table 4: The feature set for product attribute extraction.
</tableCaption>
<bodyText confidence="0.9688754">
Table 5 shows the product attribute extraction per-
formances on the test data. We can find that the
performance (F1) of our approach is better than
NPs based method for each domain. We discuss the
results as follows:
1. Comparing to the NPs based method, the re-
call of our approach increases a lot for each domain.
This demonstrates that generalized syntactic struc-
tures can cover more forms of product attributes.
2. Comparing to the NPs based method, the pre-
cision of our approach also increases for each do-
main. That’s because syntactic structures are more
specialized than the phrase forms (such as NP, VP)
in the previous work, which can filter some noises
from the phrase(NP) candidates.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9979055">
This paper describes a simple but effective way to
extract the product attribute candidates from product
</bodyText>
<table confidence="0.991565285714286">
Domain Method R (%) P (%) F1 (%)
Camera NPs based 59.62 68.38 63.70
Our approach 62.96 73.32 67.74
Car NPs based 59.94 64.87 62.31
Our approach 67.34 65.90 66.61
Phone NPs based 58.53 71.14 64.22
Our approach 67.84 76.13 71.74
</table>
<tableCaption confidence="0.956718">
Table 5: Comparisons between our approach and the
NPs based method for product attribute extraction.
</tableCaption>
<bodyText confidence="0.999606666666667">
reviews. The proposed approach is based on deep
analysis into syntactic structures of the product at-
tributes, via intuitive heuristics and syntactic struc-
ture decomposition. Experimental results indicate
that our approach is promising. In future, we will try
more syntactic structure generalization strategies.
</bodyText>
<sectionHeader confidence="0.999033" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9861458">
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, and the “863”National
High- Tech Research and Development of China via
grant 2008AA01Z144.
</bodyText>
<sectionHeader confidence="0.99776" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994988">
Wanxiang Che, Min Zhang, AiTi Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a hybrid con-
volution tree kernel for semantic role labeling. ACM
Trans. Asian Lang. Inf. Process., 7(4).
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In NIPS, pages 625–632.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI-
2004, pages 755–760.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
hltemnlp2005, pages 339–346.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Extract-
ing sentiments about a given topic using natural lan-
guage processing techniques. In Proceedings of the
IEEE International Conference on Data Mining.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,
Kang Liu, and Qi Zhang. 2008. Overview of chinese
opinion analysis evaluation 2008.
</reference>
<page confidence="0.998268">
380
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966267">
<title confidence="0.9988345">Generalizing Syntactic Structures for Product Attribute Candidate Extraction</title>
<author confidence="0.986831">Yanyan Zhao</author>
<author confidence="0.986831">Bing Qin</author>
<author confidence="0.986831">Shen Hu</author>
<author confidence="0.986831">Ting</author>
<affiliation confidence="0.992238">Harbin Institute of Technology, Harbin,</affiliation>
<abstract confidence="0.998831272727273">Noun phrases (NP) in a product review are always considered as the product attribute candidates in previous work. However, this limits the recall of the product attribute extraction. We therefore a novel approach by generalizing syntactic structures of the product attributes with two strategies: intuitive heuristics and syntactic structure similarity. Experiments show that the proposed approach is effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Min Zhang</author>
<author>AiTi Aw</author>
<author>Chew Lim Tan</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>Using a hybrid convolution tree kernel for semantic role labeling.</title>
<date>2008</date>
<journal>ACM Trans. Asian Lang. Inf. Process.,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="6509" citStr="Che et al., 2008" startWordPosition="1031" endWordPosition="1034">lization, the syntactic structure is used as a whole in exact matching all the same. Thus, as an alternative to the exact matching, tree kernel based methods can be used to implicitly explore the substructures of the syntactic structure in a highdimensional space. This kind of methods can directly calculate the similarity between two substructure vectors using a kernel function. Tree kernel based methods are effective in modeling structured features, which are widely used in many natural language processing tasks, such as syntactic parsing (Collins and Duffy, 2001) and semantic role labeling (Che et al., 2008) and so on. IP IP IP NP VP VB IP IP IP VP VB NP NN Figure 3: Substructures from a syntactic structure. In this paper, the syntactic structure for a product attribute can be decomposed into several substructures, such as in Figure 3. Correspondingly, the syntactic structure T can be represented by a vector of integer counts of each substructure type: ,b(T) _ (01(T ), 02(T), ..., 0.(T)) _ (# of substructures of type 1, _ # of substructures of type 2, ..., _ # of substructures of type n) NP VP NP Heu2 NP VP NP VB NNS NN NN NP VB Heul NN IP NP NN VP VB NP NN VP VB NP NN NP NN VP NP VP VP VB 378 Af</context>
</contexts>
<marker>Che, Zhang, Aw, Tan, Liu, Li, 2008</marker>
<rawString>Wanxiang Che, Min Zhang, AiTi Aw, Chew Lim Tan, Ting Liu, and Sheng Li. 2008. Using a hybrid convolution tree kernel for semantic role labeling. ACM Trans. Asian Lang. Inf. Process., 7(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language. In</title>
<date>2001</date>
<booktitle>NIPS,</booktitle>
<pages>625--632</pages>
<contexts>
<context position="6463" citStr="Collins and Duffy, 2001" startWordPosition="1022" endWordPosition="1025"> a good coverage. Moreover, after this kind of generalization, the syntactic structure is used as a whole in exact matching all the same. Thus, as an alternative to the exact matching, tree kernel based methods can be used to implicitly explore the substructures of the syntactic structure in a highdimensional space. This kind of methods can directly calculate the similarity between two substructure vectors using a kernel function. Tree kernel based methods are effective in modeling structured features, which are widely used in many natural language processing tasks, such as syntactic parsing (Collins and Duffy, 2001) and semantic role labeling (Che et al., 2008) and so on. IP IP IP NP VP VB IP IP IP VP VB NP NN Figure 3: Substructures from a syntactic structure. In this paper, the syntactic structure for a product attribute can be decomposed into several substructures, such as in Figure 3. Correspondingly, the syntactic structure T can be represented by a vector of integer counts of each substructure type: ,b(T) _ (01(T ), 02(T), ..., 0.(T)) _ (# of substructures of type 1, _ # of substructures of type 2, ..., _ # of substructures of type n) NP VP NP Heu2 NP VP NP VB NNS NN NN NP VB Heul NN IP NP NN VP VB</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In NIPS, pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI2004,</booktitle>
<pages>755--760</pages>
<contexts>
<context position="1219" citStr="Hu and Liu, 2004" startWordPosition="175" endWordPosition="178">e proposed approach is effective. 1 Introduction Product attribute extraction is a fundamental task of sentiment analysis. It aims to extract the product attributes from a product review, such as “picture quality” in the sentence “The picture quality of Canon is perfect.” This task is usually performed in two steps: product attribute candidate extraction and candidate classification. Almost all the previous work pays more attention to the second step, fewer researchers make in-depth research on the first step. They simply choose the NPs in a product review as the product attribute candidates (Hu and Liu, 2004; Popescu and Etzioni, 2005; Yi et al., 2003). However, this method limits the recall of the product attribute extraction for two reasons. First, there exist other structures of the product attributes except NPs. Second, the syntactic parsing is not perfect, especially for the Non-English languages, such as Chinese. Experiments on three Chinese datasetsl show that nearly 15% product attributes are lost, when only using NPs as the candidates. Obviously, if using the candidate classification techniques on these NP candidates, it would &apos;It refers to the training data in Section 3.1. lead to poor </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of AAAI2004, pages 755–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In hltemnlp2005,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="1246" citStr="Popescu and Etzioni, 2005" startWordPosition="179" endWordPosition="182">h is effective. 1 Introduction Product attribute extraction is a fundamental task of sentiment analysis. It aims to extract the product attributes from a product review, such as “picture quality” in the sentence “The picture quality of Canon is perfect.” This task is usually performed in two steps: product attribute candidate extraction and candidate classification. Almost all the previous work pays more attention to the second step, fewer researchers make in-depth research on the first step. They simply choose the NPs in a product review as the product attribute candidates (Hu and Liu, 2004; Popescu and Etzioni, 2005; Yi et al., 2003). However, this method limits the recall of the product attribute extraction for two reasons. First, there exist other structures of the product attributes except NPs. Second, the syntactic parsing is not perfect, especially for the Non-English languages, such as Chinese. Experiments on three Chinese datasetsl show that nearly 15% product attributes are lost, when only using NPs as the candidates. Obviously, if using the candidate classification techniques on these NP candidates, it would &apos;It refers to the training data in Section 3.1. lead to poor performance (especially for</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In hltemnlp2005, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Tetsuya Nasukawa</author>
<author>Razvan Bunescu</author>
<author>Wayne Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining.</booktitle>
<contexts>
<context position="1264" citStr="Yi et al., 2003" startWordPosition="183" endWordPosition="186">ion Product attribute extraction is a fundamental task of sentiment analysis. It aims to extract the product attributes from a product review, such as “picture quality” in the sentence “The picture quality of Canon is perfect.” This task is usually performed in two steps: product attribute candidate extraction and candidate classification. Almost all the previous work pays more attention to the second step, fewer researchers make in-depth research on the first step. They simply choose the NPs in a product review as the product attribute candidates (Hu and Liu, 2004; Popescu and Etzioni, 2005; Yi et al., 2003). However, this method limits the recall of the product attribute extraction for two reasons. First, there exist other structures of the product attributes except NPs. Second, the syntactic parsing is not perfect, especially for the Non-English languages, such as Chinese. Experiments on three Chinese datasetsl show that nearly 15% product attributes are lost, when only using NPs as the candidates. Obviously, if using the candidate classification techniques on these NP candidates, it would &apos;It refers to the training data in Section 3.1. lead to poor performance (especially for recall) for the f</context>
</contexts>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and Wayne Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In Proceedings of the IEEE International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhao</author>
</authors>
<title>Hongbo Xu, Xuanjing Huang, Songbo Tan,</title>
<date>2008</date>
<location>Kang</location>
<marker>Zhao, 2008</marker>
<rawString>Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, Kang Liu, and Qi Zhang. 2008. Overview of chinese opinion analysis evaluation 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>