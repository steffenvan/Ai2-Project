<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005343">
<title confidence="0.9988425">
Outclassing Wikipedia in Open-Domain Information Extraction:
Weakly-Supervised Acquisition of Attributes over Conceptual Hierarchies
</title>
<author confidence="0.950924">
Marius Pas¸ca
</author>
<affiliation confidence="0.895243">
Google Inc.
</affiliation>
<address confidence="0.902416">
Mountain View, California 94043
</address>
<email confidence="0.998964">
mars@google.com
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999436">
A set of labeled classes of instances is ex-
tracted from text and linked into an exist-
ing conceptual hierarchy. Besides a signif-
icant increase in the coverage of the class
labels assigned to individual instances, the
resulting resource of labeled classes is
more effective than similar data derived
from the manually-created Wikipedia, in
the task of attribute extraction over con-
ceptual hierarchies.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999857838709678">
Motivation: Sharing basic intuitions and long-
term goals with other tasks within the area of Web-
based information extraction (Banko and Etzioni,
2008; Davidov and Rappoport, 2008), the task
of acquiring class attributes relies on unstructured
text available on the Web, as a data source for ex-
tracting generally-useful knowledge. In the case
of attribute extraction, the knowledge to be ex-
tracted consists in quantifiable properties of var-
ious classes (e.g., top speed, body style and gas
mileage for the class of sports cars).
Existing work on large-scale attribute extraction
focuses on producing ranked lists of attributes, for
target classes of instances available in the form
of flat sets of instances (e.g., ferrari modena,
porsche carrera gt) sharing the same class label
(e.g., sports cars). Independently of how the input
target classes are populated with instances (man-
ually (Pas¸ca, 2007) or automatically (Pas¸ca and
Van Durme, 2008)), and what type of textual data
source is used for extracting attributes (Web docu-
ments or query logs), the extraction of attributes
operates at a lexical rather than semantic level.
Indeed, the class labels of the target classes may
be not more than text surface strings (e.g., sports
cars) or even artificially-created labels (e.g., Car-
toonChar in lieu of cartoon characters). More-
over, although it is commonly accepted that sports
cars are also cars, which in turn are also motor ve-
hicles, the presence of sports cars among the input
target classes does not lead to any attributes being
extracted for cars and motor vehicles, unless the
latter two class labels are also present explicitly
among the input target classes.
Contributions: The contributions of this paper
are threefold. First, we investigate the role of
classes of instances acquired automatically from
unstructured text, in the task of attribute extrac-
tion over concepts from existing conceptual hi-
erarchies. For this purpose, ranked lists of at-
tributes are acquired from query logs for various
concepts, after linking a set of more than 4,500
open-domain, automatically-acquired classes con-
taining a total of around 250,000 instances into
conceptual hierarchies available in WordNet (Fell-
baum, 1998). In comparison, previous work
extracts attributes for either manually-specified
classes of instances (Pas¸ca, 2007), or for classes of
instances derived automatically but considered as
flat rather than hierarchical classes, and manually
associated to existing semantic concepts (Pas¸ca
and Van Durme, 2008). Second, we expand the
set of classes of instances acquired from text, thus
increasing their usefulness in attribute extraction
in particular and information extraction in general.
To this effect, additional class labels (e.g., mo-
tor vehicles) are identified for existing instances
(e.g., ferrari modena) of existing class labels (e.g.,
sports cars), by exploiting IsA relations available
within the conceptual hierarchy (e.g., sports cars
are also motor vehicles). Third, we show that
large-scale, automatically-derived classes of in-
</bodyText>
<note confidence="0.922964">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 639–647,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998589">
639
</page>
<bodyText confidence="0.999544">
stances can have as much as, or even bigger, prac-
tical impact in open-domain information extrac-
tion tasks than similar data from large-scale, high-
coverage, manually-compiled resources. Specif-
ically, evaluation results indicate that the accu-
racy of the extracted lists of attributes is higher
by 8% at rank 10, 13% at rank 30 and 18% at
rank 50, when using the automatically-extracted
classes of instances rather than the comparatively
more numerous and a-priori more reliable, human-
generated, collaboratively-vetted classes of in-
stances available within Wikipedia (Remy, 2002).
</bodyText>
<sectionHeader confidence="0.969335" genericHeader="method">
2 Attribute Extraction over Hierarchies
</sectionHeader>
<bodyText confidence="0.92794325">
Extraction of Flat Labeled Classes: Unstruc-
tured text from a combination of Web documents
and query logs represents the source for deriving
a flat set of labeled classes of instances, which are
necessary as input for attribute extraction experi-
ments. The labeled classes are acquired in three
stages:
1) extraction of a noisy pool of pairs of a
class label and a potential class instance, by ap-
plying a few Is-A extraction patterns, selected
from (Hearst, 1992), to Web documents:
(fruits, apple), (fruits, corn), (fruits, mango),
(fruits, orange), (foods, broccoli), (crops, lettuce),
(flowers, rose);
2) extraction of unlabeled clusters of distribu-
tionally similar phrases, by clustering vectors of
contextual features collected around the occur-
rences of the phrases within Web documents (Lin
and Pantel, 2002):
{lettuce, broccoli, corn, ..},
{carrot, mango, apple, orange, rose, ..};
3) merging and filtering of the raw pairs and un-
labeled clusters into smaller, more accurate sets of
class instances associated with class labels, in an
attempt to use unlabeled clusters to filter noisy raw
pairs instead of merely using clusters to general-
ize class labels across raw pairs (Pas¸ca and Van
Durme, 2008):
fruits={apple, mango, orange, ..}.
To increase precision, the vocabulary of class
instances is confined to the set of queries that are
most frequently submitted to a general-purpose
Web search engine. After merging, the resulting
pairs of an instance and a class label are arranged
into instance sets (e.g., {ferrari modena, porsche
carrera gt}), each associated with a class label
(e.g., sports cars).
Linking Labeled Classes into Hierarchies:
Manually-constructed language resources such as
WordNet provide reliable, wide-coverage upper-
level conceptual hierarchies, by grouping together
phrases with the same meaning (e.g., {analgesic,
painkiller, pain pill}) into sets of synonyms
(synsets), and organizing the synsets into concep-
tual hierarchies (e.g., painkillers are a subconcept,
or a hyponym, of drugs) (Fellbaum, 1998). To de-
termine the points of insertion of automatically-
extracted labeled classes into hand-built Word-
Net hierarchies, the class labels are looked up in
WordNet using built-in morphological normaliza-
tion routines. When a class label (e.g., age-related
diseases) is not found in WordNet, it is looked up
again after iteratively removing its leading words
(e.g., related diseases, and diseases) until a poten-
tial point of insertion is found where one or more
senses exist in WordNet for the class label.
An efficient heuristic for sense selection is to
uniformly choose the first (that is, most frequent)
sense of the class label in WordNet, as point of
insertion. Due to its simplicity, the heuristic is
bound to make errors whenever the correct sense is
not the first one, thus incorrectly linking academic
journals under the sense of journals as personal
diaries rather than periodicals, and active volca-
noes under the sense of volcanoes as fissures in
the earth, rather than mountains formed by vol-
canic material. Nevertheless, choosing the first
sense is attractive for three reasons. First, Word-
Net senses are often too fine-grained, making the
task of choosing the correct sense difficult even
for humans (Palmer et al., 2007). Second, choos-
ing the first sense from WordNet is sometimes
better than more intelligent disambiguation tech-
niques (Pradhan et al., 2007). Third, previous ex-
perimental results on linking Wikipedia classes to
WordNet concepts confirm that first-sense selec-
tion is more effective in practice than other tech-
niques (Suchanek et al., 2007). Thus, a class la-
bel and its associated instances are inserted under
the first WordNet sense available for the class la-
bel. For example, silicon valley companies and its
associated instances (apple, hewlett packard etc.)
are inserted under the first of the 9 senses of com-
panies in WordNet, which corresponds to compa-
nies as institutions created to conduct business.
In order to trade off coverage for higher preci-
sion, the heuristic can be restricted to link a class
label under the first WordNet sense available, as
</bodyText>
<page confidence="0.995591">
640
</page>
<bodyText confidence="0.994571348837209">
before, but only when no other senses are avail-
able at the point of insertion beyond the first sense.
With the modified heuristic, the class label internet
search engines is linked under the first and only
sense of search engines in WordNet, but silicon
valley companies is no longer linked under the first
of the 9 senses of companies.
Extraction of Attributes for Hierarchy Con-
cepts: The labeled classes of instances linked to
conceptual hierarchies constitute the input to the
acquisition of attributes of hierarchy concepts, by
mining a collection of Web search queries. The at-
tributes capture properties that are relevant to the
concept. The extraction of attributes exploits the
sets of class instances rather than the associated
class labels. More precisely, for each hierarchy
concept for which attributes must be extracted, the
instances associated to all class labels linked un-
der the subhierarchy rooted at the concept are col-
lected as a union set of instances, thus exploiting
the transitivity of IsA relations. This step is equiv-
alent to propagating the instances upwards, from
their class labels to higher-level WordNet concepts
under which the class labels are linked, up to the
root of the hierarchy. The resulting sets of in-
stances constitute the input to the acquisition of
attributes, which consists of four stages:
1) identification of a noisy pool of candidate at-
tributes, as remainders of queries that also con-
tain one of the class instances. In the case of the
concept movies, whose instances include jay and
silent bob strike back and kill bill, the query “cast
jay and silent bob strike back” produces the can-
didate attribute cast;
2) construction of internal vector representa-
tions for each candidate attribute, based on queries
(e.g., “cast selection for kill bill”) that contain a
candidate attribute (cast) and a class instance (kill
bill). These vectors consist of counts tied to the
frequency with which an attribute occurs with a
given “templatized” query. The latter replaces spe-
cific attributes and instances from the query with
common placeholders, e.g., “Xfor Y”;
</bodyText>
<listItem confidence="0.915843875">
3) construction of a reference internal vector
representation for a small set of seed attributes
provided as input. A reference vector is the nor-
malized sum of the individual vectors correspond-
ing to the seed attributes;
4) ranking of candidate attributes with respect
to each concept, by computing the similarity be-
tween their individual vector representations and
</listItem>
<bodyText confidence="0.9885274">
the reference vector of the seed attributes.
The result of the four stages, which are de-
scribed in more detail in (Pas¸ca, 2007), is a ranked
list of attributes (e.g., [opening song, cast, charac-
ters,...]) for each concept (e.g., movies).
</bodyText>
<sectionHeader confidence="0.99218" genericHeader="method">
3 Experimental Setting
</sectionHeader>
<bodyText confidence="0.994739090909091">
Textual Data Sources: The acquisition of open-
domain knowledge relies on unstructured text
available within a combination of Web documents
maintained by, and search queries submitted to the
Google search engine. The textual data source
for extracting labeled classes of instances con-
sists of around 100 million documents in En-
glish, as available in a Web repository snapshot
from 2006. Conversely, the acquisition of open-
domain attributes relies on a random sample of
fully-anonymized queries in English submitted by
Web users in 2006. The sample contains about 50
million unique queries. Each query is accompa-
nied by its frequency of occurrence in the logs.
Other sources of similar data are available publicly
for research purposes (Gao et al., 2007).
Parameters for Extracting Labeled Classes:
When applied to the available document col-
lection, the method for extracting open-domain
classes of instances from unstructured text intro-
duced in (Pas¸ca and Van Durme, 2008) produces
4,583 class labels associated to 258,699 unique
instances, for a total of 869,118 pairs of a class
instance and an associated class label. All col-
lected instances occur among to the top five mil-
lion queries with the highest frequency within the
input query logs. The data is further filtered by
discarding labeled classes with fewer than 25 in-
stances. The classes, examples of which are shown
in Table 1, are linked under conceptual hierarchies
available within WordNet 3.0, which contains a to-
tal of 117,798 English noun phrases grouped in
82,115 concepts (or synsets).
Parameters for Extracting Attributes: For each
target concept from the hierarchy, given the union
of all instances associated to class labels linked to
the target concept or one of its subconcepts, and
given a set of five seed attributes (e.g., {quality,
speed, number of users, market share, reliabil-
ity} for search engines), the method described
in (Pas¸ca, 2007) extracts ranked lists of attributes
from the input query logs. Internally, the rank-
ing of attributes uses Jensen-Shannon (Lee, 1999)
to compute similarity scores between internal rep-
</bodyText>
<page confidence="0.975754">
641
</page>
<table confidence="0.379327333333333">
Class Label Class Size Class Instances
accounting systems 40 flexcube, myob, oracle financials, peachtree accounting, sybiz
antimicrobials 97 azithromycin, chloramphenicol, fusidic acid, quinolones, sulfa drugs
civilizations 197 ancient greece, chaldeans, etruscans, inca, indians, roman republic
elementary particles 33 axions, electrons, gravitons, leptons, muons, neutrons, positrons
farm animals 61 angora goats, burros, cattle, cows, donkeys, draft horses, mule, oxen
forages 27 alsike clover, rye grass, tall fescue, sericea lespedeza, birdsfoot trefoil
ideologies 179 egalitarianism, laissez-faire capitalism, participatory democracy
social events 436 academic conferences, afternoon teas, block parties, masquerade balls
</table>
<tableCaption confidence="0.834739">
Table 1: Examples of instances within labeled classes extracted from unstructured text, used as input for
attribute extraction experiments
</tableCaption>
<bodyText confidence="0.999687675">
resentations of seed attributes, on one hand, and
each of the newly acquired attributes, on the other
hand. Depending on the experiments, the amount
of supervision is thus limited to either 5 seed at-
tributes for each target concept, or to 5 seed at-
tributes (population, area, president, flag and cli-
mate) provided for only one of the extracted la-
beled classes, namely european countries.
Experimental Runs: The experiments consist of
four different runs, which correspond to different
choices for the source of conceptual hierarchies
and class instances linked to those hierarchies, as
illustrated in Table 2. In the first run, denoted N,
the class instances are those available within the
latest version of WordNet (3.0) itself via HasIn-
stance relations. The second run, Y, corresponds to
an extension of WordNet based on the manually-
compiled classes of instances from categories in
Wikipedia, as available in the 2007-w50-5 version
of Yago (Suchanek et al., 2007). Therefore, run Y
has the advantage of the fact that Wikipedia cat-
egories are a rich source of useful and accurate
knowledge (Nastase and Strube, 2008), which ex-
plains their previous use as a source for evaluation
gold standards (Blohm et al., 2007). The last two
runs from Table 2, E3 and Ea, correspond to the
set of open-domain labeled classes acquired from
unstructured text. In both E3 and Ea, class labels
are linked to the first sense available at the point
of insertion in WordNet. In E3, the class labels
are linked only if no other senses are available at
the point of insertion beyond the first sense, thus
promoting higher linkage precision at the expense
of fewer links. For example, since the phrases im-
pressionists, sports cars and painters have 1, 1 and
4 senses available in WordNet respectively, the
class labels french impressionists and sports cars
are linked to the respective WordNet concepts,
whereas the class label painters is not. Compar-
atively, in Ea, the class labels are uniformly linked
</bodyText>
<table confidence="0.9994558">
Description Source of Hierarchy and Instances
N Y Es Ea
Include instances V V - -
from WordNet?
Include instances - V V V
from elsewhere?
#Instances (x103) 14.3 1,296.5 108.0 257.0
#Class labels 945 30,338 1,315 4,517
#Pairs of a class label 17.4 2,839.8 191.0 859.0
and instance (x103)
</table>
<tableCaption confidence="0.929052">
Table 2: Source of class instances for various ex-
perimental runs
</tableCaption>
<bodyText confidence="0.999102142857143">
to the first sense available in WordNet, regardless
of whether other senses may or may not be avail-
able. Thus, Ea trades off potentially lower preci-
sion for the benefit of higher linkage recall, and
results in more of the class labels and their asso-
ciated instances extracted from text to be linked to
WordNet than in the case of run E3.
</bodyText>
<sectionHeader confidence="0.999798" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999709">
4.1 Evaluation of Labeled Classes
</subsectionHeader>
<bodyText confidence="0.999796555555556">
Coverage of Class Instances: In run N, the in-
put class instances are the component phrases of
synsets encoded via HasInstance relations under
other synsets in WordNet. For example, the synset
corresponding to {search engine}, defined as “a
computer program that retrieves documents or
files or data from a database or from a computer
network”, has 3 HasInstance instances in Word-
Net, namely Ask Jeeves, Google and Yahoo. Ta-
ble 3 illustrates the coverage of the class instances
extracted from unstructured text and linked to
WordNet in runs E3 and Ea respectively, relative to
all 945 WordNet synsets that contain HasInstance
instances. Note that the coverage scores are con-
servative assessments of actual coverage, since a
run (i.e., E3 or Ea) receives credit for a WordNet
instance only if the run contains an instance that
is a full-length, case-insensitive match (e.g., ask
</bodyText>
<page confidence="0.986578">
642
</page>
<table confidence="0.999926166666667">
Concept HasInstance Instances within WordNet Cvg
Synset Offset Examples Count Es Ea
{existentialist, existentialist, 10071557 Albert Camus, Beauvoir, Camus, 8 1.00 1.00
philosopher, existential philosopher} Heidegger, Jean-Paul Sartre
{search engine} 06578654 Ask Jeeves, Google, Yahoo 3 1.00 1.00
{university} 04511002 Brown, Brown University, 44 0.61 0.77
Carnegie Mellon University
{continent} 09254614 Africa, Antarctic continent, Europe, 13 0.54 0.54
Eurasia, Gondwanaland, Laurasia
{microscopist} 10313872 Anton van Leeuwenhoek, Anton 6 0.00 0.00
van Leuwenhoek, Swammerdam
Average over all 945 WordNet concepts that have HasInstance instance(s) 18.71 0.21 0.40
</table>
<tableCaption confidence="0.998506">
Table 3: Coverage of class instances extracted from text and linked to WordNet (used as input in runs Es
</tableCaption>
<bodyText confidence="0.986809114285714">
and Ea respectively), measured as the fraction of WordNet HasInstance instances (used as input in run
N) that occur among the class instances (Cvg=coverage)
jeeves) of the WordNet instance. On average, the
coverage scores for class instances of runs Es and
Ea relative to run N are 0.21 and 0.40 respectively,
as shown in the last row in Table 3. Comparatively,
the equivalent instance coverage for run Y, which
already includes most of the WordNet instances by
design (cf. (Suchanek et al., 2007)), is 0.59.
Relative Coverage of Class Labels: The link-
ing of class labels to WordNet concepts allows for
the expansion of the set of classes of instances ac-
quired from text, thus increasing its usefulness in
attribute extraction in particular and information
extraction in general. To this effect, additional
class labels are identified for existing instances,
in the form of component phrases of the synsets
that are superconcepts (or hypernyms, in WordNet
terminology) of the synset under which the class
label of the instance is linked in WordNet. For ex-
ample, since the class label sports cars is linked
under the WordNet synset {sports car, sport car},
and the latter has the synset {motor vehicle, auto-
motive vehicle} among its hypernyms, the phrases
motor vehicles and automotive vehicles are col-
lected as new class labels 1 and associated to ex-
isting instances of sports cars from the original
set, such as ferrari modena. No phrases are col-
lected from a selected set of 10 top-level Word-
Net synsets, including {entity} and {object, phys-
ical object}, which are deemed too general to be
useful as class labels. As illustrated in Table 4,
a collected pair of a new class label and an exist-
ing instance either does not have any impact, if the
pair already occurs in the original set of labeled
</bodyText>
<footnote confidence="0.997746">
1For consistency with the original labeled classes, new
class labels collected from WordNet are converted from sin-
gular (e.g., motor vehicle) to plural (e.g., motor vehicles).
</footnote>
<table confidence="0.986850307692308">
Already in original labeled classes:
painters alfred sisley
european countries austria
Expansion of existing labeled classes:
animals avocet
animals northern oriole
scientists howard gardner
scientists phil zimbardo
Creation of new labeled classes:
automotive vehicles acura nsx
automotive vehicles detomaso pantera
creative persons aaron copland
creative persons yoshitomo nara
</table>
<tableCaption confidence="0.970616">
Table 4: Examples of additional class labels col-
</tableCaption>
<bodyText confidence="0.9648555">
lected from WordNet, for existing instances of the
original labeled classes extracted from text
classes; or expands existing classes, if the class
label already occurs in the original set of labeled
classes but not in association to the instance; or
creates new classes of instances, if the class label
is not part of the original set. The latter two cases
aggregate to increases in coverage, relative to the
pairs from the original sets of labeled classes, of
53% for Es and 304% for Ea.
</bodyText>
<subsectionHeader confidence="0.999433">
4.2 Evaluation of Attributes
</subsectionHeader>
<bodyText confidence="0.938845636363636">
Target Hierarchy Concepts: The performance of
attribute extraction is assessed over a set of 25 tar-
get concepts also used for evaluation in (Pas¸ca,
2008). The set of 25 target concepts includes: Ac-
tor, Award, Battle, CelestialBody, ChemicalEle-
ment, City, Company, Country, Currency, Dig-
italCamera, Disease, Drug, FictionalCharacter,
Flower, Food, Holiday, Mountain, Movie, Nation-
alPark, Painter, Religion, River, SearchEngine,
Treaty, Wine. Each target concept represents ex-
actly one WordNet concept (synset). For instance,
</bodyText>
<page confidence="0.998632">
643
</page>
<bodyText confidence="0.999909921568627">
one of the target concepts, denoted Country, cor-
responds to a synset situated at the internal off-
set 08544813 in WordNet 3.0, which groups to-
gether the synonymous phrases country, state and
land and associates them with the definition “the
territory occupied by a nation”. The target con-
cepts exhibit variation with respect to their depths
within WordNet conceptual hierarchies, ranging
from a minimum of 5 (e.g., for Food) to a maxi-
mum of 11 (for Flower), with a mean depth of 8
over the 25 concepts.
Evaluation Procedure: The measurement of re-
call requires knowledge of the complete set of
items (in our case, attributes) to be extracted. Un-
fortunately, this number is often unavailable in in-
formation extraction tasks in general (Hasegawa
et al., 2004), and attribute extraction in particular.
Indeed, the manual enumeration of all attributes
of each target concept, to measure recall, is un-
feasible. Therefore, the evaluation focuses on the
assessment of attribute accuracy.
To remove any bias towards higher-ranked at-
tributes during the assessment of class attributes,
the ranked lists of attributes produced by each run
to be evaluated are sorted alphabetically into a
merged list. Each attribute of the merged list is
manually assigned a correctness label within its
respective class. In accordance with previously
introduced methodology, an attribute is vital if it
must be present in an ideal list of attributes of
the class (e.g., side effects for Drug); okay if it
provides useful but non-essential information; and
wrong if it is incorrect (Pas¸ca, 2007).
To compute the precision score over a ranked
list of attributes, the correctness labels are con-
verted to numeric values (vital to 1, okay to 0.5
and wrong to 0). Precision at some rank N in the
list is thus measured as the sum of the assigned
values of the first N attributes, divided by N.
Attribute Accuracy: Figure 1 plots the precision
at ranks 1 through 50 for the ranked lists of at-
tributes extracted by various runs as an average
over the 25 target concepts, along two dimensions.
In the leftmost graphs, each of the 25 target con-
cepts counts towards the computation of precision
scores of a given run, regardless of whether any
attributes were extracted or not for the target con-
cept. In the rightmost graphs, only target con-
cepts for which some attributes were extracted are
included in the precision scores of a given run.
Thus, the leftmost graphs properly penalize a run
</bodyText>
<figureCaption confidence="0.974771">
Figure 1: Accuracy of the attributes extracted for
</figureCaption>
<bodyText confidence="0.983012677419355">
various runs, as an average over the entire set of
25 target concepts (left graphs) and as an average
over (variable) subsets of the 25 target concepts
for which some attributes were extracted in each
run (right graphs). Seed attributes are provided as
input for only one target concept (top graphs), or
for each target concept (bottom graphs)
for failing to extract any attributes for some tar-
get concepts, whereas the rightmost graphs do not
include any such penalties. On the other dimen-
sion, in the graphs at the top of Figure 1, seed at-
tributes are provided only for one class (namely,
european countries), for a total of 5 attributes over
all classes. In the graphs at the bottom of the fig-
ure, there are 5 seed attributes for each of the 25
target concepts in the graphs at the bottom of Fig-
ure 1, for a total of 5×25=125 attributes.
Several conclusions can be drawn after inspect-
ing the results. First, providing more supervi-
sion, in the form of seed attributes for all concepts
rather than for only one concept, translates into
higher attribute accuracy for all runs, as shown
by the graphs at the top vs. graphs at the bot-
tom of Figure 1. Second, in the leftmost graphs,
run N has the lowest precision scores, which is in
line with the relatively small number of instances
available in the original WordNet, as confirmed by
the counts from Table 2. Third, in the leftmost
graphs, the more restrictive run Es has lower pre-
cision scores across all ranks than its less restric-
tive counterpart Ea. In other words, adding more
</bodyText>
<figure confidence="0.999513708333333">
Class: Average-Class
N
Y
Es
Ea
0 10 20 30 40 50
Rank
Class: Average-Class
N
Y
Es
Ea
0 10 20 30 40 50
Rank
Class: Average-Class
0 10 20 30 40 50
Rank
Class: Average-Class
N
Y
Es
Ea
0 10 20 30 40 50
Rank
Precision 1
0.8
0.6
0.4
0.2
Precision 1
0.8
0.6
0.4
0.2
Precision 1
Precision 0.8
0.6
0.4
0.2
1
0.8
0.6
0.4
0.2
N
Y
Es
Ea
</figure>
<page confidence="0.992436">
644
</page>
<table confidence="0.999882833333333">
Class Precision
@10 @30 @50
N Y E3 Ea N Y E3 Ea N Y E3 Ea
Actor 1.00 1.00 1.00 1.00 0.78 0.85 0.98 0.95 0.62 0.84 0.95 0.96
Award 0.00 0.50 0.95 0.85 0.00 0.35 0.80 0.73 0.00 0.29 0.70 0.69
Battle 0.80 0.90 0.00 0.90 0.76 0.80 0.00 0.80 0.74 0.72 0.00 0.73
CelestialBody 1.00 1.00 1.00 0.40 1.00 1.00 0.93 0.16 0.98 0.89 0.91 0.12
ChemicalElement 0.00 0.65 0.80 0.80 0.00 0.45 0.83 0.63 0.00 0.48 0.84 0.51
City 1.00 1.00 0.00 1.00 0.86 0.80 0.00 0.83 0.78 0.70 0.00 0.76
Company 0.00 1.00 0.90 1.00 0.00 0.90 0.93 0.88 0.00 0.77 0.82 0.80
Country 1.00 0.90 1.00 1.00 0.98 0.81 0.96 0.96 0.97 0.76 0.98 0.97
Currency 0.00 0.90 0.00 0.90 0.00 0.53 0.00 0.83 0.00 0.36 0.00 0.87
DigitalCamera 0.00 0.20 0.85 0.85 0.00 0.10 0.85 0.85 0.00 0.10 0.82 0.82
Disease 0.00 0.60 0.75 0.75 0.00 0.76 0.83 0.83 0.00 0.63 0.87 0.86
Drug 0.00 1.00 1.00 1.00 0.00 0.91 1.00 1.00 0.00 0.88 0.96 0.96
FictionalCharacter 0.80 0.70 0.00 0.55 0.65 0.48 0.00 0.38 0.42 0.41 0.00 0.34
Flower 0.00 0.65 0.00 0.70 0.00 0.26 0.00 0.55 0.00 0.16 0.00 0.53
Food 0.00 0.80 0.90 1.00 0.00 0.65 0.71 0.96 0.00 0.53 0.59 0.96
Holiday 0.00 0.60 0.80 0.80 0.00 0.50 0.48 0.48 0.00 0.37 0.41 0.41
Mountain 1.00 0.75 0.00 0.90 0.96 0.61 0.00 0.86 0.77 0.58 0.00 0.74
Movie 0.00 1.00 1.00 1.00 0.00 0.90 0.80 0.78 0.00 0.85 0.75 0.74
NationalPark 0.90 0.80 0.00 0.00 0.85 0.76 0.00 0.00 0.82 0.75 0.00 0.00
Painter 1.00 1.00 1.00 1.00 0.96 0.93 0.88 0.96 0.92 0.89 0.76 0.93
Religion 0.00 0.00 1.00 1.00 0.00 0.00 1.00 1.00 0.00 0.00 0.92 0.97
River 1.00 0.80 0.00 0.00 0.70 0.60 0.00 0.00 0.61 0.58 0.00 0.00
SearchEngine 0.40 0.00 0.25 0.25 0.23 0.00 0.35 0.35 0.32 0.00 0.43 0.43
Treaty 0.50 0.90 0.80 0.80 0.33 0.65 0.53 0.53 0.26 0.59 0.42 0.42
Wine 0.00 0.30 0.80 0.80 0.00 0.26 0.43 0.45 0.00 0.20 0.28 0.29
Average (over 25) 0.41 0.71 0.59 0.77 0.36 0.59 0.53 0.67 0.32 0.53 0.49 0.63
Average (over non-empty) 0.86 0.78 0.87 0.83 0.75 0.64 0.78 0.73 0.68 0.57 0.73 0.68
</table>
<tableCaption confidence="0.990032">
Table 5: Comparative accuracy of the attributes extracted by various runs, for individual concepts, as an
</tableCaption>
<bodyText confidence="0.974089808510638">
average over the entire set of 25 target concepts, and as an average over (variable) subsets of the 25 target
concepts for which some attributes were extracted in each run. Seed attributes are provided as input for
each target concept
restrictions may improve precision but hurts recall
of class instances, which results in lower average
precision scores for the attributes. Fourth, in the
leftmost graphs, the runs using the automatically-
extracted labeled classes (Es and Ea) not only out-
perform N, but one of them (Ea) also outperforms
Y. This is the most important result. It shows
that large-scale, automatically-derived classes of
instances can have as much as, or even bigger,
practical impact in attribute extraction than similar
data from larger (cf. Table 2), manually-compiled,
collaboratively created and maintained resources
such as Wikipedia. Concretely, in the graph on
the bottom left of Figure 1, the precision scores at
ranks 10, 30 and 50 are 0.71, 0.59 and 0.53 for run
Y, but 0.77, 0.67 and 0.63 for run Ea. The scores
correspond to attribute accuracy improvements of
8% at rank 10, 13% at rank 30, and 18% at rank
50 for run Ea over run Y. In fact, in the rightmost
graphs, that is, without taking into account target
concepts without any extracted attributes, the pre-
cision scores of both Es and Ea are higher than for
run Y across most, if not all, ranks from 1 through
50. In this case, it is El that produces the most
accurate attributes, in a task-based demonstration
that the more cautious linking of class labels to
WordNet concepts in Es vs. Ea leads to less cov-
erage but higher precision of the linked labeled
classes, which translates into extracted attributes
of higher accuracy but for fewer target concepts.
Analysis: The curves plotted in the two graphs
at the bottom of Figure 1 are computed as av-
erages over precision scores for individual target
concepts, which are shown in detail in Table 5.
Precision scores of 0.00 correspond to runs for
which no attributes are acquired from query logs,
because no instances are available in the subhier-
archy rooted at the respective concepts. For exam-
ple, precision scores for run N are 0.00 for Award
and DigitalCamera, among others concepts in Ta-
ble 5, due to the lack of any HasInstance instances
in WordNet for the respective concepts. The num-
ber of target concepts for which some attributes
are extracted is 12 for run N, 23 for Y, 17 for Es
</bodyText>
<page confidence="0.997376">
645
</page>
<bodyText confidence="0.999925939393939">
and 23 for Ea. Thus, both run N and run Es exhibit
rather binary behavior across individual classes, in
that they tend to either not retrieve any attributes or
retrieve attributes of relatively higher quality than
the other runs, causing Es and N to have the worst
precision scores in the last but one row of Table 5,
but the best precision scores in the last row of Ta-
ble 5.
The individual scores shown for Es and Ea in
Table 5 concur with the conclusion drawn earlier
from the graphs in Figure 1, that Run Es has lower
precision than Ea as an average over all target con-
cepts. Notable exceptions are the scores obtained
for the concepts CelestialBody and ChemicalEle-
ment, where Es significantly outperforms Ea in Ta-
ble 5. This is due to confusing instances (e.g., kobe
bryant) being associated with class labels (e.g.,
nba stars) that are incorrectly linked under the tar-
get concepts (e.g., Star, which is a subconcept of
CelestialBody in WordNet) in Ea, but not linked at
all and thus not causing confusion in Es.
Run Y performs better than Ea for 5 of the 25
individual concepts, including NationalPark, for
which no instances of national parks or related
class labels are available in run Ea; and River, for
which relevant instances in the labeled classes in
Ea, but they are associated to the class label river
systems, which is incorrectly linked to the Word-
Net concept systems rather than to rivers. How-
ever, run Ea outperforms Y on 12 individual con-
cepts (e.g., Award, DigitalCamera and Disease),
and also as an average over all classes (last two
rows in Table 5).
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999981">
Previous work on the automatic acquisition of at-
tributes for open-domain classes from text requires
the manual enumeration of sets of instances and
seed attributes, for each class for which attributes
are to be extracted. In contrast, the current method
operates on automatically-extracted classes. The
experiments reported in (Pas¸ca and Van Durme,
2008) also exploit automatically-extracted classes
for the purpose of attribute extraction. However,
they operate on flat classes, as opposed to concepts
organized hierarchically. Furthermore, they re-
quire manual mappings from extracted class labels
into a selected set of evaluation classes (e.g., by
mapping river systems to River, football clubs to
SoccerClub, and parks to NationalPark), whereas
the current method maps class labels to concepts
automatically, by linking class labels and their as-
sociated instances to concepts. Manually-encoded
attributes available within Wikipedia articles are
used in (Wu and Weld, 2008) in order to derive
other attributes from unstructured text within Web
documents. Comparatively, the current method
extracts attributes from query logs rather than
Web documents, using labeled classes extracted
automatically rather than available in manually-
created resources, and requiring minimal supervi-
sion in the form of only 5 seed attributes provided
for only one concept, rather than thousands of at-
tributes available in millions of manually-created
Wikipedia articles. To our knowledge, there is
only one previous study (Pas¸ca, 2008) that directly
addresses the problem of extracting attributes over
conceptual hierarchies. However, that study uses
labeled classes extracted from text with a different
method; extracts attributes for labeled classes and
propagates them upwards in the hierarchy, in order
to compute attributes of hierarchy concepts from
attributes of their subconcepts; and does not con-
sider resources similar to Wikipedia, as sources of
input labeled classes for attribute extraction.
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999856208333333">
This paper introduces an extraction framework
for exploiting labeled classes of instances to ac-
quire open-domain attributes from unstructured
text available within search query logs. The link-
ing of the labeled classes into existing conceptual
hierarchies allows for the extraction of attributes
over hierarchy concepts, without a-priori restric-
tions to specific domains of interest and with little
supervision. Experimental results show that the
extracted attributes are more accurate when us-
ing automatically-derived labeled classes, rather
than classes of instances derived from manually-
created resources such as Wikipedia. Current
work investigates the impact of the semantic dis-
tribution of the classes of instances on the overall
accuracy of attributes; the potential benefits of us-
ing more compact conceptual hierarchies (Snow
et al., 2007) on attribute accuracy; and the orga-
nization of labeled classes of instances into con-
ceptual hierarchies, as an alternative to inserting
them into existing conceptual hierarchies created
manually from scratch or automatically by filter-
ing manually-generated relations among classes
from Wikipedia (Ponzetto and Strube, 2007).
</bodyText>
<page confidence="0.998792">
646
</page>
<sectionHeader confidence="0.995887" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929678571428">
M. Banko and O. Etzioni. 2008. The tradeoffs between open
and traditional relation extraction. In Proceedings of the
46th Annual Meeting of the Association for Computational
Linguistics (ACL-08), pages 28–36, Columbus, Ohio.
S. Blohm, P. Cimiano, and E. Stemle. 2007. Harvesting re-
lations from the web - quantifiying the impact of filter-
ing functions. In Proceedings of the 22nd National Con-
ference on Artificial Intelligence (AAAI-07), pages 1316–
1321, Vancouver, British Columbia.
D. Davidov and A. Rappoport. 2008. Classification of se-
mantic relationships between nominals using pattern clus-
ters. In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08), pages
227–235, Columbus, Ohio.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical
Database and Some of its Applications. MIT Press.
W. Gao, C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and H. Hon.
2007. Cross-lingual query suggestion using query logs
of different languages. In Proceedings of the 30th ACM
Conference on Research and Development in Information
Retrieval (SIGIR-07), pages 463–470, Amsterdam, The
Netherlands.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-04), pages 415–
422, Barcelona, Spain.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
International Conference on Computational Linguistics
(COLING-92), pages 539–545, Nantes, France.
L. Lee. 1999. Measures of distributional similarity. In Pro-
ceedings of the 37th Annual Meeting of the Association of
Computational Linguistics (ACL-99), pages 25–32, Col-
lege Park, Maryland.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference on
Computational linguistics (COLING-02), pages 1–7.
V. Nastase and M. Strube. 2008. Decoding Wikipedia cat-
egories for knowledge acquisition. In Proceedings of
the 23rd National Conference on Artificial Intelligence
(AAAI-08), pages 1219–1224, Chicago, Illinois.
M. Pas¸ca and B. Van Durme. 2008. Weakly-supervised ac-
quisition of open-domain classes and class attributes from
web documents and query logs. In Proceedings of the 46th
Annual Meeting of the Association for Computational Lin-
guistics (ACL-08), pages 19–27, Columbus, Ohio.
M. Pas¸ca. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of the 16th World Wide Web Con-
ference (WWW-07), pages 101–110, Banff, Canada.
M. Pas¸ca. 2008. Turning Web text and search queries into
factual knowledge: Hierarchical class attribute extraction.
In Proceedings of the 23rd National Conference on Arti-
ficial Intelligence (AAAI-08), pages 1225–1230, Chicago,
Illinois.
M. Palmer, H. Dang, and C. Fellbaum. 2007. Making fine-
grained and coarse-grained sense distinctions, both man-
ually and automatically. Natural Language Engineering,
13(2):137–163.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings of the 22nd
National Conference on Artificial Intelligence (AAAI-07),
pages 1440–1447, Vancouver, British Columbia.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 2007.
SemEval-2007 Task-17: English lexical sample, SRL and
all words. In Proceedings of the 4th Workshop on Se-
mantic Evaluations (SemEval-07), pages 87–92, Prague,
Czech Republic.
M. Remy. 2002. Wikipedia: The free encyclopedia. Online
Information Review, 26(6):434.
R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007. Learning
to merge word senses. In Proceedings of the 2007 Con-
ference on Empirical Methods in Natural Language Pro-
cessing (EMNLP-07), pages 1005–1014, Prague, Czech
Republic.
F. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of the 16th World Wide Web
Conference (WWW-07), pages 697–706, Banff, Canada.
F. Wu and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the 17th
World Wide Web Conference (WWW-08), pages 635–644,
Beijing, China.
</reference>
<page confidence="0.998184">
647
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.809621">
<title confidence="0.958911">Outclassing Wikipedia in Open-Domain Information Extraction: Weakly-Supervised Acquisition of Attributes over Conceptual Hierarchies</title>
<affiliation confidence="0.990469">Google Inc.</affiliation>
<address confidence="0.99957">Mountain View, California 94043</address>
<email confidence="0.999172">mars@google.com</email>
<abstract confidence="0.989569636363636">A set of labeled classes of instances is extracted from text and linked into an existing conceptual hierarchy. Besides a significant increase in the coverage of the class labels assigned to individual instances, the resulting resource of labeled classes is more effective than similar data derived from the manually-created Wikipedia, in the task of attribute extraction over conceptual hierarchies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>O Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>28--36</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="779" citStr="Banko and Etzioni, 2008" startWordPosition="107" endWordPosition="110">c. Mountain View, California 94043 mars@google.com Abstract A set of labeled classes of instances is extracted from text and linked into an existing conceptual hierarchy. Besides a significant increase in the coverage of the class labels assigned to individual instances, the resulting resource of labeled classes is more effective than similar data derived from the manually-created Wikipedia, in the task of attribute extraction over conceptual hierarchies. 1 Introduction Motivation: Sharing basic intuitions and longterm goals with other tasks within the area of Webbased information extraction (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), the task of acquiring class attributes relies on unstructured text available on the Web, as a data source for extracting generally-useful knowledge. In the case of attribute extraction, the knowledge to be extracted consists in quantifiable properties of various classes (e.g., top speed, body style and gas mileage for the class of sports cars). Existing work on large-scale attribute extraction focuses on producing ranked lists of attributes, for target classes of instances available in the form of flat sets of instances (e.g., ferrari modena, porsche carrera gt)</context>
</contexts>
<marker>Banko, Etzioni, 2008</marker>
<rawString>M. Banko and O. Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 28–36, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Blohm</author>
<author>P Cimiano</author>
<author>E Stemle</author>
</authors>
<title>Harvesting relations from the web - quantifiying the impact of filtering functions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd National Conference on Artificial Intelligence (AAAI-07),</booktitle>
<pages>1316--1321</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="15521" citStr="Blohm et al., 2007" startWordPosition="2392" endWordPosition="2395">d in Table 2. In the first run, denoted N, the class instances are those available within the latest version of WordNet (3.0) itself via HasInstance relations. The second run, Y, corresponds to an extension of WordNet based on the manuallycompiled classes of instances from categories in Wikipedia, as available in the 2007-w50-5 version of Yago (Suchanek et al., 2007). Therefore, run Y has the advantage of the fact that Wikipedia categories are a rich source of useful and accurate knowledge (Nastase and Strube, 2008), which explains their previous use as a source for evaluation gold standards (Blohm et al., 2007). The last two runs from Table 2, E3 and Ea, correspond to the set of open-domain labeled classes acquired from unstructured text. In both E3 and Ea, class labels are linked to the first sense available at the point of insertion in WordNet. In E3, the class labels are linked only if no other senses are available at the point of insertion beyond the first sense, thus promoting higher linkage precision at the expense of fewer links. For example, since the phrases impressionists, sports cars and painters have 1, 1 and 4 senses available in WordNet respectively, the class labels french impressioni</context>
</contexts>
<marker>Blohm, Cimiano, Stemle, 2007</marker>
<rawString>S. Blohm, P. Cimiano, and E. Stemle. 2007. Harvesting relations from the web - quantifiying the impact of filtering functions. In Proceedings of the 22nd National Conference on Artificial Intelligence (AAAI-07), pages 1316– 1321, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>A Rappoport</author>
</authors>
<title>Classification of semantic relationships between nominals using pattern clusters.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>227--235</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="809" citStr="Davidov and Rappoport, 2008" startWordPosition="111" endWordPosition="114">nia 94043 mars@google.com Abstract A set of labeled classes of instances is extracted from text and linked into an existing conceptual hierarchy. Besides a significant increase in the coverage of the class labels assigned to individual instances, the resulting resource of labeled classes is more effective than similar data derived from the manually-created Wikipedia, in the task of attribute extraction over conceptual hierarchies. 1 Introduction Motivation: Sharing basic intuitions and longterm goals with other tasks within the area of Webbased information extraction (Banko and Etzioni, 2008; Davidov and Rappoport, 2008), the task of acquiring class attributes relies on unstructured text available on the Web, as a data source for extracting generally-useful knowledge. In the case of attribute extraction, the knowledge to be extracted consists in quantifiable properties of various classes (e.g., top speed, body style and gas mileage for the class of sports cars). Existing work on large-scale attribute extraction focuses on producing ranked lists of attributes, for target classes of instances available in the form of flat sets of instances (e.g., ferrari modena, porsche carrera gt) sharing the same class label </context>
</contexts>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>D. Davidov and A. Rappoport. 2008. Classification of semantic relationships between nominals using pattern clusters. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 227–235, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and Some of its Applications.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and Some of its Applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gao</author>
<author>C Niu</author>
<author>J Nie</author>
<author>M Zhou</author>
<author>J Hu</author>
<author>K Wong</author>
<author>H Hon</author>
</authors>
<title>Cross-lingual query suggestion using query logs of different languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th ACM Conference on Research and Development in Information Retrieval (SIGIR-07),</booktitle>
<pages>463--470</pages>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="12085" citStr="Gao et al., 2007" startWordPosition="1873" endWordPosition="1876">nts maintained by, and search queries submitted to the Google search engine. The textual data source for extracting labeled classes of instances consists of around 100 million documents in English, as available in a Web repository snapshot from 2006. Conversely, the acquisition of opendomain attributes relies on a random sample of fully-anonymized queries in English submitted by Web users in 2006. The sample contains about 50 million unique queries. Each query is accompanied by its frequency of occurrence in the logs. Other sources of similar data are available publicly for research purposes (Gao et al., 2007). Parameters for Extracting Labeled Classes: When applied to the available document collection, the method for extracting open-domain classes of instances from unstructured text introduced in (Pas¸ca and Van Durme, 2008) produces 4,583 class labels associated to 258,699 unique instances, for a total of 869,118 pairs of a class instance and an associated class label. All collected instances occur among to the top five million queries with the highest frequency within the input query logs. The data is further filtered by discarding labeled classes with fewer than 25 instances. The classes, examp</context>
</contexts>
<marker>Gao, Niu, Nie, Zhou, Hu, Wong, Hon, 2007</marker>
<rawString>W. Gao, C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and H. Hon. 2007. Cross-lingual query suggestion using query logs of different languages. In Proceedings of the 30th ACM Conference on Research and Development in Information Retrieval (SIGIR-07), pages 463–470, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hasegawa</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04),</booktitle>
<pages>415--422</pages>
<location>Barcelona,</location>
<contexts>
<context position="22882" citStr="Hasegawa et al., 2004" startWordPosition="3575" endWordPosition="3578">roups together the synonymous phrases country, state and land and associates them with the definition “the territory occupied by a nation”. The target concepts exhibit variation with respect to their depths within WordNet conceptual hierarchies, ranging from a minimum of 5 (e.g., for Food) to a maximum of 11 (for Flower), with a mean depth of 8 over the 25 concepts. Evaluation Procedure: The measurement of recall requires knowledge of the complete set of items (in our case, attributes) to be extracted. Unfortunately, this number is often unavailable in information extraction tasks in general (Hasegawa et al., 2004), and attribute extraction in particular. Indeed, the manual enumeration of all attributes of each target concept, to measure recall, is unfeasible. Therefore, the evaluation focuses on the assessment of attribute accuracy. To remove any bias towards higher-ranked attributes during the assessment of class attributes, the ranked lists of attributes produced by each run to be evaluated are sorted alphabetically into a merged list. Each attribute of the merged list is manually assigned a correctness label within its respective class. In accordance with previously introduced methodology, an attrib</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 415– 422, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92),</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="4944" citStr="Hearst, 1992" startWordPosition="747" endWordPosition="748"> reliable, humangenerated, collaboratively-vetted classes of instances available within Wikipedia (Remy, 2002). 2 Attribute Extraction over Hierarchies Extraction of Flat Labeled Classes: Unstructured text from a combination of Web documents and query logs represents the source for deriving a flat set of labeled classes of instances, which are necessary as input for attribute extraction experiments. The labeled classes are acquired in three stages: 1) extraction of a noisy pool of pairs of a class label and a potential class instance, by applying a few Is-A extraction patterns, selected from (Hearst, 1992), to Web documents: (fruits, apple), (fruits, corn), (fruits, mango), (fruits, orange), (foods, broccoli), (crops, lettuce), (flowers, rose); 2) extraction of unlabeled clusters of distributionally similar phrases, by clustering vectors of contextual features collected around the occurrences of the phrases within Web documents (Lin and Pantel, 2002): {lettuce, broccoli, corn, ..}, {carrot, mango, apple, orange, rose, ..}; 3) merging and filtering of the raw pairs and unlabeled clusters into smaller, more accurate sets of class instances associated with class labels, in an attempt to use unlabe</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics (ACL-99),</booktitle>
<pages>25--32</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="13384" citStr="Lee, 1999" startWordPosition="2081" endWordPosition="2082">in WordNet 3.0, which contains a total of 117,798 English noun phrases grouped in 82,115 concepts (or synsets). Parameters for Extracting Attributes: For each target concept from the hierarchy, given the union of all instances associated to class labels linked to the target concept or one of its subconcepts, and given a set of five seed attributes (e.g., {quality, speed, number of users, market share, reliability} for search engines), the method described in (Pas¸ca, 2007) extracts ranked lists of attributes from the input query logs. Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal rep641 Class Label Class Size Class Instances accounting systems 40 flexcube, myob, oracle financials, peachtree accounting, sybiz antimicrobials 97 azithromycin, chloramphenicol, fusidic acid, quinolones, sulfa drugs civilizations 197 ancient greece, chaldeans, etruscans, inca, indians, roman republic elementary particles 33 axions, electrons, gravitons, leptons, muons, neutrons, positrons farm animals 61 angora goats, burros, cattle, cows, donkeys, draft horses, mule, oxen forages 27 alsike clover, rye grass, tall fescue, sericea lespedeza, bird</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>L. Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics (ACL-99), pages 25–32, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational linguistics (COLING-02),</booktitle>
<pages>1--7</pages>
<contexts>
<context position="5295" citStr="Lin and Pantel, 2002" startWordPosition="793" endWordPosition="796">re necessary as input for attribute extraction experiments. The labeled classes are acquired in three stages: 1) extraction of a noisy pool of pairs of a class label and a potential class instance, by applying a few Is-A extraction patterns, selected from (Hearst, 1992), to Web documents: (fruits, apple), (fruits, corn), (fruits, mango), (fruits, orange), (foods, broccoli), (crops, lettuce), (flowers, rose); 2) extraction of unlabeled clusters of distributionally similar phrases, by clustering vectors of contextual features collected around the occurrences of the phrases within Web documents (Lin and Pantel, 2002): {lettuce, broccoli, corn, ..}, {carrot, mango, apple, orange, rose, ..}; 3) merging and filtering of the raw pairs and unlabeled clusters into smaller, more accurate sets of class instances associated with class labels, in an attempt to use unlabeled clusters to filter noisy raw pairs instead of merely using clusters to generalize class labels across raw pairs (Pas¸ca and Van Durme, 2008): fruits={apple, mango, orange, ..}. To increase precision, the vocabulary of class instances is confined to the set of queries that are most frequently submitted to a general-purpose Web search engine. Afte</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>D. Lin and P. Pantel. 2002. Concept discovery from text. In Proceedings of the 19th International Conference on Computational linguistics (COLING-02), pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nastase</author>
<author>M Strube</author>
</authors>
<title>Decoding Wikipedia categories for knowledge acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08),</booktitle>
<pages>1219--1224</pages>
<location>Chicago, Illinois.</location>
<contexts>
<context position="15423" citStr="Nastase and Strube, 2008" startWordPosition="2375" endWordPosition="2378"> for the source of conceptual hierarchies and class instances linked to those hierarchies, as illustrated in Table 2. In the first run, denoted N, the class instances are those available within the latest version of WordNet (3.0) itself via HasInstance relations. The second run, Y, corresponds to an extension of WordNet based on the manuallycompiled classes of instances from categories in Wikipedia, as available in the 2007-w50-5 version of Yago (Suchanek et al., 2007). Therefore, run Y has the advantage of the fact that Wikipedia categories are a rich source of useful and accurate knowledge (Nastase and Strube, 2008), which explains their previous use as a source for evaluation gold standards (Blohm et al., 2007). The last two runs from Table 2, E3 and Ea, correspond to the set of open-domain labeled classes acquired from unstructured text. In both E3 and Ea, class labels are linked to the first sense available at the point of insertion in WordNet. In E3, the class labels are linked only if no other senses are available at the point of insertion beyond the first sense, thus promoting higher linkage precision at the expense of fewer links. For example, since the phrases impressionists, sports cars and pain</context>
</contexts>
<marker>Nastase, Strube, 2008</marker>
<rawString>V. Nastase and M. Strube. 2008. Decoding Wikipedia categories for knowledge acquisition. In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08), pages 1219–1224, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>B Van Durme</author>
</authors>
<title>Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>pages</pages>
<location>Columbus, Ohio.</location>
<marker>Pas¸ca, Van Durme, 2008</marker>
<rawString>M. Pas¸ca and B. Van Durme. 2008. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 19–27, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>Organizing and searching the World Wide Web of facts - step two: Harnessing the wisdom of the crowds.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th World Wide Web Conference (WWW-07),</booktitle>
<pages>101--110</pages>
<location>Banff, Canada.</location>
<marker>Pas¸ca, 2007</marker>
<rawString>M. Pas¸ca. 2007. Organizing and searching the World Wide Web of facts - step two: Harnessing the wisdom of the crowds. In Proceedings of the 16th World Wide Web Conference (WWW-07), pages 101–110, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>Turning Web text and search queries into factual knowledge: Hierarchical class attribute extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08),</booktitle>
<pages>1225--1230</pages>
<location>Chicago, Illinois.</location>
<marker>Pas¸ca, 2008</marker>
<rawString>M. Pas¸ca. 2008. Turning Web text and search queries into factual knowledge: Hierarchical class attribute extraction. In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08), pages 1225–1230, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>H Dang</author>
<author>C Fellbaum</author>
</authors>
<title>Making finegrained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="7747" citStr="Palmer et al., 2007" startWordPosition="1174" endWordPosition="1177"> the class label in WordNet, as point of insertion. Due to its simplicity, the heuristic is bound to make errors whenever the correct sense is not the first one, thus incorrectly linking academic journals under the sense of journals as personal diaries rather than periodicals, and active volcanoes under the sense of volcanoes as fissures in the earth, rather than mountains formed by volcanic material. Nevertheless, choosing the first sense is attractive for three reasons. First, WordNet senses are often too fine-grained, making the task of choosing the correct sense difficult even for humans (Palmer et al., 2007). Second, choosing the first sense from WordNet is sometimes better than more intelligent disambiguation techniques (Pradhan et al., 2007). Third, previous experimental results on linking Wikipedia classes to WordNet concepts confirm that first-sense selection is more effective in practice than other techniques (Suchanek et al., 2007). Thus, a class label and its associated instances are inserted under the first WordNet sense available for the class label. For example, silicon valley companies and its associated instances (apple, hewlett packard etc.) are inserted under the first of the 9 sens</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>M. Palmer, H. Dang, and C. Fellbaum. 2007. Making finegrained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering, 13(2):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd National Conference on Artificial Intelligence (AAAI-07),</booktitle>
<pages>1440--1447</pages>
<location>Vancouver, British Columbia.</location>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S. Ponzetto and M. Strube. 2007. Deriving a large scale taxonomy from Wikipedia. In Proceedings of the 22nd National Conference on Artificial Intelligence (AAAI-07), pages 1440–1447, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>E Loper</author>
<author>D Dligach</author>
<author>M Palmer</author>
</authors>
<title>SemEval-2007 Task-17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th Workshop on Semantic Evaluations (SemEval-07),</booktitle>
<pages>87--92</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7885" citStr="Pradhan et al., 2007" startWordPosition="1195" endWordPosition="1198">se is not the first one, thus incorrectly linking academic journals under the sense of journals as personal diaries rather than periodicals, and active volcanoes under the sense of volcanoes as fissures in the earth, rather than mountains formed by volcanic material. Nevertheless, choosing the first sense is attractive for three reasons. First, WordNet senses are often too fine-grained, making the task of choosing the correct sense difficult even for humans (Palmer et al., 2007). Second, choosing the first sense from WordNet is sometimes better than more intelligent disambiguation techniques (Pradhan et al., 2007). Third, previous experimental results on linking Wikipedia classes to WordNet concepts confirm that first-sense selection is more effective in practice than other techniques (Suchanek et al., 2007). Thus, a class label and its associated instances are inserted under the first WordNet sense available for the class label. For example, silicon valley companies and its associated instances (apple, hewlett packard etc.) are inserted under the first of the 9 senses of companies in WordNet, which corresponds to companies as institutions created to conduct business. In order to trade off coverage for</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 2007. SemEval-2007 Task-17: English lexical sample, SRL and all words. In Proceedings of the 4th Workshop on Semantic Evaluations (SemEval-07), pages 87–92, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Remy</author>
</authors>
<title>Wikipedia: The free encyclopedia.</title>
<date>2002</date>
<journal>Online Information Review,</journal>
<volume>26</volume>
<issue>6</issue>
<contexts>
<context position="4441" citStr="Remy, 2002" startWordPosition="665" endWordPosition="666"> for Computational Linguistics 639 stances can have as much as, or even bigger, practical impact in open-domain information extraction tasks than similar data from large-scale, highcoverage, manually-compiled resources. Specifically, evaluation results indicate that the accuracy of the extracted lists of attributes is higher by 8% at rank 10, 13% at rank 30 and 18% at rank 50, when using the automatically-extracted classes of instances rather than the comparatively more numerous and a-priori more reliable, humangenerated, collaboratively-vetted classes of instances available within Wikipedia (Remy, 2002). 2 Attribute Extraction over Hierarchies Extraction of Flat Labeled Classes: Unstructured text from a combination of Web documents and query logs represents the source for deriving a flat set of labeled classes of instances, which are necessary as input for attribute extraction experiments. The labeled classes are acquired in three stages: 1) extraction of a noisy pool of pairs of a class label and a potential class instance, by applying a few Is-A extraction patterns, selected from (Hearst, 1992), to Web documents: (fruits, apple), (fruits, corn), (fruits, mango), (fruits, orange), (foods, b</context>
</contexts>
<marker>Remy, 2002</marker>
<rawString>M. Remy. 2002. Wikipedia: The free encyclopedia. Online Information Review, 26(6):434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>S Prakash</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Learning to merge word senses.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-07),</booktitle>
<pages>1005--1014</pages>
<location>Prague, Czech Republic.</location>
<marker>Snow, Prakash, Jurafsky, Ng, 2007</marker>
<rawString>R. Snow, S. Prakash, D. Jurafsky, and A. Ng. 2007. Learning to merge word senses. In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing (EMNLP-07), pages 1005–1014, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Suchanek</author>
<author>G Kasneci</author>
<author>G Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge unifying WordNet and Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th World Wide Web Conference (WWW-07),</booktitle>
<pages>697--706</pages>
<location>Banff, Canada.</location>
<contexts>
<context position="8083" citStr="Suchanek et al., 2007" startWordPosition="1225" endWordPosition="1228">sures in the earth, rather than mountains formed by volcanic material. Nevertheless, choosing the first sense is attractive for three reasons. First, WordNet senses are often too fine-grained, making the task of choosing the correct sense difficult even for humans (Palmer et al., 2007). Second, choosing the first sense from WordNet is sometimes better than more intelligent disambiguation techniques (Pradhan et al., 2007). Third, previous experimental results on linking Wikipedia classes to WordNet concepts confirm that first-sense selection is more effective in practice than other techniques (Suchanek et al., 2007). Thus, a class label and its associated instances are inserted under the first WordNet sense available for the class label. For example, silicon valley companies and its associated instances (apple, hewlett packard etc.) are inserted under the first of the 9 senses of companies in WordNet, which corresponds to companies as institutions created to conduct business. In order to trade off coverage for higher precision, the heuristic can be restricted to link a class label under the first WordNet sense available, as 640 before, but only when no other senses are available at the point of insertion</context>
<context position="15271" citStr="Suchanek et al., 2007" startWordPosition="2349" endWordPosition="2352"> labeled classes, namely european countries. Experimental Runs: The experiments consist of four different runs, which correspond to different choices for the source of conceptual hierarchies and class instances linked to those hierarchies, as illustrated in Table 2. In the first run, denoted N, the class instances are those available within the latest version of WordNet (3.0) itself via HasInstance relations. The second run, Y, corresponds to an extension of WordNet based on the manuallycompiled classes of instances from categories in Wikipedia, as available in the 2007-w50-5 version of Yago (Suchanek et al., 2007). Therefore, run Y has the advantage of the fact that Wikipedia categories are a rich source of useful and accurate knowledge (Nastase and Strube, 2008), which explains their previous use as a source for evaluation gold standards (Blohm et al., 2007). The last two runs from Table 2, E3 and Ea, correspond to the set of open-domain labeled classes acquired from unstructured text. In both E3 and Ea, class labels are linked to the first sense available at the point of insertion in WordNet. In E3, the class labels are linked only if no other senses are available at the point of insertion beyond the</context>
<context position="19177" citStr="Suchanek et al., 2007" startWordPosition="2983" endWordPosition="2986">(s) 18.71 0.21 0.40 Table 3: Coverage of class instances extracted from text and linked to WordNet (used as input in runs Es and Ea respectively), measured as the fraction of WordNet HasInstance instances (used as input in run N) that occur among the class instances (Cvg=coverage) jeeves) of the WordNet instance. On average, the coverage scores for class instances of runs Es and Ea relative to run N are 0.21 and 0.40 respectively, as shown in the last row in Table 3. Comparatively, the equivalent instance coverage for run Y, which already includes most of the WordNet instances by design (cf. (Suchanek et al., 2007)), is 0.59. Relative Coverage of Class Labels: The linking of class labels to WordNet concepts allows for the expansion of the set of classes of instances acquired from text, thus increasing its usefulness in attribute extraction in particular and information extraction in general. To this effect, additional class labels are identified for existing instances, in the form of component phrases of the synsets that are superconcepts (or hypernyms, in WordNet terminology) of the synset under which the class label of the instance is linked in WordNet. For example, since the class label sports cars i</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>F. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago: a core of semantic knowledge unifying WordNet and Wikipedia. In Proceedings of the 16th World Wide Web Conference (WWW-07), pages 697–706, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wu</author>
<author>D Weld</author>
</authors>
<title>Automatically refining the Wikipedia infobox ontology.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th World Wide Web Conference (WWW-08),</booktitle>
<pages>635--644</pages>
<location>Beijing, China.</location>
<contexts>
<context position="33531" citStr="Wu and Weld, 2008" startWordPosition="5447" endWordPosition="5450">o exploit automatically-extracted classes for the purpose of attribute extraction. However, they operate on flat classes, as opposed to concepts organized hierarchically. Furthermore, they require manual mappings from extracted class labels into a selected set of evaluation classes (e.g., by mapping river systems to River, football clubs to SoccerClub, and parks to NationalPark), whereas the current method maps class labels to concepts automatically, by linking class labels and their associated instances to concepts. Manually-encoded attributes available within Wikipedia articles are used in (Wu and Weld, 2008) in order to derive other attributes from unstructured text within Web documents. Comparatively, the current method extracts attributes from query logs rather than Web documents, using labeled classes extracted automatically rather than available in manuallycreated resources, and requiring minimal supervision in the form of only 5 seed attributes provided for only one concept, rather than thousands of attributes available in millions of manually-created Wikipedia articles. To our knowledge, there is only one previous study (Pas¸ca, 2008) that directly addresses the problem of extracting attrib</context>
</contexts>
<marker>Wu, Weld, 2008</marker>
<rawString>F. Wu and D. Weld. 2008. Automatically refining the Wikipedia infobox ontology. In Proceedings of the 17th World Wide Web Conference (WWW-08), pages 635–644, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>