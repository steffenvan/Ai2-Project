<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.776108">
ISOLATING DOMAIN DEPENDENCIES IN NATURAL LANGUAGE INTERFACES
</note>
<author confidence="0.864368">
R. Grishman*, L. Hirschman+, and C. Friedman**
</author>
<affiliation confidence="0.989946">
*Department of Computer Science, New York University
</affiliation>
<address confidence="0.73029025">
New York, NY 10012
+Federal and Special Systems Group, Burroughs Corporation
Paoli, PA 19301
**Linguistic String Project, New York University
</address>
<sectionHeader confidence="0.956048" genericHeader="abstract">
Abstract
</sectionHeader>
<subsectionHeader confidence="0.528396">
Isolating the domain-dependent
</subsectionHeader>
<bodyText confidence="0.997691144329897">
information within a large natural
language system offers the general
advantages of modular design and greatly
enhances the portability of the system to
new domains. We have explored the problem
of isolating the domain dependencies
within two large natural language systems,
one for generating a tabular data base
from text (&amp;quot;information formatting&amp;quot;), the
other for retrieving information from a
data base. We describe the domain
information schema which is used to
capture the --domain-specific information,
and indicate how this information is used
throughout the two systems.
Prologue
Computational linguistics is an
interesting blend of science and
engineering. It is science insofar as we
are trying to understand a natural process
verbal communication. It is
engineering insofar as we are trying to
manage complexity -- the complexity which,
from our present viewpoint, seems inherent
in natural language systems.
One tool we have for managing
complexity is modular design -- dividing a
large system into components of manageable
size. This may mean, in particular,
separating procedures from knowledge
sources and then separating different
sources of knowledge. If we &amp;quot;factor&amp;quot; our
system in an appropriate way, we may be
able to reduce the size not just of
individual components but of the system as
a whole. Because of our involvement with
large natural language systems, we have
long been concerned with these issues of
modularity [Grishman 19801. Attacking
substantial natural language applications
will require that we scale up our already
large systems, and we believe that this
will be feasible only with systems which
have been carefully divided into modules.
One such division is the separation
of domain-specific knowledge (sometimes
called &amp;quot;world knowledge&amp;quot;) from knowledge
of the language in general. Such a
division not only confers the usual
advantages of modularity in facilitating
development of a system for a single
application, but also greatly enhances the
portability of a system to new domains.
Portability is especially enhanced if the
domain-specific information can be
empirically verified and its discovery at
least partially automated. What we
require is a compact representation of the
domain-specific information needed by the
components of a natural language system,
in a form which can be efficiently
utilized by these components. Before we
review our own efforts in this direction,
a few historical comments are in order on
isolating domain-dependent information.
In the early 1970&apos;s, the prime
concern for most designers was getting
these domain-specific (&amp;quot;semantic&amp;quot;)
constraints into their system; little
emphasis was placed on isolating this
information from the rest of the system.
For example, in the LUNAR system the
constraints of operating in a moon rocks
world were interwoven with the semantic
interpretation procedures (Woods 1972].
Interestingly, one trend of the mid-70&apos;s
was a tighter integration of
domain-specific constraints with general
grammatical constraints, in the form of
semantic grammars [Burton 1976]. By
merging grammatical and &apos;semantic&apos;
constraints, semantic grammars facilitated
the construction of small natural language
systems. On the other hand, they impeded
the capture of grammatical regularities;
adding a new syntactic pattern (e.g.,
reduced relatives) might require adding
dozens of productions (one for each
allowable combination of semantic
classes). They also made it difficult to
transport a system to a new domain. As a
result, the most recent trend has been
towards a careful isolation of
domain-specific knowledge (e.g., the RUS
System [Bobrow 1980]). In particular,
some groups which developed substantial
semantic-grammar-based systems, such as
</bodyText>
<page confidence="0.998825">
46
</page>
<bodyText confidence="0.979221714285714">
LADDER at SRI [Hendrix 19781 and PLANES at
the Univ. of Illinois [Waltz 1978], have
now developed syntactic grammars with
separate domain information components.
Our systems
For all of the reasons mentioned
above -- reduced size and complexity,
</bodyText>
<subsectionHeader confidence="0.539522">
better capture of grammatical
</subsectionHeader>
<bodyText confidence="0.998687475409836">
regularities, greater portability,
empirical verifiability -- we have been
working for the past few years to factor
out the domain dependencies from two large
natural language systems. One of these is
a system for information formatting -- the
mapping of natural language text into a
tabular data base, for subsequent use in
information retrieval and statistical
analysis; this system has been used to
process radiology reports and hospital
discharge summaries [Sager 1978, Hirschman
1982b]. The other is a question-answering
system for data retrieval from relational
data bases, including in particular those
generated by information formatting
[Grishman 1978].
In both systems the initial
processing -- parsing and transformational
decomposition -- is performed by the
Linguistic String Parser [Sager 1981]. In
formatting, the transformationally
regularized parse tree is mapped into an
information format; the format is then
&amp;quot;normalized&amp;quot; to recover zeroed information
and analyze the time structure of the
narrative [Hirschman 1981]. For
question-answering the operator-operand
tree (produced by transformational
decomposition) is first translated into a
logical form based on first-order
predicate calculus; anaphoric phrases are
resolved; the logical form is translated
into a data base retrieval request; the
data is retrieved; and, if necessary, a
full-sentence answer is generated
incorporating the retrieved data.
The domain information schema (DIS)
The domain-dependent information used
by our systems has two basic aspects.
First, it characterizes the structure of
the information in the domain. Second, it
specifies the correspondence between the
information structures as they appear in
the text and the various internal
representations of information in the
system.
We call the characterization of the
structure of information in the domain a
domain information schema or DIS (Grishman
1982]. This characterization consists
primarily of a set of semantic classes,
the words and phrases which are members of
these classes, and the allowable
predicate-argument relationships among
these classes in this domain. For
example, a schema for a medical domain
would contain classes such as PT
(patient), VPT (patient-verb), INDIC
(indicator of sign or symptom), and
BODY-PART:
</bodyText>
<figure confidence="0.948234583333333">
PT VPT
patient experience
Pt complain of
• • •
INDIC BODY-PART
swelling abdominal
stiff neck
and predicate-argument patterns such as
verb-subject-object:
VPT PT INDIC
complain of patient swelling
host-adjective:
</figure>
<sectionHeader confidence="0.819958" genericHeader="categories and subject descriptors">
BODY-PART INDIC
</sectionHeader>
<subsectionHeader confidence="0.382573">
neck stiff
</subsectionHeader>
<bodyText confidence="0.988112083333333">
(here class names are given in upper case
and members of the classes in lower case).
Certain other properties of these
predicates, such as functional
dependencies among arguments, are also
included in the DIS. For example, in the
medical domain there is a functional
relationship from tests to patients
because each test is of one and only one
patient. The DIS is thus similar to data
base schemata and to the frame-slot
structures of frame-based systems.
</bodyText>
<subsectionHeader confidence="0.670186">
Using the DIS
</subsectionHeader>
<bodyText confidence="0.999973807692308">
The domain information schema is used
most extensively in the parsing stage of
the two systems. The predicate-argument
constraints of the DIS yield a sublanguage
in the sense of Harris [Harris 1968].
These constraints are enforced by a set of
selectional restrictions added to the
Linguistic String Project English grammar.
The task of enforcing these constraints is
complicated by the wide variety of surface
structures in which a subject-verb-object
pattern may appear: declarative,
interrogative, and imperative sentences;
active and passive voice; in main
clauses, relatives, and reduced relatives;
etc. The complexity of the restrictions
is reduced by the power of the Restriction
Language to operate in terms of the string
relations (e.g., subject-verb-object or
host-modifier relations) [Sager 1975], but
it is still substantial. The virtue of
this approach, however, is that these
restrictions are essentially constant
across sublanguages, while the DIS, which
will change and grow for new applications,
Is kept to a minimum.
</bodyText>
<page confidence="0.996059">
47
</page>
<bodyText confidence="0.993870263157895">
adjuncts are checked and, if lppr(p .3 e,
a computed attribute is assigned to be
used in further selectional restrictions.
The following sentence fragment
illustrates the use of selectional
restrictions to obtain the correct parse:
Blood cultures obtained in the
visit to the emergency room
prior to admission.
Here the problem is the placement of the
prepositional phrase prior to admission,
which could modify the direcEry adjacent
noun phrase (the emergency room), but in
fact modifies the preceding noun phrase
the visit. The selection for
prepositional phrases on their hosts is
given by the P-NSTGO-HOST list (part of
the DIS). The portion of this list
relevant for the preposition prior to is:
</bodyText>
<listItem confidence="0.841130666666667">
Prep. Object of Host of
Prep. Prep. phrase
PREPTIME: (INDIC: (INDIC,TEST,VMD,
</listItem>
<bodyText confidence="0.991168542056075">
VPT,VTR,VSHOW,VHAVE),
TEST: (INDIC,TEST,VMD,
VPT,VTR,VSHOW,VHAVE),
VMD: (INDIC,TEST,VMD,
VPT,VTR,VSHOW,VHAVE),
...),
This list contains the information that a
time preposition (PREPTIME, e.g., prior
to) can appear with a VMD (medical action
word) as its prepositional object (e.g.,
prior to admission), with the
prepositional phrase modifying another VMD
word (e.g., visit); this corresponds to
the P-NSTGO-HOST pattern
PREPTIME: (VMD: (VMD)). There is no
pattern PREPTIME: (VMD: (INS?)) which
would allow prior to admission to modify
the INST word (institution word) emergency
room. The application of the selectional
constraints ensures that the incorrect
parse will be eliminated and the correct
one produced.
In order to verify these constraints,
the restrictions must determine the
semantic class membership of the noun
phrases in the sentence. Usually the
class of a noun phrase is that of the
&amp;quot;core&amp;quot; noun of the phrase. In certain
cases, however, the class of the noun
phrase as a whole is a function of the
classes of both core and adjunct. For
instance, a BODY-PART word modified by an
INDIC word becomes an INDIC phrase, as in
stiff neck. In some cases, the core is
transparent&amp;quot; and the class of the noun
phrase is determined by the class of its
right or left adjunct, so that, for
example, onset of swelling would be in the
same class as swelling. To accommodate
these situations, the DIS contains rules
for such phrasal or &amp;quot;computed&amp;quot; attributes
(see the N-LN-COMP-ATT and N-RN-COMP-ATT
lists in the appendix). Each time a noun
is encountered which can participate in a
computed attribute construction, its
The selectional restrictions serve to
exclude many incorrect but syntactically
well-formed parses. Constraints on
coordinate conjunction (requiring the
conjoining of phrases from identical or
similar semantic classes), acting together
with the computed attribute mechanism,
serve to reduce the structural ambiguity
of conjoined constructs, always a serious
problem (Hirschman 1982a]. The noun
phrase anorexia and onset of a stiff neck
illustrates this process. There are two
possible parses for this phase, namely
(anorexia and onset) of a stiff neck,
which is TiEorrect; and the correct
analysis (anorexia) and (onset of a stiff
neck). The conjunction mechanism requires
that only &amp;quot;similar&amp;quot; elements be conjoined;
this rules out the conjunction of anorexia
(an INDIC word) and onset (a BEGIN word).
However, the phrase stiff neck receives a
computed attribute /ND/C; and onset is
&amp;quot;transparent&amp;quot;, so it receives a computed
attribute INDIC from its right adjunct
stiff neck. Therefore the entire noun
phrase onset of a stiff neck has a
computed attribute INDIC, and can conjoin
(as a phrase) to anorexia, giving the
correct parse.
In addition, these selectional
patterns can be used to resolve most
homographs, that is, to determine the
class assignment of words which are
members of several classes. For example,
the word injection is ambiguous: it can
mean inflammation (an INDIC word), as in
throat injection, or it can mean shot (a
VTR word), as in penicillin injection.
The DIS enables a homograph to be
disambiguated, provided that sufficient
context is present. For example, in the
phrase throat injection, the combination
INDIC: (BODY-PART) is allowed in the
compound noun (N-NPOS) relation, whereas
the combination VTR: (BODY-PART) is not
allowed (see the appendix for the N-NPOS
list). This disambiguation is important
because the subsequent mapping into an
internal representation (information
format or predicate calculus expression)
is dependent on the correct identification
of the semantic class of each
information-carrying word.
The anaphora resolution procedure in
the question-answering system relies
crucially on the DIS. The same mechanism
which uses context to resolve homographs
also serves to determine the possible
class assignment(s) for an anaphoric
phrase. For example, if given the
question
Did it show swelling?
</bodyText>
<page confidence="0.996288">
48
</page>
<bodyText confidence="0.999850157894737">
the procedure would consult the DIS to
determine what classes of subjects can
occur with a VSHOW verb (show) and an
INDIC object (swelling). The DIS
(Appendix, section 2) indicates that the
subject in this context can be a BODY-PART
or a TEST. The anaphora resolution
procedure then searches the current and
prior sentences for an antecedent
belonging to one of those classes.
The DIS also plays a role in the
translation of queries into predicate
calculus. Specifically, the information
on functional dependencies between
arguments of predicates is used in
determining the scope of quantifiers and
conjunctions. Consider the following two
sentences, which have similar syntactic
structures:
</bodyText>
<listItem confidence="0.968163">
(1) How many patients have had
an X-ray and a biopsy?
(2) How many biopsies did
patient X and patient Y have?
</listItem>
<bodyText confidence="0.999308086956521">
The first question asks for a single
number; in other words, the scope of how
many is wider than the scope of and. Th7
second question, however, asks for two
numbers: the number of biopsies X had and
the number Y had; in this case, the scope
of and is wider than that of how many. We
know that this is the otlir7 possible
interpretation of the second question
because there are no &amp;quot;group biopsies&amp;quot; --
each biopsy is of one and only one
patient. This fact is encoded in the DIS
as a functional dependency from TEST to PT
(patient) in the triple VHAVE-PT-TEST. By
using this functional dependency
information, the system is able to assign
the correct interpretation to the two
questions.
A further application of the DIS (not
yet implemented) is the retrieval of
&amp;quot;implicit&amp;quot; or omitted information. For
example, certain compound noun
constructions can be considered to result
from the omission of the connector between
the two nouns. In these cases, it may be
possible to use the verb-subject-object
list of the DIS to identify the omitted
verb. This can be done by assuming that
the head noun of the compound noun phrase
will be the subject of the verb, and the
modifying noun the object. Thus, given
the phrase infectious disease consultant,
we have a compound noun whose head is the
the DOCTOR class, and the modifying noun
in the INDIC class. If we search the
V-S-0 list of the DIS (see appendix) for
candidate verbs, we find that a verb of
class VTR (treatment) can take a DOCTOR
subject and an INDIC object. If, in
addition, each class has a distinguished
&amp;quot;default&amp;quot; member (e.g., treat for the VTR
class), it may be possible to regularize
the compound noun by restoring the omitted
information (infectious disease consultant
&lt;= consultant who treats infectious
disease).
</bodyText>
<subsectionHeader confidence="0.89315">
Generating internal Representations
</subsectionHeader>
<bodyText confidence="0.992322107142858">
The semantic classes, and the
subject-verb-object and host-adjunct
patterns are also used to specify the
correspondence between the textual and
internal representations.
In the current information format for
hospital records, most classes map into a
unique format column. The formatting
procedure records this correspondence as a
list of semantic class - format column
pairs. For some modifiers, however, such
as time modifiers, aspectuals, and
negation, the placement of the modifier in
the format depends on the class (and hence
the placement) of the host; special
transformations are provided for the
formatting of these modifiers.
The question-answering system has
provided slightly greater generality
within a two-stage mapping. Syntactically
analyzed queries are first mapped into an
extended predicate calculus. For each
subject-verb-object and host-adjunct
pattern in the DIS, we specify a predicate
or set of predicates) and a
correspondence between syntactic roles
(subject, object, sentence adjunct) and
argument positions. Later (after anaphora
resolution)
the predicate calculus expression is
mapped into a retrieval request on the
Information format; each predicate is
defined as a projection of the information
format.
Automated verification and discovery
procedures
One of the attractive features of the
DIS is that it is empirically verifiable;
some of our current research also
addresses the possibility of (at least
partial) automation of a discovery
procedure for portions of the DIS.
Semantic classes can be identified within
a sublanguage, using techniques of
distribution analysis (Hirschman 19751:
for each pair of words in a parsed,
regularized sample corpus of a
sublanguage, a similarity coefficient is
computed based on how many common
syntactic environments the two words
occurred in (e.g., as the subject of the
same verb). &amp;quot;Clusters&amp;quot; of similar words
are then formed by grouping together words
whose similarity coefficients exceed a
certain threshold value. This technique
has been used to identify the frequently
</bodyText>
<page confidence="0.998755">
49
</page>
<bodyText confidence="0.999984642857143">
occurring members of the major semantic
classes of a radiology report domain.
Given the semantic classes, it is
then possible to identify the selectional
patterns, simply by recording those
patterns that occur in (good) parses.
This provides verification of the DIS
selectional patterns. It also allows
collection of data on the relative
frequency of occurrence of the various
patterns. The frequency data would permit
use of a weighting algorithm, in order to
&amp;quot;prefer&amp;quot; a parse with more frequently
occurring patterns to an alternate parse
with less frequently occurring patterns.
The &amp;quot;preferential&amp;quot; approach may allow
significant enhancement of parsing
robustness compared to the &amp;quot;accept/reject&amp;quot;
approach currently used. (In the
&amp;quot;accept/reject&amp;quot; approach, a parse is
either acceptable, or if it violates any
constraints, it is rejected; there is no
notion of &amp;quot;relative goodness&amp;quot; of several
parses). The preferential approach would
be particularly useful for incremental
development of a DIS in a new sublanguage,
where only partial data on selectional
patterns is available, and also in highly
non-deterministic parsing, such as speech
understanding.
One of the issues in automating the
discovery procedure for the selectional
patterns of the DIS is how to prevent
patterns from bad parses from being
included in the DIS (and thus allowing
even more bad parses). The use of
weighted patterns may provide a means for
automating the discovery of the DIS, since
&amp;quot;correct&amp;quot; patterns are more likely to
outnumber random &amp;quot;incorrect&amp;quot; patterns from
bad parses. These issues are the subject
of an ongoing research project.
</bodyText>
<sectionHeader confidence="0.98318" genericHeader="discussions">
Discussion
</sectionHeader>
<bodyText confidence="0.98929806122449">
Both systems described above have
been extensively tested. The formatting
procedure has been applied to a set of 14
hospital discharge summaries containing
over 700 sentences; it is currently being
used to process other types of hospital
records. The question-answering system
has been used on a data base of simplified
formatted radiology records. In addition,
to test its portability to quite different
domains, we have applied the system to a
simple data base of student transcripts.*
In the course of this work, we have
developed a simple, compact representation
of domain-specific knowledge and have
thereby substantially reduced the
complexity and increased the portability
* The student data base was developed by
V. K. Lamson as her master&apos;s thesis
[Lamson 82].
of our systems. Our experience with the
rather different student transcript data
base has indicated that not all domain
dependencies have yet been isolated,
particularly in specifying the mapping
from textual to internal representation.
Problems arose with the characterization
of sentence adjuncts, units of time
(semesters instead of days and months),
and nouns or noun phrases implying
computations (grade point average,
enrollment), which we intend to rectify
shortly by enriching the DIS.
Our experiments also indicated that
relatively limited domain-specific
information (primarily a characterization
of the structure of information in a
domain, rather than specific facts about
the domain) can be adequate for certain
natural language applications, such as
those described. Problems arose more
often because the selectional constraints
were too &amp;quot;tight&amp;quot; than because constraints
deducible from specific facts of the
domain were not available. As a result,
we are now beginning to experiment with
the automatic selective relaxation of
these restrictions in order to improve
parsing performance.
</bodyText>
<sectionHeader confidence="0.973635" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998003833333333">
This research was supported in part
by National Science Foundation grants MCS
80-02453 from the Division of Mathematical
and Computer Sciences and IST 81-15669
from the Division of Information Science
and Technology; in part by National
Library of Medicine grant 1-R01-LM03933,
awarded by the National Institutes of
Health, Department of Health and Human
Services; and in part by Office of Naval
Research contract N00014-75-C-0571, NR
049-347.
</bodyText>
<sectionHeader confidence="0.970356" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.720347453488372">
[Bobrow 1980] Bobrow, R.J. and Webber,
B.L. Knowledge Representation for
Syntactic/Semantic Processing, First
Annual Nat&apos;l Conf. on Artificial
Intelligence, 316-323, AAAI, Stanford,
1980.
(Burton 1976] Burton, R. Semantic
grammar: An engineering technique for
constructing natural language
understanding systems. BBN Report No.
3453, Bolt, Beranek, and Newman,
Cambridge, Mass., 1976.
(Grishman 1978) Grishman, R., and
Hirschman, L. Question Answering from
Natural Language Medical Data Bases.
Artificial Intelligence 11 (1978),
25-43.
5 0
[Grishman 1980] Grishman, R. Conjunctions
and Modularity in Language Analysis
Procedures. Proc. COLING 80,
500-503.
[Grishman 1982] Grishman, R., Hirschman,
L., and Friedman, C. Natural Language
Interfaces Using Limited Semantic
Information. Proc. COLING 82, 89-94
(North-Holland, 1982).
[Harris 1968] Harris, Z. Mathematical
Structures of Language (Interscience,
New York, 1968).
[Hendrix 1978] Hendrix, G., Sacerdoti, E.,
Sagalowicz, D., and Slocum, J.
Developing a natural language
interface to complex data. ACM TODS
3, 2 (June 1978), 105-147.
(Hirschman 1975] Hirschman, L., Grishman,
R., and Sager, N.
&amp;quot;Grammatically-Based Automatic Word
Class Formation,&amp;quot; Information
Processing and Management Vol. 11
(1975), 39-57.
[Hirschman 1981] Hirschman, L., and Story,
G. Representing Implicit and Explicit
Time Relations in Narrative. Proc.
IJCAI-81, Vol. 1, 289-295.
[Hirschman 1982a] Hirschman, L.
Constraints on Noun Phrase
Conjunction: a Domain-Independent
Mechanism. COLING 82 Abstracts,
129-133 (Charles University, Prague,
1982).
[Hirschman 1982b] Hirschman, L., and
Sager, N. Automatic Information
Formatting of a Medical Sublanguage.
In Sublanguage: Studies of Language
in Restricted SirggriTUF Domains (R.
Kittredge and J. Lehrberger, eds.),
(Walter de Gruyter, Berlin, in press).
[Lamson 1982] Lamson, V. K.
Question-Answering System for an
Academic Data Base. Unpublished
Master&apos;s Thesis, Dept. of Computer
Science, New York University, 1982.
[Sager 1975] Sager, N., and Grishman, R.
The restriction language for computer
grammars of natural language. Comm.
ACM 18, 7 (July 1975), 390-400.
[Sager 1T78] Sager, N. Natural Language
Information Formatting: The Automatic
Conversion of Texts to a Structured
Data Base. In Advances in Computers
17 (M.C. Yovitg,---id.), 89-162
(Academic Press, NY, 1978).
[Sager 1981] Sager, N. Natural Language
Information Processing.
(Addison-Wesley, 1981).
[Waltz 1978] Waltz, D. An English
language question answering system for
a large relational data base. Comm.
ACM 21, 7 (July 1978), 526-539.
[Woon-1972] Woods, W. A., Kaplan, R.
M., and Nash-Webber, B. The lunar
sciences natural language information
system: Final report. Report 2378,
Bolt, Beranek, and Newman, Cambridge,
Mass., 1972.
</reference>
<sectionHeader confidence="0.981972333333333" genericHeader="method">
APPENDIX
AN EDITED DOMAIN INFORMATION SCHEMA FOR A MEDICAL SUBLANGUAGE
1. SUBLANGUAGE SEMANTIC CLASSES
</sectionHeader>
<tableCaption confidence="0.315534">
* Below are some of the sublanguage classes used in the medical
* domain information schema; note that classes may contain
* words from different syntactic classes. The 15 classes
* shown below were selected for illustrative purposes from
* the over 50 classes in the full DIS.
* Classes are given in the format: (abbreviated) CLASS NAME,
* [explanation of name], followed by a few class members.
</tableCaption>
<sectionHeader confidence="0.312414" genericHeader="method">
PT DOCTOR INST TEST RX
</sectionHeader>
<bodyText confidence="0.88249">
(patient] [doctor] (medical [test] [medication]
institution]
patient doctor hospital x-ray penicillin
Pt consultant emergency room red cell count hydration
INDIC BODY-PART AMT PREPTIME BEGIN
(indicator of [part of body] [amount] [time [beginning]
sign/symptom] preposition] onset
swelling neck severe during start
stiff throat high after beginning
disease muscle low prior to
injection abdominal
(=inflammation] ...
</bodyText>
<page confidence="0.974055">
51
</page>
<figure confidence="0.917814916666667">
VHAVE
[possession,
association]
VTR VSHOW
[treatment] [show]
VMD VPT
[medical [patient
action] verb]
admission complain of treat/ment show have
discharge experience inject/ion reveal
visit suffer from prescribe indicate
• • •
</figure>
<sectionHeader confidence="0.908739" genericHeader="method">
2. ALLOWABLE PREDICATE-ARGUMENT RELATIONSHIPS
</sectionHeader>
<reference confidence="0.891438083333333">
* The following lists are edited versions of the various patterns
of selectional relations stated in terms of the (medical) domain
semantic classes.
LIST V-S-0 =
* Verb-Subject-Object allowable patterns, given in the form:
VERB: (SUBJECT1: (OBJECT11,...OBJECT1n),
SUBJECT2: (OBJECT21,...OBJECT2m),
Thus a VPT verb (e.g., complain of) can occur with a
PT subject and INDIC object (OK: patient complained of fever),
but not with an INDIC subject and PATIENT object
(NO: fever complained of patient).
VMD: (DOCTOR: (PT)),
</reference>
<table confidence="0.514479111111111">
VPT: (PT: (INDIC)),
VTR: (DOCTOR: (INDIC, PT, RX, TEST, VTR),
INST: (INDIC, PT, RX, TEST, VTR)),
VSHOW: (BODY-PART: (INDIC),
TEST: (INDIC)),
VHAVE: (PT: (DOCTOR, INDIC, TEST, VTR),
BODY-PART: (INDIC)),
• • •
LIST N-NPOS
</table>
<sectionHeader confidence="0.758103" genericHeader="method">
* Noun-compoundNoun list
</sectionHeader>
<bodyText confidence="0.91524225">
describes which classes of head noun can be modified by
which classes of compound noun (NPOS) modifier in the form:
HEAD-NOUN1: (MODIFIER-NOUN11 ..... MODIFIER-NOUN1n),
HEAD-NOUN2: (MODIFIER-NOUN21,...,MODIFIER-NOUN2m),
Thus the compound noun INDIC :(BODY-PART), as in
throat injection, is allowable, but the compound noun
BODY-PART :(INDIC), as in injection throat, is not.
DOCTOR: (BODY-PART lent consultant], INDIC),
</bodyText>
<sectionHeader confidence="0.995355142857143" genericHeader="method">
INDIC: (BODY-PART, INDIC),
INST: (INST [hospital emergency room]),
PT: (INDIC),
VMD: (INST, INDIC),
VTR: (INST, RX, INDIC),
LIST N-ADJ =
* Noun-Adjective list
</sectionHeader>
<bodyText confidence="0.991581">
describes which classes of head noun can be modified by
which classes of adjectives in the form:
HEAD-NOUN1: (ADJECTIVE11,...,ADJECTIVE1n),
HEAD-NOUN2: (ADJECTIVE21 ..... ADJECTIVE2m),
Thus the adjective-noun combination given by BODY-PART :(INDIC),
as in stiff neck is allowed, but BODY-PART: (AMT) is not
(NO: severe neck).
</bodyText>
<sectionHeader confidence="0.975209166666667" genericHeader="method">
BODY-PART: (BODY-PART, INDIC),
INDIC: (AMT, BODY-PART, INDIC),
PT: (INDIC),
RX: (VTR [prophylactic penicillin],
BODY-PART (cardiac medication]),
TEST: (AMT, BODY-PART),
</sectionHeader>
<page confidence="0.997301">
52
</page>
<sectionHeader confidence="0.927654" genericHeader="method">
LIST P-NSTGO-HOST =
</sectionHeader>
<bodyText confidence="0.95705675">
* Preposition-NounObject-Host
describes which prepositional phrases can modify which hosts.
These patterns have the form:
PREPOSITION1: (NOUN-OBJECT11: (HOST111, HOST112,...),
NOUN-OBJECT12: (HOST121, HOST122,...),
...),
PREPOSITION2: (NOUN-OBJECT21: (HOST211, HOST212,...),
...).
Thus the prepositional phrase given by PREPTIME:(VMD:(INDIC)),
as in swelling after admission is allowable, but there
is no pattern PREPTIME:(BODY-PART:(/NDIC)), e.g., no phrase
welling after neck.
</bodyText>
<sectionHeader confidence="0.909771833333333" genericHeader="method">
&apos;IN&apos;: (BODY-PART: (INDIC, TEST),
INST: (DOCTOR, PT).
...),
PREPTIME: (INDIC: (INDIC,TEST,VMD,VPT,VTR,VSHOW,VHAVE),
TEST: (INDIC,TEST,VMD,VPT,VTR,VSHOW,VHAVE),
VMD: (INDIC,TEST,VMD,VPT,VTR,VSHOW,VHAVE),
</sectionHeader>
<bodyText confidence="0.670473">
...).
</bodyText>
<listItem confidence="0.593092">
• • •
</listItem>
<sectionHeader confidence="0.926647" genericHeader="method">
3. COMPUTED ATTRIBUTE LISTS
</sectionHeader>
<bodyText confidence="0.945247444444445">
* There are two computed attribute lists, one for head noun + left adjuncts,
and one for head noun + right adjuncts. For computed attributes,
the list must specify the class of the head, the &amp;quot;computed&amp;quot; class,
and the class of the adjunct. The lists have the form:
HEAD-NOUN1: (COMPUTED-CLASS11: (ADJUNCT111,ADJUNCT112,...),
COMPUTED-CLASS12: (ADJUNCT121,ADJUNCT122,...),
...),
HEAD-NOUN2: (COMPUTED-CLASS21: (ADJUNCT211,ADJUNCT212,...),
...),
</bodyText>
<listItem confidence="0.757656">
• • •
</listItem>
<bodyText confidence="0.99371375">
Thus a BODY-PART head noun will give a &amp;quot;computed attribute&amp;quot; INDIC
when modified by an INDIC left modifier (stiff neck); a BEGIN
head noun will give an INDIC computed ataTEUTe when modified by
either a left or a right INDIC modifier: fever onset or
</bodyText>
<reference confidence="0.729743181818182">
onset of fever.
N-LN-COMP-ATT =
* Noun = Computed Attribute with LeftNoun adjunct
BODY-PART: (INDIC: (INDIC)),
BEGIN: (INDIC: (INDIC), TEST: (TEST), RX: (RX),
VMD: (VMD), VTR: (VTR)),
• • •
N-RN-COMP-ATT =
* Noun = Computed Attribute with RightNoun adjunct
BEGIN: (INDIC: (INDIC), TEST: (TEST), RX: (RX),
VMD: (VMD), VTR: (VTR)),
</reference>
<page confidence="0.999248">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99958">ISOLATING DOMAIN DEPENDENCIES IN NATURAL LANGUAGE INTERFACES</title>
<author confidence="0.999801">R Grishman</author>
<author confidence="0.999801">L Hirschman</author>
<author confidence="0.999801">C Friedman</author>
<affiliation confidence="0.999987">Department of Computer Science, New York University</affiliation>
<address confidence="0.998138">New York, NY 10012</address>
<affiliation confidence="0.981569">Federal and Special Systems Group, Burroughs Corporation</affiliation>
<address confidence="0.991898">Paoli, PA 19301</address>
<affiliation confidence="0.94116">Linguistic String Project, New York University</affiliation>
<abstract confidence="0.987732441331">Isolating the domain-dependent information within a large natural language system offers the general advantages of modular design and greatly enhances the portability of the system to new domains. We have explored the problem of isolating the domain dependencies within two large natural language systems, one for generating a tabular data base from text (&amp;quot;information formatting&amp;quot;), the other for retrieving information from a base. We describe the informationschema which is used to the and indicate how this information is used throughout the two systems. Computational linguistics is an interesting blend of science and engineering. It is science insofar as we are trying to understand a natural process verbal communication. It is engineering insofar as we are trying to manage complexity -the complexity which, from our present viewpoint, seems inherent in natural language systems. One tool we have for managing complexity is modular design -dividing a large system into components of manageable size. This may mean, in particular, separating procedures from knowledge sources and then separating different sources of knowledge. If we &amp;quot;factor&amp;quot; our system in an appropriate way, we may be able to reduce the size not just of individual components but of the system as a whole. Because of our involvement with large natural language systems, we have long been concerned with these issues of modularity [Grishman 19801. Attacking substantial natural language applications will require that we scale up our already large systems, and we believe that this will be feasible only with systems which have been carefully divided into modules. One such division is the separation of domain-specific knowledge (sometimes called &amp;quot;world knowledge&amp;quot;) from knowledge of the language in general. Such a division not only confers the usual advantages of modularity in facilitating development of a system for a single application, but also greatly enhances the portability of a system to new domains. Portability is especially enhanced if the domain-specific information can be empirically verified and its discovery at least partially automated. What we require is a compact representation of the domain-specific information needed by the components of a natural language system, in a form which can be efficiently utilized by these components. Before we review our own efforts in this direction, a few historical comments are in order on isolating domain-dependent information. In the early 1970&apos;s, the prime concern for most designers was getting these domain-specific (&amp;quot;semantic&amp;quot;) constraints into their system; little emphasis was placed on isolating this information from the rest of the system. For example, in the LUNAR system the constraints of operating in a moon rocks world were interwoven with the semantic interpretation procedures (Woods 1972]. Interestingly, one trend of the mid-70&apos;s was a tighter integration of domain-specific constraints with general grammatical constraints, in the form of grammars[Burton 1976]. By merging grammatical and &apos;semantic&apos; constraints, semantic grammars facilitated the construction of small natural language On hand, they impeded the capture of grammatical regularities; adding a new syntactic pattern (e.g., reduced relatives) might require adding dozens of productions (one for each allowable combination of semantic classes). They also made it difficult to transport a system to a new domain. As a result, the most recent trend has been towards a careful isolation of domain-specific knowledge (e.g., the RUS System [Bobrow 1980]). In particular, some groups which developed substantial semantic-grammar-based systems, such as 46 LADDER at SRI [Hendrix 19781 and PLANES at the Univ. of Illinois [Waltz 1978], have now developed syntactic grammars with separate domain information components. For all of the reasons mentioned above -reduced size and complexity, better capture of grammatical regularities, greater portability, empirical verifiability -we have been working for the past few years to factor out the domain dependencies from two large natural language systems. One of these is a system for information formatting -the mapping of natural language text into a tabular data base, for subsequent use in information retrieval and statistical analysis; this system has been used to process radiology reports and hospital discharge summaries [Sager 1978, Hirschman 1982b]. The other is a question-answering system for data retrieval from relational data bases, including in particular those generated by information formatting [Grishman 1978]. In both systems the initial processing -parsing and transformational decomposition -is performed by the Linguistic String Parser [Sager 1981]. In formatting, the transformationally regularized parse tree is mapped into an information format; the format is then &amp;quot;normalized&amp;quot; to recover zeroed information and analyze the time structure of the narrative [Hirschman 1981]. For question-answering the operator-operand tree (produced by transformational decomposition) is first translated into a logical form based on first-order predicate calculus; anaphoric phrases are resolved; the logical form is translated into a data base retrieval request; the data is retrieved; and, if necessary, a full-sentence answer is generated incorporating the retrieved data. information schema (DIS) The domain-dependent information used by our systems has two basic aspects. First, it characterizes the structure of the information in the domain. Second, it specifies the correspondence between the information structures as they appear in the text and the various internal representations of information in the system. We call the characterization of the structure of information in the domain a information schemaor DIS (Grishman 1982]. This characterization consists primarily of a set of semantic classes, the words and phrases which are members of these classes, and the allowable predicate-argument relationships among classes in this domain. example, a schema for a medical domain would contain classes such as PT (patient), VPT (patient-verb), INDIC (indicator of sign or symptom), and BODY-PART: PT VPT patient experience Pt complain of • • • INDIC BODY-PART swelling abdominal stiff neck and predicate-argument patterns such as verb-subject-object: VPT PT INDIC complain of patient swelling host-adjective: BODY-PART INDIC neck stiff class names given in upper case and members of the classes in lower case). other properties of predicates, such as functional dependencies among arguments, are also included in the DIS. For example, in the medical domain there is a functional relationship from tests to patients because each test is of one and only one patient. The DIS is thus similar to data base schemata and to the frame-slot structures of frame-based systems. Usingthe DIS domain information schema used most extensively in the parsing stage of systems. The predicate-argument constraints of the DIS yield a sublanguage in the sense of Harris [Harris 1968]. constraints are enforced by set of selectional restrictions added to the Linguistic String Project English grammar. The task of enforcing these constraints is complicated by the wide variety of surface structures in which a subject-verb-object pattern may appear: declarative, interrogative, and imperative sentences; active and passive voice; in main clauses, relatives, and reduced relatives; etc. The complexity of the restrictions is reduced by the power of the Restriction operate in terms of the string relations (e.g., subject-verb-object or host-modifier relations) [Sager 1975], but it is still substantial. The virtue of this approach, however, is that these restrictions are essentially constant across sublanguages, while the DIS, which will change and grow for new applications, Is kept to a minimum. 47 are checked and, if lppr(p a computed attribute is assigned to be used in further selectional restrictions. The following sentence fragment illustrates the use of selectional restrictions to obtain the correct parse: Blood cultures obtained in the visit to the emergency room prior to admission. Here the problem is the placement of the phrase priorto admission, which could modify the direcEry adjacent phrase (the emergencyroom), but in fact modifies the preceding noun phrase the visit. The selection for prepositional phrases on their hosts is given by the P-NSTGO-HOST list (part of the DIS). The portion of this list for the preposition priorto is: Objectof Host of Prep. Prep. phrase PREPTIME: (INDIC: (INDIC,TEST,VMD, VPT,VTR,VSHOW,VHAVE), TEST: (INDIC,TEST,VMD, VPT,VTR,VSHOW,VHAVE), VMD: (INDIC,TEST,VMD, VPT,VTR,VSHOW,VHAVE), ...), This list contains the information that a preposition (PREPTIME, e.g., to) can appear with a VMD (medical action word) as its prepositional object (e.g., priorto with the prepositional phrase modifying another VMD word (e.g., visit); this corresponds to the P-NSTGO-HOST pattern PREPTIME: (VMD: (VMD)). There is no pattern PREPTIME: (VMD: (INS?)) which allow priorto admissionto modify INST word (institution word) room. The application of the selectional constraints ensures that the incorrect parse will be eliminated and the correct one produced. In order to verify these constraints, the restrictions must determine the semantic class membership of the noun phrases in the sentence. Usually the class of a noun phrase is that of the &amp;quot;core&amp;quot; noun of the phrase. In certain cases, however, the class of the noun phrase as a whole is a function of the classes of both core and adjunct. For instance, a BODY-PART word modified by an INDIC word becomes an INDIC phrase, as in stiff neck. In some cases, the core is transparent&amp;quot; and the class of the noun phrase is determined by the class of its right or left adjunct, so that, for onset of swellingwould be in the class as swelling.To accommodate these situations, the DIS contains rules for such phrasal or &amp;quot;computed&amp;quot; attributes (see the N-LN-COMP-ATT and N-RN-COMP-ATT lists in the appendix). Each time a noun is encountered which can participate in a computed attribute construction, its The selectional restrictions serve to exclude many incorrect but syntactically well-formed parses. Constraints on coordinate conjunction (requiring the conjoining of phrases from identical or similar semantic classes), acting together with the computed attribute mechanism, serve to reduce the structural ambiguity of conjoined constructs, always a serious problem (Hirschman 1982a]. The noun anorexiaand onsetof a stiffneck illustrates this process. There are two possible parses for this phase, namely (anorexiaand onset)of a stiff neck, is the correct (anorexia)and (onsetof a stiff The conjunction mechanism requires that only &amp;quot;similar&amp;quot; elements be conjoined; rules out the conjunction of INDIC word) and onset(a BEGIN word). the phrase stiffneck receives a computed attribute /ND/C; and onset is &amp;quot;transparent&amp;quot;, so it receives a computed attribute INDIC from its right adjunct stiff neck. Therefore the entire noun onset of a stiffneck has a and can conjoin a phrase) to anorexia,giving the correct parse. In addition, these selectional patterns can be used to resolve most homographs, that is, to determine the class assignment of words which are members of several classes. For example, word injectionis ambiguous: it can mean inflammation (an INDIC word), as in injection,or it can mean shot word), as in penicillininjection. The DIS enables a homograph to be disambiguated, provided that sufficient context is present. For example, in the injection,the combination INDIC: (BODY-PART) is allowed in the compound noun (N-NPOS) relation, whereas the combination VTR: (BODY-PART) is not allowed (see the appendix for the N-NPOS list). This disambiguation is important because the subsequent mapping into an internal representation (information format or predicate calculus expression) is dependent on the correct identification of the semantic class of each information-carrying word. The anaphora resolution procedure in the question-answering system relies crucially on the DIS. The same mechanism which uses context to resolve homographs also serves to determine the possible class assignment(s) for an anaphoric phrase. For example, if given the question Did it show swelling? 48 the procedure would consult the DIS to determine what classes of subjects can occur with a VSHOW verb (show) and an object (swelling).The DIS (Appendix, section 2) indicates that the subject in this context can be a BODY-PART or a TEST. The anaphora resolution procedure then searches the current and prior sentences for an antecedent belonging to one of those classes. The DIS also plays a role in the translation of queries into predicate calculus. Specifically, the information on functional dependencies between arguments of predicates is used in determining the scope of quantifiers and conjunctions. Consider the following two sentences, which have similar syntactic structures: (1) How many patients have had an X-ray and a biopsy? (2) How many biopsies did patient X and patient Y have? The first question asks for a single number; in other words, the scope of how manyis wider than the scope of and. second question, however, asks for two numbers: the number of biopsies X had and the number Y had; in this case, the scope and is wider than that of how many.We know that this is the otlir7 possible interpretation of the second question because there are no &amp;quot;group biopsies&amp;quot; -each biopsy is of one and only one patient. This fact is encoded in the DIS as a functional dependency from TEST to PT (patient) in the triple VHAVE-PT-TEST. By using this functional dependency information, the system is able to assign the correct interpretation to the two questions. application of the DIS (not yet implemented) is the retrieval of &amp;quot;implicit&amp;quot; or omitted information. For example, certain compound noun constructions can be considered to result from the omission of the connector between the two nouns. In these cases, it may be possible to use the verb-subject-object of DIS to identify the omitted verb. This can be done by assuming that the head noun of the compound noun phrase the subject of the verb, and the modifying noun the object. Thus, given phrase disease consultant, we have a compound noun whose head is the the DOCTOR class, and the modifying noun in the INDIC class. If we search the list the DIS appendix) for candidate verbs, we find that a verb of class VTR (treatment) can take a DOCTOR subject and an INDIC object. If, in addition, each class has a distinguished &amp;quot;default&amp;quot; member (e.g., treat for the VTR class), it may be possible to regularize the compound noun by restoring the omitted disease consultant consultantwho infectious disease). Generating internal Representations The semantic classes, and the subject-verb-object and host-adjunct are also used specify the correspondence between the textual and internal representations. the current information for hospital records, most classes map into a unique format column. The formatting procedure records this correspondence as a list of semantic class format column pairs. For some modifiers, however, such as time modifiers, aspectuals, and negation, the placement of the modifier in the format depends on the class (and hence the placement) of the host; special provided for the formatting of these modifiers. The question-answering system has provided slightly greater generality two-stage mapping. Syntactically queries are first into an extended predicate calculus. For each subject-verb-object and host-adjunct in the DIS, we specify predicate of predicates) a between roles (subject, object, sentence adjunct) and argument positions. Later (after anaphora resolution) the predicate calculus expression is mapped into a retrieval request on the Information format; each predicate is defined as a projection of the information format. verificationand procedures One of the attractive features of the DIS is that it is empirically verifiable; some of our current research also addresses the possibility of (at least partial) automation of a discovery procedure for portions of the DIS. classes be identified within a sublanguage, using techniques of analysis (Hirschman each pair words in a parsed, regularized sample corpus of a sublanguage, a similarity coefficient is computed based on how many common syntactic environments the two words occurred in (e.g., as the subject of the same verb). &amp;quot;Clusters&amp;quot; of similar words are then formed by grouping together words whose similarity coefficients exceed a certain threshold value. This technique has been used to identify the frequently 49 occurring members of the major semantic classes of a radiology report domain. Given the semantic classes, it is then possible to identify the selectional patterns, simply by recording those patterns that occur in (good) parses. This provides verification of the DIS selectional patterns. It also allows collection of data on the relative frequency of occurrence of the various patterns. The frequency data would permit use of a weighting algorithm, in order to &amp;quot;prefer&amp;quot; a parse with more frequently occurring patterns to an alternate parse with less frequently occurring patterns. The &amp;quot;preferential&amp;quot; approach may allow significant enhancement of parsing robustness compared to the &amp;quot;accept/reject&amp;quot; approach currently used. (In the &amp;quot;accept/reject&amp;quot; approach, a parse is either acceptable, or if it violates any constraints, it is rejected; there is no notion of &amp;quot;relative goodness&amp;quot; of several parses). The preferential approach would be particularly useful for incremental development of a DIS in a new sublanguage, where only partial data on selectional patterns is available, and also in highly non-deterministic parsing, such as speech understanding. One of the issues in automating the discovery procedure for the selectional patterns of the DIS is how to prevent patterns from bad parses from being included in the DIS (and thus allowing even more bad parses). The use of weighted patterns may provide a means for automating the discovery of the DIS, since &amp;quot;correct&amp;quot; patterns are more likely to outnumber random &amp;quot;incorrect&amp;quot; patterns from bad parses. These issues are the subject of an ongoing research project. Discussion Both systems described above have been extensively tested. The formatting procedure has been applied to a set of 14 hospital discharge summaries containing over 700 sentences; it is currently being used to process other types of hospital records. The question-answering system has been used on a data base of simplified formatted radiology records. In addition, to test its portability to quite different domains, we have applied the system to a simple data base of student transcripts.* In the course of this work, we have developed a simple, compact representation of domain-specific knowledge and have thereby substantially reduced the complexity and increased the portability * The student data base was developed by V. K. Lamson as her master&apos;s thesis [Lamson 82]. of our systems. Our experience with the rather different student transcript data base has indicated that not all domain dependencies have yet been isolated, particularly in specifying the mapping from textual to internal representation. Problems arose with the characterization of sentence adjuncts, units of time (semesters instead of days and months), and nouns or noun phrases implying point average, which we intend to rectify shortly by enriching the DIS. Our experiments also indicated that relatively limited domain-specific information (primarily a characterization of the structure of information in a domain, rather than specific facts about the domain) can be adequate for certain natural language applications, such as those described. Problems arose more often because the selectional constraints were too &amp;quot;tight&amp;quot; than because constraints deducible from specific facts of the domain were not available. As a result, we are now beginning to experiment with the automatic selective relaxation of these restrictions in order to improve parsing performance. Acknowledgements</abstract>
<note confidence="0.75673765">This research was supported in part by National Science Foundation grants MCS 80-02453 from the Division of Mathematical Computer Sciences and from the Division of Information Science and Technology; in part by National Library of Medicine grant 1-R01-LM03933, awarded by the National Institutes of Health, Department of Health and Human Services; and in part by Office of Naval Research contract N00014-75-C-0571, NR 049-347. References [Bobrow 1980] Bobrow, R.J. and Webber, B.L. Knowledge Representation for Syntactic/Semantic Processing, First Nat&apos;l Conf.on Intelligence,316-323, AAAI, Stanford, 1980. (Burton 1976] Burton, R. Semantic</note>
<abstract confidence="0.926979">grammar: An engineering technique for constructing natural language understanding systems. BBN Report No.</abstract>
<address confidence="0.985223">3453, Bolt, Beranek, and Newman, Cambridge, Mass., 1976.</address>
<note confidence="0.953539073529412">(Grishman 1978) Grishman, R., and Hirschman, L. Question Answering from Natural Language Medical Data Bases. Intelligence11 (1978), 25-43. 5 0 [Grishman 1980] Grishman, R. Conjunctions and Modularity in Language Analysis Proc. COLING80, 500-503. [Grishman 1982] Grishman, R., Hirschman, L., and Friedman, C. Natural Language Interfaces Using Limited Semantic COLING82, 89-94 (North-Holland, 1982). 1968] Harris, Z. Structuresof Language(Interscience, New York, 1968). [Hendrix 1978] Hendrix, G., Sacerdoti, E., Sagalowicz, D., and Slocum, J. Developing a natural language interface to complex data. ACM TODS 3, 2 (June 1978), 105-147. (Hirschman 1975] Hirschman, L., Grishman, R., and Sager, N. &amp;quot;Grammatically-Based Automatic Word Formation,&amp;quot; Processingand ManagementVol. 11 (1975), 39-57. [Hirschman 1981] Hirschman, L., and Story, G. Representing Implicit and Explicit Time Relations in Narrative. Proc. IJCAI-81, Vol. 1, 289-295. [Hirschman 1982a] Hirschman, L. Constraints on Noun Phrase Conjunction: a Domain-Independent COLING82 Abstracts, 129-133 (Charles University, Prague, 1982). [Hirschman 1982b] Hirschman, L., and Sager, N. Automatic Information Formatting of a Medical Sublanguage. Sublanguage:Studies of SirggriTUF Domains(R. Kittredge and J. Lehrberger, eds.), (Walter de Gruyter, Berlin, in press). [Lamson 1982] Lamson, V. K. Question-Answering System for an Academic Data Base. Unpublished Master&apos;s Thesis, Dept. of Computer Science, New York University, 1982. [Sager 1975] Sager, N., and Grishman, R. The restriction language for computer natural language. Comm. ACM 18, 7 (July 1975), 390-400. [Sager 1T78] Sager, N. Natural Language Information Formatting: The Automatic Conversion of Texts to a Structured Base. In Advances in (M.C. 89-162 (Academic Press, NY, 1978). 1981] Sager, N. Language (Addison-Wesley, 1981). [Waltz 1978] Waltz, D. An English language question answering system for a large relational data base. Comm. ACM 21, 7 (July 1978), 526-539. Woods, W. A., Kaplan, R.</note>
<abstract confidence="0.316074666666667">M., and Nash-Webber, B. The lunar sciences natural language information system: Final report. Report 2378,</abstract>
<address confidence="0.8286795">Bolt, Beranek, and Newman, Cambridge, Mass., 1972.</address>
<abstract confidence="0.971904872340425">APPENDIX AN EDITED DOMAIN INFORMATION SCHEMA FOR A MEDICAL SUBLANGUAGE 1. SUBLANGUAGE SEMANTIC CLASSES * Below are some of the sublanguage classes used in the medical * domain information schema; note that classes may contain * words from different syntactic classes. The 15 classes * shown below were selected for illustrative purposes from * the over 50 classes in the full DIS. * Classes are given in the format: (abbreviated) CLASS NAME, * [explanation of name], followed by a few class members. PT DOCTOR INST TEST RX (patient] [doctor] (medical [test] [medication] institution] patient doctor hospital x-ray penicillin Pt consultant emergency room red cell count hydration INDIC BODY-PART AMT PREPTIME BEGIN (indicator of [part of body] [amount] [time [beginning] sign/symptom] preposition] during after prior to onset start beginning swelling neck severe stiff throat high disease muscle low injection abdominal (=inflammation] ... VHAVE [possession, association] VTR VSHOW [treatment] [show] VMD VPT [medical [patient action] verb] admission complain of treat/ment show have discharge experience inject/ion reveal visit suffer from prescribe indicate • • • 2. ALLOWABLE PREDICATE-ARGUMENT RELATIONSHIPS * The following lists are edited versions of the various patterns of selectional relations stated in terms of the (medical) domain semantic classes. LIST V-S-0 = * Verb-Subject-Object allowable patterns, given in the form: VERB: (SUBJECT1: (OBJECT11,...OBJECT1n), SUBJECT2: (OBJECT21,...OBJECT2m), a VPT verb (e.g., complainof) can occur with a subject and INDIC object (OK: complainedof fever), but not with an INDIC subject and PATIENT object complainedof patient).</abstract>
<keyword confidence="0.8253626">VMD: (DOCTOR: (PT)), VPT: (PT: (INDIC)), VTR: (DOCTOR: (INDIC, PT, RX, TEST, VTR), (INDIC, PT, RX, TEST, VTR)), (INDIC), (INDIC)), INST: (DOCTOR, INDIC, TEST, VTR), (INDIC)), VSHOW: (BODY-PART:</keyword>
<title confidence="0.5489424">TEST: VHAVE: (PT: BODY-PART: • • • LIST N-NPOS</title>
<abstract confidence="0.96060825">Noun-compoundNoun list describes which classes of head noun can be modified by which classes of compound noun (NPOS) modifier in the form: HEAD-NOUN1: (MODIFIER-NOUN11 ..... MODIFIER-NOUN1n), HEAD-NOUN2: (MODIFIER-NOUN21,...,MODIFIER-NOUN2m), Thus the compound noun INDIC :(BODY-PART), as in injection,is allowable, but the compound noun :(INDIC), as in throat,is not.</abstract>
<keyword confidence="0.821287666666667">DOCTOR: (BODY-PART lent consultant], INDIC), INDIC: (BODY-PART, INDIC), INST: (INST [hospital emergency room]), PT: (INDIC), VMD: (INST, INDIC), VTR: (INST, RX, INDIC),</keyword>
<abstract confidence="0.947636555555556">LIST N-ADJ = * Noun-Adjective list describes which classes of head noun can be modified by which classes of adjectives in the form: HEAD-NOUN1: (ADJECTIVE11,...,ADJECTIVE1n), HEAD-NOUN2: (ADJECTIVE21 ..... ADJECTIVE2m), Thus the adjective-noun combination given by BODY-PART :(INDIC), in stiffneck is allowed, but BODY-PART: (AMT) is not (NO: severe neck).</abstract>
<keyword confidence="0.813345833333333">BODY-PART: (BODY-PART, INDIC), INDIC: (AMT, BODY-PART, INDIC), PT: (INDIC), RX: (VTR [prophylactic penicillin], BODY-PART (cardiac medication]), TEST: (AMT, BODY-PART),</keyword>
<abstract confidence="0.784492785714286">52 LIST P-NSTGO-HOST = * Preposition-NounObject-Host describes which prepositional phrases can modify which hosts. These patterns have the form: PREPOSITION1: (NOUN-OBJECT11: (HOST111, HOST112,...), NOUN-OBJECT12: (HOST121, HOST122,...), ...), PREPOSITION2: (NOUN-OBJECT21: (HOST211, HOST212,...), ...). Thus the prepositional phrase given by PREPTIME:(VMD:(INDIC)), in swellingafter admissionis allowable, but there is no pattern PREPTIME:(BODY-PART:(/NDIC)), e.g., no phrase wellingafter neck.</abstract>
<keyword confidence="0.875171333333333">BODY-PART: (INDIC, TEST), INST: (DOCTOR, PT). ...), PREPTIME: (INDIC: (INDIC,TEST,VMD,VPT,VTR,VSHOW,VHAVE), TEST: (INDIC,TEST,VMD,VPT,VTR,VSHOW,VHAVE), VMD: (INDIC,TEST,VMD,VPT,VTR,VSHOW,VHAVE),</keyword>
<abstract confidence="0.728190555555556">3. COMPUTED ATTRIBUTE LISTS * There are two computed attribute lists, one for head noun + left adjuncts, and one for head noun + right adjuncts. For computed attributes, the list must specify the class of the head, the &amp;quot;computed&amp;quot; class, and the class of the adjunct. The lists have the form: HEAD-NOUN1: (COMPUTED-CLASS11: (ADJUNCT111,ADJUNCT112,...), COMPUTED-CLASS12: (ADJUNCT121,ADJUNCT122,...), ...), HEAD-NOUN2: (COMPUTED-CLASS21: (ADJUNCT211,ADJUNCT212,...), ...), • • • Thus a BODY-PART head noun will give a &amp;quot;computed attribute&amp;quot; INDIC when modified by an INDIC left modifier (stiff neck); a BEGIN noun will give an INDIC computed modified by either a left or a right INDIC modifier: fever onset or onsetof fever. N-LN-COMP-ATT = * Noun = Computed Attribute with LeftNoun adjunct BODY-PART: (INDIC: (INDIC)), BEGIN: (INDIC: (INDIC), TEST: (TEST), RX: (RX), VMD: (VMD), VTR: (VTR)), • • • N-RN-COMP-ATT = * Noun = Computed Attribute with RightNoun adjunct BEGIN: (INDIC: (INDIC), TEST: (TEST), RX: (RX),</abstract>
<affiliation confidence="0.836423">VMD: (VMD), VTR: (VTR)),</affiliation>
<address confidence="0.78684">53</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
<author>B L Webber</author>
</authors>
<title>Knowledge Representation for Syntactic/Semantic Processing, First Annual Nat&apos;l</title>
<date>1980</date>
<booktitle>Conf. on Artificial Intelligence, 316-323,</booktitle>
<location>AAAI, Stanford,</location>
<marker>Bobrow, Webber, 1980</marker>
<rawString>[Bobrow 1980] Bobrow, R.J. and Webber, B.L. Knowledge Representation for Syntactic/Semantic Processing, First Annual Nat&apos;l Conf. on Artificial Intelligence, 316-323, AAAI, Stanford, 1980.</rawString>
</citation>
<citation valid="true">
<title>Semantic grammar: An engineering technique for constructing natural language understanding systems.</title>
<date>1976</date>
<tech>BBN Report No. 3453,</tech>
<location>Bolt, Beranek, and Newman, Cambridge, Mass.,</location>
<marker>1976</marker>
<rawString>(Burton 1976] Burton, R. Semantic grammar: An engineering technique for constructing natural language understanding systems. BBN Report No. 3453, Bolt, Beranek, and Newman, Cambridge, Mass., 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>L Hirschman</author>
</authors>
<title>Question Answering from Natural Language Medical Data Bases.</title>
<date>1978</date>
<journal>Artificial Intelligence</journal>
<volume>11</volume>
<pages>25--43</pages>
<marker>Grishman, Hirschman, 1978</marker>
<rawString>(Grishman 1978) Grishman, R., and Hirschman, L. Question Answering from Natural Language Medical Data Bases. Artificial Intelligence 11 (1978), 25-43.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Grishman</author>
</authors>
<title>Conjunctions and Modularity in Language Analysis Procedures.</title>
<booktitle>Proc. COLING</booktitle>
<volume>80</volume>
<pages>500--503</pages>
<marker>Grishman, </marker>
<rawString>[Grishman 1980] Grishman, R. Conjunctions and Modularity in Language Analysis Procedures. Proc. COLING 80, 500-503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>L Hirschman</author>
<author>C Friedman</author>
</authors>
<title>Natural Language Interfaces Using Limited Semantic Information.</title>
<date>1982</date>
<booktitle>Proc. COLING</booktitle>
<volume>82</volume>
<pages>89--94</pages>
<marker>Grishman, Hirschman, Friedman, 1982</marker>
<rawString>[Grishman 1982] Grishman, R., Hirschman, L., and Friedman, C. Natural Language Interfaces Using Limited Semantic Information. Proc. COLING 82, 89-94 (North-Holland, 1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Mathematical Structures of Language (Interscience,</title>
<date>1968</date>
<location>New York,</location>
<contexts>
<context position="7541" citStr="Harris 1968" startWordPosition="1118" endWordPosition="1119"> the classes in lower case). Certain other properties of these predicates, such as functional dependencies among arguments, are also included in the DIS. For example, in the medical domain there is a functional relationship from tests to patients because each test is of one and only one patient. The DIS is thus similar to data base schemata and to the frame-slot structures of frame-based systems. Using the DIS The domain information schema is used most extensively in the parsing stage of the two systems. The predicate-argument constraints of the DIS yield a sublanguage in the sense of Harris [Harris 1968]. These constraints are enforced by a set of selectional restrictions added to the Linguistic String Project English grammar. The task of enforcing these constraints is complicated by the wide variety of surface structures in which a subject-verb-object pattern may appear: declarative, interrogative, and imperative sentences; active and passive voice; in main clauses, relatives, and reduced relatives; etc. The complexity of the restrictions is reduced by the power of the Restriction Language to operate in terms of the string relations (e.g., subject-verb-object or host-modifier relations) [Sa</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>[Harris 1968] Harris, Z. Mathematical Structures of Language (Interscience, New York, 1968).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hendrix</author>
<author>E Sacerdoti</author>
<author>D Sagalowicz</author>
<author>J Slocum</author>
</authors>
<title>Developing a natural language interface to complex data.</title>
<date>1978</date>
<journal>ACM TODS</journal>
<volume>3</volume>
<pages>105--147</pages>
<marker>Hendrix, Sacerdoti, Sagalowicz, Slocum, 1978</marker>
<rawString>[Hendrix 1978] Hendrix, G., Sacerdoti, E., Sagalowicz, D., and Slocum, J. Developing a natural language interface to complex data. ACM TODS 3, 2 (June 1978), 105-147.</rawString>
</citation>
<citation valid="true">
<title>Grammatically-Based Automatic Word Class Formation,&amp;quot;</title>
<date>1975</date>
<journal>Information Processing and Management</journal>
<volume>11</volume>
<pages>39--57</pages>
<marker>1975</marker>
<rawString>(Hirschman 1975] Hirschman, L., Grishman, R., and Sager, N. &amp;quot;Grammatically-Based Automatic Word Class Formation,&amp;quot; Information Processing and Management Vol. 11 (1975), 39-57.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Hirschman</author>
<author>G Story</author>
</authors>
<title>Representing Implicit and Explicit Time Relations in Narrative.</title>
<booktitle>Proc. IJCAI-81,</booktitle>
<volume>1</volume>
<pages>289--295</pages>
<marker>Hirschman, Story, </marker>
<rawString>[Hirschman 1981] Hirschman, L., and Story, G. Representing Implicit and Explicit Time Relations in Narrative. Proc. IJCAI-81, Vol. 1, 289-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
</authors>
<title>Constraints on Noun Phrase Conjunction: a Domain-Independent Mechanism.</title>
<date>1982</date>
<journal>COLING 82 Abstracts,</journal>
<pages>129--133</pages>
<institution>Charles University,</institution>
<location>Prague,</location>
<contexts>
<context position="4840" citStr="Hirschman 1982" startWordPosition="717" endWordPosition="718"> components. Our systems For all of the reasons mentioned above -- reduced size and complexity, better capture of grammatical regularities, greater portability, empirical verifiability -- we have been working for the past few years to factor out the domain dependencies from two large natural language systems. One of these is a system for information formatting -- the mapping of natural language text into a tabular data base, for subsequent use in information retrieval and statistical analysis; this system has been used to process radiology reports and hospital discharge summaries [Sager 1978, Hirschman 1982b]. The other is a question-answering system for data retrieval from relational data bases, including in particular those generated by information formatting [Grishman 1978]. In both systems the initial processing -- parsing and transformational decomposition -- is performed by the Linguistic String Parser [Sager 1981]. In formatting, the transformationally regularized parse tree is mapped into an information format; the format is then &amp;quot;normalized&amp;quot; to recover zeroed information and analyze the time structure of the narrative [Hirschman 1981]. For question-answering the operator-operand tree (p</context>
<context position="11158" citStr="Hirschman 1982" startWordPosition="1667" endWordPosition="1668">ations, the DIS contains rules for such phrasal or &amp;quot;computed&amp;quot; attributes (see the N-LN-COMP-ATT and N-RN-COMP-ATT lists in the appendix). Each time a noun is encountered which can participate in a computed attribute construction, its The selectional restrictions serve to exclude many incorrect but syntactically well-formed parses. Constraints on coordinate conjunction (requiring the conjoining of phrases from identical or similar semantic classes), acting together with the computed attribute mechanism, serve to reduce the structural ambiguity of conjoined constructs, always a serious problem (Hirschman 1982a]. The noun phrase anorexia and onset of a stiff neck illustrates this process. There are two possible parses for this phase, namely (anorexia and onset) of a stiff neck, which is TiEorrect; and the correct analysis (anorexia) and (onset of a stiff neck). The conjunction mechanism requires that only &amp;quot;similar&amp;quot; elements be conjoined; this rules out the conjunction of anorexia (an INDIC word) and onset (a BEGIN word). However, the phrase stiff neck receives a computed attribute /ND/C; and onset is &amp;quot;transparent&amp;quot;, so it receives a computed attribute INDIC from its right adjunct stiff neck. Therefo</context>
</contexts>
<marker>Hirschman, 1982</marker>
<rawString>[Hirschman 1982a] Hirschman, L. Constraints on Noun Phrase Conjunction: a Domain-Independent Mechanism. COLING 82 Abstracts, 129-133 (Charles University, Prague, 1982).</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Hirschman</author>
<author>N Sager</author>
</authors>
<title>Automatic Information Formatting of a Medical Sublanguage.</title>
<booktitle>In Sublanguage: Studies of Language in Restricted SirggriTUF Domains</booktitle>
<editor>(R. Kittredge and J. Lehrberger, eds.), (Walter de Gruyter,</editor>
<location>Berlin,</location>
<note>in press).</note>
<marker>Hirschman, Sager, </marker>
<rawString>[Hirschman 1982b] Hirschman, L., and Sager, N. Automatic Information Formatting of a Medical Sublanguage. In Sublanguage: Studies of Language in Restricted SirggriTUF Domains (R. Kittredge and J. Lehrberger, eds.), (Walter de Gruyter, Berlin, in press).</rawString>
</citation>
<citation valid="false">
<authors>
<author>V K Lamson</author>
</authors>
<title>Question-Answering System for an Academic Data Base.</title>
<publisher>Unpublished</publisher>
<marker>Lamson, </marker>
<rawString>[Lamson 1982] Lamson, V. K. Question-Answering System for an Academic Data Base. Unpublished</rawString>
</citation>
<citation valid="false">
<date>1982</date>
<institution>Master&apos;s Thesis, Dept. of Computer Science, New York University,</institution>
<marker>1982</marker>
<rawString>Master&apos;s Thesis, Dept. of Computer Science, New York University, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sager</author>
<author>R Grishman</author>
</authors>
<title>The restriction language for computer grammars of natural language.</title>
<date>1975</date>
<journal>Comm. ACM</journal>
<volume>18</volume>
<pages>390--400</pages>
<marker>Sager, Grishman, 1975</marker>
<rawString>[Sager 1975] Sager, N., and Grishman, R. The restriction language for computer grammars of natural language. Comm. ACM 18, 7 (July 1975), 390-400.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Sager</author>
</authors>
<title>Natural Language Information Formatting: The Automatic Conversion of Texts to a Structured</title>
<marker>Sager, </marker>
<rawString>[Sager 1T78] Sager, N. Natural Language Information Formatting: The Automatic Conversion of Texts to a Structured</rawString>
</citation>
<citation valid="true">
<authors>
<author>Data Base</author>
</authors>
<date>1978</date>
<booktitle>In Advances in Computers 17 (M.C. Yovitg,---id.),</booktitle>
<pages>89--162</pages>
<publisher>Academic Press,</publisher>
<location>NY,</location>
<marker>Base, 1978</marker>
<rawString>Data Base. In Advances in Computers 17 (M.C. Yovitg,---id.), 89-162 (Academic Press, NY, 1978).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sager</author>
</authors>
<title>Natural Language Information Processing.</title>
<date>1981</date>
<publisher>Addison-Wesley,</publisher>
<contexts>
<context position="5159" citStr="Sager 1981" startWordPosition="761" endWordPosition="762"> is a system for information formatting -- the mapping of natural language text into a tabular data base, for subsequent use in information retrieval and statistical analysis; this system has been used to process radiology reports and hospital discharge summaries [Sager 1978, Hirschman 1982b]. The other is a question-answering system for data retrieval from relational data bases, including in particular those generated by information formatting [Grishman 1978]. In both systems the initial processing -- parsing and transformational decomposition -- is performed by the Linguistic String Parser [Sager 1981]. In formatting, the transformationally regularized parse tree is mapped into an information format; the format is then &amp;quot;normalized&amp;quot; to recover zeroed information and analyze the time structure of the narrative [Hirschman 1981]. For question-answering the operator-operand tree (produced by transformational decomposition) is first translated into a logical form based on first-order predicate calculus; anaphoric phrases are resolved; the logical form is translated into a data base retrieval request; the data is retrieved; and, if necessary, a full-sentence answer is generated incorporating the </context>
</contexts>
<marker>Sager, 1981</marker>
<rawString>[Sager 1981] Sager, N. Natural Language Information Processing. (Addison-Wesley, 1981).</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Waltz</author>
</authors>
<title>An English language question answering system for</title>
<marker>Waltz, </marker>
<rawString>[Waltz 1978] Waltz, D. An English language question answering system for</rawString>
</citation>
<citation valid="true">
<title>a large relational data base.</title>
<date>1978</date>
<journal>Comm. ACM</journal>
<volume>21</volume>
<pages>526--539</pages>
<marker>1978</marker>
<rawString>a large relational data base. Comm. ACM 21, 7 (July 1978), 526-539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
<author>R M Kaplan</author>
<author>B Nash-Webber</author>
</authors>
<title>The lunar sciences natural language information system: Final report.</title>
<date>1972</date>
<tech>Report 2378,</tech>
<location>Bolt, Beranek, and Newman, Cambridge, Mass.,</location>
<marker>Woods, Kaplan, Nash-Webber, 1972</marker>
<rawString>[Woon-1972] Woods, W. A., Kaplan, R. M., and Nash-Webber, B. The lunar sciences natural language information system: Final report. Report 2378, Bolt, Beranek, and Newman, Cambridge, Mass., 1972.</rawString>
</citation>
<citation valid="false">
<title>The following lists are edited versions of the various patterns of selectional relations stated in terms of the (medical) domain semantic classes.</title>
<marker></marker>
<rawString>* The following lists are edited versions of the various patterns of selectional relations stated in terms of the (medical) domain semantic classes.</rawString>
</citation>
<citation valid="false">
<authors>
<author>LIST V-S-0</author>
</authors>
<title>Verb-Subject-Object allowable patterns, given in the form: VERB: (SUBJECT1: (OBJECT11,...OBJECT1n), SUBJECT2: (OBJECT21,...OBJECT2m), Thus a VPT verb (e.g., complain of) can occur with a PT subject and INDIC object (OK: patient complained of fever), but not with an INDIC subject and PATIENT object (NO: fever complained of patient). VMD: (DOCTOR: (PT)), onset of fever.</title>
<journal>N-LN-COMP-ATT = * Noun = Computed Attribute with LeftNoun adjunct BODY-PART:</journal>
<publisher>(VTR)),</publisher>
<location>INDIC: (INDIC)), BEGIN: (INDIC: (INDIC), TEST: (TEST), RX: (RX), VMD: (VMD), VTR:</location>
<marker>V-S-0, </marker>
<rawString>LIST V-S-0 = * Verb-Subject-Object allowable patterns, given in the form: VERB: (SUBJECT1: (OBJECT11,...OBJECT1n), SUBJECT2: (OBJECT21,...OBJECT2m), Thus a VPT verb (e.g., complain of) can occur with a PT subject and INDIC object (OK: patient complained of fever), but not with an INDIC subject and PATIENT object (NO: fever complained of patient). VMD: (DOCTOR: (PT)), onset of fever. N-LN-COMP-ATT = * Noun = Computed Attribute with LeftNoun adjunct BODY-PART: (INDIC: (INDIC)), BEGIN: (INDIC: (INDIC), TEST: (TEST), RX: (RX), VMD: (VMD), VTR: (VTR)), • • • N-RN-COMP-ATT = * Noun = Computed Attribute with RightNoun adjunct BEGIN: (INDIC: (INDIC), TEST: (TEST), RX: (RX), VMD: (VMD), VTR: (VTR)),</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>