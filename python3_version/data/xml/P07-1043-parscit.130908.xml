<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020818">
<title confidence="0.972147">
Sentence generation as a planning problem
</title>
<author confidence="0.998924">
Alexander Koller Matthew Stone
</author>
<affiliation confidence="0.9996995">
Center for Computational Learning Systems Computer Science
Columbia University Rutgers University
</affiliation>
<email confidence="0.99833">
koller@cs.columbia.edu Matthew.Stone@rutgers.edu
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946444444444">
We translate sentence generation from TAG
grammars with semantic and pragmatic in-
formation into a planning problem by encod-
ing the contribution of each word declara-
tively and explicitly. This allows us to ex-
ploit the performance of off-the-shelf plan-
ners. It also opens up new perspectives on
referring expression generation and the rela-
tionship between language and action.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931745762712">
Systems that produce natural language must synthe-
size the primitives of linguistic structure into well-
formed utterances that make desired contributions to
discourse. This is fundamentally a planning prob-
lem: Each linguistic primitive makes certain con-
tributions while potentially introducing new goals.
In this paper, we make this perspective explicit by
translating the sentence generation problem of TAG
grammars with semantic and pragmatic information
into a planning problem stated in the widely used
Planning Domain Definition Language (PDDL, Mc-
Dermott (2000)). The encoding provides a clean
separation between computation and linguistic mod-
elling and is open to future extensions. It also allows
us to benefit from the past and ongoing advances in
the performance of off-the-shelf planners (Blum and
Furst, 1997; Kautz and Selman, 1998; Hoffmann
and Nebel, 2001).
While there have been previous systems that en-
code generation as planning (Cohen and Perrault,
1979; Appelt, 1985; Heeman and Hirst, 1995), our
approach is distinguished from these systems by its
focus on the grammatically specified contributions
of each individual word (and the TAG tree it an-
chors) to syntax, semantics, and local pragmatics
(Hobbs et al., 1993). For example, words directly
achieve content goals by adding a corresponding se-
mantic primitive to the conversational record. We
deliberately avoid reasoning about utterances as co-
ordinated rational behavior, as earlier systems did;
this allows us to get by with a much simpler logic.
The problem we solve encompasses the genera-
tion of referring expressions (REs) as a special case.
Unlike some approaches (Dale and Reiter, 1995;
Heeman and Hirst, 1995), we do not have to dis-
tinguish between generating NPs and expressions of
other syntactic categories. We develop a new per-
spective on the lifecycle of a distractor, which allows
us to generate more succinct REs by taking the rest
of the utterance into account. More generally, we do
not split the process of sentence generation into two
separate steps of sentence planning and realization,
as most other systems do, but solve the joint prob-
lem in a single integrated step. This can potentially
allow us to generate higher-quality sentences. We
share these advantages with systems such as SPUD
(Stone et al., 2003).
Crucially, however, our approach describes the
dynamics of interpretation explicitly and declara-
tively. We do not need to assume extra machin-
ery beyond the encoding of words as PDDL plan-
ning operators; for example, our planning opera-
tors give a self-contained description of how each
individual word contributes to resolving references.
This makes our encoding more direct and transpar-
ent than those in work like Thomason and Hobbs
(1997) and Stone et al. (2003).
We present our encoding in a sequence of steps,
each of which adds more linguistic information to
</bodyText>
<page confidence="0.98395">
336
</page>
<note confidence="0.925458">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 336–343,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999859272727273">
the planning operators. After a brief review of LTAG
and PDDL, we first focus on syntax alone and show
how to cast the problem of generating grammatically
well-formed LTAG trees as a planning problem in
Section 2. In Section 3, we add semantics to the ele-
mentary trees and add goals to communicate specific
content (this corresponds to surface realization). We
complete the account by modeling referring expres-
sions and go through an example. Finally, we assess
the practical efficiency of our approach and discuss
future work in Section 4.
</bodyText>
<sectionHeader confidence="0.913261" genericHeader="method">
2 Grammaticality as planning
</sectionHeader>
<bodyText confidence="0.999974">
We start by reviewing the LTAG grammar formal-
ism and giving an intuition of how LTAG gen-
eration is planning. We then add semantic roles
to the LTAG elementary trees in order to distin-
guish different substitution nodes. Finally, we re-
view the PDDL planning specification language and
show how LTAG grammaticality can be encoded as
a PDDL problem and how we can reconstruct an
LTAG derivation from the plan.
</bodyText>
<subsectionHeader confidence="0.995235">
2.1 Tree-adjoining grammars
</subsectionHeader>
<bodyText confidence="0.994392434782609">
The grammar formalism we use here is that of lex-
icalized tree-adjoining grammars (LTAG; Joshi and
Schabes (1997)). An LTAG grammar consists of a
finite set of lexicalized elementary trees as shown in
Fig. 1a. Each elementary tree contains exactly one
anchor node, which is labelled by a word. Elemen-
tary trees can contain substitution nodes, which are
marked by down arrows (1). Those elementary trees
that are auxiliary trees also contain exactly one foot
node, which is marked with an asterisk (*). Trees
that are not auxiliary trees are called initial trees.
Elementary trees can be combined by substitution
and adjunction to form larger trees. Substitution
is the operation of replacing a substitution node of
some tree by another initial tree with the same root
label. Adjunction is the operation of splicing an aux-
iliary tree into some node v of a tree, in such a way
that the root of the auxiliary tree becomes the child
of v’s parent, and the foot node becomes the parent
of v’s children. If a node carries a null adjunction
constraint (indicated by no-adjoin), no adjunction is
allowed at this node; if it carries an obligatory ad-
junction constraint (indicated by adjoin!), an auxil-
</bodyText>
<figureCaption confidence="0.8458295">
Figure 1: Building a derived (b) and a derivation tree
(c) by combining elementary trees (a).
</figureCaption>
<bodyText confidence="0.9624069">
iary tree must be adjoined there.
In Fig. 1a, we have combined some ele-
mentary trees by substitution (indicated by the
dashed/magenta arrows) and adjunction (dotted/blue
arrows). The result of these operations is the derived
tree in Fig. 1b. The derivation tree in Fig. 1c rep-
resents the tree combination operations we used by
having one node per elementary tree and drawing a
solid edge if we combined the two trees by substitu-
tion, and a dashed edge for adjunctions.
</bodyText>
<subsectionHeader confidence="0.999752">
2.2 The basic idea
</subsectionHeader>
<bodyText confidence="0.999946">
Consider the process of constructing a derivation
tree top-down. To build the tree in Fig. 1c, say, we
start with the empty derivation tree and an obligation
to generate an expression of category S. We satisfy
this obligation by adding the tree for “likes” as the
root of the derivation; but in doing so, we have in-
troduced new unfilled substitution nodes of category
NP, i.e. the derivation tree is not complete. We use
the NP tree for “Mary” to fill one substitution node
and the NP tree for “rabbit” to fill the other. This
fills both substitution nodes, but the “rabbit” tree in-
troduces an obligatory adjunction constraint, which
we must satisfy by adjoining the auxiliary tree for
“the”. We now have a grammatical derivation tree,
but we are free to continue by adding more auxiliary
trees, such as the one for “white”.
As we have just presented it, the generation of
derivation trees is essentially a planning problem.
A planning problem involves states and actions that
can move from one state to another. The task is to
find a sequence of actions that moves us from the
</bodyText>
<figure confidence="0.999651028571429">
(b)
(a) S
S
NP
PN
PN
Mary
V
likes
the
no-adjoin
NP ↓ VP
V
likes
NP VP
N
white N
rabbit
likes
Mary rabbit
white
Mary
NP
NP *
the
NP adjoin!
(c)
the
N
white N -
NP ↓
NP
NP
N
rabbit
</figure>
<page confidence="0.984842">
337
</page>
<bodyText confidence="0.999972083333333">
initial state to a state that satisfies all the goals. In
our case, the states are defined by the unfilled sub-
stitution nodes, the unsatisfied obligatory adjunction
constraints, and the nodes that are available for ad-
junction in some (possibly incomplete) derivation
tree. Each action adds a single elementary tree to the
derivation, removing some of these “open nodes”
while introducing new ones. The initial state is asso-
ciated with the empty derivation tree and a require-
ment to generate an expression for the given root cat-
egory. The goal is for the current derivation tree to
be grammatically complete.
</bodyText>
<subsectionHeader confidence="0.998184">
2.3 Semantic roles
</subsectionHeader>
<bodyText confidence="0.999957117647059">
Formalizing this intuition requires unique names for
each node in the derived tree. Such names are nec-
essary to distinguish the different open substitution
nodes that still need to be filled, or the different
available adjunction sites; in the example, the plan-
ner needed to be aware that “likes” introduces two
separate NP substitution nodes to fill.
There are many ways to assign these names. One
that works particularly well in the context of PDDL
(as we will see below) is to assume that each node
in an elementary tree, except for ones with null ad-
junction constraints, is marked with a semantic role,
and that all substitution nodes are marked with dif-
ferent roles. Nothing hinges on the particular role in-
ventory; here we assume an inventory including the
roles ag for “agent” and pat for “patient”. We also
assume one special role self, which must be used for
the root of each elementary tree and must never be
used for substitution nodes.
We can now assign a unique name to every sub-
stitution node in a derived tree by assigning arbitrary
but distinct indices to each use of an elementary tree,
and giving the substitution node with role r in the el-
ementary tree with index i the identity i.r. In the ex-
ample, let’s say the “likes” tree has index 1 and the
semantic roles for the substitution nodes were ag and
pat, respectively. The planner action that adds this
tree would then require substitution of one NP with
identity 1.ag and another NP with identity 1.pat; the
“Mary” tree would satisfy the first requirement and
the “rabbit” tree the second. If we assume that no
elementary tree contains two internal nodes with the
same category and role, we can refer to adjunction
opportunities in a similar way.
</bodyText>
<equation confidence="0.9967228">
Action S-likes-1(u). Precond: subst(S,u),step(1)
Effect: -subst(S,u),subst(NP,1.ag),
subst(NP,1.pat),-step(1),step(2)
Action NP-Mary-2(u). Precond: subst(NP,u),step(2)
Effect: -subst(NP,u),-step(2),step(3)
Action NP-rabbit-3(u). Precond: subst(NP,u),step(3)
Effect: -subst(NP,u),canadjoin(NP,u),
mustadjoin(NP,u),-step(3),step(4)
Action NP-the-4(u). Precond: canadjoin(NP,u),step(4)
Effect: -mustadjoin(NP,u),-step(4),step(5)
</equation>
<figureCaption confidence="0.993275">
Figure 2: Some actions for the grammar in Fig. 1.
</figureCaption>
<subsectionHeader confidence="0.988961">
2.4 Encoding in PDDL
</subsectionHeader>
<bodyText confidence="0.9997352">
Now we are ready to encode the problem of generat-
ing grammatical LTAG derivation trees into PDDL.
PDDL (McDermott, 2000) is the standard input lan-
guage for modern planning systems. It is based on
the well-known STRIPS language (Fikes and Nils-
son, 1971). In this paradigm, a planning state is
defined as a finite set of ground atoms of predicate
logic that are true in this state; all other atoms are as-
sumed to be false. Actions have a number of param-
eters, as well as a precondition and effect, both of
which are logical formulas. When a planner tries to
apply an action, it will first create an action instance
by binding all parameters to constants from the do-
main. It must then verify that the precondition of the
action instance is satisfied in the current state. If so,
the action can be applied, in which case the effect is
processed in order to change the state. In STRIPS,
the precondition and effect both had to be conjunc-
tions of atoms or negated atoms; positive effects are
interpreted as making the atom true in the new state,
and negative ones as making it false. PDDL per-
mits numerous extensions to the formulas that can
be used as preconditions and effects.
Each action in our planning problem encodes the
effect of adding some elementary tree to the deriva-
tion tree. An initial tree with root category A trans-
lates to an action with a parameter u for the iden-
tity of the node that the current tree is substituted
into. The action carries the precondition subst(A,u),
and so can only be applied if u is an open substi-
tution node in the current derivation with the cor-
rect category A. Auxiliary trees are analogous, but
carry the precondition canadjoin(A,u). The effect
of an initial tree is to remove the subst condition
from the planning state (to record that the substitu-
</bodyText>
<page confidence="0.997018">
338
</page>
<figureCaption confidence="0.997819">
Figure 3: A plan for the actions in Fig. 2.
</figureCaption>
<equation confidence="0.6703318">
step(1)
subst(S,1.self)
S-likes-1
(1.self)
subst(NP,1.ag)
step(2)
NP-Mary-2
(1.ag)
step(3)
subst(NP,1.pat)
NP-rabbit-3
(1.pat)
canadjoin(NP,1.pat)
step(4)
NP-the-4
(1.pat)
canadjoin(NP,1.pat)
step(5)
subst(NP,1.pat)
mustadjoin(NP,1.pat)
</equation>
<bodyText confidence="0.998868428571429">
tion node u is now filled); an auxiliary tree has an
effect -,mustadjoin(A,u) to indicate that any oblig-
atory adjunction constraint is satisfied but leaves the
canadjoin condition in place to allow multiple ad-
junctions into the same node. In both cases, effects
add subst, canadjoin and mustadjoin atoms repre-
senting the substitution nodes and adjunction sites
that are introduced by the new elementary tree.
One remaining complication is that an action must
assign new identities to the nodes it introduces; thus
it must have access to a tree index that was not used
in the derivation tree so far. We use the number of
the current plan step as the index. We add an atom
step(1) to the initial state of the planning problem,
and we introduce k different copies of the actions for
each elementary tree, where k is some upper limit
on the plan size. These actions are identical, except
that the i-th copy has an extra precondition step(i)
and effects -,step(i) and step(i+ 1). It is no restric-
tion to assume an upper limit on the plan size, as
most modern planners search for plans smaller than
a given maximum length anyway.
Fig. 2 shows some of the actions into which the
grammar in Fig. 1 translates. We display only one
copy of each action and have left out most of the
canadjoin effects. In addition, we use an initial state
containing the atoms subst(S,1.self) and step(1)
and a final state consisting of the following goal:
</bodyText>
<equation confidence="0.52685">
VA,u.-,subst(A,u) ∧ VA,u.-,mustadjoin(A,u).
</equation>
<bodyText confidence="0.999933684210526">
We can then send the actions and the initial state
and goal specifications to any off-the-shelf planner
and obtain the plan in Fig. 3. The straight arrows in
the picture link the actions to their preconditions and
(positive) effects; the curved arrows indicate atoms
that carry over from one state to the next without
being changed by the action. Atoms are printed in
boldface iff they contradict the goal.
This plan can be read as a derivation tree that has
one node for each action instance in the plan, and an
edge from node u to node v if u establishes a subst
or canadjoin fact that is a precondition of v. These
causal links are drawn as bold edges in Fig. 3. The
mapping is unique for substitution edges because
subst atoms are removed by every action that has
them as their precondition. There may be multiple
action instances in the plan that introduce the same
atom canadjoin(A,u). In this case, we can freely
choose one of these instances as the parent.
</bodyText>
<sectionHeader confidence="0.971873" genericHeader="method">
3 Sentence generation as planning
</sectionHeader>
<bodyText confidence="0.9993265">
Now we extend this encoding to deal with semantics
and referring expressions.
</bodyText>
<subsectionHeader confidence="0.996966">
3.1 Communicative goals
</subsectionHeader>
<bodyText confidence="0.999917678571429">
In order to use the planner as a surface realiza-
tion algorithm for TAG along the lines of Koller
and Striegnitz (2002), we attach semantic content to
each elementary tree and require that the sentence
achieves a certain communicative goal. We also use
a knowledge base that specifies the speaker’s knowl-
edge, and require that we can only use trees that ex-
press information in this knowledge base.
We follow Stone et al. (2003) in formalizing the
semantic content of a lexicalized elementary tree t as
a finite set of atoms; but unlike in earlier approaches,
we use the semantic roles in t as the arguments of
these atoms. For instance, the semantic content of
the “likes” tree in Fig. 1 is {like(self,ag,pat)} (see
also the semcon entries in Fig. 4). The knowledge
base is some finite set of ground atoms; in the exam-
ple, it could contain such entries as like(e,m,r) and
rabbit(r). Finally, the communicative goal is some
subset of the knowledge base, such as {like(e,m,r)}.
We implement unsatisfied communicative goals
as flaws that the plan must remedy. To this end,
we add an atom cg(P,a1,...,an) for each element
P(a1,...,an) of the communicative goal to the ini-
tial state, and we add a corresponding conjunct
VP,x1,...,xn.-,cg(P,x1,...,xn) to the goal. In ad-
dition, we add an atom skb(P,a1,...,an) to the
initial state for each element P(a1,...,an) of the
(speaker’s) knowledge base.
</bodyText>
<page confidence="0.99826">
339
</page>
<bodyText confidence="0.999984896551724">
We then add parameters x1,...,xn to each action
with n semantic roles (including self). These new
parameters are intended to be bound to individual
constants in the knowledge base by the planner. For
each elementary tree t and possible step index i, we
establish the relationship between these parameters
and the roles in two steps. First we fix a function id
that maps the semantic roles of t to node identities.
It maps self to u and each other role r to i.r. Second,
we fix a function ref that maps the outputs of id bi-
jectively to the parameters x1,...,xn, in such a way
that ref(u) = x1.
We can then capture the contribution of the i-th
action for t to the communicative goal by giving it
an effect -cg(P,ref(id(r1)),...,ref(id(rn))) for each
element P(r1,...,rn) of the elementary tree’s seman-
tic content. We restrict ourselves to only expressing
true statements by giving the action a precondition
skb(P,ref(id(r1)),...,ref(id(rn))) for each element
of the semantic content.
In order to keep track of the connection between
node identities and individuals for future reference,
each action gets an effect referent(id(r),ref(id(r)))
for each semantic role r except self. We enforce the
connection between u and x1 by adding a precondi-
tion referent(u,x1).
In the example, the most interesting action in this
respect is the one for the elementary tree for “likes”.
This action looks as follows:
</bodyText>
<equation confidence="0.984948857142857">
Action S-likes-1(u,x1,x2,x3).
Precond: subst(S,u),step(1),referent(u,x1),
skb(like,x1,x2,x3)
Effect: -subst(S,u),subst(NP,1.ag),subst(NP,1.pat),
-step(1),step(2),
referent(1.ag,x2),referent(1.pat,x3),
-cg(like,x1,x2,x3)
</equation>
<bodyText confidence="0.999266555555556">
We can run a planner and interpret the plan as
above; the main difference is that complete plans not
only correspond to grammatical derivation trees, but
also express all communicative goals. Notice that
this encoding models some aspects of lexical choice:
The semantic content sets of the elementary trees
need not be singletons, and so there may be multiple
ways of partitioning the communicative goal into the
content sets of various elementary trees.
</bodyText>
<subsectionHeader confidence="0.998878">
3.2 Referring expressions
</subsectionHeader>
<bodyText confidence="0.99998286">
Finally, we extend the system to deal with the gen-
eration of referring expressions. While this prob-
lem is typically taken to require the generation of a
noun phrase that refers uniquely to some individual,
we don’t need to make any assumptions about the
syntactic category here. Moreover, we consider the
problem in the wider context of generating referring
expressions within a sentence, which can allow us to
generate more succinct expressions.
Because a referring expression must allow the
hearer to identify the intended referent uniquely,
we keep track of the hearer’s knowledge base sep-
arately. We use atoms hkb(P,a1,...,an), as with
skb above. In addition, we assume pragmatic
information of the form pkb(P,a1,...,an). The
three pragmatic predicates that we will use here are
hearer-new, indicating that the hearer does not know
about the existence of an individual and can’t infer it
(Stone et al., 2003), hearer-old for the opposite, and
contextset. The context set of an intended referent is
the set of all individuals that the hearer might possi-
bly confuse it with (DeVault et al., 2004). It is empty
for hearer-new individuals. To say that b is in a’s
context set, we put the atom pkb(contextset,a,b)
into the initial state.
In addition to the semantic content, we equip ev-
ery elementary tree in the grammar with a seman-
tic requirement and a pragmatic condition (Stone
et al., 2003). The semantic requirement is a set of
atoms spelling out presuppositions of an elementary
tree that can help the hearer identify what its argu-
ments refer to. For instance, “likes” has the selec-
tional restriction that its agent must be animate; thus
the hearer will not consider inanimate individuals as
distractors for the referring expression in agent posi-
tion. The pragmatic condition is a set of atoms over
the predicates in the pragmatic knowledge base.
In our setting, every substitution node that is in-
troduced during the derivation introduces a new re-
ferring expression. This means that we can dis-
tinguish the referring expressions by the identity
of the substitution node that introduced them. For
each referring expression u (where u is a node iden-
tity), we keep track of the distractors in atoms
of the form distractor(u,x). The presence of an
atom distractor(u,a) in some planning state repre-
sents the fact that the current derivation tree is not
yet informative enough to allow the hearer to iden-
tify the intended referent for u uniquely; a is an-
other individual that is not the intended referent,
</bodyText>
<page confidence="0.984144">
340
</page>
<figure confidence="0.999364192307692">
likes
semcon: {like(self,ag,pat)}
semreq: {animate(ag)}
adjoin! NP:self
N:self
rabbit
semcon: {rabbit(self)}
semcon: {name(self, mary)}
N:self
white N:self
semcon: {white(self)}
VP:self
NP:pat ↓
S:self
NP:ag ↓
V:self
NP:self
PN:self
Mary
semcon: { } semcon: { }
semreq: { } semreq: { }
pragcon: {hearer-old(self)} pragcon: {hearer-new(self)}
NP:self
the NP:self *
NP:self
a NP:self *
</figure>
<bodyText confidence="0.999155157894737">
but consistent with the partial referring expression
we have constructed so far. We enforce uniqueness
of all referring expressions by adding the conjunct
∀u,x¬distractor(u,x) to the planning goal.
Now whenever an action introduces a new substi-
tution node u, it will also introduce some distractor
atoms to record the initial distractors for the refer-
ring expression at u. An individual a is in the initial
distractor set for the substitution node with role r
if (a) it is not the intended referent, (b) it is in the
context set of the intended referent, and (c) there
is a choice of individuals for the other parameters
of the action that satisfies the semantic requirement
together with a. This is expressed by adding the
following effect for each substitution node; the con-
junction is over the elements P(r1,...,r„) of the se-
mantic requirement, and there is one universal quan-
tifier for y and for each parameter xj of the action
except for ref(id(r)).
</bodyText>
<equation confidence="0.994292">
∀y,x1,...,x„
(y # ref(id(r)) ∧ pkb(contextset,ref(id(r)),y)∧
Ahkb(P,ref(id(r1)),...,ref(id(r„)))[y/ref(id(r))])
→ distractor(id(r),y)
</equation>
<bodyText confidence="0.999933555555556">
On the other hand, a distractor a for a referring ex-
pression introduced at u is removed when we substi-
tute or adjoin an elementary tree into u which rules
a out. For instance, the elementary tree for “rabbit”
will remove all non-rabbits from the distractor set of
the substitution node into which it is substituted. We
achieve this by adding the following effect to each
action; here the conjunction is over all elements of
the semantic content.
</bodyText>
<equation confidence="0.5876815">
∀y.(¬Ahkb(P,ref(id(r1)),...,ref(id(r„))))[y/x1]
→ ¬distractor(u,y),
</equation>
<bodyText confidence="0.9860345">
Finally, each action gets its pragmatic condition
as a precondition.
</bodyText>
<subsectionHeader confidence="0.999589">
3.3 The example
</subsectionHeader>
<bodyText confidence="0.9958622">
By way of example, Fig. 5 shows the full versions
of the actions from Fig. 2, for the extended gram-
mar in Fig. 4. Let’s say that the hearer knows
about two rabbits r (which is white) and r0 (which
is not), about a person m with the name Mary, and
about an event e, and that the context set of r is
{r,r0,m,e}. Let’s also say that our communicative
goal is {like(e,m,r)}. In this case, the first action
instance in Fig. 3, S-likes-l(1.self,e,m,r), intro-
duces a substitution node with identity 1.pat. The
</bodyText>
<figureCaption confidence="0.995186">
Figure 4: The extended example grammar.
</figureCaption>
<bodyText confidence="0.99957715">
initial distractor set of this node is {r0,m} – the set
of all individuals in r’s context set except for inan-
imate objects (which violate the semantic require-
ment) and r itself. The NP-rabbit-3 action removes
m from the distractor set, but at the end of the plan in
Fig. 3, r0 is still a distractor, i.e. we have not reached
a goal state. We can complete the plan by perform-
ing a final action NP-white-5(1.pat,r), which will
remove this distractor and achieve the planning goal.
We can still reconstruct a derivation tree from the
complete plan literally as described in Section 2.
Now let’s say that the hearer did not know about
the existence of the individual r before the utterance
we are generating. We model this by marking r as
hearer-new in the pragmatic knowledge base and as-
signing it an empty context set. In this case, the re-
ferring expression 1.pat would be initialized with an
empty distractor set. This entitles us to use the action
NP-a-4 and generate the four-step plan correspond-
ing to the sentence “Mary likes a rabbit.”
</bodyText>
<sectionHeader confidence="0.998857" genericHeader="evaluation">
4 Discussion and future work
</sectionHeader>
<bodyText confidence="0.997759333333333">
In conclusion, let’s look in more detail at computa-
tional issues and the role of mutually constraining
referring expressions.
</bodyText>
<page confidence="0.969573">
341
</page>
<equation confidence="0.896100533333333">
Action S-likes-1(u,x1,x2,x3).
Precond: referent(u,x1),skb(like,x1,x2,x3),subst(S,u),step(1)
Effect: -cg(like,x1,x2,x3),-subst(S,u),-step(1),step(2),subst(NP,1.ag),subst(NP,1.pat),
by.-hkb(like,y,x2,x3) → -distractor(u,y),
by,x1,x3.x2 # yn pkb(contextset,x2,y) nanimate(y) → distractor(1.ag,y),
by,x1,x2.x3 # y n pkb(contextset,x3,y) → distractor(1.pat,y)
Action NP-Mary-2(u,x1).
Precond: referent(u,x1),skb(name,x1,mary),
subst(NP,u),step(2)
Effect: -cg(name,x1,mary),-subst(NP,u),
-step(2),step(3),
by.-hkb(name,y,mary) → -distractor(u,y)
Action NP-the-4(u,x1).
Precond: referent(u,x1),canadjoin(NP,u),step(4),
pkb(hearer-old,x1)
Effect: -mustadjoin(NP,u),-step(4),step(5)
Action NP-rabbit-3(u,x1).
Precond: referent(u,x1),skb(rabbit,x1),
subst(N,u),step(3)
Effect: -cg(rabbit,x1),-subst(N,u),-step(3),step(4),
canadjoin(NP,u),mustadjoin(NP,u),
by.-hkb(rabbit,y) → -distractor(u,y)
Action NP-a-4(u,x1).
Precond: referent(u,x1),canadjoin(NP,u),step(4),
pkb(hearer-new,x1)
Effect: -mustadjoin(NP,u),-step(4),step(5)
Action NP-white-5(u,x1).
Precond: referent(u,x1),skb(white,x1),canadjoin(NP,u),step(5)
Effect: -cg(white,x1),-mustadjoin(NP,u),-step(5),step(6),
by.-hkb(white,y) → -distractor(u,y)
</equation>
<figureCaption confidence="0.996909">
Figure 5: Some of the actions corresponding to the grammar in Fig. 4.
</figureCaption>
<subsectionHeader confidence="0.980722">
4.1 Computational issues
</subsectionHeader>
<bodyText confidence="0.999988644444444">
We lack the space to present the formal definition
of the sentence generation problem we encode into
PDDL. However, this problem is NP-complete, by
reduction of Hamiltonian Cycle – unsurprisingly,
given that it encompasses realization, and the very
similar realization problem in Koller and Striegnitz
(2002) is NP-hard. So any algorithm for our prob-
lem must be prepared for exponential runtimes.
We have implemented the translation described in
this paper and experimented with a number of differ-
ent grammars, knowledge bases, and planners. The
FF planner (Hoffmann and Nebel, 2001) can com-
pute the plans in Section 3.3 in under 100 ms us-
ing the grammar in Fig. 4. If we add 10 more lex-
icon entries to the grammar, the runtime grows to
190 ms; and for 20 more entries, to 360 ms. The
runtime also grows with the plan length: It takes
410 ms to generate a sentence “Mary likes the Adj
... Adj rabbit” with four adjectives and 890 ms for
six adjectives, corresponding to a plan length of 10.
We compared these results against a planning-based
reimplementation of SPUD’s greedy search heuris-
tic (Stone et al., 2003). This system is faster than FF
for small inputs (360 ms for four adjectives), but be-
comes slower as inputs grow larger (1000 ms for six
adjectives); but notice that while FF is also a heuris-
tic planner, it is guaranteed to find a solution if one
exists, unlike SPUD.
Planners have made tremendous progress in effi-
ciency in the past decade, and by encoding sentence
generation as a planning problem, we are set to profit
from any future improvements; it is an advantage
of the planning approach that we can compare very
different search strategies like FF’s and SPUD’s in
the same framework. However, our PDDL problems
are challenging for modern planners because most
planners start by computing all instances of atoms
and actions. In our experiments, FF generally spent
only about 10% of the runtime on search and the
rest on computing the instances; that is, there is a lot
of room for optimization. For larger grammars and
knowledge bases, the number of instances can easily
grow into the billions. In future work, we will there-
fore collaborate with experts on planning systems to
compute action instances only by need.
</bodyText>
<subsectionHeader confidence="0.987303">
4.2 Referring expressions
</subsectionHeader>
<bodyText confidence="0.999969625">
In our analysis of referring expressions, the tree t
that introduces the new substitution nodes typically
initializes the distractor sets with proper subsets of
the entire domain. This allows us to generate suc-
cinct descriptions by encoding t’s presuppositions
as semantic requirements, and localizes the inter-
actions between the referring expressions generated
for different substitution nodes within t’s action.
</bodyText>
<page confidence="0.993929">
342
</page>
<bodyText confidence="0.999968352941177">
However, an important detail in the encoding of
referring expressions above is that an individual a
counts as a distractor for the role r if there is any
tuple of values that satisfies the semantic require-
ment and has a in the r-component. This is correct,
but can sometimes lead to overly complicated refer-
ring expressions. An example is the construction “X
takes Y from Z”, which presupposes that Y is in Z.
In a scenario that involves multiple rabbits, multiple
hats, and multiple individuals that are inside other
individuals, but only one pair of a rabbit r inside a
hat h, the expression “X takes the rabbit from the
hat” is sufficient to refer uniquely to r and h (Stone
and Webber, 1998). Our system would try to gen-
erate an expression for Y that suffices by itself to
distinguish r from all distractors, and similarly for
Z. We will explore this issue further in future work.
</bodyText>
<sectionHeader confidence="0.999775" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9993985">
In this paper, we have shown how sentence gener-
ation with TAG grammars and semantic and prag-
matic information can be encoded into PDDL. Our
encoding is declarative in that it can be used with
any correct planning algorithm, and explicit in that
the actions capture the complete effect of a word on
the syntactic, semantic, and local pragmatic goals.
In terms of expressive power, it captures the core of
SPUD, except for its inference capabilities.
This work is practically relevant because it opens
up the possibility of using efficient planners to make
generators faster and more flexible. Conversely, our
PDDL problems are a challenge for current plan-
ners and open up NLG as an application domain that
planning research itself can target.
Theoretically, our encoding provides a new
framework for understanding and exploring the gen-
eral relationships between language and action. It
suggests new ways of going beyond SPUD’s expres-
sive power, to formulate utterances that describe and
disambiguate concurrent real-world actions or ex-
ploit the dynamics of linguistic context within and
across sentences.
Acknowledgments. This work was funded by a DFG re-
search fellowship and the NSF grants HLC 0308121, IGERT
0549115, and HSD 0624191. We are indebted to Henry Kautz
for his advice on planning systems, and to Owen Rambow, Bon-
nie Webber, and the anonymous reviewers for feedback.
</bodyText>
<sectionHeader confidence="0.998446" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999525042553192">
D. Appelt. 1985. Planning English Sentences. Cam-
bridge University Press, Cambridge England.
A. Blum and M. Furst. 1997. Fast planning through
graph analysis. Artificial Intelligence, 90:281–300.
P. R. Cohen and C. R. Perrault. 1979. Elements of a
plan-based theory of speech acts. Cognitive Science,
3(3):177–212.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19.
D. DeVault, C. Rich, and C. Sidner. 2004. Natural lan-
guage generation and discourse context: Computing
distractor sets from the focus stack. In Proc. FLAIRS.
R. Fikes and N. Nilsson. 1971. STRIPS: A new approach
in the application of theorem proving to problem solv-
ing. Artificial Intelligence, 2:189–208.
P. Heeman and G. Hirst. 1995. Collaborating on
referring expressions. Computational Linguistics,
21(3):351–382.
J. Hobbs, M. Stickel, D. Appelt, and P. Martin. 1993.
Interpretation as abduction. Artificial Intelligence,
63:69–142.
J. Hoffmann and B. Nebel. 2001. The FF planning
system: Fast plan generation through heuristic search.
Journal of Artificial Intelligence Research, 14.
A. Joshi and Y. Schabes. 1997. Tree-Adjoining Gram-
mars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, chapter 2, pages 69–
123. Springer-Verlag, Berlin.
H. Kautz and B. Selman. 1998. Blackbox: A new ap-
proach to the application of theorem proving to prob-
lem solving. In Workshop Planning as Combinatorial
Search, AIPS-98.
A. Koller and K. Striegnitz. 2002. Generation as depen-
dency parsing. In Proc. 40th ACL, Philadelphia.
D. V. McDermott. 2000. The 1998 AI Planning Systems
Competition. AI Magazine, 21(2):35–55.
M. Stone and B. Webber. 1998. Textual economy
through close coupling of syntax and semantics. In
Proc. INLG.
M. Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer.
2003. Microplanning with communicative inten-
tions: The SPUD system. Computational Intelligence,
19(4):311–381.
R. Thomason and J. Hobbs. 1997. Interrelating interpre-
tation and generation in an abductive framework. In
AAAI Fall Symposium on Communicative Action.
</reference>
<page confidence="0.999356">
343
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959204">
<title confidence="0.999753">Sentence generation as a planning problem</title>
<author confidence="0.999959">Alexander Koller Matthew Stone</author>
<affiliation confidence="0.999913">Center for Computational Learning Systems Computer Science Columbia University Rutgers University</affiliation>
<email confidence="0.991949">koller@cs.columbia.eduMatthew.Stone@rutgers.edu</email>
<abstract confidence="0.9967341">We translate sentence generation from TAG grammars with semantic and pragmatic information into a planning problem by encoding the contribution of each word declaratively and explicitly. This allows us to exploit the performance of off-the-shelf planners. It also opens up new perspectives on referring expression generation and the relationship between language and action.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Appelt</author>
</authors>
<title>Planning English Sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge England.</location>
<contexts>
<context position="1602" citStr="Appelt, 1985" startWordPosition="230" endWordPosition="231">sentence generation problem of TAG grammars with semantic and pragmatic information into a planning problem stated in the widely used Planning Domain Definition Language (PDDL, McDermott (2000)). The encoding provides a clean separation between computation and linguistic modelling and is open to future extensions. It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners (Blum and Furst, 1997; Kautz and Selman, 1998; Hoffmann and Nebel, 2001). While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavior, as earlier systems did; this allows us to get by with a much simpler logic. The problem we solve encompasses the generation of referring ex</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>D. Appelt. 1985. Planning English Sentences. Cambridge University Press, Cambridge England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>M Furst</author>
</authors>
<title>Fast planning through graph analysis.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<pages>90--281</pages>
<contexts>
<context position="1436" citStr="Blum and Furst, 1997" startWordPosition="202" endWordPosition="205">g problem: Each linguistic primitive makes certain contributions while potentially introducing new goals. In this paper, we make this perspective explicit by translating the sentence generation problem of TAG grammars with semantic and pragmatic information into a planning problem stated in the widely used Planning Domain Definition Language (PDDL, McDermott (2000)). The encoding provides a clean separation between computation and linguistic modelling and is open to future extensions. It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners (Blum and Furst, 1997; Kautz and Selman, 1998; Hoffmann and Nebel, 2001). While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coor</context>
</contexts>
<marker>Blum, Furst, 1997</marker>
<rawString>A. Blum and M. Furst. 1997. Fast planning through graph analysis. Artificial Intelligence, 90:281–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>C R Perrault</author>
</authors>
<title>Elements of a plan-based theory of speech acts.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="1588" citStr="Cohen and Perrault, 1979" startWordPosition="226" endWordPosition="229">plicit by translating the sentence generation problem of TAG grammars with semantic and pragmatic information into a planning problem stated in the widely used Planning Domain Definition Language (PDDL, McDermott (2000)). The encoding provides a clean separation between computation and linguistic modelling and is open to future extensions. It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners (Blum and Furst, 1997; Kautz and Selman, 1998; Hoffmann and Nebel, 2001). While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavior, as earlier systems did; this allows us to get by with a much simpler logic. The problem we solve encompasses the generation o</context>
</contexts>
<marker>Cohen, Perrault, 1979</marker>
<rawString>P. R. Cohen and C. R. Perrault. 1979. Elements of a plan-based theory of speech acts. Cognitive Science, 3(3):177–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<contexts>
<context position="2282" citStr="Dale and Reiter, 1995" startWordPosition="335" endWordPosition="338">om these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavior, as earlier systems did; this allows us to get by with a much simpler logic. The problem we solve encompasses the generation of referring expressions (REs) as a special case. Unlike some approaches (Dale and Reiter, 1995; Heeman and Hirst, 1995), we do not have to distinguish between generating NPs and expressions of other syntactic categories. We develop a new perspective on the lifecycle of a distractor, which allows us to generate more succinct REs by taking the rest of the utterance into account. More generally, we do not split the process of sentence generation into two separate steps of sentence planning and realization, as most other systems do, but solve the joint problem in a single integrated step. This can potentially allow us to generate higher-quality sentences. We share these advantages with sys</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D DeVault</author>
<author>C Rich</author>
<author>C Sidner</author>
</authors>
<title>Natural language generation and discourse context: Computing distractor sets from the focus stack. In</title>
<date>2004</date>
<booktitle>Proc. FLAIRS.</booktitle>
<contexts>
<context position="19794" citStr="DeVault et al., 2004" startWordPosition="3232" endWordPosition="3235"> must allow the hearer to identify the intended referent uniquely, we keep track of the hearer’s knowledge base separately. We use atoms hkb(P,a1,...,an), as with skb above. In addition, we assume pragmatic information of the form pkb(P,a1,...,an). The three pragmatic predicates that we will use here are hearer-new, indicating that the hearer does not know about the existence of an individual and can’t infer it (Stone et al., 2003), hearer-old for the opposite, and contextset. The context set of an intended referent is the set of all individuals that the hearer might possibly confuse it with (DeVault et al., 2004). It is empty for hearer-new individuals. To say that b is in a’s context set, we put the atom pkb(contextset,a,b) into the initial state. In addition to the semantic content, we equip every elementary tree in the grammar with a semantic requirement and a pragmatic condition (Stone et al., 2003). The semantic requirement is a set of atoms spelling out presuppositions of an elementary tree that can help the hearer identify what its arguments refer to. For instance, “likes” has the selectional restriction that its agent must be animate; thus the hearer will not consider inanimate individuals as </context>
</contexts>
<marker>DeVault, Rich, Sidner, 2004</marker>
<rawString>D. DeVault, C. Rich, and C. Sidner. 2004. Natural language generation and discourse context: Computing distractor sets from the focus stack. In Proc. FLAIRS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fikes</author>
<author>N Nilsson</author>
</authors>
<title>STRIPS: A new approach in the application of theorem proving to problem solving.</title>
<date>1971</date>
<journal>Artificial Intelligence,</journal>
<pages>2--189</pages>
<contexts>
<context position="10807" citStr="Fikes and Nilsson, 1971" startWordPosition="1749" endWordPosition="1753">NP-Mary-2(u). Precond: subst(NP,u),step(2) Effect: -subst(NP,u),-step(2),step(3) Action NP-rabbit-3(u). Precond: subst(NP,u),step(3) Effect: -subst(NP,u),canadjoin(NP,u), mustadjoin(NP,u),-step(3),step(4) Action NP-the-4(u). Precond: canadjoin(NP,u),step(4) Effect: -mustadjoin(NP,u),-step(4),step(5) Figure 2: Some actions for the grammar in Fig. 1. 2.4 Encoding in PDDL Now we are ready to encode the problem of generating grammatical LTAG derivation trees into PDDL. PDDL (McDermott, 2000) is the standard input language for modern planning systems. It is based on the well-known STRIPS language (Fikes and Nilsson, 1971). In this paradigm, a planning state is defined as a finite set of ground atoms of predicate logic that are true in this state; all other atoms are assumed to be false. Actions have a number of parameters, as well as a precondition and effect, both of which are logical formulas. When a planner tries to apply an action, it will first create an action instance by binding all parameters to constants from the domain. It must then verify that the precondition of the action instance is satisfied in the current state. If so, the action can be applied, in which case the effect is processed in order to</context>
</contexts>
<marker>Fikes, Nilsson, 1971</marker>
<rawString>R. Fikes and N. Nilsson. 1971. STRIPS: A new approach in the application of theorem proving to problem solving. Artificial Intelligence, 2:189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heeman</author>
<author>G Hirst</author>
</authors>
<title>Collaborating on referring expressions.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="1627" citStr="Heeman and Hirst, 1995" startWordPosition="232" endWordPosition="235">ation problem of TAG grammars with semantic and pragmatic information into a planning problem stated in the widely used Planning Domain Definition Language (PDDL, McDermott (2000)). The encoding provides a clean separation between computation and linguistic modelling and is open to future extensions. It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners (Blum and Furst, 1997; Kautz and Selman, 1998; Hoffmann and Nebel, 2001). While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavior, as earlier systems did; this allows us to get by with a much simpler logic. The problem we solve encompasses the generation of referring expressions (REs) as a spec</context>
</contexts>
<marker>Heeman, Hirst, 1995</marker>
<rawString>P. Heeman and G. Hirst. 1995. Collaborating on referring expressions. Computational Linguistics, 21(3):351–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>M Stickel</author>
<author>D Appelt</author>
<author>P Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--69</pages>
<contexts>
<context position="1853" citStr="Hobbs et al., 1993" startWordPosition="268" endWordPosition="271"> computation and linguistic modelling and is open to future extensions. It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners (Blum and Furst, 1997; Kautz and Selman, 1998; Hoffmann and Nebel, 2001). While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavior, as earlier systems did; this allows us to get by with a much simpler logic. The problem we solve encompasses the generation of referring expressions (REs) as a special case. Unlike some approaches (Dale and Reiter, 1995; Heeman and Hirst, 1995), we do not have to distinguish between generating NPs and expressions of other syntactic categories. We develop a new perspective on the lifecycl</context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>J. Hobbs, M. Stickel, D. Appelt, and P. Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63:69–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoffmann</author>
<author>B Nebel</author>
</authors>
<title>The FF planning system: Fast plan generation through heuristic search.</title>
<date>2001</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>14</volume>
<contexts>
<context position="1487" citStr="Hoffmann and Nebel, 2001" startWordPosition="210" endWordPosition="213">tain contributions while potentially introducing new goals. In this paper, we make this perspective explicit by translating the sentence generation problem of TAG grammars with semantic and pragmatic information into a planning problem stated in the widely used Planning Domain Definition Language (PDDL, McDermott (2000)). The encoding provides a clean separation between computation and linguistic modelling and is open to future extensions. It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners (Blum and Furst, 1997; Kautz and Selman, 1998; Hoffmann and Nebel, 2001). While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavior, as earlier systems did; </context>
<context position="26893" citStr="Hoffmann and Nebel, 2001" startWordPosition="4270" endWordPosition="4273">ammar in Fig. 4. 4.1 Computational issues We lack the space to present the formal definition of the sentence generation problem we encode into PDDL. However, this problem is NP-complete, by reduction of Hamiltonian Cycle – unsurprisingly, given that it encompasses realization, and the very similar realization problem in Koller and Striegnitz (2002) is NP-hard. So any algorithm for our problem must be prepared for exponential runtimes. We have implemented the translation described in this paper and experimented with a number of different grammars, knowledge bases, and planners. The FF planner (Hoffmann and Nebel, 2001) can compute the plans in Section 3.3 in under 100 ms using the grammar in Fig. 4. If we add 10 more lexicon entries to the grammar, the runtime grows to 190 ms; and for 20 more entries, to 360 ms. The runtime also grows with the plan length: It takes 410 ms to generate a sentence “Mary likes the Adj ... Adj rabbit” with four adjectives and 890 ms for six adjectives, corresponding to a plan length of 10. We compared these results against a planning-based reimplementation of SPUD’s greedy search heuristic (Stone et al., 2003). This system is faster than FF for small inputs (360 ms for four adje</context>
</contexts>
<marker>Hoffmann, Nebel, 2001</marker>
<rawString>J. Hoffmann and B. Nebel. 2001. The FF planning system: Fast plan generation through heuristic search. Journal of Artificial Intelligence Research, 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-Adjoining Grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages, chapter 2,</booktitle>
<pages>69--123</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="4791" citStr="Joshi and Schabes (1997)" startWordPosition="743" endWordPosition="746">r approach and discuss future work in Section 4. 2 Grammaticality as planning We start by reviewing the LTAG grammar formalism and giving an intuition of how LTAG generation is planning. We then add semantic roles to the LTAG elementary trees in order to distinguish different substitution nodes. Finally, we review the PDDL planning specification language and show how LTAG grammaticality can be encoded as a PDDL problem and how we can reconstruct an LTAG derivation from the plan. 2.1 Tree-adjoining grammars The grammar formalism we use here is that of lexicalized tree-adjoining grammars (LTAG; Joshi and Schabes (1997)). An LTAG grammar consists of a finite set of lexicalized elementary trees as shown in Fig. 1a. Each elementary tree contains exactly one anchor node, which is labelled by a word. Elementary trees can contain substitution nodes, which are marked by down arrows (1). Those elementary trees that are auxiliary trees also contain exactly one foot node, which is marked with an asterisk (*). Trees that are not auxiliary trees are called initial trees. Elementary trees can be combined by substitution and adjunction to form larger trees. Substitution is the operation of replacing a substitution node o</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A. Joshi and Y. Schabes. 1997. Tree-Adjoining Grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, chapter 2, pages 69– 123. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kautz</author>
<author>B Selman</author>
</authors>
<title>Blackbox: A new approach to the application of theorem proving to problem solving.</title>
<date>1998</date>
<booktitle>In Workshop Planning as Combinatorial Search, AIPS-98.</booktitle>
<contexts>
<context position="1460" citStr="Kautz and Selman, 1998" startWordPosition="206" endWordPosition="209">stic primitive makes certain contributions while potentially introducing new goals. In this paper, we make this perspective explicit by translating the sentence generation problem of TAG grammars with semantic and pragmatic information into a planning problem stated in the widely used Planning Domain Definition Language (PDDL, McDermott (2000)). The encoding provides a clean separation between computation and linguistic modelling and is open to future extensions. It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners (Blum and Furst, 1997; Kautz and Selman, 1998; Hoffmann and Nebel, 2001). While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993). For example, words directly achieve content goals by adding a corresponding semantic primitive to the conversational record. We deliberately avoid reasoning about utterances as coordinated rational behavio</context>
</contexts>
<marker>Kautz, Selman, 1998</marker>
<rawString>H. Kautz and B. Selman. 1998. Blackbox: A new approach to the application of theorem proving to problem solving. In Workshop Planning as Combinatorial Search, AIPS-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>K Striegnitz</author>
</authors>
<title>Generation as dependency parsing.</title>
<date>2002</date>
<journal>AI Magazine,</journal>
<booktitle>In Proc. 40th ACL,</booktitle>
<volume>21</volume>
<issue>2</issue>
<publisher>The</publisher>
<contexts>
<context position="15321" citStr="Koller and Striegnitz (2002)" startWordPosition="2526" endWordPosition="2529">a precondition of v. These causal links are drawn as bold edges in Fig. 3. The mapping is unique for substitution edges because subst atoms are removed by every action that has them as their precondition. There may be multiple action instances in the plan that introduce the same atom canadjoin(A,u). In this case, we can freely choose one of these instances as the parent. 3 Sentence generation as planning Now we extend this encoding to deal with semantics and referring expressions. 3.1 Communicative goals In order to use the planner as a surface realization algorithm for TAG along the lines of Koller and Striegnitz (2002), we attach semantic content to each elementary tree and require that the sentence achieves a certain communicative goal. We also use a knowledge base that specifies the speaker’s knowledge, and require that we can only use trees that express information in this knowledge base. We follow Stone et al. (2003) in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms; but unlike in earlier approaches, we use the semantic roles in t as the arguments of these atoms. For instance, the semantic content of the “likes” tree in Fig. 1 is {like(self,ag,pat)} (see als</context>
<context position="26618" citStr="Koller and Striegnitz (2002)" startWordPosition="4226" endWordPosition="4229"> Effect: -mustadjoin(NP,u),-step(4),step(5) Action NP-white-5(u,x1). Precond: referent(u,x1),skb(white,x1),canadjoin(NP,u),step(5) Effect: -cg(white,x1),-mustadjoin(NP,u),-step(5),step(6), by.-hkb(white,y) → -distractor(u,y) Figure 5: Some of the actions corresponding to the grammar in Fig. 4. 4.1 Computational issues We lack the space to present the formal definition of the sentence generation problem we encode into PDDL. However, this problem is NP-complete, by reduction of Hamiltonian Cycle – unsurprisingly, given that it encompasses realization, and the very similar realization problem in Koller and Striegnitz (2002) is NP-hard. So any algorithm for our problem must be prepared for exponential runtimes. We have implemented the translation described in this paper and experimented with a number of different grammars, knowledge bases, and planners. The FF planner (Hoffmann and Nebel, 2001) can compute the plans in Section 3.3 in under 100 ms using the grammar in Fig. 4. If we add 10 more lexicon entries to the grammar, the runtime grows to 190 ms; and for 20 more entries, to 360 ms. The runtime also grows with the plan length: It takes 410 ms to generate a sentence “Mary likes the Adj ... Adj rabbit” with fo</context>
</contexts>
<marker>Koller, Striegnitz, 2002</marker>
<rawString>A. Koller and K. Striegnitz. 2002. Generation as dependency parsing. In Proc. 40th ACL, Philadelphia. D. V. McDermott. 2000. The 1998 AI Planning Systems Competition. AI Magazine, 21(2):35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>B Webber</author>
</authors>
<title>Textual economy through close coupling of syntax and semantics.</title>
<date>1998</date>
<booktitle>In Proc. INLG.</booktitle>
<contexts>
<context position="29685" citStr="Stone and Webber, 1998" startWordPosition="4750" endWordPosition="4753">ove is that an individual a counts as a distractor for the role r if there is any tuple of values that satisfies the semantic requirement and has a in the r-component. This is correct, but can sometimes lead to overly complicated referring expressions. An example is the construction “X takes Y from Z”, which presupposes that Y is in Z. In a scenario that involves multiple rabbits, multiple hats, and multiple individuals that are inside other individuals, but only one pair of a rabbit r inside a hat h, the expression “X takes the rabbit from the hat” is sufficient to refer uniquely to r and h (Stone and Webber, 1998). Our system would try to generate an expression for Y that suffices by itself to distinguish r from all distractors, and similarly for Z. We will explore this issue further in future work. 5 Conclusion In this paper, we have shown how sentence generation with TAG grammars and semantic and pragmatic information can be encoded into PDDL. Our encoding is declarative in that it can be used with any correct planning algorithm, and explicit in that the actions capture the complete effect of a word on the syntactic, semantic, and local pragmatic goals. In terms of expressive power, it captures the c</context>
</contexts>
<marker>Stone, Webber, 1998</marker>
<rawString>M. Stone and B. Webber. 1998. Textual economy through close coupling of syntax and semantics. In Proc. INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>C Doran</author>
<author>B Webber</author>
<author>T Bleam</author>
<author>M Palmer</author>
</authors>
<title>Microplanning with communicative intentions: The SPUD system.</title>
<date>2003</date>
<journal>Computational Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="2920" citStr="Stone et al., 2003" startWordPosition="442" endWordPosition="445"> 1995), we do not have to distinguish between generating NPs and expressions of other syntactic categories. We develop a new perspective on the lifecycle of a distractor, which allows us to generate more succinct REs by taking the rest of the utterance into account. More generally, we do not split the process of sentence generation into two separate steps of sentence planning and realization, as most other systems do, but solve the joint problem in a single integrated step. This can potentially allow us to generate higher-quality sentences. We share these advantages with systems such as SPUD (Stone et al., 2003). Crucially, however, our approach describes the dynamics of interpretation explicitly and declaratively. We do not need to assume extra machinery beyond the encoding of words as PDDL planning operators; for example, our planning operators give a self-contained description of how each individual word contributes to resolving references. This makes our encoding more direct and transparent than those in work like Thomason and Hobbs (1997) and Stone et al. (2003). We present our encoding in a sequence of steps, each of which adds more linguistic information to 336 Proceedings of the 45th Annual M</context>
<context position="15629" citStr="Stone et al. (2003)" startWordPosition="2578" endWordPosition="2581"> can freely choose one of these instances as the parent. 3 Sentence generation as planning Now we extend this encoding to deal with semantics and referring expressions. 3.1 Communicative goals In order to use the planner as a surface realization algorithm for TAG along the lines of Koller and Striegnitz (2002), we attach semantic content to each elementary tree and require that the sentence achieves a certain communicative goal. We also use a knowledge base that specifies the speaker’s knowledge, and require that we can only use trees that express information in this knowledge base. We follow Stone et al. (2003) in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms; but unlike in earlier approaches, we use the semantic roles in t as the arguments of these atoms. For instance, the semantic content of the “likes” tree in Fig. 1 is {like(self,ag,pat)} (see also the semcon entries in Fig. 4). The knowledge base is some finite set of ground atoms; in the example, it could contain such entries as like(e,m,r) and rabbit(r). Finally, the communicative goal is some subset of the knowledge base, such as {like(e,m,r)}. We implement unsatisfied communicative goals as fla</context>
<context position="19608" citStr="Stone et al., 2003" startWordPosition="3200" endWordPosition="3203">onsider the problem in the wider context of generating referring expressions within a sentence, which can allow us to generate more succinct expressions. Because a referring expression must allow the hearer to identify the intended referent uniquely, we keep track of the hearer’s knowledge base separately. We use atoms hkb(P,a1,...,an), as with skb above. In addition, we assume pragmatic information of the form pkb(P,a1,...,an). The three pragmatic predicates that we will use here are hearer-new, indicating that the hearer does not know about the existence of an individual and can’t infer it (Stone et al., 2003), hearer-old for the opposite, and contextset. The context set of an intended referent is the set of all individuals that the hearer might possibly confuse it with (DeVault et al., 2004). It is empty for hearer-new individuals. To say that b is in a’s context set, we put the atom pkb(contextset,a,b) into the initial state. In addition to the semantic content, we equip every elementary tree in the grammar with a semantic requirement and a pragmatic condition (Stone et al., 2003). The semantic requirement is a set of atoms spelling out presuppositions of an elementary tree that can help the hear</context>
<context position="27423" citStr="Stone et al., 2003" startWordPosition="4371" endWordPosition="4374">ferent grammars, knowledge bases, and planners. The FF planner (Hoffmann and Nebel, 2001) can compute the plans in Section 3.3 in under 100 ms using the grammar in Fig. 4. If we add 10 more lexicon entries to the grammar, the runtime grows to 190 ms; and for 20 more entries, to 360 ms. The runtime also grows with the plan length: It takes 410 ms to generate a sentence “Mary likes the Adj ... Adj rabbit” with four adjectives and 890 ms for six adjectives, corresponding to a plan length of 10. We compared these results against a planning-based reimplementation of SPUD’s greedy search heuristic (Stone et al., 2003). This system is faster than FF for small inputs (360 ms for four adjectives), but becomes slower as inputs grow larger (1000 ms for six adjectives); but notice that while FF is also a heuristic planner, it is guaranteed to find a solution if one exists, unlike SPUD. Planners have made tremendous progress in efficiency in the past decade, and by encoding sentence generation as a planning problem, we are set to profit from any future improvements; it is an advantage of the planning approach that we can compare very different search strategies like FF’s and SPUD’s in the same framework. However,</context>
</contexts>
<marker>Stone, Doran, Webber, Bleam, Palmer, 2003</marker>
<rawString>M. Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer. 2003. Microplanning with communicative intentions: The SPUD system. Computational Intelligence, 19(4):311–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Thomason</author>
<author>J Hobbs</author>
</authors>
<title>Interrelating interpretation and generation in an abductive framework.</title>
<date>1997</date>
<booktitle>In AAAI Fall Symposium on Communicative Action.</booktitle>
<contexts>
<context position="3360" citStr="Thomason and Hobbs (1997)" startWordPosition="511" endWordPosition="514">e joint problem in a single integrated step. This can potentially allow us to generate higher-quality sentences. We share these advantages with systems such as SPUD (Stone et al., 2003). Crucially, however, our approach describes the dynamics of interpretation explicitly and declaratively. We do not need to assume extra machinery beyond the encoding of words as PDDL planning operators; for example, our planning operators give a self-contained description of how each individual word contributes to resolving references. This makes our encoding more direct and transparent than those in work like Thomason and Hobbs (1997) and Stone et al. (2003). We present our encoding in a sequence of steps, each of which adds more linguistic information to 336 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 336–343, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics the planning operators. After a brief review of LTAG and PDDL, we first focus on syntax alone and show how to cast the problem of generating grammatically well-formed LTAG trees as a planning problem in Section 2. In Section 3, we add semantics to the elementary trees and add goals to</context>
</contexts>
<marker>Thomason, Hobbs, 1997</marker>
<rawString>R. Thomason and J. Hobbs. 1997. Interrelating interpretation and generation in an abductive framework. In AAAI Fall Symposium on Communicative Action.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>