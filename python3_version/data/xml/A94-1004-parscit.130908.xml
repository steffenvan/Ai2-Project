<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000083">
<title confidence="0.6873724">
Modeling Content Identification from Document Images
Takehiro Nakayama
Fuji Xerox Palo Alto Laboratory
3400 Hillview Avenue
Palo Alto, CA 94304 USA
</title>
<email confidence="0.72878">
nakayamagpal.xerox.com
</email>
<sectionHeader confidence="0.986738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920466666667">
A new technique to locate content-represent-
ing words for a given document image using
abstract representation of character shapes is
described. A character shape code representa-
tion defined by the location of a character in a
text line has been developed. Character shape
code generation avoids the computational
expense of conventional optical character rec-
ognition (OCR). Because character shape
codes are an abstraction of standard character
code (e.g., ASCII), the mapping is ambiguous.
In this paper, the ambiguity is shown to be
practically limited to an acceptable level. It is
illustrated that: first, punctuation marks are
clearly distinguished from the other charac-
ters; second, stop words are generally distin-
guishable from other words, because the
permutations of character shape codes in func-
tion words are characteristically different from
those in content words; and third, numerals
and acronyms in capital letters are distinguish-
able from other words. With these classifica-
tions, potential content-representing words are
identified, and an analysis of their distribution
yields their rank. Consequently, introducing
character shape codes makes it possible to
inexpensively and robustly bridge the gap
between electronic documents and hard-copy
documents for the purpose of content identifi-
cation.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961666666667">
Documents are becoming increasingly available in
machine-readable form. As they are stored automati-
cally and transferred on networks, many natural lan-
guage processing techniques that identify their content
have been developed to assist users with information
retrieval and document classification. Conventionally,
stored records are identified by sets of keywords or
phrases, known as index terms (Salton, 1991).
Although documents are increasingly being com-
puter generated, they are still printed on paper for read-
ing, dissemination, and markup. As it is believed that
paper will remain a comfortable medium for reading
and modification (O&apos;Gorman and Kasturi, 1992), devel-
opment of content identification techniques from a doc-
ument image is still important. OCR is often used to
convert a document image into machine-readable form,
but processing performance is limited by the overhead
of OCR (Mori et al., 1992; Nagy, 1992; Rice et al.,
1993). Because of the inaccuracy and expense of OCR,
we decided to avoid using it.
Instead, we have developed a method that first
makes generalizations about images of characters, then
performs gross classification of the isolated characters
and agglomerates these character shape codes into spa-
tially isolated (word shape) tokens (Nakayama and
Spitz, 1993; Sibun and Spitz, this volume). Generating
word shape tokens is inexpensive, fast, and robust.
Word shape tokens are a potential alternative to charac-
ter coded words when they are used for language deter-
mination and part-of-speech tagging (Nakayama and
Spitz, 1993; Sibun and Spitz, this volume; Sibun and
Farrar, 1994). In this paper, we describe an extension of
our approach to content identification.
</bodyText>
<sectionHeader confidence="0.505936" genericHeader="introduction">
2 Word shape token generation from image
</sectionHeader>
<bodyText confidence="0.999767461538462">
In this section, we introduce word shape tokens and
their generation from document images.
First, we classify characters by determining the
characteristics of the text line. We identify the positions
of the baseline and the x-height as shown in figure 1
(Spitz, 1993).
Next, we count the number of connected compo-
nents in each character cell and note the position of
those connected components with respect to the base-
line and x-height (Nakayama and Spitz, 1993; Sibun
and Spitz, this volume). The basic character classes A
xigjU&apos;-,.:=!I and the members which constitute
those classes are shown in Table 1. In this paper, they
</bodyText>
<page confidence="0.992477">
22
</page>
<bodyText confidence="0.565785">
are represented in bold-face type (e.g., Aigxx). Note
that a character shape code subset { - , . : ! } includes
only punctuation marks. This is important for our clean-
ing process which will be described later.
</bodyText>
<figure confidence="0.93311525">
top
tiger x-height
baseline
bottom
</figure>
<figureCaption confidence="0.999967">
Figure 1: Text line parameter positions
</figureCaption>
<bodyText confidence="0.990704272727273">
Character shape codes are grouped by word bound-
ary into word shape tokens (see Sibun and Spitz, this
volume). The correspondence between the scanned
word image and the word shape token is one-to-one;
that is, when a certain word shape token is selected, its
original word image can be immediately located.
Recognizing word shape tokens from images is two
or three orders of magnitude faster than conventional
OCR (Spitz, 1994), and is robust for real-world docu-
ments which are sometimes degraded by poor printing
and which sometimes use more than a single font.
</bodyText>
<tableCaption confidence="0.99772">
Table 1: Shape class code membership
</tableCaption>
<table confidence="0.72802625">
character members
shape code
A A-Z bdfhldt8 0-9 +#$&amp;()/&lt;&gt;0@{ II
x acemnorsuvwxz
i idaideerofit
g glx1Y9
j j
U itetKioe
</table>
<bodyText confidence="0.98912832">
The use of only 13 character shape codes instead of
approximately 100 standard character codes results in a
one-to-many correspondence between word shape
tokens and words.
Figure 2 shows how much character shape codes
reduce word variation using the most frequent 10,000
English words (Carroll et al., 1971) in order of fre-
quency. A word is defined as a string of graphic charac-
ters bounded on the left and right by spaces. Words are
distinguished by their graphic characters. For example,
&amp;quot;apple&amp;quot;, &amp;quot;Apple&amp;quot;, and &amp;quot;apples&amp;quot; are three different
words, while &amp;quot;will&amp;quot; (modal) and &amp;quot;will&amp;quot; (noun) are the
same. For the purpose of comparing the character shape
code representation with the standard character code
representation, the x axis represents the number of fre-
quent words, and the y axis represents the number of
distinct words represented in both ASCII and character
shape codes. The number of words in ASCII naturally
corresponds to the number of original words one-to-
one. On the other hand, the number of words in charac-
ter shape codes (the number of word shape tokens) is
less than half of the number of original words. This gap
is a constraint on the accuracy of our approach, but we
show it is not a serious limitation in the following sec-
tion.
</bodyText>
<sectionHeader confidence="0.979753" genericHeader="method">
3 Content identification
</sectionHeader>
<bodyText confidence="0.9996126">
Text characterization is an important domain for natural
language processing. Many published techniques utilize
word frequencies of a text for information retrieval and
text categorization (Jacobs, 1992; Cutting etal., 1993).
We also characterize the content of the document
image by finding words that seem to specify the topic of
the document. Briefly, our strategy is to identify the fre-
quently occurring word shape tokens.
In this section, we first describe a process of clean-
ing the input sequence which precedes the main proce-
dures. Then, we illustrate how to collect the important
tokens, introducing a stop list of common word shape
tokens which is used to remove the tokens that are
insufficiently specific to represent the content of the
documents.
</bodyText>
<subsectionHeader confidence="0.999743">
3.1 Cleaning input sequence
</subsectionHeader>
<bodyText confidence="0.9998028">
Given a sequence of word shape tokens, the system
removes the specific character shape codes `-&apos;, `,&apos;, `.&apos;,
:&apos;, and !&apos; that do not contribute important linguistic
information to the words to which they adhere, but that
change the shape of the tokens. Otherwise, word shape
would vary according to position and punctuation,
which would interfere with token distribution analysis
downstream. We ignore possible sentence initial word
shape alteration by capitalization simply because it is
almost impossible to presume the original shape. In this
</bodyText>
<figure confidence="0.997565">
10000
8000
6000
4000
2000
0
0 2000 4000 6000 8000 10000
</figure>
<figureCaption confidence="0.849576">
number of words (frequency order)
Figure 2: ASCII and character shape code
</figureCaption>
<figure confidence="0.721">
number of coded words
— ASCII
— character shape code
1 1111 1 111 111 1 111
</figure>
<page confidence="0.99424">
23
</page>
<bodyText confidence="0.999744933333333">
paper, capitalized words are counted differently from
their uncapitalized counterparts.
Our cleaning process concatenates word shape
tokens before and after the hyphen at the end of line.
The process also deletes intended hyphens (e.g.,
AxxxA-xixAxA [broad-minded] --&gt; AxxxAxixAxA).
Eliminating hyphens reduces the variation of word
shape tokens. We measured this effect using the afore-
mentioned frequent 10,000 words. Forty-two words of
10,000 are hyphenated. In character shape code repre-
sentation, 10,000 words map into 3,679 word shape
tokens (figure 2). When hyphens are eliminated, the
10,000 words fall into 3,670 word shape tokens. This
small reduction implies that eliminating hyphens does
not practically affect the following process.
</bodyText>
<subsectionHeader confidence="0.999846">
3.2 Introducing a word shape token stop list
</subsectionHeader>
<bodyText confidence="0.99992665">
After cleaning is done, the system analyzes word shape
token distribution. Word shape tokens are counted on
the hypothesis that frequent ones correspond to words
that represent content; however, tokens that correspond
to function words are also very frequent. One problem
awaiting solution is that of developing a technique to
separate these two classes.
In this paper, we define function words as the set of
(prepositions, determiners, conjunctions, pronouns,
modals, be and have surface forms), and content words
as the set of (nouns, verbs (excluding modals and be
and have surface forms), adjectives). Words that belong
in both categories are defined as function words. We
exclude adverbs from both, because they sometimes
behave as function words and sometimes as content
words. Words that can be adverbs but also can be either
a function or a context word are not counted as adverbs.
In English, function words tend to be short whereas
content words tend to be long. For the purpose of inves-
tigating characteristics of function and content words in
character shape code representation, we compiled a lex-
icon of 71,372 distinct word shape token envies from
an ASCII-represented lexicon of 245,085 word entries
which was provided by Xerox PARC and was modified
in our laboratory. 254 word shape token entries of the
lexicon correspond to 515 function words, 63,356
entries correspond to 226,648 content words, and 209
entries correspond to both function and content words.
Finally, 8,921 word shape token entries correspond to
17,922 adverbs. Figure 3 shows the distribution of word
shape token length. Frequency of occurrence of word
shape tokens was not taken into account; that is, we
simply counted the length of each entry and computed
the population ratio. The distribution of content words
is apparently different from that of function words. In
the figure, we also record the distribution of word shape
tokens corresponding to the 100 most frequent words
(75 function words, 16 content words, and 9 adverbs)
from the source (Carroll et al., 1971). It illustrates that
very common words are short.
</bodyText>
<figureCaption confidence="0.991616">
Figure 3: Word shape token length distribution
</figureCaption>
<bodyText confidence="0.999968236842105">
A stop list of the most common function word
shape tokens was constructed so that they could be
removed from sequences of word shape tokens. It is
important to select the right word shape tokens for this
list, which must selectively remove more function
words than content words. In general, the larger the list,
the more it removes both function and content words.
Thinking back to our goal of finding frequent content
words, we don&apos;t need to try to remove all function
words. We need only to remove the function words that
are usually more frequent than content-representing fre-
quent words in the text on the assumption that the fre-
quency of individual function words is almost
independent of topic. Infrequent function words that
remain after using the word shape token stop list are
distinguishable from frequent content words by com-
paring their frequencies.
We generated a word shape token stop list using
Carroll&apos;s list of frequent function words. We selected
several sets of the most frecr,ent function words, by
limiting the minimum frequency of words in the set to
1%, 0.5%, 0.1%, 0.09%, 0.08%, ..., 0%, then converted
them into word shape tokens. We tested these word
shape tokens on the aforementioned lexicon to count
the number of matching entries. Table 2 gives part of
the results, where Freq.FW stands for frequencies of
the selected function words, # FW for the number of
them, # stop-tokens for the number of word shape
tokens derived from them, FW.Match for a ratio of the
number of matching function words to the total number
of function words in the lexicon (515), and CW.Match
for a ratio of the number of matching content words to
the total number of content words (226,648). A word
shape token stop list, for instance, from function words
whose frequencies are more than 0.5% removes 0.4%
of content words and 18% of function words from the
lexicon; a word shape token stop list from function
words with frequencies more than 001% removes 4.2%
</bodyText>
<figure confidence="0.994224888888889">
• 0.4
0
1 0.3
0
0_
t 0.2
0_
-5 0.1
172
0
0.0
0
A-A function words
st—v content words
100 frequent words
5 10 15 20
The length of word shape token
25
</figure>
<page confidence="0.99298">
24
</page>
<bodyText confidence="0.998018666666667">
of content and 56% of function words; and a word
shape token stop list from all function words in the lexi-
con removes 9.5% of content words.
</bodyText>
<tableCaption confidence="0.9099795">
Table 2: Application of word shape token stop list to
lexicon
</tableCaption>
<table confidence="0.965070230769231">
Freq.FW # FW # stop- FW. CW.
(%) tokens Match Match
(%) (%)
&gt;1 7 6 6.2 0.1
&gt;0.5 19 15 18 0.4
&gt;0.1 77 44 39 1.8
&gt; 0.09 81 44 39 1.8
&gt; 0.07 87 47 41 2.4
&gt;005
&gt; 0.03 115 68 49 3.7
&gt;0.01 153 95 56 4.2
&gt;0 515 254 100 9.5
Function words (frequency &gt;0.05%)
</table>
<bodyText confidence="0.999345111111111">
the of and a to in is you that it he for was on are as with
his they at be this from I have or by one had but what
all were when we there can an your which their if will
each about up out them she many some so these would
other into has her like him could no than been its who
now my over down only may after where most through
before our me any same around another must because
such off every between should under us along while
might next below something both few those
</bodyText>
<subsectionHeader confidence="0.793362">
Word shape token stop list from above words
</subsectionHeader>
<bodyText confidence="0.904694125">
AAx xA xxA x Ax ix gxx Afoot iA Axx XXX xx
xiAA Aix AAxg AAix AXXX A Ag AxA xAxA
xAA xxx7c xAxx AAxxx gx,xx xAixA AAxix
xxxA XAXXA xg AAxx xAx xxxg xxxAA xAAxx
ixAx AiAx iAx xxAg xxg xAxxx AAxxxgA
AXAXXX 300LXXA voLAAxx Axxxxxx xxxxg
AxAxxxx xAxxAA xxAxx xAxxg xAiAx xigAA
AxAxx xxxxAAixg AxAA
</bodyText>
<figureCaption confidence="0.9979965">
Figure 4: Selected function words and word shape
token stop list
</figureCaption>
<bodyText confidence="0.999371454545455">
We also tested these word shape token stop lists on
ASCII encoded documents, and discovered that good
results are obtained with the lists derived from function
words with frequencies of more than 0.05%. This list
identifies all words that occur more than 5 times per
10,000 in the document. Figure 4 shows the selected
function words and the corresponding word shape token
stop list. The number of stop tokens is 57 for 101 func-
tion words. Table 2 shows that the list removes 2.9% of
content words and 44% of function words from the lex-
icon.
</bodyText>
<subsectionHeader confidence="0.999889">
3.3 Augmentation of the word shape token stop list
</subsectionHeader>
<bodyText confidence="0.9986523">
In our character classification, all numeric characters
are represented by the character shape code A (Table 1).
Therefore, after cleaning is done, all numerals in a text
fall into word shape tokens A*, where * means zero or
more repetitions of A. This sometimes makes the fre-
quency of A* unreasonably high though numerals are
often of little importance in content identification.
A* matches all numerals, but since it matches few
content words except for acronyms in capital letters, we
decided to add A* to the word shape token stop list.
</bodyText>
<tableCaption confidence="0.9852335">
Table 3: Testing the word shape token stop list on
sample documents
</tableCaption>
<table confidence="0.9998895">
sample CW.1 CW.R FW.1 FW.R
-4 CW.2 (%) -, FW.2 (%)
doc.1 347 -0 321 7.5 74 -+18 76
doc.2 246 -4 221 10 63 -*7 89
doc.3 245 -4 225 8.2 61 -0 10 85
doc.4 292 -0 272 6.8 61 -0 7 89
doc.5 279 --0 265 5.0 71 --0 16 78
doc.6 255 -0 236 73 56 -0 12 79
doc.7 177 --0 164 7.3 53 -* 14 74
doc.8 253 -0 231 8.7 71 -* 17 76
doc.9 227 -4 214 5.7 64 -* 11 83
doc.10 239 -0 218 8.8 63 -*14 78
doc.11 233 -9 212 9.0 62 -*10 84
doc.12 294 -0 265 9.9 58 -0 12 79
doc.13 233 -0 212 9.0 57 -&apos;12 79
doc.14 271 -4 248 8.5 59 -4 13 78
doc.15 130 -0 115 12 42 -&apos;5 88
doc.16 1582 -0 1513 4.4 150-* 45 70
doc.17 453 --0 409 9.7 99 --0 17 83
doc.18 292 --0 249 15 75 -&apos;8 89
doc.19 1189--s 1046 12 157-+35 78
doc.20 309 -0 286 7.4 73 --0 6 92
</table>
<subsectionHeader confidence="0.997912">
3.4 Testing the word shape token stop list
</subsectionHeader>
<bodyText confidence="0.99578675">
Our word shape token stop list was tested on 20 ASCII
encoded sample documents, ranged in length from 571
to 13,756 words, from a variety of sources including
business reports, travel guides, advertisements, techni-
</bodyText>
<page confidence="0.995309">
25
</page>
<bodyText confidence="0.999255333333333">
cal reports, and private letters. First, we generated word
shape tokens, and cleaned as described earlier. Next, we
removed tokens that were on the word shape token stop
list. Table 3 shows the number of content and function
words which the documents consist of before and after
using the list. In the table, CW.1 and CW.2 stand for the
number of distinct content words in the original docu-
ment and the number after using the word shape token
stop list, respectively. CW.R stands for a ratio of
(CW.1 - CW.2) to CW.1. Similarly, FW.1 and FVV.2
stand for the number of distinct function words before
and after using the list, and F&apos;W.R is a ratio of (FW.1 -
FW.2) to FW.1. FW.R is much larger than CW.R in
all sample documents, which shows the good perfor-
mance of the word shape token stop list. We should note
that the values of CW.R are larger than the 2.9% that
we get from testing the list on the lexicon. This is
because the lexicon includes many uncommon words
and these tend to be longer than the function words
selected to make the word shape token stop list. This
implies that our list removes more ordinary content
words than uncommon ones. We believe that removing
such words affects content identification little since
ordinary content words in many topics usually don&apos;t
</bodyText>
<figure confidence="0.944947181818182">
Original document: word shape token ranking and corresponding words
(count)
67 AAx (the, The
55 Ax Ito, be, Fr, In, As, On, An, (el
35 AAAA (1988, 1989, 2000, 1990, 1987, +7%), +5%), +28%, +27%, +18%, +14%)
33 xA {of, at}
32 xxA (and, out, not, end, act)
31 ix isl
29 Mot {for, due, ten, low, For, Enz}
28 XYLXX (some, over, were, more, same, rose, ease)
23 AA {6%, 5%, At, 9%, 7%, 4%, 3%, 1%, 8%, 2%, 11, 10, +6}
22 AxiAAixg (building, Building)
22A {4, 9, 8, 6,3, 2, 1, 0, R, A, 5}
19 XXXA (work, real, cost, such, much, most)
18 AAA (90%, 83%, 847, 80%, 5%),49%, 29%, 27%, 26%, 23%, 21%, 19%, 175, 14%, 13%)
16x (a, s)
16 gxxx (year, pace, grew)
16 Ag (by, By)
14 YOLK (was, are, new, can, saw, own, one)
12 xxxxAxxxAixx (construction)
10 xxgxxAxA (expanded, expected, reported)
9 XX (on, as, or)
</figure>
<bodyText confidence="0.565217">
After using the word shape token stop list: word shape token ranking and corresponding words
</bodyText>
<sectionHeader confidence="0.976069307692308" genericHeader="method">
22 AxiAAixg
12 xxxxAxxxAbtx
10 xxgxxAxA
8 XXXAXX
7 xxgix,xxxixg
7 XXAXXX
7 ixAxxAxixA
7 gxxxAA
7 gxixxx
7 AxAxAxx
6 vomixgx
6 MocAx
6 AxxAAxg
</sectionHeader>
<figure confidence="0.810957076923077">
(building, Building)
(construction)
(expanded, expected, reported)
(sector, number)
(engineering)
(volume, orders, return)
(industrial)
(growth)
(prices)
(October)
(earnings)
( trade )
(backlog)
</figure>
<sectionHeader confidence="0.949577461538462" genericHeader="method">
6 Aim
6 AixxA
5 boa/ma
5 AxxxxA
5 Axxxixg
5 AiAAixx
4 xxxAxxxAx
4 xxAx
4 iXAXXIDLX
4 gxxxxAixg
4 gxixA
4 MocxxAxx
4 Axixx
</sectionHeader>
<figure confidence="0.930039153846154">
(firms, Since)
(first, fixed)
(increase)
(demand, traced, lowest)
(housing)
(billion)
(contracts)
(rate)
(interior)
(preceding)
(point)
(branches)
(Swiss)
</figure>
<figureCaption confidence="0.999137">
Figure 5: Most frequent word shape tokens and corresponding words
</figureCaption>
<page confidence="0.978815">
26
</page>
<bodyText confidence="0.999661904761905">
specify the content of the document well (Salton et al.,
1975). Likewise, the values of FW.R are larger than
44% for the same reason.
After using the word shape token stop list, we
counted the remaining tokens to obtain content-repre-
senting words in their frequency order. All samples suc-
cessfully indicated their content by appropriate words.
Data for a sample document reporting the growth
of the building industry in Switzerland in 1988 and its
outlook for 1989, consisting of 1013 words, are shown
in figure 5. It shows top frequent word shape token
ranking of the original document and the new ranking
after using the word shape token stop list. The number
of removed tokens was 544. Most of them represented
common words and numerals. The top ranking after
using the word shape token stop list consists of content
words and represents the content of the document much
better than the ranking before using it.
Figure 5 also suggests that we can inexpensively
locate key words by performing OCR on the few fre-
quent word shape tokens.
</bodyText>
<sectionHeader confidence="0.995138" genericHeader="conclusions">
4 Conclusions and further directions
</sectionHeader>
<bodyText confidence="0.999983535714286">
Generating word shape tokens from images is inexpen-
sive and robust for real-world documents. Word shape
tokens do not carry alphabetical information, but they
are potentially usable for content identification by locat-
ing content-representing word images. Our method uses
a word shape token stop list and analyzes the distribu-
tion of tokens. This technique depends on the observa-
tion that, in English, the characteristics of word shape
differ between function and content words, and between
frequent and infrequent words.
We expect to be able to extend the technique to
many other European languages that have similar char-
acteristics. For example, German function words tend
to be shorter than nouns, which are always capitalized.
In addition, by drawing on our language determination
technique, which uses the same word shape tokens
(Nakayama and Spitz, 1993; Sibun and Spitz, this vol-
ume), we could enhance the technique described here
for multilingual sources.
Other future work involves examining automatic
document categorization in which an input document
image is assigned to some pre-existing subject category
(Cavnar and Trenlde, 1994). With reliable training data,
we feel we can identify the configuration of word shape
tokens across subjects. Using a statistical method to
compute the distance between input and configurations
of categories would be a good approach. This might be
useful for document sorting service for fax machines.
</bodyText>
<sectionHeader confidence="0.99895" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990196">
The author gratefully acknowledges helpful suggestions
by Larry Spitz and Penni Sibun.
</bodyText>
<sectionHeader confidence="0.995649" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905632653061">
John B. Carroll, Peter Davies, and Barry Richman, The
American Heritage word frequency book, Boston,
Houghton-Mifflin, 1971.
William B. Cavnar and John M. Trenlde, N-Gram-Based Text
Categorization, Proceedings of the Third Annual
Symposium on Document Analysis and Information
Retrieval, Las Vegas, U.S.A., 1994.
Douglass R. Cutting, David R. Karger, and Jan 0. Pedersen,
Constant Interaction-Tune Scatter/Gather Browsing of
Very Large Document Collections, Proceedings of the
16th Annual International ACM SIGIR Conference,
Pittsburgh, U.S.A., 1993.
Paul S. Jacob, Joining Statistics with NLP for Text Categori-
zation, Proceedings of the Third Conference on Applied
Natural Language Processing, Trento, Italy, 1992.
Shunji Mori, Ching Y. Suen, and Kazuhiko Yamamoto, His-
torical Review of OCR Research and Development, Pro-
ceedings of IEEE, Vol. 80, No. 7, 1992.
George Nagy, At the Frontiers of OCR, Proceedings of IEEE,
Vol. 80, No. 7, 1992.
Takehiro Nakayama and A. Lawrence Spitz, European Lan-
guage Determination from Image. Proceedings of the
Second International Conference on Document Analysis
and Recognition, Tsukuba Science City, Japan, 1993.
Lawrence O&apos;Gorman and Rangachar Kasturi, Document
Image Analysis Systems, Computer July 1992 Vol. 25.
Stephen V. Rice, Junichi Kanai and Thomas A. Nartker, An
Evaluation of OCR Accuracy, Information Science
Research Institute 1993 Annual Research Report, Univer-
sity of Nevada, Las Vegas.
Gerard Salton, Developments in Automatic Text Retrieval,
Science Vol. 253, No. 5023, Aug. 30, 1991.
Gerard Salton, A. Wong, and C. S. Young, A Vector Space
Model for Automatic Indexing, Communications of the
ACM November 1975 Vol. 18.
Penelope Sibun and David S. Farrar, Content Characterization
Using Word Shape Tokens, Proceedings of the 15th Inter-
national Conference on Computational Linguistics,
Kyoto, Japan, 1994.
Penelope Sibun and A. Lawrence Spitz, Language Determina-
tion: Natural Language Processing from Scanned Docu-
ment Images, this volume.
A. Lawrence Spitz, Generalized Line Word and Character
Finding, Proceedings of the International Conference on
Image Analysis and Processing, Bari, Italy, 1993.
A. Lawrence Spitz, Using Character Shape Codes for Word
Spotting in Document Images, Proceedings of the Third
International Workshop on Syntactic and Structural
Pattern Recognition, Haifa, Israel, 1994.
</reference>
<page confidence="0.99881">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.726616">
<title confidence="0.999993">Modeling Content Identification from Document Images</title>
<author confidence="0.998359">Takehiro Nakayama</author>
<affiliation confidence="0.995226">Fuji Xerox Palo Alto Laboratory</affiliation>
<address confidence="0.9989435">3400 Hillview Avenue Palo Alto, CA 94304 USA</address>
<email confidence="0.999788">nakayamagpal.xerox.com</email>
<abstract confidence="0.991010032258064">A new technique to locate content-representing words for a given document image using abstract representation of character shapes is described. A character shape code representation defined by the location of a character in a text line has been developed. Character shape code generation avoids the computational expense of conventional optical character recognition (OCR). Because character shape codes are an abstraction of standard character code (e.g., ASCII), the mapping is ambiguous. In this paper, the ambiguity is shown to be practically limited to an acceptable level. It is illustrated that: first, punctuation marks are clearly distinguished from the other characters; second, stop words are generally distinguishable from other words, because the permutations of character shape codes in function words are characteristically different from those in content words; and third, numerals and acronyms in capital letters are distinguishable from other words. With these classifications, potential content-representing words are identified, and an analysis of their distribution yields their rank. Consequently, introducing character shape codes makes it possible to inexpensively and robustly bridge the gap between electronic documents and hard-copy documents for the purpose of content identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John B Carroll</author>
<author>Peter Davies</author>
<author>Barry Richman</author>
</authors>
<title>The American Heritage word frequency book,</title>
<date>1971</date>
<location>Boston, Houghton-Mifflin,</location>
<contexts>
<context position="5189" citStr="Carroll et al., 1971" startWordPosition="798" endWordPosition="801">onal OCR (Spitz, 1994), and is robust for real-world documents which are sometimes degraded by poor printing and which sometimes use more than a single font. Table 1: Shape class code membership character members shape code A A-Z bdfhldt8 0-9 +#$&amp;()/&lt;&gt;0@{ II x acemnorsuvwxz i idaideerofit g glx1Y9 j j U itetKioe The use of only 13 character shape codes instead of approximately 100 standard character codes results in a one-to-many correspondence between word shape tokens and words. Figure 2 shows how much character shape codes reduce word variation using the most frequent 10,000 English words (Carroll et al., 1971) in order of frequency. A word is defined as a string of graphic characters bounded on the left and right by spaces. Words are distinguished by their graphic characters. For example, &amp;quot;apple&amp;quot;, &amp;quot;Apple&amp;quot;, and &amp;quot;apples&amp;quot; are three different words, while &amp;quot;will&amp;quot; (modal) and &amp;quot;will&amp;quot; (noun) are the same. For the purpose of comparing the character shape code representation with the standard character code representation, the x axis represents the number of frequent words, and the y axis represents the number of distinct words represented in both ASCII and character shape codes. The number of words in ASCII</context>
<context position="10545" citStr="Carroll et al., 1971" startWordPosition="1661" endWordPosition="1664">ond to both function and content words. Finally, 8,921 word shape token entries correspond to 17,922 adverbs. Figure 3 shows the distribution of word shape token length. Frequency of occurrence of word shape tokens was not taken into account; that is, we simply counted the length of each entry and computed the population ratio. The distribution of content words is apparently different from that of function words. In the figure, we also record the distribution of word shape tokens corresponding to the 100 most frequent words (75 function words, 16 content words, and 9 adverbs) from the source (Carroll et al., 1971). It illustrates that very common words are short. Figure 3: Word shape token length distribution A stop list of the most common function word shape tokens was constructed so that they could be removed from sequences of word shape tokens. It is important to select the right word shape tokens for this list, which must selectively remove more function words than content words. In general, the larger the list, the more it removes both function and content words. Thinking back to our goal of finding frequent content words, we don&apos;t need to try to remove all function words. We need only to remove t</context>
</contexts>
<marker>Carroll, Davies, Richman, 1971</marker>
<rawString>John B. Carroll, Peter Davies, and Barry Richman, The American Heritage word frequency book, Boston, Houghton-Mifflin, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>M John</author>
</authors>
<title>Trenlde, N-Gram-Based Text Categorization,</title>
<date>1994</date>
<booktitle>Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<location>Las Vegas, U.S.A.,</location>
<marker>Cavnar, John, 1994</marker>
<rawString>William B. Cavnar and John M. Trenlde, N-Gram-Based Text Categorization, Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval, Las Vegas, U.S.A., 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglass R Cutting</author>
<author>David R Karger</author>
<author>Jan</author>
</authors>
<title>Constant Interaction-Tune Scatter/Gather Browsing of Very Large Document Collections,</title>
<date>1993</date>
<booktitle>Proceedings of the 16th Annual International ACM SIGIR Conference,</booktitle>
<location>Pittsburgh, U.S.A.,</location>
<marker>Cutting, Karger, Jan, 1993</marker>
<rawString>Douglass R. Cutting, David R. Karger, and Jan 0. Pedersen, Constant Interaction-Tune Scatter/Gather Browsing of Very Large Document Collections, Proceedings of the 16th Annual International ACM SIGIR Conference, Pittsburgh, U.S.A., 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacob</author>
</authors>
<title>Joining Statistics with NLP for Text Categorization,</title>
<date>1992</date>
<booktitle>Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<location>Trento, Italy,</location>
<marker>Jacob, 1992</marker>
<rawString>Paul S. Jacob, Joining Statistics with NLP for Text Categorization, Proceedings of the Third Conference on Applied Natural Language Processing, Trento, Italy, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shunji Mori</author>
<author>Ching Y Suen</author>
<author>Kazuhiko Yamamoto</author>
</authors>
<date>1992</date>
<journal>Historical Review of OCR Research and Development, Proceedings of IEEE,</journal>
<volume>80</volume>
<contexts>
<context position="2420" citStr="Mori et al., 1992" startWordPosition="349" endWordPosition="352">n. Conventionally, stored records are identified by sets of keywords or phrases, known as index terms (Salton, 1991). Although documents are increasingly being computer generated, they are still printed on paper for reading, dissemination, and markup. As it is believed that paper will remain a comfortable medium for reading and modification (O&apos;Gorman and Kasturi, 1992), development of content identification techniques from a document image is still important. OCR is often used to convert a document image into machine-readable form, but processing performance is limited by the overhead of OCR (Mori et al., 1992; Nagy, 1992; Rice et al., 1993). Because of the inaccuracy and expense of OCR, we decided to avoid using it. Instead, we have developed a method that first makes generalizations about images of characters, then performs gross classification of the isolated characters and agglomerates these character shape codes into spatially isolated (word shape) tokens (Nakayama and Spitz, 1993; Sibun and Spitz, this volume). Generating word shape tokens is inexpensive, fast, and robust. Word shape tokens are a potential alternative to character coded words when they are used for language determination and </context>
</contexts>
<marker>Mori, Suen, Yamamoto, 1992</marker>
<rawString>Shunji Mori, Ching Y. Suen, and Kazuhiko Yamamoto, Historical Review of OCR Research and Development, Proceedings of IEEE, Vol. 80, No. 7, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Nagy</author>
</authors>
<title>At the Frontiers of OCR,</title>
<date>1992</date>
<journal>Proceedings of IEEE,</journal>
<volume>80</volume>
<contexts>
<context position="2432" citStr="Nagy, 1992" startWordPosition="353" endWordPosition="354">stored records are identified by sets of keywords or phrases, known as index terms (Salton, 1991). Although documents are increasingly being computer generated, they are still printed on paper for reading, dissemination, and markup. As it is believed that paper will remain a comfortable medium for reading and modification (O&apos;Gorman and Kasturi, 1992), development of content identification techniques from a document image is still important. OCR is often used to convert a document image into machine-readable form, but processing performance is limited by the overhead of OCR (Mori et al., 1992; Nagy, 1992; Rice et al., 1993). Because of the inaccuracy and expense of OCR, we decided to avoid using it. Instead, we have developed a method that first makes generalizations about images of characters, then performs gross classification of the isolated characters and agglomerates these character shape codes into spatially isolated (word shape) tokens (Nakayama and Spitz, 1993; Sibun and Spitz, this volume). Generating word shape tokens is inexpensive, fast, and robust. Word shape tokens are a potential alternative to character coded words when they are used for language determination and part-of-spee</context>
</contexts>
<marker>Nagy, 1992</marker>
<rawString>George Nagy, At the Frontiers of OCR, Proceedings of IEEE, Vol. 80, No. 7, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takehiro Nakayama</author>
<author>A Lawrence Spitz</author>
</authors>
<title>European Language Determination from Image.</title>
<date>1993</date>
<booktitle>Proceedings of the Second International Conference on Document Analysis and Recognition,</booktitle>
<location>Tsukuba Science City, Japan,</location>
<contexts>
<context position="2803" citStr="Nakayama and Spitz, 1993" startWordPosition="408" endWordPosition="411">elopment of content identification techniques from a document image is still important. OCR is often used to convert a document image into machine-readable form, but processing performance is limited by the overhead of OCR (Mori et al., 1992; Nagy, 1992; Rice et al., 1993). Because of the inaccuracy and expense of OCR, we decided to avoid using it. Instead, we have developed a method that first makes generalizations about images of characters, then performs gross classification of the isolated characters and agglomerates these character shape codes into spatially isolated (word shape) tokens (Nakayama and Spitz, 1993; Sibun and Spitz, this volume). Generating word shape tokens is inexpensive, fast, and robust. Word shape tokens are a potential alternative to character coded words when they are used for language determination and part-of-speech tagging (Nakayama and Spitz, 1993; Sibun and Spitz, this volume; Sibun and Farrar, 1994). In this paper, we describe an extension of our approach to content identification. 2 Word shape token generation from image In this section, we introduce word shape tokens and their generation from document images. First, we classify characters by determining the characteristic</context>
<context position="21059" citStr="Nakayama and Spitz, 1993" startWordPosition="3569" endWordPosition="3572">senting word images. Our method uses a word shape token stop list and analyzes the distribution of tokens. This technique depends on the observation that, in English, the characteristics of word shape differ between function and content words, and between frequent and infrequent words. We expect to be able to extend the technique to many other European languages that have similar characteristics. For example, German function words tend to be shorter than nouns, which are always capitalized. In addition, by drawing on our language determination technique, which uses the same word shape tokens (Nakayama and Spitz, 1993; Sibun and Spitz, this volume), we could enhance the technique described here for multilingual sources. Other future work involves examining automatic document categorization in which an input document image is assigned to some pre-existing subject category (Cavnar and Trenlde, 1994). With reliable training data, we feel we can identify the configuration of word shape tokens across subjects. Using a statistical method to compute the distance between input and configurations of categories would be a good approach. This might be useful for document sorting service for fax machines. Acknowledgme</context>
</contexts>
<marker>Nakayama, Spitz, 1993</marker>
<rawString>Takehiro Nakayama and A. Lawrence Spitz, European Language Determination from Image. Proceedings of the Second International Conference on Document Analysis and Recognition, Tsukuba Science City, Japan, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence O&apos;Gorman</author>
<author>Rangachar Kasturi</author>
</authors>
<date>1992</date>
<journal>Document Image Analysis Systems, Computer</journal>
<volume>25</volume>
<contexts>
<context position="2174" citStr="O&apos;Gorman and Kasturi, 1992" startWordPosition="309" endWordPosition="312">ilable in machine-readable form. As they are stored automatically and transferred on networks, many natural language processing techniques that identify their content have been developed to assist users with information retrieval and document classification. Conventionally, stored records are identified by sets of keywords or phrases, known as index terms (Salton, 1991). Although documents are increasingly being computer generated, they are still printed on paper for reading, dissemination, and markup. As it is believed that paper will remain a comfortable medium for reading and modification (O&apos;Gorman and Kasturi, 1992), development of content identification techniques from a document image is still important. OCR is often used to convert a document image into machine-readable form, but processing performance is limited by the overhead of OCR (Mori et al., 1992; Nagy, 1992; Rice et al., 1993). Because of the inaccuracy and expense of OCR, we decided to avoid using it. Instead, we have developed a method that first makes generalizations about images of characters, then performs gross classification of the isolated characters and agglomerates these character shape codes into spatially isolated (word shape) tok</context>
</contexts>
<marker>O&apos;Gorman, Kasturi, 1992</marker>
<rawString>Lawrence O&apos;Gorman and Rangachar Kasturi, Document Image Analysis Systems, Computer July 1992 Vol. 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen V Rice</author>
<author>Junichi Kanai</author>
<author>Thomas A Nartker</author>
</authors>
<title>An Evaluation of OCR Accuracy,</title>
<date>1993</date>
<journal>Information Science Research Institute</journal>
<institution>University of Nevada, Las Vegas.</institution>
<contexts>
<context position="2452" citStr="Rice et al., 1993" startWordPosition="355" endWordPosition="358">ds are identified by sets of keywords or phrases, known as index terms (Salton, 1991). Although documents are increasingly being computer generated, they are still printed on paper for reading, dissemination, and markup. As it is believed that paper will remain a comfortable medium for reading and modification (O&apos;Gorman and Kasturi, 1992), development of content identification techniques from a document image is still important. OCR is often used to convert a document image into machine-readable form, but processing performance is limited by the overhead of OCR (Mori et al., 1992; Nagy, 1992; Rice et al., 1993). Because of the inaccuracy and expense of OCR, we decided to avoid using it. Instead, we have developed a method that first makes generalizations about images of characters, then performs gross classification of the isolated characters and agglomerates these character shape codes into spatially isolated (word shape) tokens (Nakayama and Spitz, 1993; Sibun and Spitz, this volume). Generating word shape tokens is inexpensive, fast, and robust. Word shape tokens are a potential alternative to character coded words when they are used for language determination and part-of-speech tagging (Nakayama</context>
</contexts>
<marker>Rice, Kanai, Nartker, 1993</marker>
<rawString>Stephen V. Rice, Junichi Kanai and Thomas A. Nartker, An Evaluation of OCR Accuracy, Information Science Research Institute 1993 Annual Research Report, University of Nevada, Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Developments in Automatic Text Retrieval,</title>
<date></date>
<journal>Science</journal>
<volume>253</volume>
<marker>Salton, </marker>
<rawString>Gerard Salton, Developments in Automatic Text Retrieval, Science Vol. 253, No. 5023, Aug. 30, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>A Wong</author>
<author>C S Young</author>
</authors>
<title>A Vector Space Model for Automatic Indexing,</title>
<date>1975</date>
<journal>Communications of the ACM</journal>
<volume>18</volume>
<contexts>
<context position="19188" citStr="Salton et al., 1975" startWordPosition="3265" endWordPosition="3268"> MocAx 6 AxxAAxg (building, Building) (construction) (expanded, expected, reported) (sector, number) (engineering) (volume, orders, return) (industrial) (growth) (prices) (October) (earnings) ( trade ) (backlog) 6 Aim 6 AixxA 5 boa/ma 5 AxxxxA 5 Axxxixg 5 AiAAixx 4 xxxAxxxAx 4 xxAx 4 iXAXXIDLX 4 gxxxxAixg 4 gxixA 4 MocxxAxx 4 Axixx (firms, Since) (first, fixed) (increase) (demand, traced, lowest) (housing) (billion) (contracts) (rate) (interior) (preceding) (point) (branches) (Swiss) Figure 5: Most frequent word shape tokens and corresponding words 26 specify the content of the document well (Salton et al., 1975). Likewise, the values of FW.R are larger than 44% for the same reason. After using the word shape token stop list, we counted the remaining tokens to obtain content-representing words in their frequency order. All samples successfully indicated their content by appropriate words. Data for a sample document reporting the growth of the building industry in Switzerland in 1988 and its outlook for 1989, consisting of 1013 words, are shown in figure 5. It shows top frequent word shape token ranking of the original document and the new ranking after using the word shape token stop list. The number </context>
</contexts>
<marker>Salton, Wong, Young, 1975</marker>
<rawString>Gerard Salton, A. Wong, and C. S. Young, A Vector Space Model for Automatic Indexing, Communications of the ACM November 1975 Vol. 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Sibun</author>
<author>David S Farrar</author>
</authors>
<title>Content Characterization Using Word Shape Tokens,</title>
<date>1994</date>
<booktitle>Proceedings of the 15th International Conference on Computational Linguistics, Kyoto,</booktitle>
<contexts>
<context position="3123" citStr="Sibun and Farrar, 1994" startWordPosition="458" endWordPosition="461">we decided to avoid using it. Instead, we have developed a method that first makes generalizations about images of characters, then performs gross classification of the isolated characters and agglomerates these character shape codes into spatially isolated (word shape) tokens (Nakayama and Spitz, 1993; Sibun and Spitz, this volume). Generating word shape tokens is inexpensive, fast, and robust. Word shape tokens are a potential alternative to character coded words when they are used for language determination and part-of-speech tagging (Nakayama and Spitz, 1993; Sibun and Spitz, this volume; Sibun and Farrar, 1994). In this paper, we describe an extension of our approach to content identification. 2 Word shape token generation from image In this section, we introduce word shape tokens and their generation from document images. First, we classify characters by determining the characteristics of the text line. We identify the positions of the baseline and the x-height as shown in figure 1 (Spitz, 1993). Next, we count the number of connected components in each character cell and note the position of those connected components with respect to the baseline and x-height (Nakayama and Spitz, 1993; Sibun and S</context>
</contexts>
<marker>Sibun, Farrar, 1994</marker>
<rawString>Penelope Sibun and David S. Farrar, Content Characterization Using Word Shape Tokens, Proceedings of the 15th International Conference on Computational Linguistics, Kyoto, Japan, 1994.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Penelope Sibun</author>
<author>A Lawrence Spitz</author>
</authors>
<title>Language Determination: Natural Language Processing from Scanned Document Images, this volume.</title>
<marker>Sibun, Spitz, </marker>
<rawString>Penelope Sibun and A. Lawrence Spitz, Language Determination: Natural Language Processing from Scanned Document Images, this volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lawrence Spitz</author>
</authors>
<title>Generalized Line Word and Character Finding,</title>
<date>1993</date>
<booktitle>Proceedings of the International Conference on Image Analysis and Processing,</booktitle>
<location>Bari, Italy,</location>
<contexts>
<context position="2803" citStr="Spitz, 1993" startWordPosition="410" endWordPosition="411">ontent identification techniques from a document image is still important. OCR is often used to convert a document image into machine-readable form, but processing performance is limited by the overhead of OCR (Mori et al., 1992; Nagy, 1992; Rice et al., 1993). Because of the inaccuracy and expense of OCR, we decided to avoid using it. Instead, we have developed a method that first makes generalizations about images of characters, then performs gross classification of the isolated characters and agglomerates these character shape codes into spatially isolated (word shape) tokens (Nakayama and Spitz, 1993; Sibun and Spitz, this volume). Generating word shape tokens is inexpensive, fast, and robust. Word shape tokens are a potential alternative to character coded words when they are used for language determination and part-of-speech tagging (Nakayama and Spitz, 1993; Sibun and Spitz, this volume; Sibun and Farrar, 1994). In this paper, we describe an extension of our approach to content identification. 2 Word shape token generation from image In this section, we introduce word shape tokens and their generation from document images. First, we classify characters by determining the characteristic</context>
<context position="21059" citStr="Spitz, 1993" startWordPosition="3571" endWordPosition="3572">images. Our method uses a word shape token stop list and analyzes the distribution of tokens. This technique depends on the observation that, in English, the characteristics of word shape differ between function and content words, and between frequent and infrequent words. We expect to be able to extend the technique to many other European languages that have similar characteristics. For example, German function words tend to be shorter than nouns, which are always capitalized. In addition, by drawing on our language determination technique, which uses the same word shape tokens (Nakayama and Spitz, 1993; Sibun and Spitz, this volume), we could enhance the technique described here for multilingual sources. Other future work involves examining automatic document categorization in which an input document image is assigned to some pre-existing subject category (Cavnar and Trenlde, 1994). With reliable training data, we feel we can identify the configuration of word shape tokens across subjects. Using a statistical method to compute the distance between input and configurations of categories would be a good approach. This might be useful for document sorting service for fax machines. Acknowledgme</context>
</contexts>
<marker>Spitz, 1993</marker>
<rawString>A. Lawrence Spitz, Generalized Line Word and Character Finding, Proceedings of the International Conference on Image Analysis and Processing, Bari, Italy, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lawrence Spitz</author>
</authors>
<title>Using Character Shape Codes for Word Spotting in Document Images,</title>
<date>1994</date>
<booktitle>Proceedings of the Third International Workshop on Syntactic and Structural Pattern Recognition,</booktitle>
<location>Haifa,</location>
<contexts>
<context position="4590" citStr="Spitz, 1994" startWordPosition="703" endWordPosition="704">! } includes only punctuation marks. This is important for our cleaning process which will be described later. top tiger x-height baseline bottom Figure 1: Text line parameter positions Character shape codes are grouped by word boundary into word shape tokens (see Sibun and Spitz, this volume). The correspondence between the scanned word image and the word shape token is one-to-one; that is, when a certain word shape token is selected, its original word image can be immediately located. Recognizing word shape tokens from images is two or three orders of magnitude faster than conventional OCR (Spitz, 1994), and is robust for real-world documents which are sometimes degraded by poor printing and which sometimes use more than a single font. Table 1: Shape class code membership character members shape code A A-Z bdfhldt8 0-9 +#$&amp;()/&lt;&gt;0@{ II x acemnorsuvwxz i idaideerofit g glx1Y9 j j U itetKioe The use of only 13 character shape codes instead of approximately 100 standard character codes results in a one-to-many correspondence between word shape tokens and words. Figure 2 shows how much character shape codes reduce word variation using the most frequent 10,000 English words (Carroll et al., 1971) </context>
</contexts>
<marker>Spitz, 1994</marker>
<rawString>A. Lawrence Spitz, Using Character Shape Codes for Word Spotting in Document Images, Proceedings of the Third International Workshop on Syntactic and Structural Pattern Recognition, Haifa, Israel, 1994.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>