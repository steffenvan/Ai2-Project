<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9988585">
That’s Not What I Meant!
Using Parsers to Avoid Structural Ambiguities in Generated Text
</title>
<author confidence="0.993313">
Manjuan Duan and Michael White
</author>
<affiliation confidence="0.995657">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.737575">
Columbus, OH 43210, USA
</address>
<email confidence="0.999728">
{duan,mwhite}@ling.osu.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929740740741">
We investigate whether parsers can be
used for self-monitoring in surface real-
ization in order to avoid egregious errors
involving “vicious” ambiguities, namely
those where the intended interpretation
fails to be considerably more likely than
alternative ones. Using parse accuracy
in a simple reranking strategy for self-
monitoring, we find that with a state-
of-the-art averaged perceptron realization
ranking model, BLEU scores cannot be
improved with any of the well-known
Treebank parsers we tested, since these
parsers too often make errors that human
readers would be unlikely to make. How-
ever, by using an SVM ranker to combine
the realizer’s model score together with
features from multiple parsers, including
ones designed to make the ranker more ro-
bust to parsing mistakes, we show that sig-
nificant increases in BLEU scores can be
achieved. Moreover, via a targeted man-
ual analysis, we demonstrate that the SVM
reranker frequently manages to avoid vi-
cious ambiguities, while its ranking errors
tend to affect fluency much more often
than adequacy.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999435921568627">
Rajkumar &amp; White (2011; 2012) have recently
shown that some rather egregious surface realiza-
tion errors—in the sense that the reader would
likely end up with the wrong interpretation—can
be avoided by making use of features inspired by
psycholinguistics research together with an other-
wise state-of-the-art averaged perceptron realiza-
tion ranking model (White and Rajkumar, 2009),
as reviewed in the next section. However, one is
apt to wonder: could one use a parser to check
whether the intended interpretation is easy to re-
cover, either as an alternative or to catch additional
mistakes? Doing so would be tantamount to self-
monitoring in Levelt’s (1989) model of language
production.
Neumann &amp; van Noord (1992) pursued the idea
of self-monitoring for generation in early work
with reversible grammars. As Neumann &amp; van
Noord observed, a simple, brute-force way to gen-
erate unambiguous sentences is to enumerate pos-
sible realizations of an input logical form, then
to parse each realization to see how many inter-
pretations it has, keeping only those that have
a single reading; they then went on to devise a
more efficient method of using self-monitoring to
avoid generating ambiguous sentences, targeted to
the ambiguous portion of the output. We might
question, however, whether it is really possible
to avoid ambiguity entirely in the general case,
since Abney (1996) and others have argued that
nearly every sentence is potentially ambiguous,
though we (as human comprehenders) may not
notice the ambiguities if they are unlikely. Tak-
ing up this issue, Khan et al. (2008)—building on
Chantree et al.’s (2006) approach to identifying
“innocuous” ambiguities—conducted several ex-
periments to test whether ambiguity could be bal-
anced against length or fluency in the context of
generating referring expressions involving coordi-
nate structures. Though Khan et al.’s study was
limited to this one kind of structural ambiguity,
they do observe that generating the brief variants
when the intended interpretation is clear instanti-
ates Van Deemter’s (2004) general strategy of only
avoiding vicious ambiguities—that is, ambigui-
ties where the intended interpretation fails to be
considerably more likely than any other distractor
interpretations—rather than trying to avoid all am-
biguities.
In this paper, we investigate whether Neumann
&amp; van Noord’s brute-force strategy for avoid-
</bodyText>
<page confidence="0.987387">
413
</page>
<note confidence="0.831685">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 413–423,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999848454545455">
ing ambiguities in surface realization can be up-
dated to only avoid vicious ambiguities, extend-
ing (and revising) Van Deemter’s general strategy
to all kinds of structural ambiguity, not just the
one investigated by Khan et al. To do so—in a
nutshell—we enumerate an n-best list of realiza-
tions and rerank them if necessary to avoid vicious
ambiguities, as determined by one or more auto-
matic parsers. A potential obstacle, of course, is
that automatic parsers may not be sufficiently rep-
resentative of human readers, insofar as errors that
a parser makes may not be problematic for human
comprehension; moreover, parsers are rarely suc-
cessful in fully recovering the intended interpreta-
tion for sentences of moderate length, even with
carefully edited news text. Consequently, we ex-
amine two reranking strategies, one a simple base-
line approach and the other using an SVM reranker
(Joachims, 2002).
Our simple reranking strategy for self-
monitoring is to rerank the realizer’s n-best list
by parse accuracy, preserving the original order in
case of ties. In this way, if there is a realization in
the n-best list that can be parsed more accurately
than the top-ranked realization—even if the
intended interpretation cannot be recovered with
100% accuracy—it will become the preferred
output of the combined realization-with-self-
monitoring system. With this simple reranking
strategy and each of three different Treebank
parsers, we find that it is possible to improve
BLEU scores on Penn Treebank development data
with White &amp; Rajkumar’s (2011; 2012) baseline
generative model, but not with their averaged
perceptron model. In inspecting the results of
reranking with this strategy, we observe that while
it does sometimes succeed in avoiding egregious
errors involving vicious ambiguities, common
parsing mistakes such as PP-attachment errors
lead to unnecessarily sacrificing conciseness or
fluency in order to avoid ambiguities that would be
easily tolerated by human readers. Therefore, to
develop a more nuanced self-monitoring reranker
that is more robust to such parsing mistakes, we
trained an SVM using dependency precision and
recall features for all three parses, their n-best
parsing results, and per-label precision and recall
for each type of dependency, together with the
realizer’s normalized perceptron model score as
a feature. With the SVM reranker, we obtain a
significant improvement in BLEU scores over
White &amp; Rajkumar’s averaged perceptron model
on both development and test data. Additionally,
in a targeted manual analysis, we find that in cases
where the SVM reranker improves the BLEU
score, improvements to fluency and adequacy are
roughly balanced, while in cases where the BLEU
score goes down, it is mostly fluency that is made
worse (with reranking yielding an acceptable
paraphrase roughly one third of the time in both
cases).
The paper is structured as follows. In Sec-
tion 2, we review the realization ranking mod-
els that serve as a starting point for the paper.
In Section 3, we report on our experiments with
the simple reranking strategy, including a discus-
sion of the ways in which this method typically
fails. In Section 4, we describe how we trained an
SVM reranker and report our results using BLEU
scores (Papineni et al., 2002). In Section 5, we
present a targeted manual analysis of the devel-
opment set sentences with the greatest change in
BLEU scores, discussing both successes and er-
rors. In Section 6, we briefly review related work
on broad coverage surface realization. Finally, in
Section 7, we sum up and discuss opportunities for
future work in this direction.
</bodyText>
<sectionHeader confidence="0.987774" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999386909090909">
We use the OpenCCG1 surface realizer for the ex-
periments reported in this paper. The OpenCCG
realizer generates surface strings for input seman-
tic dependency graphs (or logical forms) using a
chart-based algorithm (White, 2006) for Combi-
natory Categorial Grammar (Steedman, 2000) to-
gether with a “hypertagger” for probabilistically
assigning lexical categories to lexical predicates
in the input (Espinosa et al., 2008). An exam-
ple input appears in Figure 1. In the figure,
nodes correspond to discourse referents labeled
with lexical predicates, and dependency relations
between nodes encode argument structure (gold
standard CCG lexical categories are also shown);
note that semantically empty function words such
as infinitival-to are missing. The grammar is ex-
tracted from a version of the CCGbank (Hocken-
maier and Steedman, 2007) enhanced for realiza-
tion; the enhancements include: better analyses of
punctuation (White and Rajkumar, 2008); less er-
ror prone handling of named entities (Rajkumar et
al., 2009); re-inserting quotes into the CCGbank;
</bodyText>
<footnote confidence="0.989979">
1http://openccg.sf.net
</footnote>
<page confidence="0.995605">
414
</page>
<figure confidence="0.640436">
s[dcl]\np/np
</figure>
<figureCaption confidence="0.750053333333333">
Figure 1: Example OpenCCG semantic depen-
dency input for he has a point he wants to make,
with gold standard lexical categories for each node
</figureCaption>
<bodyText confidence="0.994753536585367">
and assignment of consistent semantic roles across
diathesis alternations (Boxwell and White, 2008),
using PropBank (Palmer et al., 2005).
To select preferred outputs from the chart, we
use White &amp; Rajkumar’s (2009; 2012) realization
ranking model, recently augmented with a large-
scale 5-gram model based on the Gigaword cor-
pus. The ranking model makes choices addressing
all three interrelated sub-tasks traditionally con-
sidered part of the surface realization task in natu-
ral language generation research (Reiter and Dale,
2000; Reiter, 2010): inflecting lemmas with gram-
matical word forms, inserting function words and
linearizing the words in a grammatical and natu-
ral order. The model takes as its starting point two
probabilistic models of syntax that have been de-
veloped for CCG parsing, Hockenmaier &amp; Steed-
man’s (2002) generative model and Clark &amp; Cur-
ran’s (2007) normal-form model. Using the aver-
aged perceptron algorithm (Collins, 2002), White
&amp; Rajkumar (2009) trained a structured predic-
tion ranking model to combine these existing syn-
tactic models with several n-gram language mod-
els. This model improved upon the state-of-the-art
in terms of automatic evaluation scores on held-
out test data, but nevertheless an error analysis re-
vealed a surprising number of word order, func-
tion word and inflection errors. For each kind of
error, subsequent work investigated the utility of
employing more linguistically motivated features
to improve the ranking model.
To improve word ordering decisions, White &amp;
Rajkumar (2012) demonstrated that incorporat-
ing a feature into the ranker inspired by Gib-
son’s (2000) dependency locality theory can de-
liver statistically significant improvements in au-
tomatic evaluation scores, better match the distri-
butional characteristics of sentence orderings, and
significantly reduce the number of serious order-
ing errors (some involving vicious ambiguities) as
confirmed by a targeted human evaluation. Sup-
porting Gibson’s theory, comprehension and cor-
pus studies have found that the tendency to min-
imize dependency length has a strong influence
on constituent ordering choices; see Temperley
(2007) and Gildea and Temperley (2010) for an
overview.
Table 1 shows examples from White and Rajku-
mar (2012) of how the dependency length feature
(DEPLEN) affects the OpenCCG realizer’s output
even in comparison to a model (DEPORD) with
a rich set of discriminative syntactic and depen-
dency ordering features, but no features directly
targeting relative weight. In wsj 0015.7, the de-
pendency length model produces an exact match,
while the DEPORD model fails to shift the short
temporal adverbial next year next to the verb, leav-
ing a confusingly repetitive this year next year at
the end of the sentence. Note how shifting next
year from its canonical VP-final position to appear
next to the verb shortens its dependency length
considerably, while barely lengthening the depen-
dency to based on; at the same time, it avoids
ambiguity in what next year is modifying. In
wsj 0020.1 we see the reverse case: the depen-
dency length model produces a nearly exact match
with just an equally acceptable inversion of closely
watching, keeping the direct object in its canoni-
cal position. By contrast, the DEPORD model mis-
takenly shifts the direct object South Korea, Tai-
wan and Saudia Arabia to the end of the sentence
where it is difficult to understand following two
very long intervening phrases.
With function words, Rajkumar and White
(2011) showed that they could improve upon the
earlier model’s predictions for when to employ
that-complementizers using features inspired by
Jaeger’s (2010) work on using the principle of
uniform information density, which holds that
human language use tends to keep information
density relatively constant in order to optimize
communicative efficiency. In news text, com-
</bodyText>
<figure confidence="0.999222066666666">
np/n
&lt;Arg1&gt;
&lt;Arg0&gt;
np
&lt;Det&gt;
have.03
&lt;TENSE&gt;pres
&lt;Arg0&gt; &lt;Arg1&gt;
h1
n
&lt;Arg1&gt;
s[dcl]\np/(s[to]\np)
he
h2
&lt;NUM&gt;sg
point
P1
want.01
a1
a
W1
&lt;TENSE&gt;pres
M1
make.03
s[b]\np/np
np
h3
he
&lt;Arg0&gt;
&lt;GenRel&gt;
</figure>
<page confidence="0.993735">
415
</page>
<bodyText confidence="0.890338142857143">
wsj 0015.7 the exact amount of the refund will be determined next year based on actual collections made
until Dec. 31 of this year.
DEPLEN [same]
DEPORD the exact amount of the refund will be determined based on actual collections made until Dec.
31 of this year next year.
wsj 0020.1 the U.S. , claiming some success in its trade diplomacy , removed South Korea , Taiwan and
Saudi Arabia from a list of countries it is closely watching for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights .
DEPLEN the U.S. claiming some success in its trade diplomacy , removed South Korea, Taiwan and
Saudi Arabia from a list of countries it is watching closely for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights .
DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights , claiming some success in its trade
diplomacy, South Korea, Taiwan and Saudi Arabia.
</bodyText>
<tableCaption confidence="0.8012025">
Table 1: Examples of realized output for full models with and without the dependency length feature
(White and Rajkumar, 2012)
</tableCaption>
<bodyText confidence="0.997567604651163">
plementizers are left out two times out of three,
but in some cases the presence of that is cru-
cial to the interpretation. Generally, inserting a
complementizer makes the onset of a complement
clause more predictable, and thus less informa-
tion dense, thereby avoiding a potential spike in
information density that is associated with com-
prehension difficulty. Rajkumar &amp; White’s exper-
iments confirmed the efficacy of the features based
on Jaeger’s work, including information density–
based features, in a local classification model.2
Their experiments also showed that the improve-
ments in prediction accuracy apply to cases in
which the presence of a that-complementizer ar-
guably makes a substantial difference to fluency
or intelligiblity. For example, in (1), the pres-
ence of that avoids a local ambiguity, helping the
reader to understand that for the second month in
a row modifies the reporting of the shortage; with-
out that, it is very easy to mis-parse the sentence
as having for the second month in a row modifying
the saying event.
(1) He said that/∅? for the second month in a row,
food processors reported a shortage of nonfat
dry milk. (PTB WSJ0036.61)
Finally, to reduce the number of subject-verb
agreement errors, Rajkumar and White (2010) ex-
tended the earlier model with features enabling it
to make correct verb form choices in sentences
involving complex coordinate constructions and
2Note that the features from the local classification model
for that-complementizer choice have not yet been incorpo-
rated into OpenCCG’s global realization ranking model, and
thus do not inform the baseline realization choices in this
work.
with expressions such as a lot of where the correct
choice is not determined solely by the head noun.
They also improved animacy agreement with rela-
tivizers, reducing the number of errors where that
or which was chosen to modify an animate noun
rather than who or whom (and vice-versa), while
also allowing both choices where corpus evidence
was mixed.
</bodyText>
<sectionHeader confidence="0.905973" genericHeader="method">
3 Simple Reranking
</sectionHeader>
<subsectionHeader confidence="0.994847">
3.1 Methods
</subsectionHeader>
<bodyText confidence="0.999261333333333">
We ran two OpenCCG surface realization models
on the CCGbank dev set (derived from Section 00
of the Penn Treebank) and obtained n-best (n =
10) realizations. The first one is the baseline gen-
erative model (hereafter, generative model) used
in training the averaged perceptron model. This
model ranks realizations using the product of the
Hockenmaier syntax model, n-gram models over
words, POS tags and supertags in the training sec-
tions of the CCGbank, and the large-scale 5-gram
model from Gigaword. The second one is the
averaged perceptron model (hereafter, perceptron
model), which uses all the features reviewed in
Section 2. In order to experiment with multiple
parsers, we used the Stanford dependencies (de
Marneffe et al., 2006), obtaining gold dependen-
cies from the gold-standard PTB parses and auto-
matic dependencies from the automatic parses of
each realization. Using dependencies allowed us
to measure parse accuracy independently of word
order. We chose the Berkeley parser (Petrov et
al., 2006), Brown parser (Charniak and Johnson,
2005) and Stanford parser (Klein and Manning,
2003) to parse the realizations generated by the
</bodyText>
<page confidence="0.995777">
416
</page>
<table confidence="0.809184875">
rers tut
Berkeley Brown Stanford
No reranking 87.93 87.93 87.93
Mann Dua
Labeled 87.77 87.87 87.12
Unlabeled 87.90 87.97 86.97
partm
uus,
</table>
<tableCaption confidence="0.969104">
Table 2: Devset BLEU scores for simple ranking
on top of n-best perceptron model realizations
</tableCaption>
<figure confidence="0.9936628">
�dun,mw
... is propelling the region toward economic integration
(a) gold dependency
dobj
... is propelling toward economic integration the region
(b) simple ranker
pobj
... is propelling the region toward economic integration
(c) perceptron best
1: s s -
</figure>
<figureCaption confidence="0.6507845">
Figure 2: Example parsing mistake in PP-
s
</figureCaption>
<bodyText confidence="0.558836">
attachment (wsj 0043.1)
two realization models and calculated precision,
</bodyText>
<sectionHeader confidence="0.776985" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.998182727272727">
recall and F1 of the dependencies for each realiza-
Wiiga.
tion by comparing them with the gold dependen-
cies. We then ranked the realizations by their F1
score of parse accuracy, keeping the original rank-
ing in case of ties. We also tried using unlabeled
(and unordered) dependencies, in order to possi-
bly make better use of parses that were close to
being correct. In this setting, as long as the right
pair of tokens occur in a dependency relation, it
was counted as a correctly recovered dependency.
</bodyText>
<subsectionHeader confidence="0.655569">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999940833333333">
Simple ranking with the Berkeley parser of the
generative model’s n-best realizations raised the
BLEU score from 85.55 to 86.07, well below
the averaged perceptron model’s BLEU score of
87.93. However, as shown in Table 2, none of the
parsers yielded significant improvements on the
top of the perceptron model.
Inspecting the results of simple ranking re-
vealed that while simple ranking did success-
fully avoid vicious ambiguities in some cases,
parser mistakes with PP-attachments, noun-noun
compounds and coordinate structures too often
</bodyText>
<subsectionHeader confidence="0.417175">
g
</subsectionHeader>
<bodyText confidence="0.9972655">
blocked the gold realization from emerging on top.
To illustrate, Figure 2 shows an example with a
</bodyText>
<subsectionHeader confidence="0.609905">
Mihal White
</subsectionHeader>
<bodyText confidence="0.981309294117647">
PP-attachment mistake. In the figure, the key gold
Ligiic
dependencies of the reference sentence are shown
Univeriy
in (a), the dependencies of the realization selected
43210 USA
by the simple ranker are shown in (b), and the de-
pendencies of the realization selected by the per-
ig.osu.edu
ceptron ranker (same as gold) appear in (c), with
the parsing mistake indicated by the dashed line.
The simple ranker ends up choosing (b) as the best
realization because it has the most accurate parse
compared to the reference sentence, given the mis-
take with (c).
Other common parse errors are illustrated in
Figure 3. Here, (b) ends up getting chosen by the
simple ranker as the realization with the most ac-
curate parse given the failures in (c), where the ad-
ditional technology, personnel training is mistak-
enly analyzed as one noun phrase, a reading un-
likely to be considered by human readers.
In sum, although simple ranking helps to avoid
vicious ambiguity in some cases, the overall re-
sults of simple ranking are no better than the per-
ceptron model (according to BLEU, at least), as
parse failures that are not reflective of human in-
tepretive tendencies too often lead the ranker to
choose dispreferred realizations. As such, we turn
now to a more nuanced model for combining the
results of multiple parsers in a way that is less sen-
sitive to such parsing mistakes, while also letting
the perceptron model have a say in the final rank-
ing.
</bodyText>
<sectionHeader confidence="0.992827" genericHeader="method">
4 Reranking with SVMs
</sectionHeader>
<subsectionHeader confidence="0.993464">
4.1 Methods
</subsectionHeader>
<bodyText confidence="0.9999460625">
Since different parsers make different errors, we
conjectured that dependencies in the intersection
of the output of multiple parsers may be more re-
liable and thus may more reliably reflect human
comprehension preferences. Similarly, we conjec-
tured that large differences in the realizer’s percep-
tron model score may more reliably reflect human
fluency preferences than small ones, and thus we
combined this score with features for parser accu-
racy in an SVM ranker. Additionally, given that
parsers may more reliably recover some kinds of
dependencies than others, we included features for
each dependency type, so that the SVM ranker
might learn how to weight them appropriately.
Finally, since the differences among the n-best
parses reflect the least certain parsing decisions,
</bodyText>
<figure confidence="0.9785495">
prep pobj
dobj
aux
pobj
aux
prep
dobj
prep
aux
417
conj
the additional technology, personnel training and promotional efforts
(a) gold dependency
conj
the additional technology, training personnel and promotional efforts
(b) simple ranker
the additional technology, personnel training and promotional efforts
(c) perceptron best
</figure>
<figureCaption confidence="0.961944">
Figure 23: Example parsing mistakes in a noun-noun compound and a coordinate structure (wsj 0085.45)
</figureCaption>
<figure confidence="0.994662647058823">
det
cc
conj
amod nn
amod
det
conj
cc
amod nn
amod
det
amod
conj
nn
dep
cc
amod
</figure>
<bodyText confidence="0.99973806122449">
and thus ones that may require more common
sense inference that is easy for humans but not
machines, we conjectured that including features
from the n-best parses may help to better match
human performance. In more detail, we made use
of the following feature classes for each candidate
realization:
perceptron model score the score from the real-
izer’s model, normalized to [0,1] for the real-
izations in the n-best list
precision and recall labeled and unlabeled preci-
sion and recall for each parser’s best parse
per-label precision and recall (dep) precision
and recall for each type of dependency
obtained from each parser’s best parse (using
zero if not defined for lack of predicted or
gold dependencies with a given label)
n-best precision and recall (nbest) labeled and
unlabeled precision and recall for each
parser’s top five parses, along with the same
features for the most accurate of these parses
In training, we used the BLEU scores of each
realization compared with its reference sentence
to establish a preference order over pairs of candi-
date realizations, assuming that the original corpus
sentences are generally better than related alterna-
tives, and that BLEU can somewhat reliably pre-
dict human preference judgments.
We trained the SVM ranker (Joachims, 2002)
with a linear kernel and chose the hyper-parameter
c, which tunes the trade-off between training error
and margin, with 6-fold cross-validation on the de-
vset. We trained different models to investigate the
contribution made by different parsers and differ-
ent types of features, with the perceptron model
score included as a feature in all models. For each
parser, we trained a model with its overall preci-
sion and recall features, as shown at the top of Ta-
ble 3. Then we combined these three models to get
a new model (Bkl+Brw+St in the table) . Next,
to this combined model we separately added (i)
the per-label precision and recall features from all
the parsers (BBS+dep), and (ii) the n-best features
from the parsers (BBS+nbest). The full model
(BBS+dep+nbest) includes all the features listed
above. Finally, since the Berkeley parser yielded
the best results on its own, we also tested mod-
els using all the feature classes but only using this
parser by itself.
</bodyText>
<sectionHeader confidence="0.715802" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.99870225">
Table 3 shows the results of different SVM rank-
ing models on the devset. We calculated signifi-
cance using paired bootstrap resampling (Koehn,
2004).3 Both the per-label precision &amp; recall fea-
</bodyText>
<footnote confidence="0.998927666666667">
3Kudos to Kevin Gimpel for making his implementa-
tion available: http://www.ark.cs.cmu.edu/MT/
paired_bootstrap_v13a.tar.gz
</footnote>
<page confidence="0.983868">
418
</page>
<table confidence="0.999468833333333">
BLEU sig.
perceptron baseline 87.93 –
Berkeley 88.45 *
Brown 88.34
Stanford 88.18
Bkl+Brw+St 88.44 *
BBS+dep 88.63 **
BBS+nbest 88.60 **
BBS+dep+nbest 88.73 **
Bkl+dep 88.63 **
Bkl+nbest 88.48 *
Bkl +dep+nbest 88.68 **
</table>
<tableCaption confidence="0.9020675">
Table 3: Devset results of SVM ranking on top
of perceptron model. Significance codes: ** for
</tableCaption>
<figure confidence="0.42434725">
p &lt; 0.05, * for p &lt; 0.1.
BLEU sig.
perceptron baseline 86.94 –
BBS+dep+nbest 87.64 **
</figure>
<tableCaption confidence="0.91565">
Table 4: Final test results of SVM ranking on top
</tableCaption>
<bodyText confidence="0.912176230769231">
of perceptron model. Significance codes: ** for
p &lt; 0.05, * for p &lt; 0.1.
tures and the n-best parse features contributed to
achieving a significant improvement compared to
the perceptron model. Somewhat surprisingly, the
Berkeley parser did as well as all three parsers us-
ing just the overall precision and recall features,
but not quite as well using all features. The com-
plete model, BBS+dep+nbest, achieved a BLEU
score of 88.73, significantly improving upon the
perceptron model (p &lt; 0.02). We then confirmed
this result on the final test set, Section 23 of the
CCGbank, as shown in Table 4 (p &lt; 0.02 as well).
</bodyText>
<sectionHeader confidence="0.827863" genericHeader="evaluation">
5 Analysis and Discussion
</sectionHeader>
<subsectionHeader confidence="0.994429">
5.1 Targeted Manual Analysis
</subsectionHeader>
<bodyText confidence="0.999991827586207">
In order to gain a better understanding of the suc-
cesses and failures of our SVM ranker, we present
here a targeted manual analysis of the develop-
ment set sentences with the greatest change in
BLEU scores, carried out by the second author
(a native speaker). In this analysis, we consider
whether the reranked realization improves upon
or detracts from realization quality—in terms of
adequacy, fluency, both or neither—along with
a linguistic categorization of the differences be-
tween the reranked realization and the original
top-ranked realization according to the averaged
perceptron model. Unlike the broad-based and ob-
jective evaluation in terms of BLEU scores pre-
sented above, this analysis is narrowly targeted
and subjective, though the interested reader is in-
vited to review the complete set of analyzed ex-
amples that accompany the paper as a supplement.
We leave a more broad-based human evaluation by
naive subjects for future work.
Table 5 shows the results of the analysis, both
overall and for the most frequent categories of
changes. Of the 50 sentences where the BLEU
score went up the most, 15 showed an improve-
ment in adequacy (i.e., in conveying the intended
meaning), 22 showed an improvement in fluency
(with 3 cases also improving adequacy), and 16
yielded no discernible change in fluency or ade-
quacy. By contrast, with the 50 sentences where
the BLEU score went down the most, adequacy
was only affected 4 times, though fluency was af-
fected 32 times, and 15 remained essentially un-
changed.4 The table also shows that differences
in the order of VP constituents usually led to a
change in adequacy or fluency, as did ordering
changes within NPs, with noun-noun compounds
and named entities as the most frequent subcate-
gories of NP-ordering changes. Of the cases where
adequacy and fluency were not affected, contrac-
tions and subject-verb inversions were the most
frequent differences.
Examples of the changes yielded by the SVM
ranker appear in Table 6. With wsj 0036.54,
the averaged perceptron model selects a realiza-
tion that regrettably (though amusingly) swaps
purchasing and more than 250—yielding a sen-
tence that suggests that the executives have been
purchased!—while the SVM ranker succeeds in
ranking the original sentence above all competing
realizations. With wsj 0088.25, self-monitoring
with the SVM ranker yields a realization nearly
identical to the original except for an extra comma,
where it is clear that in public modifies do this;
by contrast, in the perceptron-best realization, in
public mistakenly appears to modify be disclosed.
With wsj 0041.18, the SVM ranker unfortunately
prefers a realization where presumably seems to
modify shows rather than of two politicians as
</bodyText>
<footnote confidence="0.9985514">
4The difference in the distribution of adequacy change,
fluency change and no change counts between the two condi-
tions is highly significant statistically (x2 = 9.3, df = 2, p &lt;
0.01). In this comparison, items where both fluency and ade-
quacy were affected were counted as adequacy cases.
</footnote>
<page confidence="0.993174">
419
</page>
<table confidence="0.861813666666667">
±adq ±flu =eq ±vpord ±npord ±nn ±ne =vpord =sbjinv =cntrc
BLEU wins 15 22 16 10 9 7 3 4 - 11
BLEU losses 4 32 15 8 13 5 5 4 7 -
</table>
<tableCaption confidence="0.861718">
Table 5: Manual analysis of devset sentences where the SVM ranker achieved the greatest in-
</tableCaption>
<bodyText confidence="0.938063428571429">
crease/decrease in BLEU scores (50 each of wins/losses) compared to the averaged perceptron baseline
model in terms of positive or negative changes in adequacy (±adq), fluency (±flu) or neither (=eq);
changes in VP ordering (±vpord), NP ordering (±npord), noun-noun compound ordering (±nn) and
named entities (±ne); and neither positive nor negative changes in VP ordering (=vpord), subject-
inversion (=sbjinv) and contractions (=cntrc). In all but one case (counted as =eq here), the BLEU
wins saw positive changes and the BLEU losses saw negative changes.
wsj 0036.54 the purchasing managers ’ report is based on data provided by more than 250 purchasing executives .
</bodyText>
<table confidence="0.949410857142857">
SVM RANKER [same]
PERCEP BEST the purchasing managers ’ report is based on data provided by purchasing more than 250 executives .
wsj 0088.25 Markey said we could have done this in public because so little sensitive information was disclosed ,
the aide said.
SVM RANKER Markey said, we could have done this in public because so little sensitive information was disclosed,
the aide said.
PERCEP BEST Markey said, we could have done this because so little sensitive information was disclosed in public ,
the aide said.
wsj 0041.18 the screen shows two distorted, unrecognizable photos , presumably of two politicians .
SVM RANKER the screen shows two distorted , unrecognizable photos presumably , of two politicians .
PERCEP BEST [same as original]
wsj 0044.111 “ I was dumbfounded ” , Mrs. Ward recalls.
SVM RANKER “ I was dumbfounded ” , recalls Mrs. Ward.
PERCEP BEST [same as original]
</table>
<tableCaption confidence="0.724063">
Table 6: Examples of devset sentences where the SVM ranker improved adequacy (top), made it worse
(middle) or left it the same (bottom)
</tableCaption>
<bodyText confidence="0.9986775">
in the original, which the averaged perceptron
model prefers. Finally, wsj 0044.111 is an exam-
ple where a subject-inversion makes no difference
to adequacy or fluency.
</bodyText>
<subsectionHeader confidence="0.996422">
5.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999981866666667">
The BLEU evaluation and targeted manual analy-
sis together show that the SVM ranker increases
the similarity to the original corpus of realizations
produced with self-monitoring, often in ways that
are crucial for the intended meaning to be apparent
to human readers.
A limitation of the experiments reported in this
paper is that OpenCCG’s input semantic depen-
dency graphs are not the same as the Stanford de-
pendencies used with the Treebank parsers, and
thus we have had to rely on the gold parses in
the PTB to derive gold dependencies for measur-
ing accuracy of parser dependency recovery. In a
realistic application scenario, however, we would
need to measure parser accuracy relative to the re-
alizer’s input. We initially tried using OpenCCG’s
parser in a simple ranking approach, but found that
it did not improve upon the averaged perceptron
model, like the three parsers used subsequently.
Given that with the more refined SVM ranker, the
Berkeley parser worked nearly as well as all three
parsers together using the complete feature set,
the prospects for future work on a more realistic
scenario using the OpenCCG parser in an SVM
ranker for self-monitoring now appear much more
promising, either using OpenCCG’s reimplemen-
tation of Hockenmaier &amp; Steedman’s generative
CCG model, or using the Berkeley parser trained
on OpenCCG’s enhanced version of the CCG-
bank, along the lines of Fowler and Penn (2010).
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9997322">
Approaches to surface realization have been de-
veloped for LFG, HPSG, and TAG, in addition
to CCG, and recently statistical dependency-based
approaches have been developed as well; see the
report from the first surface realization shared
</bodyText>
<page confidence="0.991531">
420
</page>
<bodyText confidence="0.999652111111111">
task (Belz et al., 2010; Belz et al., 2011) for an
overview. To our knowledge, however, a com-
prehensive investigation of avoiding vicious struc-
tural ambiguities with broad coverage statistical
parsers has not been previously explored. As
our SVM ranking model does not make use of
CCG-specific features, we would expect our self-
monitoring method to be equally applicable to re-
alizers using other frameworks.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999980441860465">
In this paper, we have shown that while using
parse accuracy in a simple reranking strategy for
self-monitoring fails to improve BLEU scores
over a state-of-the-art averaged perceptron realiza-
tion ranking model, it is possible to significantly
increase BLEU scores using an SVM ranker that
combines the realizer’s model score together with
features from multiple parsers, including ones de-
signed to make the ranker more robust to parsing
mistakes that human readers would be unlikely to
make. Additionally, via a targeted manual analy-
sis, we showed that the SVM reranker frequently
manages to avoid egregious errors involving “vi-
cious” ambiguities, of the kind that would mislead
human readers as to the intended meaning.
As noted in Reiter’s (2010) survey, many NLG
systems use surface realizers as off-the-shelf com-
ponents. In this paper, we have focused on
broad coverage surface realization using widely-
available PTB data—where there are many sen-
tences of varying complexity with gold-standard
annotations—following the common assumption
that experiments with broad coverage realization
are (or eventually will be) relevant for NLG ap-
plications. Of course, the kinds of ambiguity that
can be problematic in news text may or may not be
the same as the ones encountered in particular ap-
plications. Moreover, for certain applications (e.g.
ones with medical or legal implications), it may be
better to err on the side of ambiguity avoidance,
even at some expense to fluency, thereby requir-
ing training data reflecting the desired trade-off to
adapt the methods described here. We leave these
application-centered issues for investigation in fu-
ture work.
The current approach is primarily suitable for
offline use, for example in report generation where
there are no real-time interaction demands. In fu-
ture work, we also plan to investigate ways that
self-monitoring might be implemented more effi-
ciently as a combined process, rather than running
independent parsers as a post-process following
realization.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997914">
We thank Mark Johnson, Micha Elsner, the OSU
Clippers Group and the anonymous reviewers for
helpful comments and discussion. This work was
supported in part by NSF grants IIS-1143635 and
IIS-1319318.
</bodyText>
<sectionHeader confidence="0.998961" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998621128205128">
S. Abney. 1996. Statistical methods and linguistics. In
Judith Klavans and Philip Resnik, editors, The bal-
ancing act: Combining symbolic and statistical ap-
proaches to language, pages 1–26. MIT Press, Cam-
bridge, MA.
Anja Belz, Mike White, Josef van Genabith, Deirdre
Hogan, and Amanda Stent. 2010. Finding common
ground: Towards a surface realisation shared task.
In Proceedings of INLG-10, Generation Challenges,
pages 267–272.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 217–226,
Nancy, France, September. Association for Compu-
tational Linguistics.
Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-
08.
F. Chantree, B. Nuseibeh, A. De Roeck, and A. Willis.
2006. Identifying nocuous ambiguities in natural
language requirements. In Requirements Engineer-
ing, 14th IEEE International Conference, pages 59–
68. IEEE.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173–180,
Ann Arbor, Michigan. Association for Computa-
tional Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493–552.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP-02.
</reference>
<page confidence="0.992425">
421
</page>
<reference confidence="0.999593980952381">
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proceedings of ACL-08: HLT,
pages 183–191, Columbus, Ohio, June. Association
for Computational Linguistics.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with Combinatory Cat-
egorial Grammar. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 335–344, Uppsala, Sweden, July.
Association for Computational Linguistics.
Edward Gibson. 2000. Dependency locality theory:
A distance-based theory of linguistic complexity.
In Alec Marantz, Yasushi Miyashita, and Wayne
O’Neil, editors, Image, Language, brain: Papers
from the First Mind Articulation Project Symposium.
MIT Press, Cambridge, MA.
Daniel Gildea and David Temperley. 2010. Do gram-
mars minimize dependency length? Cognitive Sci-
ence, 34(2):286–310.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proc. ACL-02.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355–396.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage information density. Cognitive
Psychology, 61(1):23–62, August.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.
I.H. Khan, K. Van Deemter, and G. Ritchie. 2008.
Generation of referring expressions: Managing
structural ambiguities. In Proceedings of the 22nd
International Conference on Computational Lin-
guistics, pages 433–440. Association for Computa-
tional Linguistics.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 423–430.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Willem J. M. Levelt. 1989. Speaking: From Intention
to Articulation. MIT Press.
G¨unter Neumann and Gertjan van Noord. 1992. Self-
monitoring with reversible grammars. In Proceed-
ings of the 14th conference on Computational lin-
guistics - Volume 2, COLING ’92, pages 700–706,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL-02.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL.
Rajakrishnan Rajkumar and Michael White. 2010. De-
signing agreement features for realization ranking.
In Proc. Coling 2010: Posters, pages 1032–1040,
Beijing, China, August.
Rajakrishnan Rajkumar and Michael White. 2011.
Linguistically motivated complementizer choice in
surface realization. In Proceedings of the UC-
NLG+Eval: Language Generation and Evaluation
Workshop, pages 39–44, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proc. NAACL HLT 2009
Short Papers.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral generation systems. Studies in Natural Language
Processing. Cambridge University Press.
Ehud Reiter. 2010. Natural language generation. In
Alexander Clark, Chris Fox, and Shalom Lappin,
editors, The Handbook of Computational Linguistics
and Natural Language Processing (Blackwell Hand-
books in Linguistics), Blackwell Handbooks in Lin-
guistics, chapter 20. Wiley-Blackwell, 1 edition.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
David Temperley. 2007. Minimization of dependency
length in written English. Cognition, 105(2):300–
333.
K. Van Deemter. 2004. Towards a probabilistic version
of bidirectional OT syntax and semantics. Journal of
Semantics, 21(3):251–280.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Coling
2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 17–24.
</reference>
<page confidence="0.980635">
422
</page>
<reference confidence="0.999522941176471">
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 410–
419, Singapore, August. Association for Computa-
tional Linguistics.
Michael White and Rajakrishnan Rajkumar. 2012.
Minimal dependency length in realization ranking.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 244–255, Jeju Island, Korea, July. Association
for Computational Linguistics.
Michael White. 2006. Efficient Realization of
Coordinate Structures in Combinatory Categorial
Grammar. Research on Language &amp; Computation,
4(1):39–75.
</reference>
<page confidence="0.999423">
423
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.909884">
<title confidence="0.988832">That’s Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text</title>
<author confidence="0.987101">Duan</author>
<affiliation confidence="0.983908">Department of The Ohio State</affiliation>
<address confidence="0.999108">Columbus, OH 43210,</address>
<abstract confidence="0.998931642857143">We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving “vicious” ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for selfmonitoring, we find that with a stateof-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. However, by using an SVM ranker to combine the realizer’s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Statistical methods and linguistics.</title>
<date>1996</date>
<pages>1--26</pages>
<editor>In Judith Klavans and Philip Resnik, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2675" citStr="Abney (1996)" startWordPosition="417" endWordPosition="418">tion in early work with reversible grammars. As Neumann &amp; van Noord observed, a simple, brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form, then to parse each realization to see how many interpretations it has, keeping only those that have a single reading; they then went on to devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences, targeted to the ambiguous portion of the output. We might question, however, whether it is really possible to avoid ambiguity entirely in the general case, since Abney (1996) and others have argued that nearly every sentence is potentially ambiguous, though we (as human comprehenders) may not notice the ambiguities if they are unlikely. Taking up this issue, Khan et al. (2008)—building on Chantree et al.’s (2006) approach to identifying “innocuous” ambiguities—conducted several experiments to test whether ambiguity could be balanced against length or fluency in the context of generating referring expressions involving coordinate structures. Though Khan et al.’s study was limited to this one kind of structural ambiguity, they do observe that generating the brief va</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>S. Abney. 1996. Statistical methods and linguistics. In Judith Klavans and Philip Resnik, editors, The balancing act: Combining symbolic and statistical approaches to language, pages 1–26. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Mike White</author>
<author>Josef van Genabith</author>
<author>Deirdre Hogan</author>
<author>Amanda Stent</author>
</authors>
<title>Finding common ground: Towards a surface realisation shared task.</title>
<date>2010</date>
<booktitle>In Proceedings of INLG-10, Generation Challenges,</booktitle>
<pages>267--272</pages>
<marker>Belz, White, van Genabith, Hogan, Stent, 2010</marker>
<rawString>Anja Belz, Mike White, Josef van Genabith, Deirdre Hogan, and Amanda Stent. 2010. Finding common ground: Towards a surface realisation shared task. In Proceedings of INLG-10, Generation Challenges, pages 267–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Michael White</author>
<author>Dominic Espinosa</author>
<author>Eric Kow</author>
<author>Deirdre Hogan</author>
<author>Amanda Stent</author>
</authors>
<title>The first surface realisation shared task: Overview and evaluation results.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>217--226</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="32176" citStr="Belz et al., 2011" startWordPosition="5152" endWordPosition="5155">scenario using the OpenCCG parser in an SVM ranker for self-monitoring now appear much more promising, either using OpenCCG’s reimplementation of Hockenmaier &amp; Steedman’s generative CCG model, or using the Berkeley parser trained on OpenCCG’s enhanced version of the CCGbank, along the lines of Fowler and Penn (2010). 6 Related Work Approaches to surface realization have been developed for LFG, HPSG, and TAG, in addition to CCG, and recently statistical dependency-based approaches have been developed as well; see the report from the first surface realization shared 420 task (Belz et al., 2010; Belz et al., 2011) for an overview. To our knowledge, however, a comprehensive investigation of avoiding vicious structural ambiguities with broad coverage statistical parsers has not been previously explored. As our SVM ranking model does not make use of CCG-specific features, we would expect our selfmonitoring method to be equally applicable to realizers using other frameworks. 7 Conclusion In this paper, we have shown that while using parse accuracy in a simple reranking strategy for self-monitoring fails to improve BLEU scores over a state-of-the-art averaged perceptron realization ranking model, it is poss</context>
</contexts>
<marker>Belz, White, Espinosa, Kow, Hogan, Stent, 2011</marker>
<rawString>Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 217–226, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boxwell</author>
<author>Michael White</author>
</authors>
<title>Projecting Propbank roles onto the CCGbank. In</title>
<date>2008</date>
<booktitle>Proc. LREC08.</booktitle>
<contexts>
<context position="8843" citStr="Boxwell and White, 2008" startWordPosition="1367" endWordPosition="1370">s such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; 1http://openccg.sf.net 414 s[dcl]\np/np Figure 1: Example OpenCCG semantic dependency input for he has a point he wants to make, with gold standard lexical categories for each node and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White &amp; Rajkumar’s (2009; 2012) realization ranking model, recently augmented with a largescale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as </context>
</contexts>
<marker>Boxwell, White, 2008</marker>
<rawString>Stephen Boxwell and Michael White. 2008. Projecting Propbank roles onto the CCGbank. In Proc. LREC08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Chantree</author>
<author>B Nuseibeh</author>
<author>A De Roeck</author>
<author>A Willis</author>
</authors>
<title>Identifying nocuous ambiguities in natural language requirements.</title>
<date>2006</date>
<booktitle>In Requirements Engineering, 14th IEEE International Conference,</booktitle>
<pages>59--68</pages>
<publisher>IEEE.</publisher>
<marker>Chantree, Nuseibeh, De Roeck, Willis, 2006</marker>
<rawString>F. Chantree, B. Nuseibeh, A. De Roeck, and A. Willis. 2006. Identifying nocuous ambiguities in natural language requirements. In Requirements Engineering, 14th IEEE International Conference, pages 59– 68. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="17028" citStr="Charniak and Johnson, 2005" startWordPosition="2670" endWordPosition="2673">g sections of the CCGbank, and the large-scale 5-gram model from Gigaword. The second one is the averaged perceptron model (hereafter, perceptron model), which uses all the features reviewed in Section 2. In order to experiment with multiple parsers, we used the Stanford dependencies (de Marneffe et al., 2006), obtaining gold dependencies from the gold-standard PTB parses and automatic dependencies from the automatic parses of each realization. Using dependencies allowed us to measure parse accuracy independently of word order. We chose the Berkeley parser (Petrov et al., 2006), Brown parser (Charniak and Johnson, 2005) and Stanford parser (Klein and Manning, 2003) to parse the realizations generated by the 416 rers tut Berkeley Brown Stanford No reranking 87.93 87.93 87.93 Mann Dua Labeled 87.77 87.87 87.12 Unlabeled 87.90 87.97 86.97 partm uus, Table 2: Devset BLEU scores for simple ranking on top of n-best perceptron model realizations �dun,mw ... is propelling the region toward economic integration (a) gold dependency dobj ... is propelling toward economic integration the region (b) simple ranker pobj ... is propelling the region toward economic integration (c) perceptron best 1: s s - Figure 2: Example </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of ACL, pages 173–180, Ann Arbor, Michigan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP-02.</booktitle>
<contexts>
<context position="9690" citStr="Collins, 2002" startWordPosition="1501" endWordPosition="1502"> ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier &amp; Steedman’s (2002) generative model and Clark &amp; Curran’s (2007) normal-form model. Using the averaged perceptron algorithm (Collins, 2002), White &amp; Rajkumar (2009) trained a structured prediction ranking model to combine these existing syntactic models with several n-gram language models. This model improved upon the state-of-the-art in terms of automatic evaluation scores on heldout test data, but nevertheless an error analysis revealed a surprising number of word order, function word and inflection errors. For each kind of error, subsequent work investigated the utility of employing more linguistically motivated features to improve the ranking model. To improve word ordering decisions, White &amp; Rajkumar (2012) demonstrated that</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proc. EMNLP-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Espinosa</author>
<author>Michael White</author>
<author>Dennis Mehay</author>
</authors>
<title>Hypertagging: Supertagging for surface realization with CCG.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>183--191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7929" citStr="Espinosa et al., 2008" startWordPosition="1231" endWordPosition="1234">and errors. In Section 6, we briefly review related work on broad coverage surface realization. Finally, in Section 7, we sum up and discuss opportunities for future work in this direction. 2 Background We use the OpenCCG1 surface realizer for the experiments reported in this paper. The OpenCCG realizer generates surface strings for input semantic dependency graphs (or logical forms) using a chart-based algorithm (White, 2006) for Combinatory Categorial Grammar (Steedman, 2000) together with a “hypertagger” for probabilistically assigning lexical categories to lexical predicates in the input (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-i</context>
</contexts>
<marker>Espinosa, White, Mehay, 2008</marker>
<rawString>Dominic Espinosa, Michael White, and Dennis Mehay. 2008. Hypertagging: Supertagging for surface realization with CCG. In Proceedings of ACL-08: HLT, pages 183–191, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy A D Fowler</author>
<author>Gerald Penn</author>
</authors>
<title>Accurate context-free parsing with Combinatory Categorial Grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>335--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="31875" citStr="Fowler and Penn (2010)" startWordPosition="5103" endWordPosition="5106">und that it did not improve upon the averaged perceptron model, like the three parsers used subsequently. Given that with the more refined SVM ranker, the Berkeley parser worked nearly as well as all three parsers together using the complete feature set, the prospects for future work on a more realistic scenario using the OpenCCG parser in an SVM ranker for self-monitoring now appear much more promising, either using OpenCCG’s reimplementation of Hockenmaier &amp; Steedman’s generative CCG model, or using the Berkeley parser trained on OpenCCG’s enhanced version of the CCGbank, along the lines of Fowler and Penn (2010). 6 Related Work Approaches to surface realization have been developed for LFG, HPSG, and TAG, in addition to CCG, and recently statistical dependency-based approaches have been developed as well; see the report from the first surface realization shared 420 task (Belz et al., 2010; Belz et al., 2011) for an overview. To our knowledge, however, a comprehensive investigation of avoiding vicious structural ambiguities with broad coverage statistical parsers has not been previously explored. As our SVM ranking model does not make use of CCG-specific features, we would expect our selfmonitoring met</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>Timothy A. D. Fowler and Gerald Penn. 2010. Accurate context-free parsing with Combinatory Categorial Grammar. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 335–344, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Dependency locality theory: A distance-based theory of linguistic complexity.</title>
<date>2000</date>
<booktitle>Papers from the First Mind Articulation Project Symposium.</booktitle>
<editor>In Alec Marantz, Yasushi Miyashita, and Wayne O’Neil, editors, Image, Language, brain:</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Gibson, 2000</marker>
<rawString>Edward Gibson. 2000. Dependency locality theory: A distance-based theory of linguistic complexity. In Alec Marantz, Yasushi Miyashita, and Wayne O’Neil, editors, Image, Language, brain: Papers from the First Mind Articulation Project Symposium. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>David Temperley</author>
</authors>
<title>Do grammars minimize dependency length?</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="10913" citStr="Gildea and Temperley (2010)" startWordPosition="1682" endWordPosition="1685">that incorporating a feature into the ranker inspired by Gibson’s (2000) dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation. Supporting Gibson’s theory, comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices; see Temperley (2007) and Gildea and Temperley (2010) for an overview. Table 1 shows examples from White and Rajkumar (2012) of how the dependency length feature (DEPLEN) affects the OpenCCG realizer’s output even in comparison to a model (DEPORD) with a rich set of discriminative syntactic and dependency ordering features, but no features directly targeting relative weight. In wsj 0015.7, the dependency length model produces an exact match, while the DEPORD model fails to shift the short temporal adverbial next year next to the verb, leaving a confusingly repetitive this year next year at the end of the sentence. Note how shifting next year fro</context>
</contexts>
<marker>Gildea, Temperley, 2010</marker>
<rawString>Daniel Gildea and David Temperley. 2010. Do grammars minimize dependency length? Cognitive Science, 34(2):286–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar. In</title>
<date>2002</date>
<booktitle>Proc. ACL-02.</booktitle>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proc. ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="8344" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1293" endWordPosition="1297"> algorithm (White, 2006) for Combinatory Categorial Grammar (Steedman, 2000) together with a “hypertagger” for probabilistically assigning lexical categories to lexical predicates in the input (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; 1http://openccg.sf.net 414 s[dcl]\np/np Figure 1: Example OpenCCG semantic dependency input for he has a point he wants to make, with gold standard lexical categories for each node and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White &amp; Ra</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Florian Jaeger</author>
</authors>
<title>Redundancy and reduction: Speakers manage information density.</title>
<date>2010</date>
<journal>Cognitive Psychology,</journal>
<volume>61</volume>
<issue>1</issue>
<marker>Jaeger, 2010</marker>
<rawString>T. Florian Jaeger. 2010. Redundancy and reduction: Speakers manage information density. Cognitive Psychology, 61(1):23–62, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>Proc. KDD.</booktitle>
<contexts>
<context position="4779" citStr="Joachims, 2002" startWordPosition="737" endWordPosition="738">ealizations and rerank them if necessary to avoid vicious ambiguities, as determined by one or more automatic parsers. A potential obstacle, of course, is that automatic parsers may not be sufficiently representative of human readers, insofar as errors that a parser makes may not be problematic for human comprehension; moreover, parsers are rarely successful in fully recovering the intended interpretation for sentences of moderate length, even with carefully edited news text. Consequently, we examine two reranking strategies, one a simple baseline approach and the other using an SVM reranker (Joachims, 2002). Our simple reranking strategy for selfmonitoring is to rerank the realizer’s n-best list by parse accuracy, preserving the original order in case of ties. In this way, if there is a realization in the n-best list that can be parsed more accurately than the top-ranked realization—even if the intended interpretation cannot be recovered with 100% accuracy—it will become the preferred output of the combined realization-with-selfmonitoring system. With this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank d</context>
<context position="22936" citStr="Joachims, 2002" startWordPosition="3634" endWordPosition="3635"> for lack of predicted or gold dependencies with a given label) n-best precision and recall (nbest) labeled and unlabeled precision and recall for each parser’s top five parses, along with the same features for the most accurate of these parses In training, we used the BLEU scores of each realization compared with its reference sentence to establish a preference order over pairs of candidate realizations, assuming that the original corpus sentences are generally better than related alternatives, and that BLEU can somewhat reliably predict human preference judgments. We trained the SVM ranker (Joachims, 2002) with a linear kernel and chose the hyper-parameter c, which tunes the trade-off between training error and margin, with 6-fold cross-validation on the devset. We trained different models to investigate the contribution made by different parsers and different types of features, with the perceptron model score included as a feature in all models. For each parser, we trained a model with its overall precision and recall features, as shown at the top of Table 3. Then we combined these three models to get a new model (Bkl+Brw+St in the table) . Next, to this combined model we separately added (i) </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proc. KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Khan</author>
<author>K Van Deemter</author>
<author>G Ritchie</author>
</authors>
<title>Generation of referring expressions: Managing structural ambiguities.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Khan, Van Deemter, Ritchie, 2008</marker>
<rawString>I.H. Khan, K. Van Deemter, and G. Ritchie. 2008. Generation of referring expressions: Managing structural ambiguities. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 433–440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="17074" citStr="Klein and Manning, 2003" startWordPosition="2677" endWordPosition="2680">gram model from Gigaword. The second one is the averaged perceptron model (hereafter, perceptron model), which uses all the features reviewed in Section 2. In order to experiment with multiple parsers, we used the Stanford dependencies (de Marneffe et al., 2006), obtaining gold dependencies from the gold-standard PTB parses and automatic dependencies from the automatic parses of each realization. Using dependencies allowed us to measure parse accuracy independently of word order. We chose the Berkeley parser (Petrov et al., 2006), Brown parser (Charniak and Johnson, 2005) and Stanford parser (Klein and Manning, 2003) to parse the realizations generated by the 416 rers tut Berkeley Brown Stanford No reranking 87.93 87.93 87.93 Mann Dua Labeled 87.77 87.87 87.12 Unlabeled 87.90 87.97 86.97 partm uus, Table 2: Devset BLEU scores for simple ranking on top of n-best perceptron model realizations �dun,mw ... is propelling the region toward economic integration (a) gold dependency dobj ... is propelling toward economic integration the region (b) simple ranker pobj ... is propelling the region toward economic integration (c) perceptron best 1: s s - Figure 2: Example parsing mistake in PPs attachment (wsj 0043.1)</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="24063" citStr="Koehn, 2004" startWordPosition="3823" endWordPosition="3824">del (Bkl+Brw+St in the table) . Next, to this combined model we separately added (i) the per-label precision and recall features from all the parsers (BBS+dep), and (ii) the n-best features from the parsers (BBS+nbest). The full model (BBS+dep+nbest) includes all the features listed above. Finally, since the Berkeley parser yielded the best results on its own, we also tested models using all the feature classes but only using this parser by itself. 4.2 Results Table 3 shows the results of different SVM ranking models on the devset. We calculated significance using paired bootstrap resampling (Koehn, 2004).3 Both the per-label precision &amp; recall fea3Kudos to Kevin Gimpel for making his implementation available: http://www.ark.cs.cmu.edu/MT/ paired_bootstrap_v13a.tar.gz 418 BLEU sig. perceptron baseline 87.93 – Berkeley 88.45 * Brown 88.34 Stanford 88.18 Bkl+Brw+St 88.44 * BBS+dep 88.63 ** BBS+nbest 88.60 ** BBS+dep+nbest 88.73 ** Bkl+dep 88.63 ** Bkl+nbest 88.48 * Bkl +dep+nbest 88.68 ** Table 3: Devset results of SVM ranking on top of perceptron model. Significance codes: ** for p &lt; 0.05, * for p &lt; 0.1. BLEU sig. perceptron baseline 86.94 – BBS+dep+nbest 87.64 ** Table 4: Final test results of</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem J M Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<marker>Levelt, 1989</marker>
<rawString>Willem J. M. Levelt. 1989. Speaking: From Intention to Articulation. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unter Neumann</author>
<author>Gertjan van Noord</author>
</authors>
<title>Selfmonitoring with reversible grammars.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics - Volume 2, COLING ’92,</booktitle>
<pages>700--706</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Neumann, van Noord, 1992</marker>
<rawString>G¨unter Neumann and Gertjan van Noord. 1992. Selfmonitoring with reversible grammars. In Proceedings of the 14th conference on Computational linguistics - Volume 2, COLING ’92, pages 700–706, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="8881" citStr="Palmer et al., 2005" startWordPosition="1373" endWordPosition="1376">grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; 1http://openccg.sf.net 414 s[dcl]\np/np Figure 1: Example OpenCCG semantic dependency input for he has a point he wants to make, with gold standard lexical categories for each node and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White &amp; Rajkumar’s (2009; 2012) realization ranking model, recently augmented with a largescale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two probabilistic m</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL-02.</booktitle>
<contexts>
<context position="7153" citStr="Papineni et al., 2002" startWordPosition="1110" endWordPosition="1113">equacy are roughly balanced, while in cases where the BLEU score goes down, it is mostly fluency that is made worse (with reranking yielding an acceptable paraphrase roughly one third of the time in both cases). The paper is structured as follows. In Section 2, we review the realization ranking models that serve as a starting point for the paper. In Section 3, we report on our experiments with the simple reranking strategy, including a discussion of the ways in which this method typically fails. In Section 4, we describe how we trained an SVM reranker and report our results using BLEU scores (Papineni et al., 2002). In Section 5, we present a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, discussing both successes and errors. In Section 6, we briefly review related work on broad coverage surface realization. Finally, in Section 7, we sum up and discuss opportunities for future work in this direction. 2 Background We use the OpenCCG1 surface realizer for the experiments reported in this paper. The OpenCCG realizer generates surface strings for input semantic dependency graphs (or logical forms) using a chart-based algorithm (White, 2006) for Combinatory</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="16985" citStr="Petrov et al., 2006" startWordPosition="2664" endWordPosition="2667">OS tags and supertags in the training sections of the CCGbank, and the large-scale 5-gram model from Gigaword. The second one is the averaged perceptron model (hereafter, perceptron model), which uses all the features reviewed in Section 2. In order to experiment with multiple parsers, we used the Stanford dependencies (de Marneffe et al., 2006), obtaining gold dependencies from the gold-standard PTB parses and automatic dependencies from the automatic parses of each realization. Using dependencies allowed us to measure parse accuracy independently of word order. We chose the Berkeley parser (Petrov et al., 2006), Brown parser (Charniak and Johnson, 2005) and Stanford parser (Klein and Manning, 2003) to parse the realizations generated by the 416 rers tut Berkeley Brown Stanford No reranking 87.93 87.93 87.93 Mann Dua Labeled 87.77 87.87 87.12 Unlabeled 87.90 87.97 86.97 partm uus, Table 2: Devset BLEU scores for simple ranking on top of n-best perceptron model realizations �dun,mw ... is propelling the region toward economic integration (a) gold dependency dobj ... is propelling toward economic integration the region (b) simple ranker pobj ... is propelling the region toward economic integration (c) </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajakrishnan Rajkumar</author>
<author>Michael White</author>
</authors>
<title>Designing agreement features for realization ranking.</title>
<date>2010</date>
<booktitle>In Proc. Coling 2010: Posters,</booktitle>
<pages>1032--1040</pages>
<location>Beijing, China,</location>
<contexts>
<context position="15200" citStr="Rajkumar and White (2010)" startWordPosition="2382" endWordPosition="2385">ich the presence of a that-complementizer arguably makes a substantial difference to fluency or intelligiblity. For example, in (1), the presence of that avoids a local ambiguity, helping the reader to understand that for the second month in a row modifies the reporting of the shortage; without that, it is very easy to mis-parse the sentence as having for the second month in a row modifying the saying event. (1) He said that/∅? for the second month in a row, food processors reported a shortage of nonfat dry milk. (PTB WSJ0036.61) Finally, to reduce the number of subject-verb agreement errors, Rajkumar and White (2010) extended the earlier model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and 2Note that the features from the local classification model for that-complementizer choice have not yet been incorporated into OpenCCG’s global realization ranking model, and thus do not inform the baseline realization choices in this work. with expressions such as a lot of where the correct choice is not determined solely by the head noun. They also improved animacy agreement with relativizers, reducing the number of errors where that or which was</context>
</contexts>
<marker>Rajkumar, White, 2010</marker>
<rawString>Rajakrishnan Rajkumar and Michael White. 2010. Designing agreement features for realization ranking. In Proc. Coling 2010: Posters, pages 1032–1040, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajakrishnan Rajkumar</author>
<author>Michael White</author>
</authors>
<title>Linguistically motivated complementizer choice in surface realization.</title>
<date>2011</date>
<booktitle>In Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop,</booktitle>
<pages>39--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="12213" citStr="Rajkumar and White (2011)" startWordPosition="1899" endWordPosition="1902">ependency length considerably, while barely lengthening the dependency to based on; at the same time, it avoids ambiguity in what next year is modifying. In wsj 0020.1 we see the reverse case: the dependency length model produces a nearly exact match with just an equally acceptable inversion of closely watching, keeping the direct object in its canonical position. By contrast, the DEPORD model mistakenly shifts the direct object South Korea, Taiwan and Saudia Arabia to the end of the sentence where it is difficult to understand following two very long intervening phrases. With function words, Rajkumar and White (2011) showed that they could improve upon the earlier model’s predictions for when to employ that-complementizers using features inspired by Jaeger’s (2010) work on using the principle of uniform information density, which holds that human language use tends to keep information density relatively constant in order to optimize communicative efficiency. In news text, comnp/n &lt;Arg1&gt; &lt;Arg0&gt; np &lt;Det&gt; have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; &lt;Arg1&gt; h1 n &lt;Arg1&gt; s[dcl]\np/(s[to]\np) he h2 &lt;NUM&gt;sg point P1 want.01 a1 a W1 &lt;TENSE&gt;pres M1 make.03 s[b]\np/np np h3 he &lt;Arg0&gt; &lt;GenRel&gt; 415 wsj 0015.7 the exact amount of the re</context>
<context position="1325" citStr="Rajkumar &amp; White (2011" startWordPosition="200" endWordPosition="203">wn Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. However, by using an SVM ranker to combine the realizer’s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy. 1 Introduction Rajkumar &amp; White (2011; 2012) have recently shown that some rather egregious surface realization errors—in the sense that the reader would likely end up with the wrong interpretation—can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model (White and Rajkumar, 2009), as reviewed in the next section. However, one is apt to wonder: could one use a parser to check whether the intended interpretation is easy to recover, either as an alternative or to catch additional mistakes? Doing so would be tantamount to</context>
</contexts>
<marker>Rajkumar, White, 2011</marker>
<rawString>Rajakrishnan Rajkumar and Michael White. 2011. Linguistically motivated complementizer choice in surface realization. In Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 39–44, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajakrishnan Rajkumar</author>
<author>Michael White</author>
<author>Dominic Espinosa</author>
</authors>
<title>Exploiting named entity classes in CCG surface realization.</title>
<date>2009</date>
<booktitle>In Proc. NAACL HLT</booktitle>
<note>Short Papers.</note>
<contexts>
<context position="8523" citStr="Rajkumar et al., 2009" startWordPosition="1321" endWordPosition="1324">nput (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; 1http://openccg.sf.net 414 s[dcl]\np/np Figure 1: Example OpenCCG semantic dependency input for he has a point he wants to make, with gold standard lexical categories for each node and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White &amp; Rajkumar’s (2009; 2012) realization ranking model, recently augmented with a largescale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all thr</context>
</contexts>
<marker>Rajkumar, White, Espinosa, 2009</marker>
<rawString>Rajakrishnan Rajkumar, Michael White, and Dominic Espinosa. 2009. Exploiting named entity classes in CCG surface realization. In Proc. NAACL HLT 2009 Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building natural generation systems.</title>
<date>2000</date>
<booktitle>Studies in Natural Language Processing.</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9273" citStr="Reiter and Dale, 2000" startWordPosition="1433" endWordPosition="1436">ut for he has a point he wants to make, with gold standard lexical categories for each node and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White &amp; Rajkumar’s (2009; 2012) realization ranking model, recently augmented with a largescale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier &amp; Steedman’s (2002) generative model and Clark &amp; Curran’s (2007) normal-form model. Using the averaged perceptron algorithm (Collins, 2002), White &amp; Rajkumar (2009) trained a structured prediction ranking model to combine these existing syntactic models with several n-gram language models. This model improved upon the st</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building natural generation systems. Studies in Natural Language Processing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>Natural language generation. In Alexander</title>
<date>2010</date>
<booktitle>The Handbook of Computational Linguistics and Natural Language Processing (Blackwell Handbooks in Linguistics), Blackwell Handbooks in Linguistics, chapter 20. Wiley-Blackwell,</booktitle>
<volume>1</volume>
<pages>edition.</pages>
<editor>Clark, Chris Fox, and Shalom Lappin, editors,</editor>
<contexts>
<context position="9288" citStr="Reiter, 2010" startWordPosition="1437" endWordPosition="1438">e wants to make, with gold standard lexical categories for each node and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White &amp; Rajkumar’s (2009; 2012) realization ranking model, recently augmented with a largescale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier &amp; Steedman’s (2002) generative model and Clark &amp; Curran’s (2007) normal-form model. Using the averaged perceptron algorithm (Collins, 2002), White &amp; Rajkumar (2009) trained a structured prediction ranking model to combine these existing syntactic models with several n-gram language models. This model improved upon the state-of-the-art </context>
</contexts>
<marker>Reiter, 2010</marker>
<rawString>Ehud Reiter. 2010. Natural language generation. In Alexander Clark, Chris Fox, and Shalom Lappin, editors, The Handbook of Computational Linguistics and Natural Language Processing (Blackwell Handbooks in Linguistics), Blackwell Handbooks in Linguistics, chapter 20. Wiley-Blackwell, 1 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="7789" citStr="Steedman, 2000" startWordPosition="1213" endWordPosition="1214">esent a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, discussing both successes and errors. In Section 6, we briefly review related work on broad coverage surface realization. Finally, in Section 7, we sum up and discuss opportunities for future work in this direction. 2 Background We use the OpenCCG1 surface realizer for the experiments reported in this paper. The OpenCCG realizer generates surface strings for input semantic dependency graphs (or logical forms) using a chart-based algorithm (White, 2006) for Combinatory Categorial Grammar (Steedman, 2000) together with a “hypertagger” for probabilistically assigning lexical categories to lexical predicates in the input (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements i</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Temperley</author>
</authors>
<title>Minimization of dependency length in written English.</title>
<date>2007</date>
<journal>Cognition,</journal>
<volume>105</volume>
<issue>2</issue>
<pages>333</pages>
<contexts>
<context position="10881" citStr="Temperley (2007)" startWordPosition="1679" endWordPosition="1680"> (2012) demonstrated that incorporating a feature into the ranker inspired by Gibson’s (2000) dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation. Supporting Gibson’s theory, comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices; see Temperley (2007) and Gildea and Temperley (2010) for an overview. Table 1 shows examples from White and Rajkumar (2012) of how the dependency length feature (DEPLEN) affects the OpenCCG realizer’s output even in comparison to a model (DEPORD) with a rich set of discriminative syntactic and dependency ordering features, but no features directly targeting relative weight. In wsj 0015.7, the dependency length model produces an exact match, while the DEPORD model fails to shift the short temporal adverbial next year next to the verb, leaving a confusingly repetitive this year next year at the end of the sentence.</context>
</contexts>
<marker>Temperley, 2007</marker>
<rawString>David Temperley. 2007. Minimization of dependency length in written English. Cognition, 105(2):300– 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Van Deemter</author>
</authors>
<title>Towards a probabilistic version of bidirectional OT syntax and semantics.</title>
<date>2004</date>
<journal>Journal of Semantics,</journal>
<volume>21</volume>
<issue>3</issue>
<marker>Van Deemter, 2004</marker>
<rawString>K. Van Deemter. 2004. Towards a probabilistic version of bidirectional OT syntax and semantics. Journal of Semantics, 21(3):251–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>A more precise analysis of punctuation for broadcoverage surface realization with CCG.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="8454" citStr="White and Rajkumar, 2008" startWordPosition="1309" endWordPosition="1312">bilistically assigning lexical categories to lexical predicates in the input (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; 1http://openccg.sf.net 414 s[dcl]\np/np Figure 1: Example OpenCCG semantic dependency input for he has a point he wants to make, with gold standard lexical categories for each node and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White &amp; Rajkumar’s (2009; 2012) realization ranking model, recently augmented with a largescale 5-gram model based on th</context>
</contexts>
<marker>White, Rajkumar, 2008</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2008. A more precise analysis of punctuation for broadcoverage surface realization with CCG. In Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>Perceptron reranking for CCG realization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>410--419</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="1682" citStr="White and Rajkumar, 2009" startWordPosition="252" endWordPosition="255">scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy. 1 Introduction Rajkumar &amp; White (2011; 2012) have recently shown that some rather egregious surface realization errors—in the sense that the reader would likely end up with the wrong interpretation—can be avoided by making use of features inspired by psycholinguistics research together with an otherwise state-of-the-art averaged perceptron realization ranking model (White and Rajkumar, 2009), as reviewed in the next section. However, one is apt to wonder: could one use a parser to check whether the intended interpretation is easy to recover, either as an alternative or to catch additional mistakes? Doing so would be tantamount to selfmonitoring in Levelt’s (1989) model of language production. Neumann &amp; van Noord (1992) pursued the idea of self-monitoring for generation in early work with reversible grammars. As Neumann &amp; van Noord observed, a simple, brute-force way to generate unambiguous sentences is to enumerate possible realizations of an input logical form, then to parse eac</context>
<context position="9715" citStr="White &amp; Rajkumar (2009)" startWordPosition="1503" endWordPosition="1506">akes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier &amp; Steedman’s (2002) generative model and Clark &amp; Curran’s (2007) normal-form model. Using the averaged perceptron algorithm (Collins, 2002), White &amp; Rajkumar (2009) trained a structured prediction ranking model to combine these existing syntactic models with several n-gram language models. This model improved upon the state-of-the-art in terms of automatic evaluation scores on heldout test data, but nevertheless an error analysis revealed a surprising number of word order, function word and inflection errors. For each kind of error, subsequent work investigated the utility of employing more linguistically motivated features to improve the ranking model. To improve word ordering decisions, White &amp; Rajkumar (2012) demonstrated that incorporating a feature </context>
</contexts>
<marker>White, Rajkumar, 2009</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2009. Perceptron reranking for CCG realization. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410– 419, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>Minimal dependency length in realization ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>244--255</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="10984" citStr="White and Rajkumar (2012)" startWordPosition="1694" endWordPosition="1698">dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation. Supporting Gibson’s theory, comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices; see Temperley (2007) and Gildea and Temperley (2010) for an overview. Table 1 shows examples from White and Rajkumar (2012) of how the dependency length feature (DEPLEN) affects the OpenCCG realizer’s output even in comparison to a model (DEPORD) with a rich set of discriminative syntactic and dependency ordering features, but no features directly targeting relative weight. In wsj 0015.7, the dependency length model produces an exact match, while the DEPORD model fails to shift the short temporal adverbial next year next to the verb, leaving a confusingly repetitive this year next year at the end of the sentence. Note how shifting next year from its canonical VP-final position to appear next to the verb shortens i</context>
<context position="13945" citStr="White and Rajkumar, 2012" startWordPosition="2178" endWordPosition="2181">e U.S. claiming some success in its trade diplomacy , removed South Korea, Taiwan and Saudi Arabia from a list of countries it is watching closely for allegedly failing to honor U.S. patents , copyrights and other intellectual-property rights . DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S. patents , copyrights and other intellectual-property rights , claiming some success in its trade diplomacy, South Korea, Taiwan and Saudi Arabia. Table 1: Examples of realized output for full models with and without the dependency length feature (White and Rajkumar, 2012) plementizers are left out two times out of three, but in some cases the presence of that is crucial to the interpretation. Generally, inserting a complementizer makes the onset of a complement clause more predictable, and thus less information dense, thereby avoiding a potential spike in information density that is associated with comprehension difficulty. Rajkumar &amp; White’s experiments confirmed the efficacy of the features based on Jaeger’s work, including information density– based features, in a local classification model.2 Their experiments also showed that the improvements in prediction</context>
<context position="10272" citStr="White &amp; Rajkumar (2012)" startWordPosition="1589" endWordPosition="1592">aged perceptron algorithm (Collins, 2002), White &amp; Rajkumar (2009) trained a structured prediction ranking model to combine these existing syntactic models with several n-gram language models. This model improved upon the state-of-the-art in terms of automatic evaluation scores on heldout test data, but nevertheless an error analysis revealed a surprising number of word order, function word and inflection errors. For each kind of error, subsequent work investigated the utility of employing more linguistically motivated features to improve the ranking model. To improve word ordering decisions, White &amp; Rajkumar (2012) demonstrated that incorporating a feature into the ranker inspired by Gibson’s (2000) dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation. Supporting Gibson’s theory, comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices; see Temperl</context>
</contexts>
<marker>White, Rajkumar, 2012</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2012. Minimal dependency length in realization ranking. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 244–255, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Efficient Realization of Coordinate Structures in Combinatory Categorial Grammar.</title>
<date>2006</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="7737" citStr="White, 2006" startWordPosition="1206" endWordPosition="1207">ores (Papineni et al., 2002). In Section 5, we present a targeted manual analysis of the development set sentences with the greatest change in BLEU scores, discussing both successes and errors. In Section 6, we briefly review related work on broad coverage surface realization. Finally, in Section 7, we sum up and discuss opportunities for future work in this direction. 2 Background We use the OpenCCG1 surface realizer for the experiments reported in this paper. The OpenCCG realizer generates surface strings for input semantic dependency graphs (or logical forms) using a chart-based algorithm (White, 2006) for Combinatory Categorial Grammar (Steedman, 2000) together with a “hypertagger” for probabilistically assigning lexical categories to lexical predicates in the input (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman</context>
</contexts>
<marker>White, 2006</marker>
<rawString>Michael White. 2006. Efficient Realization of Coordinate Structures in Combinatory Categorial Grammar. Research on Language &amp; Computation, 4(1):39–75.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>