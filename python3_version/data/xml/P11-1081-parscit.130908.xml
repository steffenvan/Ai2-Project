<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000196">
<title confidence="0.996065">
A Cross-Lingual ILP Solution to Zero Anaphora Resolution
</title>
<author confidence="0.996719">
Ryu Iida
</author>
<affiliation confidence="0.999285">
Tokyo Institute of Technology
</affiliation>
<address confidence="0.8078175">
2-12-1, ˆOokayama, Meguro,
Tokyo 152-8552, Japan
</address>
<email confidence="0.998801">
ryu-i@cl.cs.titech.ac.jp
</email>
<author confidence="0.986276">
Massimo Poesio
</author>
<affiliation confidence="0.964382">
Universit`a di Trento,
Center for Mind / Brain Sciences
University of Essex,
Language and Computation Group
</affiliation>
<email confidence="0.996087">
massimo.poesio@unitn.it
</email>
<sectionHeader confidence="0.99561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998777294117647">
We present an ILP-based model of zero
anaphora detection and resolution that builds
on the joint determination of anaphoricity and
coreference model proposed by Denis and
Baldridge (2007), but revises it and extends it
into a three-way ILP problem also incorporat-
ing subject detection. We show that this new
model outperforms several baselines and com-
peting models, as well as a direct translation of
the Denis / Baldridge model, for both Italian
and Japanese zero anaphora. We incorporate
our model in complete anaphoric resolvers for
both Italian and Japanese, showing that our
approach leads to improved performance also
when not used in isolation, provided that sep-
arate classifiers are used for zeros and for ex-
plicitly realized anaphors.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9962725">
In so-called ‘pro-drop’ languages such as Japanese
and many romance languages including Italian,
phonetic realization is not required for anaphoric
references in contexts in which in English non-
contrastive pronouns are used: e.g., the subjects of
Italian and Japanese translations of buy in (1b) and
(1c) are not explicitly realized. We call these non-
realized mandatory arguments zero anaphors.
</bodyText>
<listItem confidence="0.9291755">
(1) a. [EN] [John]i went to visit some friends. On
the way, [he]i bought some wine.
b. [IT] [Giovanni]i and`o a far visita a degli am-
ici. Per via, φi compr`o del vino.
c. [JA] [John]i-wa yujin-o houmon-sita.
Tochu-de φi wain-o ka-tta.
</listItem>
<page confidence="0.980016">
804
</page>
<bodyText confidence="0.99993884375">
The felicitousness of zero anaphoric reference
depends on the referred entity being sufficiently
salient, hence this type of data–particularly in
Japanese and Italian–played a key role in early
work in coreference resolution, e.g., in the devel-
opment of Centering (Kameyama, 1985; Walker et
al., 1994; Di Eugenio, 1998). This research high-
lighted both commonalities and differences between
the phenomenon in such languages. Zero anaphora
resolution has remained a very active area of study
for researchers working on Japanese, because of the
prevalence of zeros in such languages1 (Seki et al.,
2002; Isozaki and Hirao, 2003; Iida et al., 2007a;
Taira et al., 2008; Imamura et al., 2009; Sasano et
al., 2009; Taira et al., 2010). But now the availabil-
ity of corpora annotated to study anaphora, includ-
ing zero anaphora, in languages such as Italian (e.g.,
Rodriguez et al. (2010)), and their use in competi-
tions such as SEMEVAL 2010 Task 1 on Multilin-
gual Coreference (Recasens et al., 2010), is lead-
ing to a renewed interest in zero anaphora resolu-
tion, particularly at the light of the mediocre results
obtained on zero anaphors by most systems partici-
pating in SEMEVAL.
Resolving zero anaphora requires the simulta-
neous decision that one of the arguments of a
verb is phonetically unrealized (and which argu-
ment exactly–in this paper, we will only be con-
cerned with subject zeros as these are the only
type to occur in Italian) and that a particular en-
tity is its antecedent. It is therefore natural to
view zero anaphora resolution as a joint inference
</bodyText>
<note confidence="0.88574675">
1As shown in Table 1, 64.3% of anaphors in the NAIST Text
Corpus of Anaphora are zeros.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 804–813,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figure confidence="0.6349668">
1: min cCO,j) ·x(i,j) + c−C
(i,j)∈P o,j~ · (1 − xo,j�)
1: cAj · yj + c−A
+ j · (1 − yj) (2)
j∈M
</figure>
<bodyText confidence="0.999858678571429">
task, for which Integer Linear Programming (ILP)–
introduced to NLP by Roth and Yih (2004) and suc-
cessfully applied by Denis and Baldridge (2007) to
the task of jointly inferring anaphoricity and deter-
mining the antecedent–would be appropriate.
In this work we developed, starting from the ILP
system proposed by Denis and Baldridge, an ILP
approach to zero anaphora detection and resolu-
tion that integrates (revised) versions of Denis and
Baldridge’s constraints with additional constraints
between the values of three distinct classifiers, one
of which is a novel one for subject prediction. We
demonstrate that treating zero anaphora resolution
as a three-way inference problem is successful for
both Italian and Japanese. We integrate the zero
anaphora resolver with a coreference resolver and
demonstrate that the approach leads to improved re-
sults for both Italian and Japanese.
The rest of the paper is organized as follows.
Section 2 briefly summarizes the approach proposed
by Denis and Baldridge (2007). We next present our
new ILP formulation in Section 3. In Section 4 we
show the experimental results with zero anaphora
only. In Section 5 we discuss experiments testing
that adding our zero anaphora detector and resolver
to a full coreference resolver would result in overall
increase in performance. We conclude and discuss
future work in Section 7.
</bodyText>
<equation confidence="0.962702333333333">
subject to
x(i,j) ∈ {0, 1} ∀(i, j) ∈ P
yj ∈ {0, 1} ∀j ∈ M
</equation>
<bodyText confidence="0.990053352941176">
M stands for the set of mentions in the document,
and P the set of possible coreference links over these
mentions. x(i,j) is an indicator variable that is set to
1 if mentions i and j are coreferent, and 0 otherwise.
yj is an indicator variable that is set to 1 if mention
j is anaphoric, and 0 otherwise. The costs cC �i,j) =
−log(P(COREF|i, j)) are (logs of) probabilities
produced by an antecedent identification classifier
with −log, whereas cA j = −log(P(ANAPH|j)),
are the probabilities produced by an anaphoricity de-
termination classifier with −log. In the Denis &amp;
Baldridge model, the search for a solution to an-
tecedent identification and anaphoricity determina-
tion is guided by the following three constraints.
Resolve only anaphors: if a pair of mentions (i, j)
is coreferent (x�i,j� = 1), then mention j must be
anaphoric (yj = 1).
</bodyText>
<equation confidence="0.863138">
x(i,j� ≤ yj ∀(i, j) ∈ P (3)
</equation>
<bodyText confidence="0.93795815">
Resolve anaphors: if a mention is anaphoric (yj =
1), it must be coreferent with at least one antecedent.
2 Using ILP for joint anaphoricity and 1: x(i,j) ∀j ∈ M (4)
coreference determination yj ≤
i∈Mj
Integer Linear Programming (ILP) is a method for
constraint-based inference aimed at finding the val-
ues for a set of variables that maximize a (linear) ob-
jective function while satisfying a number of con-
straints. Roth and Yih (2004) advocated ILP as a
general solution for a number of NLP tasks that re-
quire combining multiple classifiers and which the
traditional pipeline architecture is not appropriate,
such as entity disambiguation and relation extrac-
tion.
Denis and Baldridge (2007) defined the following
object function for the joint anaphoricity and coref-
erence determination problem.
Do not resolve non-anaphors: if a mention is non-
anaphoric (yj = 0), it should have no antecedents.
</bodyText>
<equation confidence="0.971363333333333">
yj ≥ 1
|Mj |1:x(i,j) ∀j ∈ M (5)
i∈Mj
</equation>
<sectionHeader confidence="0.987287" genericHeader="introduction">
3 An ILP-based account of zero anaphora
detection and resolution
</sectionHeader>
<bodyText confidence="0.999218">
In the corpora used in our experiments, zero
anaphora is annotated using as markable the first
verbal form (not necessarily the head) following the
position where the argument would have been real-
ized, as in the following example.
</bodyText>
<page confidence="0.995506">
805
</page>
<listItem confidence="0.642525">
(6) [Pahor]i e` nato a Trieste, allora porto princi-
pale dell’Impero Austro-Ungarico.
</listItem>
<bodyText confidence="0.9935685">
A sette anni [vide]i l’incendio del Narodni
dom,
The proposal of Denis and Baldridge (2007) can be
easily turned into a proposal for the task of detecting
and resolving zero anaphora in this type of data by
reinterpreting the indicator variables as follows:
</bodyText>
<listItem confidence="0.8854362">
• yj is 1 if markable j (a verbal form) initiates a
verbal complex whose subject is unrealized, 0
otherwise;
• x(i,j) is 1 if the empty mention realizing the
subject argument of markable j and markable
</listItem>
<bodyText confidence="0.99741075">
i are mentions of the same entity, 0 otherwise.
There are however a number of ways in which this
direct adaptation can be modified and extended. We
discuss them in turn.
</bodyText>
<subsectionHeader confidence="0.999345">
3.1 Best First
</subsectionHeader>
<bodyText confidence="0.9706505">
In the context of zero anaphora resolution, the ‘Do
not resolve non-anaphors’ constraint (5) is too weak,
as it allows the redundant choice of more than one
candidate antecedent. We developed therefore the
following alternative, that blocks selection of more
than one antecedent.
Best First (BF): x(i,j) bj E M (7)
�
yj �
i∈Mj
</bodyText>
<subsectionHeader confidence="0.998808">
3.2 A subject detection model
</subsectionHeader>
<bodyText confidence="0.999744722222222">
The greatest difficulty in zero anaphora resolution
in comparison to, say, pronoun resolution, is zero
anaphora detection. Simply relying for this on the
parser is not enough: most dependency parsers are
not very accurate at identifying cases in which the
verb does not have a subject on syntactic grounds
only. Again, it seems reasonable to suppose this
is because zero anaphora detection requires a com-
bination of syntactic information and information
about the current context. Within the ILP frame-
work, this hypothesis can be implemented by turn-
ing the zero anaphora resolution optimization prob-
lem into one with three indicator variables, with the
objective function in (8). The third variable, zj, en-
codes the information provided by the parser: it is
1 with cost cSj = −log(P(5UBJ1j)) if the parser
thinks that verb j has an explicit subject with proba-
bility P(5UBJ1j), otherwise it is 0.
</bodyText>
<equation confidence="0.987055833333333">
cSj · zj + c−S
j · (1 − zj) (8)
subject to
x(i,j) E {0, 11 b(i, j) E P
yj E {0, 11 bj E M
zj E {0, 11 bj E M
</equation>
<bodyText confidence="0.999250625">
The crucial fact about the relation between zj and
yj is that a verb has either a syntactically realized NP
or a zero pronoun as a subject, but not both. This is
encoded by the following constraint.
Resolve only non-subjects: if a predicate j syntac-
tically depends on a subject (zj = 1), then the predi-
cate j should have no antecedents of its subject zero
pronoun.
</bodyText>
<equation confidence="0.801961">
yj + zj G 1 bj E M (9)
</equation>
<sectionHeader confidence="0.916493" genericHeader="method">
4 Experiment 1: zero anaphora resolution
</sectionHeader>
<bodyText confidence="0.999952">
In a first round of experiments, we evaluated the per-
formance of the model proposed in Section 3 on zero
anaphora only (i.e., not attempting to resolve other
types of anaphoric expressions).
</bodyText>
<subsectionHeader confidence="0.998305">
4.1 Data sets
</subsectionHeader>
<bodyText confidence="0.999953">
We use the two data sets summarized in Table 1.
The table shows that NP anaphora occurs more fre-
quently than zero-anaphora in Italian, whereas in
Japanese the frequency of anaphoric zero-anaphors2
is almost double the frequency of the remaining
anaphoric expressions.
Italian For Italian coreference, we used the anno-
tated data set presented in Rodriguez et al. (2010)
and developed for the Semeval 2010 task ‘Corefer-
ence Resolution in Multiple Languages’ (Recasens
et al., 2010), where both zero-anaphora and NP
</bodyText>
<footnote confidence="0.631484333333333">
2In Japanese, like in Italian, zero anaphors are often used
non-anaphorically, to refer to situationally introduced entities,
as in I went to John’s office, but they told me that he had left.
</footnote>
<figure confidence="0.786204875">
�min cCO,j) ·x(i,j) + c−C
(i,j)∈P o,j~ · (1 − x(i,j�)
� cAj · yj + c−A
+ j · (1
j∈M − yj)
�
+
j∈M
</figure>
<page confidence="0.849003">
806
</page>
<table confidence="0.685275833333333">
#instances (anaphoric/total)
language type #docs #sentences #words zero-anaphors others all
Italian train 97 3,294 98,304 1,093 / 1,160 6,747 / 27,187 7,840 / 28,347
test 46 1,478 41,587 792 / 837 3,058 / 11,880 3,850 / 12,717
Japanese train 1,753 24,263 651,986 18,526 / 29,544 10,206 / 161,124 28,732 / 190,668
test 696 9,287 250,901 7,877 / 11,205 4,396 / 61,652 12,273 / 72,857
</table>
<bodyText confidence="0.870404333333333">
In the 6th column we use the term ‘anaphoric’ to indicate the number of zero anaphors that have an antecedent in
the text, whereas the total figure is the sum of anaphoric and exophoric zero-anaphors - zeros with a vague / generic
reference.
</bodyText>
<tableCaption confidence="0.994535">
Table 1: Italian and Japanese Data Sets
</tableCaption>
<bodyText confidence="0.999935818181818">
coreference are annotated. This dataset consists
of articles from Italian Wikipedia, tokenized, POS-
tagged and morphologically analyzed using TextPro,
a freely available Italian pipeline (Pianta et al.,
2008). We parsed the corpus using the Italian ver-
sion of the DESR dependency parser (Attardi et al.,
2007).
In Italian, zero pronouns may only occur as omit-
ted subjects of verbs. Therefore, in the task of
zero-anaphora resolution all verbs appearing in a
text are considered candidates for zero pronouns,
and all gold mentions or system mentions preced-
ing a candidate zero pronoun are considered as can-
didate antecedents. (In contrast, in the experiments
on coreference resolution discussed in the following
section, all mentions are considered as both candi-
date anaphors and candidate antecedents. To com-
pare the results with gold mentions and with system
detected mentions, we carried out an evaluation us-
ing the mentions automatically detected by the Ital-
ian version of the BART system (I-BART) (Poesio
et al., 2010), which is freely downloadable.3
Japanese For Japanese coreference we used the
NAIST Text Corpus (Iida et al., 2007b) version
1.4β, which contains the annotated data about NP
coreference and zero-anaphoric relations. We also
used the Kyoto University Text Corpus4 that pro-
vides dependency relations information for the same
articles as the NAIST Text Corpus. In addition, we
also used a Japanese named entity tagger, CaboCha5
for automatically tagging named entity labels. In
the NAIST Text Corpus mention boundaries are not
annotated, only the heads. Thus, we considered
</bodyText>
<footnote confidence="0.99916225">
3http://www.bart-coref.org/
4http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/corpus.html
5http://chasen.org˜taku/software/cabocha/
</footnote>
<bodyText confidence="0.999839928571429">
as pseudo-mentions all bunsetsu chunks (i.e. base
phrases in Japanese) whose head part-of-speech was
automatically tagged by the Japanese morphologi-
cal analyser Chasen6 as either ‘noun’ or ‘unknown
word’ according to the NAIST-jdic dictionary.7
For evaluation, articles published from January
1st to January 11th and the editorials from January
to August were used for training and articles dated
January 14th to 17th and editorials dated October
to December are used for testing as done by Taira
et al. (2008) and Imamura et al. (2009). Further-
more, in the experiments we only considered subject
zero pronouns for a fair comparison to Italian zero-
anaphora.
</bodyText>
<subsectionHeader confidence="0.973457">
4.2 Models
</subsectionHeader>
<bodyText confidence="0.999906052631579">
In these first experiments we compared the three
ILP-based models discussed in Section 3: the direct
reimplementation of the Denis and Baldridge pro-
posal (i.e., using the same constrains), a version re-
placing Do-Not-Resolve-Not-Anaphors with Best-
First, and a version with Subject Detection as well.
As discussed by Iida et al. (2007a) and Imamura
et al. (2009), useful features in intra-sentential zero-
anaphora are different from ones in inter-sentential
zero-anaphora because in the former problem syn-
tactic information between a zero pronoun and its
candidate antecedent is essential, while the lat-
ter needs to capture the significance of saliency
based on Centering Theory (Grosz et al., 1995).
To directly reflect this difference, we created two
antecedent identification models; one for intra-
sentential zero-anaphora, induced using the training
instances which a zero pronoun and its candidate an-
tecedent appear in the same sentences, the other for
</bodyText>
<footnote confidence="0.99995">
6http://chasen-legacy.sourceforge.jp/
7http://sourceforge.jp/projects/naist-jdic/
</footnote>
<page confidence="0.99374">
807
</page>
<bodyText confidence="0.997116882352941">
inter-sentential cases, induced from the remaining
training instances.
To estimate the feature weights of each classifier,
we used MEGAM8, an implementation of the Max-
imum Entropy model, with default parameter set-
tings. The ILP-based models were compared with
the following baselines.
PAIRWISE: as in the work by Soon et al. (2001),
antecedent identification and anaphoricity determi-
nation are simultaneously executed by a single clas-
sifier.
DS-CASCADE: the model first filters out non-
anaphoric candidate anaphors using an anaphoric-
ity determination model, then selects an antecedent
from a set of candidate antecedents of anaphoric
candidate anaphors using an antecedent identifica-
tion model.
</bodyText>
<subsectionHeader confidence="0.886673">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.999990285714286">
The feature sets for antecedent identification and
anaphoricity determination are briefly summarized
in Table 2 and Table 3, respectively. The agreement
features such as NUM AGREE and GEN AGREE are
automatically derived using TextPro. Such agree-
ment features are not available in Japanese because
Japanese words do not contain such information.
</bodyText>
<subsectionHeader confidence="0.997147">
4.4 Creating subject detection models
</subsectionHeader>
<bodyText confidence="0.9998616875">
To create a subject detection model for Italian, we
used the TUT corpus9 (Bosco et al., 2010), which
contains manually annotated dependency relations
and their labels, consisting of 80,878 tokens in
CoNLL format. We induced an maximum entropy
classifier by using as items all arcs of dependency
relations, each of which is used as a positive instance
if its label is subject; otherwise it is used as a nega-
tive instance.
To train the Japanese subject detection model we
used 1,753 articles contained both in the NAIST
Text Corpus and the Kyoto University Text Corpus.
By merging these two corpora, we can obtain the an-
notated data including which dependency arc is sub-
ject10. To create the training instances, any pair of
a predicate and its dependent are extracted, each of
</bodyText>
<footnote confidence="0.90577275">
8http://www.cs.utah.edu/˜hal/megam/
9http://www.di.unito.it/˜tutreeb/
10Note that Iida et al. (2007b) referred to this relation as
‘nominative’.
</footnote>
<table confidence="0.999635142857143">
feature description
SUBJ PRE 1 if subject is included in the preceding
words of ZERO in a sentence; otherwise 0.
TOPIC PRE* 1 iftopic case marker appears in the preced-
ing words of ZERO in a sentence; otherwise
0.
NUM PRE 1 if a candidate which agrees with ZERO
(GEN PRE) with regards to number (gender) is included
in the set of NP; otherwise 0.
FIRST SENT 1 if ZERO appears in the first sentence of a
text; otherwise 0.
FIRST WORD 1 if the predicate which has ZERO is the
first word in a sentence; otherwise 0.
POS / LEMMA part-of-speech / dependency label / lemma
/ DEP LABEL of the predicate which has ZERO.
D POS / part-of-speech / dependency label / lemma
D LEMMA / ofthe dependents of the predicate which has
D DEP LABEL ZERO.
PATH* dependency labels (functional words) of
words intervening between a ZERO and the
sentence head
</table>
<tableCaption confidence="0.8927815">
The features marked with ‘*’ used only in Japanese.
Table 3: Features for anaphoricity determination
</tableCaption>
<bodyText confidence="0.999233733333333">
which is judged as positive if its relation is subject;
as negative otherwise.
As features for Italian, we used lemmas, PoS tag
of a predicate and its dependents as well as their
morphological information (i.e. gender and num-
ber) automatically computed by TextPro (Pianta et
al., 2008). For Japanese, the head lemmas of predi-
cate and dependent chunks as well as the functional
words involved with these two chunks were used as
features. One case specially treated is when a de-
pendent is placed as an adnominal constituent of a
predicate, as in this case relation estimation of de-
pendency arcs is difficult. In such case we instead
use the features shown in Table 2 for accurate esti-
mation.
</bodyText>
<subsectionHeader confidence="0.9677">
4.5 Results with zero anaphora only
</subsectionHeader>
<bodyText confidence="0.999984333333333">
In zero anaphora resolution, we need to find all pred-
icates that have anaphoric unrealized subjects (i.e.
zero pronouns which have an antecedent in a text),
and then identify an antecedent for each such argu-
ment.
The Italian and Japanese test data sets contain
4,065 and 25,467 verbal predicates respectively. The
performance of each model at zero-anaphora detec-
tion and resolution is shown in Table 4, using recall
</bodyText>
<page confidence="0.995677">
808
</page>
<table confidence="0.998567157894737">
feature description
HEAD LEMMA characters of the head lemma in NP.
POS part-of-speech of NP.
DEFINITE 1 ifNP contains the article corresponding to DEFINITE ‘the’; otherwise 0.
DEMONSTRATIVE 1 ifNP contains the article corresponding to DEMONSTRATIVE such as ‘that’ and ‘this’; otherwise 0.
POSSESSIVE 1 if NP contains the article corresponding to POSSESSIVE such as ‘his’ and ‘their’; otherwise 0.
CASE MARKER** case marker followed by NP, such as ‘wa (topic)’, ‘ga (subject)’, ‘o (object)’.
DEP LABEL* dependency label of NP.
COOC MI** the score of well-formedness model estimated from a large number of triplets ( NP, Case, Predicate).
FIRST SENT 1 ifNP appears in the first sentence of a text; otherwise 0.
FIRST MENTION 1 ifNP first appears in the set of candidate antecedents; otherwise 0.
CL RANK** a rank of NP in forward looking-center list based on Centering Theory (Grosz et al., 1995)
CL ORDER** a order of NP in forward looking-center list based on Centering Theory (Grosz et al., 1995)
PATH dependency labels (functional words) of words intervening between a ZERO and NP
NUM (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to number; otherwise 0.
GEN (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to gender; otherwise 0.
HEAD MATCH 1 if ANA and NP have the same head lemma; otherwise 0.
REGEX MATCH 1 if the string of NP subsumes the string of ANA; otherwise 0.
COMP MATCH 1 if ANA and NP have the same string; otherwise 0.
</table>
<tableCaption confidence="0.922680333333333">
NP, ANA and ZERO stand for a candidate antecedent, a candidate anaphor and a candidate zero pronoun respectively. The features
marked with ‘*’ are only used in Italian, while the features marked with ‘**’ are only used in Japanese.
Table 2: Features used for antecedent identification
</tableCaption>
<table confidence="0.999735222222222">
Italian Japanese
system mentions gold mentions
model R P F R P F R P F
PAIRWISE 0.864 0.172 0.287 0.864 0.172 0.287 0.286 0.308 0.296
DS-CASCADE 0.396 0.684 0.502 0.404 0.697 0.511 0.345 0.194 0.248
ILP 0.905 0.034 0.065 0.929 0.028 0.055 0.379 0.238 0.293
ILP +BF 0.803 0.375 0.511 0.834 0.369 0.511 0.353 0.256 0.297
ILP+SUBJ 0.900 0.034 0.066 0.927 0.028 0.055 0.371 0.315 0.341
ILP +BF +SUBJ 0.777 0.398 0.526 0.815 0.398 0.534 0.345 0.348 0.346
</table>
<tableCaption confidence="0.999933">
Table 4: Results on zero pronouns
</tableCaption>
<bodyText confidence="0.999930230769231">
/ precision / F over link detection as a metric (model
theoretic metrics do not apply for this task as only
subsets of coreference chains are considered). As
can be seen from Table 4, the ILP version with Do-
Not-Resolve-Non-Anaphors performs no better than
the baselines for either languages, but in both lan-
guages replacing that constraint with Best-First re-
sults in a performance above the baselines; adding
Subject Detection results in further improvement for
both languages. Notice also that the performance of
the models on Italian is quite a bit higher than for
Japanese although the dataset is much smaller, pos-
sibly meaning that the task is easier in Italian.
</bodyText>
<sectionHeader confidence="0.892194" genericHeader="method">
5 Experiment 2: coreference resolution for
all anaphors
</sectionHeader>
<bodyText confidence="0.99923225">
In a second series of experiments we evaluated the
performance of our models together with a full
coreference system resolving all anaphors, not just
zeros.
</bodyText>
<subsectionHeader confidence="0.999081">
5.1 Separating vs combining classifiers
</subsectionHeader>
<bodyText confidence="0.999960285714286">
Different types of nominal expressions display very
different anaphoric behavior: e.g., pronoun res-
olution involves very different types of informa-
tion from nominal expression resolution, depend-
ing more on syntactic information and on the local
context and less on commonsense knowledge. But
the most common approach to coreference resolu-
</bodyText>
<page confidence="0.996546">
809
</page>
<bodyText confidence="0.99998696">
tion (Soon et al., 2001; Ng and Cardie, 2002, etc.)
is to use a single classifier to identify antecedents of
all anaphoric expressions, relying on the ability of
the machine learning algorithm to learn these differ-
ences. These models, however, often fail to capture
the differences in anaphoric behavior between dif-
ferent types of expressions–one of the reasons be-
ing that the amount of training instances is often too
small to learn such differences.11 Using different
models would appear to be key in the case of zero-
anaphora resolution, which differs even more from
the rest of anaphora resolution, e.g., in being partic-
ularly sensitive to local salience, as amply discussed
in the literature on Centering discussed earlier.
To test the hypothesis that using what we will
call separated models for zero anaphora and every-
thing else would work better than combined mod-
els induced from all the learning instances, we man-
ually split the training instances in terms of these
two anaphora types and then created two classifiers
for antecedent identification: one for zero-anaphora,
the other for NP-anaphora, separately induced from
the corresponding training instances. Likewise,
anaphoricity determination models were separately
induced with regards to these two anaphora types.
</bodyText>
<subsectionHeader confidence="0.997103">
5.2 Results with all anaphors
</subsectionHeader>
<bodyText confidence="0.999987222222222">
In Table 5 and Table 6 we show the (MUC scorer)
results obtained by adding the zero anaphoric reso-
lution models proposed in this paper to both a com-
bined and a separated classifier. For the separated
classifier, we use the ILP+BF model for explicitly
realized NPs, and different ILP models for zeros.
The results show that the separated classi-
fier works systematically better than a combined
classifier. For both Italian and Japanese the
ILP+BF+SUBJ model works clearly better than the
baselines, whereas simply applying the original De-
nis and Baldridge model unchanged to this case we
obtain worse results than the baselines. For Italian
we could also compare our results with those ob-
tained on the same dataset by one of the two sys-
tems that participated to the Italian section of SE-
MEVAL, I-BART. I-BART’s results are clearly bet-
ter than those with both baselines, but also clearly in-
</bodyText>
<footnote confidence="0.6501035">
11E.g., the entire MUC-6 corpus contains a grand total of 3
reflexive pronouns.
</footnote>
<table confidence="0.998823888888889">
model Japanese
combined separated
R P F R P F
PAIRWISE 0.345 0.236 0.280 0.427 0.240 0.308
DS-CASCADE 0.207 0.592 0.307 0.291 0.488 0.365
ILP 0.381 0.330 0.353 0.490 0.304 0.375
ILP +BF 0.349 0.390 0.368 0.446 0.340 0.386
ILP +SUBJ 0.376 0.366 0.371 0.484 0.353 0.408
ILP +BF +SUBJ 0.344 0.450 0.390 0.441 0.415 0.427
</table>
<tableCaption confidence="0.9747935">
Table 6: Results for overall coreference: Japanese (MUC
score)
</tableCaption>
<bodyText confidence="0.999963333333333">
ferior to the results obtained with our models. In par-
ticular, the effect of introducing the separated model
with ILP+BF+SUBJ is more significant when us-
ing the system detected mentions; it obtained perfor-
mance more than 13 points better than I-BART when
the model referred to the system detected mentions.
</bodyText>
<sectionHeader confidence="0.999896" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999991785714286">
We are not aware of any previous machine learn-
ing model for zero anaphora in Italian, but there
has been quite a lot of work on Japanese zero-
anaphora (Iida et al., 2007a; Taira et al., 2008; Ima-
mura et al., 2009; Taira et al., 2010; Sasano et al.,
2009). In work such as Taira et al. (2008) and Ima-
mura et al. (2009), zero-anaphora resolution is con-
sidered as a sub-task of predicate argument structure
analysis, taking the NAIST text corpus as a target
data set. Taira et al. (2008) and Taira et al. (2010) ap-
plied decision lists and transformation-based learn-
ing respectively in order to manually analyze which
clues are important for each argument assignment.
Imamura et al. (2009) also tackled to the same prob-
lem setting by applying a pairwise classifier for each
argument. In their approach, a ‘null’ argument is ex-
plicitly added into the set of candidate argument to
learn the situation where an argument of a predicate
is ‘exophoric’. They reported their model achieved
better performance than the work by Taira et al.
(2008).
Iida et al. (2007a) also used the NAIST text
corpus. They adopted the BACT learning algo-
rithm (Kudo and Matsumoto, 2004) to effectively
learn subtrees useful for both antecedent identifica-
tion and zero pronoun detection. Their model drasti-
cally outperformed a simple pairwise model, but it is
still performed as a cascaded process. Incorporating
</bodyText>
<page confidence="0.990579">
810
</page>
<table confidence="0.995063083333333">
model Italian
system mentions gold mentions
combined separated combined separated
R P F R P F R P F R P F
PAIRWISE 0.508 0.208 0.295 0.472 0.241 0.319 0.582 0.261 0.361 0.566 0.314 0.404
DS-CASCADE 0.225 0.553 0.320 0.217 0.574 0.315 0.245 0.609 0.349 0.246 0.686 0.362
I-BART 0.324 0.294 0.308 – – – 0.532 0.441 0.482 – – –
ILP 0.539 0.321 0.403 0.535 0.316 0.397 0.614 0.369 0.461 0.607 0.384 0.470
ILP +BF 0.471 0.404 0.435 0.483 0.409 0.443 0.545 0.517 0.530 0.563 0.519 0.540
ILP +SUBJ 0.537 0.325 0.405 0.534 0.318 0.399 0.611 0.372 0.463 0.606 0.387 0.473
ILP +BF +SUBJ 0.464 0.410 0.435 0.478 0.418 0.446 0.538 0.527 0.533 0.559 0.536 0.547
R: Recall, P: Precision, F: f-score, BF: best first constraint, SUBJ: subject detection model.
</table>
<tableCaption confidence="0.998433">
Table 5: Results for overall coreference: Italian (MUC score)
</tableCaption>
<bodyText confidence="0.999956888888889">
their model into the ILP formulation proposed here
looks like a promising further extension.
Sasano et al. (2009) obtained interesting experi-
mental results about the relationship between zero-
anaphora resolution and the scale of automatically
acquired case frames. In their work, their case
frames were acquired from a very large corpus con-
sisting of 100 billion words. They also proposed
a probabilistic model to Japanese zero-anaphora
in which an argument assignment score is esti-
mated based on the automatically acquired case
frames. They concluded that case frames acquired
from larger corpora lead to better f-score on zero-
anaphora resolution.
In contrast to these approaches in Japanese, the
participants to Semeval 2010 task 1 (especially the
Italian coreference task) simply solved the prob-
lems using one coreference classifier, not distin-
guishing zero-anaphora from the other types of
anaphora (Kobdani and Sch¨utze, 2010; Poesio et al.,
2010). On the other hand, our approach shows sep-
arating problems contributes to improving perfor-
mance in Italian zero-anaphora. Although we used
gold mentions in our evaluations, mention detection
is also essential. As a next step, we also need to take
into account ways of incorporating a mention detec-
tion model into the ILP formulation.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999952333333333">
In this paper, we developed a new ILP-based model
of zero anaphora detection and resolution that ex-
tends the coreference resolution model proposed by
Denis and Baldridge (2007) by introducing modi-
fied constraints and a subject detection model. We
evaluated this model both individually and as part
of the overall coreference task for both Italian and
Japanese zero anaphora, obtaining clear improve-
ments in performance.
One avenue for future research is motivated by the
observation that whereas introducing the subject de-
tection model and the best-first constraint results in
higher precision maintaining the recall compared to
the baselines, that precision is still low. One of the
major source of the errors is that zero pronouns are
frequently used in Italian and Japanese in contexts in
which in English as so-called generic they would be
used: “I walked into the hotel and (they) said ..”. In
such case, the zero pronoun detection model is often
incorrect. We are considering adding a generic they
detection component.
We also intend to experiment with introducing
more sophisticated antecedent identification models
in the ILP framework. In this paper, we used a very
basic pairwise classifier; however Yang et al. (2008)
and Iida et al. (2003) showed that the relative com-
parison of two candidate antecedents leads to obtain-
ing better accuracy than the pairwise model. How-
ever, these approaches do not output absolute prob-
abilities, but relative significance between two can-
didates, and therefore cannot be directly integrated
with the ILP-framework. We plan to examine ways
of appropriately estimating an absolute score from a
set of relative scores for further refinement.
Finally, we would like to test our model with
English constructions which closely resemble zero
anaphora. One example were studied in the Semeval
2010 ‘Linking Events and their Participants in Dis-
course’ task, which provides data about null instan-
</bodyText>
<page confidence="0.992312">
811
</page>
<bodyText confidence="0.9997466">
tiation, omitted arguments of predicates like “We
arrived φgoal at 8pm.”. (Unfortunately the dataset
available for SEMEVAL was very small.) Another
interesting area of application of these techniques
would be VP ellipsis.
</bodyText>
<sectionHeader confidence="0.9955" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991223">
Ryu Iida’s stay in Trento was supported by the Ex-
cellent Young Researcher Overseas Visit Program
of the Japan Society for the Promotion of Science
(JSPS). Massimo Poesio was supported in part by
the Provincia di Trento Grande Progetto LiveMem-
ories, which also funded the creation of the Italian
corpus used in this study. We also wish to thank
Francesca Delogu, Kepa Rodriguez, Olga Uryupina
and Yannick Versley for much help with the corpus
and BART.
</bodyText>
<sectionHeader confidence="0.998911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999342">
G. Attardi, F. Dell’Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
Prague.
C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo,
F. Dell’Orletta, A. Lenci, L. Lesmo, G. Attardi,
M. Simi, A. Lavelli, J. Hall, J. Nilsson, and J. Nivre.
2010. Comparing the influence of different treebank
annotations on dependency parsing. In Proceedings of
LREC, pages 1794–1801.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proc. of HLT/NAACL, pages 236–
243.
B. Di Eugenio. 1998. Centering in Italian. In M. A.
Walker, A. K. Joshi, and E. F. Prince, editors, Cen-
tering Theory in Discourse, chapter 7, pages 115–138.
Oxford.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Center-
ing: A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2):203–226.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003.
Incorporating contextual cues in trainable models for
coreference resolution. In Proceedings of the 10th
EACL Workshop on The Computational Treatment of
Anaphora, pages 23–30.
R. Iida, K. Inui, and Y. Matsumoto. 2007a. Zero-
anaphora resolution by learning rich syntactic pattern
features. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 6(4).
R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007b.
Annotating a Japanese text corpus with predicate-
argument and coreference relations. In Proceeding of
the ACL Workshop ‘Linguistic Annotation Workshop’,
pages 132–139.
K. Imamura, K. Saito, and T. Izumi. 2009. Discrimi-
native approach to predicate-argument structure anal-
ysis with zero-anaphora resolution. In Proceedings of
ACL-IJCNLP, Short Papers, pages 85–88.
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun
resolution based on ranking rules and machine learn-
ing. In Proceedings ofEMNLP, pages 184–191.
M. Kameyama. 1985. Zero Anaphora: The case of
Japanese. Ph.D. thesis, Stanford University.
H. Kobdani and H. Sch¨utze. 2010. Sucre: A modular
system for coreference resolution. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 92–95.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proceed-
ings ofEMNLP, pages 301–308.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of the 40th ACL, pages 104–111.
E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro
tool suite. In In Proceedings ofLREC, pages 28–30.
M. Poesio, O. Uryupina, and Y. Versley. 2010. Creating a
coreference resolution system for Italian. In Proceed-
ings ofLREC.
M. Recasens, L. M`arquez, E. Sapena, M. A. Marti,
M. Taul´e, V. Hoste, M. Poesio, and Y. Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1–8.
K-J. Rodriguez, F. Delogu, Y. Versley, E. Stemle, and
M. Poesio. 2010. Anaphoric annotation of wikipedia
and blogs in the live memories corpus. In Proc. LREC.
D. Roth and W.-T. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of CONLL.
R. Sasano, D. Kawahara, and S. Kurohashi. 2009. The
effect of corpus size on case frame acquisition for dis-
course analysis. In Proceedings ofHLT/NAACL, pages
521–529.
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic
method for analyzing Japanese anaphora integrating
zero pronoun detection and resolution. In Proceedings
of the 19th COLING, pages 911–917.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521–
544.
</reference>
<page confidence="0.977961">
812
</page>
<reference confidence="0.999335538461539">
H. Taira, S. Fujita, and M. Nagata. 2008. A Japanese
predicate argument structure analysis using decision
lists. In Proceedings of EMNLP, pages 523–532.
H. Taira, S. Fujita, and M. Nagata. 2010. Predicate ar-
gument structure analysis using transformation based
learning. In Proceedings of the ACL 2010 Conference
Short Papers, pages 162–167.
M. A. Walker, M. Iida, and S. Cote. 1994. Japanese
discourse and the process of centering. Computational
Linguistics, 20(2):193–232.
X. Yang, J. Su, and C. L. Tan. 2008. Twin-candidate
model for learning-based anaphora resolution. Com-
putational Linguistics, 34(3):327–356.
</reference>
<page confidence="0.999189">
813
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.522995">
<title confidence="0.999403">A Cross-Lingual ILP Solution to Zero Anaphora Resolution</title>
<author confidence="0.983552">Ryu</author>
<affiliation confidence="0.997655">Tokyo Institute of</affiliation>
<address confidence="0.957797">Tokyo 152-8552,</address>
<email confidence="0.928471">ryu-i@cl.cs.titech.ac.jp</email>
<author confidence="0.899977">Massimo</author>
<affiliation confidence="0.995526">Universit`a di Center for Mind / Brain University of</affiliation>
<title confidence="0.729327">Language and Computation</title>
<email confidence="0.941101">massimo.poesio@unitn.it</email>
<abstract confidence="0.998454611111111">We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. We show that this new model outperforms several baselines and competing models, as well as a direct translation of the Denis / Baldridge model, for both Italian and Japanese zero anaphora. We incorporate our model in complete anaphoric resolvers for both Italian and Japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classifiers are used for zeros and for explicitly realized anaphors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
<author>F Dell’Orletta</author>
<author>M Simi</author>
<author>A Chanev</author>
<author>M Ciaramita</author>
</authors>
<title>Multilingual dependency parsing and domain adaptation using desr.</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<location>Prague.</location>
<marker>Attardi, Dell’Orletta, Simi, Chanev, Ciaramita, 2007</marker>
<rawString>G. Attardi, F. Dell’Orletta, M. Simi, A. Chanev, and M. Ciaramita. 2007. Multilingual dependency parsing and domain adaptation using desr. In Proc. of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bosco</author>
<author>S Montemagni</author>
<author>A Mazzei</author>
<author>V Lombardo</author>
<author>F Dell’Orletta</author>
<author>A Lenci</author>
<author>L Lesmo</author>
<author>G Attardi</author>
<author>M Simi</author>
<author>A Lavelli</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>J Nivre</author>
</authors>
<title>Comparing the influence of different treebank annotations on dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1794--1801</pages>
<marker>Bosco, Montemagni, Mazzei, Lombardo, Dell’Orletta, Lenci, Lesmo, Attardi, Simi, Lavelli, Hall, Nilsson, Nivre, 2010</marker>
<rawString>C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo, F. Dell’Orletta, A. Lenci, L. Lesmo, G. Attardi, M. Simi, A. Lavelli, J. Hall, J. Nilsson, and J. Nivre. 2010. Comparing the influence of different treebank annotations on dependency parsing. In Proceedings of LREC, pages 1794–1801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>236--243</pages>
<contexts>
<context position="3794" citStr="Denis and Baldridge (2007)" startWordPosition="608" endWordPosition="611">lar entity is its antecedent. It is therefore natural to view zero anaphora resolution as a joint inference 1As shown in Table 1, 64.3% of anaphors in the NAIST Text Corpus of Anaphora are zeros. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 804–813, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 1: min cCO,j) ·x(i,j) + c−C (i,j)∈P o,j~ · (1 − xo,j�) 1: cAj · yj + c−A + j · (1 − yj) (2) j∈M task, for which Integer Linear Programming (ILP)– introduced to NLP by Roth and Yih (2004) and successfully applied by Denis and Baldridge (2007) to the task of jointly inferring anaphoricity and determining the antecedent–would be appropriate. In this work we developed, starting from the ILP system proposed by Denis and Baldridge, an ILP approach to zero anaphora detection and resolution that integrates (revised) versions of Denis and Baldridge’s constraints with additional constraints between the values of three distinct classifiers, one of which is a novel one for subject prediction. We demonstrate that treating zero anaphora resolution as a three-way inference problem is successful for both Italian and Japanese. We integrate the ze</context>
<context position="6634" citStr="Denis and Baldridge (2007)" startWordPosition="1084" endWordPosition="1087"> coreferent with at least one antecedent. 2 Using ILP for joint anaphoricity and 1: x(i,j) ∀j ∈ M (4) coreference determination yj ≤ i∈Mj Integer Linear Programming (ILP) is a method for constraint-based inference aimed at finding the values for a set of variables that maximize a (linear) objective function while satisfying a number of constraints. Roth and Yih (2004) advocated ILP as a general solution for a number of NLP tasks that require combining multiple classifiers and which the traditional pipeline architecture is not appropriate, such as entity disambiguation and relation extraction. Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. Do not resolve non-anaphors: if a mention is nonanaphoric (yj = 0), it should have no antecedents. yj ≥ 1 |Mj |1:x(i,j) ∀j ∈ M (5) i∈Mj 3 An ILP-based account of zero anaphora detection and resolution In the corpora used in our experiments, zero anaphora is annotated using as markable the first verbal form (not necessarily the head) following the position where the argument would have been realized, as in the following example. 805 (6) [Pahor]i e` nato a Trieste, allora porto principale del</context>
<context position="29021" citStr="Denis and Baldridge (2007)" startWordPosition="4753" endWordPosition="4756">shing zero-anaphora from the other types of anaphora (Kobdani and Sch¨utze, 2010; Poesio et al., 2010). On the other hand, our approach shows separating problems contributes to improving performance in Italian zero-anaphora. Although we used gold mentions in our evaluations, mention detection is also essential. As a next step, we also need to take into account ways of incorporating a mention detection model into the ILP formulation. 7 Conclusion In this paper, we developed a new ILP-based model of zero anaphora detection and resolution that extends the coreference resolution model proposed by Denis and Baldridge (2007) by introducing modified constraints and a subject detection model. We evaluated this model both individually and as part of the overall coreference task for both Italian and Japanese zero anaphora, obtaining clear improvements in performance. One avenue for future research is motivated by the observation that whereas introducing the subject detection model and the best-first constraint results in higher precision maintaining the recall compared to the baselines, that precision is still low. One of the major source of the errors is that zero pronouns are frequently used in Italian and Japanese</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In Proc. of HLT/NAACL, pages 236– 243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Di Eugenio</author>
</authors>
<title>Centering in Italian.</title>
<date>1998</date>
<booktitle>Centering Theory in Discourse, chapter 7,</booktitle>
<pages>115--138</pages>
<editor>In M. A. Walker, A. K. Joshi, and E. F. Prince, editors,</editor>
<location>Oxford.</location>
<marker>Di Eugenio, 1998</marker>
<rawString>B. Di Eugenio. 1998. Centering in Italian. In M. A. Walker, A. K. Joshi, and E. F. Prince, editors, Centering Theory in Discourse, chapter 7, pages 115–138. Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="14460" citStr="Grosz et al., 1995" startWordPosition="2377" endWordPosition="2380">n Section 3: the direct reimplementation of the Denis and Baldridge proposal (i.e., using the same constrains), a version replacing Do-Not-Resolve-Not-Anaphors with BestFirst, and a version with Subject Detection as well. As discussed by Iida et al. (2007a) and Imamura et al. (2009), useful features in intra-sentential zeroanaphora are different from ones in inter-sentential zero-anaphora because in the former problem syntactic information between a zero pronoun and its candidate antecedent is essential, while the latter needs to capture the significance of saliency based on Centering Theory (Grosz et al., 1995). To directly reflect this difference, we created two antecedent identification models; one for intrasentential zero-anaphora, induced using the training instances which a zero pronoun and its candidate antecedent appear in the same sentences, the other for 6http://chasen-legacy.sourceforge.jp/ 7http://sourceforge.jp/projects/naist-jdic/ 807 inter-sentential cases, induced from the remaining training instances. To estimate the feature weights of each classifier, we used MEGAM8, an implementation of the Maximum Entropy model, with default parameter settings. The ILP-based models were compared w</context>
<context position="19789" citStr="Grosz et al., 1995" startWordPosition="3230" endWordPosition="3233"> otherwise 0. POSSESSIVE 1 if NP contains the article corresponding to POSSESSIVE such as ‘his’ and ‘their’; otherwise 0. CASE MARKER** case marker followed by NP, such as ‘wa (topic)’, ‘ga (subject)’, ‘o (object)’. DEP LABEL* dependency label of NP. COOC MI** the score of well-formedness model estimated from a large number of triplets ( NP, Case, Predicate). FIRST SENT 1 ifNP appears in the first sentence of a text; otherwise 0. FIRST MENTION 1 ifNP first appears in the set of candidate antecedents; otherwise 0. CL RANK** a rank of NP in forward looking-center list based on Centering Theory (Grosz et al., 1995) CL ORDER** a order of NP in forward looking-center list based on Centering Theory (Grosz et al., 1995) PATH dependency labels (functional words) of words intervening between a ZERO and NP NUM (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to number; otherwise 0. GEN (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to gender; otherwise 0. HEAD MATCH 1 if ANA and NP have the same head lemma; otherwise 0. REGEX MATCH 1 if the string of NP subsumes the string of ANA; otherwise 0. COMP MATCH 1 if ANA and NP have the same string; otherwise 0. NP, ANA and ZERO stand for a candidate antec</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>H Takamura</author>
<author>Y Matsumoto</author>
</authors>
<title>Incorporating contextual cues in trainable models for coreference resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th EACL Workshop on The Computational Treatment of Anaphora,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="30096" citStr="Iida et al. (2003)" startWordPosition="4925" endWordPosition="4928">baselines, that precision is still low. One of the major source of the errors is that zero pronouns are frequently used in Italian and Japanese in contexts in which in English as so-called generic they would be used: “I walked into the hotel and (they) said ..”. In such case, the zero pronoun detection model is often incorrect. We are considering adding a generic they detection component. We also intend to experiment with introducing more sophisticated antecedent identification models in the ILP framework. In this paper, we used a very basic pairwise classifier; however Yang et al. (2008) and Iida et al. (2003) showed that the relative comparison of two candidate antecedents leads to obtaining better accuracy than the pairwise model. However, these approaches do not output absolute probabilities, but relative significance between two candidates, and therefore cannot be directly integrated with the ILP-framework. We plan to examine ways of appropriately estimating an absolute score from a set of relative scores for further refinement. Finally, we would like to test our model with English constructions which closely resemble zero anaphora. One example were studied in the Semeval 2010 ‘Linking Events a</context>
</contexts>
<marker>Iida, Inui, Takamura, Matsumoto, 2003</marker>
<rawString>R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. Incorporating contextual cues in trainable models for coreference resolution. In Proceedings of the 10th EACL Workshop on The Computational Treatment of Anaphora, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Zeroanaphora resolution by learning rich syntactic pattern features.</title>
<date>2007</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>6</volume>
<issue>4</issue>
<contexts>
<context position="2362" citStr="Iida et al., 2007" startWordPosition="358" endWordPosition="361"> anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of</context>
<context position="12508" citStr="Iida et al., 2007" startWordPosition="2092" endWordPosition="2095"> mentions or system mentions preceding a candidate zero pronoun are considered as candidate antecedents. (In contrast, in the experiments on coreference resolution discussed in the following section, all mentions are considered as both candidate anaphors and candidate antecedents. To compare the results with gold mentions and with system detected mentions, we carried out an evaluation using the mentions automatically detected by the Italian version of the BART system (I-BART) (Poesio et al., 2010), which is freely downloadable.3 Japanese For Japanese coreference we used the NAIST Text Corpus (Iida et al., 2007b) version 1.4β, which contains the annotated data about NP coreference and zero-anaphoric relations. We also used the Kyoto University Text Corpus4 that provides dependency relations information for the same articles as the NAIST Text Corpus. In addition, we also used a Japanese named entity tagger, CaboCha5 for automatically tagging named entity labels. In the NAIST Text Corpus mention boundaries are not annotated, only the heads. Thus, we considered 3http://www.bart-coref.org/ 4http://www-lab25.kuee.kyoto-u.ac.jp/nlresource/corpus.html 5http://chasen.org˜taku/software/cabocha/ as pseudo-men</context>
<context position="14096" citStr="Iida et al. (2007" startWordPosition="2322" endWordPosition="2325">ticles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009). Furthermore, in the experiments we only considered subject zero pronouns for a fair comparison to Italian zeroanaphora. 4.2 Models In these first experiments we compared the three ILP-based models discussed in Section 3: the direct reimplementation of the Denis and Baldridge proposal (i.e., using the same constrains), a version replacing Do-Not-Resolve-Not-Anaphors with BestFirst, and a version with Subject Detection as well. As discussed by Iida et al. (2007a) and Imamura et al. (2009), useful features in intra-sentential zeroanaphora are different from ones in inter-sentential zero-anaphora because in the former problem syntactic information between a zero pronoun and its candidate antecedent is essential, while the latter needs to capture the significance of saliency based on Centering Theory (Grosz et al., 1995). To directly reflect this difference, we created two antecedent identification models; one for intrasentential zero-anaphora, induced using the training instances which a zero pronoun and its candidate antecedent appear in the same sen</context>
<context position="16768" citStr="Iida et al. (2007" startWordPosition="2716" endWordPosition="2719">using as items all arcs of dependency relations, each of which is used as a positive instance if its label is subject; otherwise it is used as a negative instance. To train the Japanese subject detection model we used 1,753 articles contained both in the NAIST Text Corpus and the Kyoto University Text Corpus. By merging these two corpora, we can obtain the annotated data including which dependency arc is subject10. To create the training instances, any pair of a predicate and its dependent are extracted, each of 8http://www.cs.utah.edu/˜hal/megam/ 9http://www.di.unito.it/˜tutreeb/ 10Note that Iida et al. (2007b) referred to this relation as ‘nominative’. feature description SUBJ PRE 1 if subject is included in the preceding words of ZERO in a sentence; otherwise 0. TOPIC PRE* 1 iftopic case marker appears in the preceding words of ZERO in a sentence; otherwise 0. NUM PRE 1 if a candidate which agrees with ZERO (GEN PRE) with regards to number (gender) is included in the set of NP; otherwise 0. FIRST SENT 1 if ZERO appears in the first sentence of a text; otherwise 0. FIRST WORD 1 if the predicate which has ZERO is the first word in a sentence; otherwise 0. POS / LEMMA part-of-speech / dependency la</context>
<context position="25523" citStr="Iida et al., 2007" startWordPosition="4181" endWordPosition="4184">4 0.353 0.408 ILP +BF +SUBJ 0.344 0.450 0.390 0.441 0.415 0.427 Table 6: Results for overall coreference: Japanese (MUC score) ferior to the results obtained with our models. In particular, the effect of introducing the separated model with ILP+BF+SUBJ is more significant when using the system detected mentions; it obtained performance more than 13 points better than I-BART when the model referred to the system detected mentions. 6 Related work We are not aware of any previous machine learning model for zero anaphora in Italian, but there has been quite a lot of work on Japanese zeroanaphora (Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Taira et al., 2010; Sasano et al., 2009). In work such as Taira et al. (2008) and Imamura et al. (2009), zero-anaphora resolution is considered as a sub-task of predicate argument structure analysis, taking the NAIST text corpus as a target data set. Taira et al. (2008) and Taira et al. (2010) applied decision lists and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each</context>
</contexts>
<marker>Iida, Inui, Matsumoto, 2007</marker>
<rawString>R. Iida, K. Inui, and Y. Matsumoto. 2007a. Zeroanaphora resolution by learning rich syntactic pattern features. ACM Transactions on Asian Language Information Processing (TALIP), 6(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>M Komachi</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
</authors>
<title>Annotating a Japanese text corpus with predicateargument and coreference relations.</title>
<date>2007</date>
<booktitle>In Proceeding of the ACL Workshop ‘Linguistic Annotation Workshop’,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="2362" citStr="Iida et al., 2007" startWordPosition="358" endWordPosition="361"> anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of</context>
<context position="12508" citStr="Iida et al., 2007" startWordPosition="2092" endWordPosition="2095"> mentions or system mentions preceding a candidate zero pronoun are considered as candidate antecedents. (In contrast, in the experiments on coreference resolution discussed in the following section, all mentions are considered as both candidate anaphors and candidate antecedents. To compare the results with gold mentions and with system detected mentions, we carried out an evaluation using the mentions automatically detected by the Italian version of the BART system (I-BART) (Poesio et al., 2010), which is freely downloadable.3 Japanese For Japanese coreference we used the NAIST Text Corpus (Iida et al., 2007b) version 1.4β, which contains the annotated data about NP coreference and zero-anaphoric relations. We also used the Kyoto University Text Corpus4 that provides dependency relations information for the same articles as the NAIST Text Corpus. In addition, we also used a Japanese named entity tagger, CaboCha5 for automatically tagging named entity labels. In the NAIST Text Corpus mention boundaries are not annotated, only the heads. Thus, we considered 3http://www.bart-coref.org/ 4http://www-lab25.kuee.kyoto-u.ac.jp/nlresource/corpus.html 5http://chasen.org˜taku/software/cabocha/ as pseudo-men</context>
<context position="14096" citStr="Iida et al. (2007" startWordPosition="2322" endWordPosition="2325">ticles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009). Furthermore, in the experiments we only considered subject zero pronouns for a fair comparison to Italian zeroanaphora. 4.2 Models In these first experiments we compared the three ILP-based models discussed in Section 3: the direct reimplementation of the Denis and Baldridge proposal (i.e., using the same constrains), a version replacing Do-Not-Resolve-Not-Anaphors with BestFirst, and a version with Subject Detection as well. As discussed by Iida et al. (2007a) and Imamura et al. (2009), useful features in intra-sentential zeroanaphora are different from ones in inter-sentential zero-anaphora because in the former problem syntactic information between a zero pronoun and its candidate antecedent is essential, while the latter needs to capture the significance of saliency based on Centering Theory (Grosz et al., 1995). To directly reflect this difference, we created two antecedent identification models; one for intrasentential zero-anaphora, induced using the training instances which a zero pronoun and its candidate antecedent appear in the same sen</context>
<context position="16768" citStr="Iida et al. (2007" startWordPosition="2716" endWordPosition="2719">using as items all arcs of dependency relations, each of which is used as a positive instance if its label is subject; otherwise it is used as a negative instance. To train the Japanese subject detection model we used 1,753 articles contained both in the NAIST Text Corpus and the Kyoto University Text Corpus. By merging these two corpora, we can obtain the annotated data including which dependency arc is subject10. To create the training instances, any pair of a predicate and its dependent are extracted, each of 8http://www.cs.utah.edu/˜hal/megam/ 9http://www.di.unito.it/˜tutreeb/ 10Note that Iida et al. (2007b) referred to this relation as ‘nominative’. feature description SUBJ PRE 1 if subject is included in the preceding words of ZERO in a sentence; otherwise 0. TOPIC PRE* 1 iftopic case marker appears in the preceding words of ZERO in a sentence; otherwise 0. NUM PRE 1 if a candidate which agrees with ZERO (GEN PRE) with regards to number (gender) is included in the set of NP; otherwise 0. FIRST SENT 1 if ZERO appears in the first sentence of a text; otherwise 0. FIRST WORD 1 if the predicate which has ZERO is the first word in a sentence; otherwise 0. POS / LEMMA part-of-speech / dependency la</context>
<context position="25523" citStr="Iida et al., 2007" startWordPosition="4181" endWordPosition="4184">4 0.353 0.408 ILP +BF +SUBJ 0.344 0.450 0.390 0.441 0.415 0.427 Table 6: Results for overall coreference: Japanese (MUC score) ferior to the results obtained with our models. In particular, the effect of introducing the separated model with ILP+BF+SUBJ is more significant when using the system detected mentions; it obtained performance more than 13 points better than I-BART when the model referred to the system detected mentions. 6 Related work We are not aware of any previous machine learning model for zero anaphora in Italian, but there has been quite a lot of work on Japanese zeroanaphora (Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Taira et al., 2010; Sasano et al., 2009). In work such as Taira et al. (2008) and Imamura et al. (2009), zero-anaphora resolution is considered as a sub-task of predicate argument structure analysis, taking the NAIST text corpus as a target data set. Taira et al. (2008) and Taira et al. (2010) applied decision lists and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each</context>
</contexts>
<marker>Iida, Komachi, Inui, Matsumoto, 2007</marker>
<rawString>R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007b. Annotating a Japanese text corpus with predicateargument and coreference relations. In Proceeding of the ACL Workshop ‘Linguistic Annotation Workshop’, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Imamura</author>
<author>K Saito</author>
<author>T Izumi</author>
</authors>
<title>Discriminative approach to predicate-argument structure analysis with zero-anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP, Short Papers,</booktitle>
<pages>85--88</pages>
<contexts>
<context position="2405" citStr="Imamura et al., 2009" startWordPosition="366" endWordPosition="369">rred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically un</context>
<context position="13631" citStr="Imamura et al. (2009)" startWordPosition="2249" endWordPosition="2252">.kuee.kyoto-u.ac.jp/nlresource/corpus.html 5http://chasen.org˜taku/software/cabocha/ as pseudo-mentions all bunsetsu chunks (i.e. base phrases in Japanese) whose head part-of-speech was automatically tagged by the Japanese morphological analyser Chasen6 as either ‘noun’ or ‘unknown word’ according to the NAIST-jdic dictionary.7 For evaluation, articles published from January 1st to January 11th and the editorials from January to August were used for training and articles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009). Furthermore, in the experiments we only considered subject zero pronouns for a fair comparison to Italian zeroanaphora. 4.2 Models In these first experiments we compared the three ILP-based models discussed in Section 3: the direct reimplementation of the Denis and Baldridge proposal (i.e., using the same constrains), a version replacing Do-Not-Resolve-Not-Anaphors with BestFirst, and a version with Subject Detection as well. As discussed by Iida et al. (2007a) and Imamura et al. (2009), useful features in intra-sentential zeroanaphora are different from ones in inter-sentential zero-anaphor</context>
<context position="25566" citStr="Imamura et al., 2009" startWordPosition="4189" endWordPosition="4193">0.390 0.441 0.415 0.427 Table 6: Results for overall coreference: Japanese (MUC score) ferior to the results obtained with our models. In particular, the effect of introducing the separated model with ILP+BF+SUBJ is more significant when using the system detected mentions; it obtained performance more than 13 points better than I-BART when the model referred to the system detected mentions. 6 Related work We are not aware of any previous machine learning model for zero anaphora in Italian, but there has been quite a lot of work on Japanese zeroanaphora (Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Taira et al., 2010; Sasano et al., 2009). In work such as Taira et al. (2008) and Imamura et al. (2009), zero-anaphora resolution is considered as a sub-task of predicate argument structure analysis, taking the NAIST text corpus as a target data set. Taira et al. (2008) and Taira et al. (2010) applied decision lists and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each argument. In their approach, a ‘null’ argu</context>
</contexts>
<marker>Imamura, Saito, Izumi, 2009</marker>
<rawString>K. Imamura, K. Saito, and T. Izumi. 2009. Discriminative approach to predicate-argument structure analysis with zero-anaphora resolution. In Proceedings of ACL-IJCNLP, Short Papers, pages 85–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>T Hirao</author>
</authors>
<title>Japanese zero pronoun resolution based on ranking rules and machine learning.</title>
<date>2003</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="2343" citStr="Isozaki and Hirao, 2003" startWordPosition="354" endWordPosition="357">he felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous d</context>
</contexts>
<marker>Isozaki, Hirao, 2003</marker>
<rawString>H. Isozaki and T. Hirao. 2003. Japanese zero pronoun resolution based on ranking rules and machine learning. In Proceedings ofEMNLP, pages 184–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kameyama</author>
</authors>
<title>Zero Anaphora: The case of Japanese.</title>
<date>1985</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1998" citStr="Kameyama, 1985" startWordPosition="302" endWordPosition="303"> (1c) are not explicitly realized. We call these nonrealized mandatory arguments zero anaphors. (1) a. [EN] [John]i went to visit some friends. On the way, [he]i bought some wine. b. [IT] [Giovanni]i and`o a far visita a degli amici. Per via, φi compr`o del vino. c. [JA] [John]i-wa yujin-o houmon-sita. Tochu-de φi wain-o ka-tta. 804 The felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)</context>
</contexts>
<marker>Kameyama, 1985</marker>
<rawString>M. Kameyama. 1985. Zero Anaphora: The case of Japanese. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kobdani</author>
<author>H Sch¨utze</author>
</authors>
<title>Sucre: A modular system for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>92--95</pages>
<marker>Kobdani, Sch¨utze, 2010</marker>
<rawString>H. Kobdani and H. Sch¨utze. 2010. Sucre: A modular system for coreference resolution. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 92–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>A boosting algorithm for classification of semi-structured text.</title>
<date>2004</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>301--308</pages>
<contexts>
<context position="26510" citStr="Kudo and Matsumoto, 2004" startWordPosition="4350" endWordPosition="4353"> and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each argument. In their approach, a ‘null’ argument is explicitly added into the set of candidate argument to learn the situation where an argument of a predicate is ‘exophoric’. They reported their model achieved better performance than the work by Taira et al. (2008). Iida et al. (2007a) also used the NAIST text corpus. They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. Their model drastically outperformed a simple pairwise model, but it is still performed as a cascaded process. Incorporating 810 model Italian system mentions gold mentions combined separated combined separated R P F R P F R P F R P F PAIRWISE 0.508 0.208 0.295 0.472 0.241 0.319 0.582 0.261 0.361 0.566 0.314 0.404 DS-CASCADE 0.225 0.553 0.320 0.217 0.574 0.315 0.245 0.609 0.349 0.246 0.686 0.362 I-BART 0.324 0.294 0.308 – – – 0.532 0.441 0.482 – – – ILP 0.539 0.321 0.403 0.535 0.316 0.397 0.614</context>
</contexts>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for classification of semi-structured text. In Proceedings ofEMNLP, pages 301–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="22420" citStr="Ng and Cardie, 2002" startWordPosition="3670" endWordPosition="3673">ference resolution for all anaphors In a second series of experiments we evaluated the performance of our models together with a full coreference system resolving all anaphors, not just zeros. 5.1 Separating vs combining classifiers Different types of nominal expressions display very different anaphoric behavior: e.g., pronoun resolution involves very different types of information from nominal expression resolution, depending more on syntactic information and on the local context and less on commonsense knowledge. But the most common approach to coreference resolu809 tion (Soon et al., 2001; Ng and Cardie, 2002, etc.) is to use a single classifier to identify antecedents of all anaphoric expressions, relying on the ability of the machine learning algorithm to learn these differences. These models, however, often fail to capture the differences in anaphoric behavior between different types of expressions–one of the reasons being that the amount of training instances is often too small to learn such differences.11 Using different models would appear to be key in the case of zeroanaphora resolution, which differs even more from the rest of anaphora resolution, e.g., in being particularly sensitive to l</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pianta</author>
<author>C Girardi</author>
<author>R Zanoli</author>
</authors>
<title>The TextPro tool suite. In</title>
<date>2008</date>
<booktitle>In Proceedings ofLREC,</booktitle>
<pages>28--30</pages>
<contexts>
<context position="11579" citStr="Pianta et al., 2008" startWordPosition="1942" endWordPosition="1945">263 651,986 18,526 / 29,544 10,206 / 161,124 28,732 / 190,668 test 696 9,287 250,901 7,877 / 11,205 4,396 / 61,652 12,273 / 72,857 In the 6th column we use the term ‘anaphoric’ to indicate the number of zero anaphors that have an antecedent in the text, whereas the total figure is the sum of anaphoric and exophoric zero-anaphors - zeros with a vague / generic reference. Table 1: Italian and Japanese Data Sets coreference are annotated. This dataset consists of articles from Italian Wikipedia, tokenized, POStagged and morphologically analyzed using TextPro, a freely available Italian pipeline (Pianta et al., 2008). We parsed the corpus using the Italian version of the DESR dependency parser (Attardi et al., 2007). In Italian, zero pronouns may only occur as omitted subjects of verbs. Therefore, in the task of zero-anaphora resolution all verbs appearing in a text are considered candidates for zero pronouns, and all gold mentions or system mentions preceding a candidate zero pronoun are considered as candidate antecedents. (In contrast, in the experiments on coreference resolution discussed in the following section, all mentions are considered as both candidate anaphors and candidate antecedents. To com</context>
<context position="18034" citStr="Pianta et al., 2008" startWordPosition="2938" endWordPosition="2941"> has ZERO. D POS / part-of-speech / dependency label / lemma D LEMMA / ofthe dependents of the predicate which has D DEP LABEL ZERO. PATH* dependency labels (functional words) of words intervening between a ZERO and the sentence head The features marked with ‘*’ used only in Japanese. Table 3: Features for anaphoricity determination which is judged as positive if its relation is subject; as negative otherwise. As features for Italian, we used lemmas, PoS tag of a predicate and its dependents as well as their morphological information (i.e. gender and number) automatically computed by TextPro (Pianta et al., 2008). For Japanese, the head lemmas of predicate and dependent chunks as well as the functional words involved with these two chunks were used as features. One case specially treated is when a dependent is placed as an adnominal constituent of a predicate, as in this case relation estimation of dependency arcs is difficult. In such case we instead use the features shown in Table 2 for accurate estimation. 4.5 Results with zero anaphora only In zero anaphora resolution, we need to find all predicates that have anaphoric unrealized subjects (i.e. zero pronouns which have an antecedent in a text), an</context>
</contexts>
<marker>Pianta, Girardi, Zanoli, 2008</marker>
<rawString>E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro tool suite. In In Proceedings ofLREC, pages 28–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>O Uryupina</author>
<author>Y Versley</author>
</authors>
<title>Creating a coreference resolution system for Italian.</title>
<date>2010</date>
<booktitle>In Proceedings ofLREC.</booktitle>
<contexts>
<context position="12393" citStr="Poesio et al., 2010" startWordPosition="2074" endWordPosition="2077">sk of zero-anaphora resolution all verbs appearing in a text are considered candidates for zero pronouns, and all gold mentions or system mentions preceding a candidate zero pronoun are considered as candidate antecedents. (In contrast, in the experiments on coreference resolution discussed in the following section, all mentions are considered as both candidate anaphors and candidate antecedents. To compare the results with gold mentions and with system detected mentions, we carried out an evaluation using the mentions automatically detected by the Italian version of the BART system (I-BART) (Poesio et al., 2010), which is freely downloadable.3 Japanese For Japanese coreference we used the NAIST Text Corpus (Iida et al., 2007b) version 1.4β, which contains the annotated data about NP coreference and zero-anaphoric relations. We also used the Kyoto University Text Corpus4 that provides dependency relations information for the same articles as the NAIST Text Corpus. In addition, we also used a Japanese named entity tagger, CaboCha5 for automatically tagging named entity labels. In the NAIST Text Corpus mention boundaries are not annotated, only the heads. Thus, we considered 3http://www.bart-coref.org/ </context>
<context position="28497" citStr="Poesio et al., 2010" startWordPosition="4669" endWordPosition="4672">rpus consisting of 100 billion words. They also proposed a probabilistic model to Japanese zero-anaphora in which an argument assignment score is estimated based on the automatically acquired case frames. They concluded that case frames acquired from larger corpora lead to better f-score on zeroanaphora resolution. In contrast to these approaches in Japanese, the participants to Semeval 2010 task 1 (especially the Italian coreference task) simply solved the problems using one coreference classifier, not distinguishing zero-anaphora from the other types of anaphora (Kobdani and Sch¨utze, 2010; Poesio et al., 2010). On the other hand, our approach shows separating problems contributes to improving performance in Italian zero-anaphora. Although we used gold mentions in our evaluations, mention detection is also essential. As a next step, we also need to take into account ways of incorporating a mention detection model into the ILP formulation. 7 Conclusion In this paper, we developed a new ILP-based model of zero anaphora detection and resolution that extends the coreference resolution model proposed by Denis and Baldridge (2007) by introducing modified constraints and a subject detection model. We evalu</context>
</contexts>
<marker>Poesio, Uryupina, Versley, 2010</marker>
<rawString>M. Poesio, O. Uryupina, and Y. Versley. 2010. Creating a coreference resolution system for Italian. In Proceedings ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Recasens</author>
<author>L M`arquez</author>
<author>E Sapena</author>
<author>M A Marti</author>
<author>M Taul´e</author>
<author>V Hoste</author>
<author>M Poesio</author>
<author>Y Versley</author>
</authors>
<title>Semeval-2010 task 1: Coreference resolution in multiple languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>1--8</pages>
<marker>Recasens, M`arquez, Sapena, Marti, Taul´e, Hoste, Poesio, Versley, 2010</marker>
<rawString>M. Recasens, L. M`arquez, E. Sapena, M. A. Marti, M. Taul´e, V. Hoste, M. Poesio, and Y. Versley. 2010. Semeval-2010 task 1: Coreference resolution in multiple languages. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-J Rodriguez</author>
<author>F Delogu</author>
<author>Y Versley</author>
<author>E Stemle</author>
<author>M Poesio</author>
</authors>
<title>Anaphoric annotation of wikipedia and blogs in the live memories corpus.</title>
<date>2010</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="2598" citStr="Rodriguez et al. (2010)" startWordPosition="399" endWordPosition="402">ntering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which argument exactly–in this paper, we will only be concerned with subject zeros as these are the only type to occur in Italian) and that a particular entity is its antecedent. </context>
<context position="10269" citStr="Rodriguez et al. (2010)" startWordPosition="1718" endWordPosition="1721"> E M (9) 4 Experiment 1: zero anaphora resolution In a first round of experiments, we evaluated the performance of the model proposed in Section 3 on zero anaphora only (i.e., not attempting to resolve other types of anaphoric expressions). 4.1 Data sets We use the two data sets summarized in Table 1. The table shows that NP anaphora occurs more frequently than zero-anaphora in Italian, whereas in Japanese the frequency of anaphoric zero-anaphors2 is almost double the frequency of the remaining anaphoric expressions. Italian For Italian coreference, we used the annotated data set presented in Rodriguez et al. (2010) and developed for the Semeval 2010 task ‘Coreference Resolution in Multiple Languages’ (Recasens et al., 2010), where both zero-anaphora and NP 2In Japanese, like in Italian, zero anaphors are often used non-anaphorically, to refer to situationally introduced entities, as in I went to John’s office, but they told me that he had left. �min cCO,j) ·x(i,j) + c−C (i,j)∈P o,j~ · (1 − x(i,j�) � cAj · yj + c−A + j · (1 j∈M − yj) � + j∈M 806 #instances (anaphoric/total) language type #docs #sentences #words zero-anaphors others all Italian train 97 3,294 98,304 1,093 / 1,160 6,747 / 27,187 7,840 / 28</context>
</contexts>
<marker>Rodriguez, Delogu, Versley, Stemle, Poesio, 2010</marker>
<rawString>K-J. Rodriguez, F. Delogu, Y. Versley, E. Stemle, and M. Poesio. 2010. Anaphoric annotation of wikipedia and blogs in the live memories corpus. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W-T Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proc. of CONLL.</booktitle>
<contexts>
<context position="3739" citStr="Roth and Yih (2004)" startWordPosition="599" endWordPosition="602">nly type to occur in Italian) and that a particular entity is its antecedent. It is therefore natural to view zero anaphora resolution as a joint inference 1As shown in Table 1, 64.3% of anaphors in the NAIST Text Corpus of Anaphora are zeros. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 804–813, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 1: min cCO,j) ·x(i,j) + c−C (i,j)∈P o,j~ · (1 − xo,j�) 1: cAj · yj + c−A + j · (1 − yj) (2) j∈M task, for which Integer Linear Programming (ILP)– introduced to NLP by Roth and Yih (2004) and successfully applied by Denis and Baldridge (2007) to the task of jointly inferring anaphoricity and determining the antecedent–would be appropriate. In this work we developed, starting from the ILP system proposed by Denis and Baldridge, an ILP approach to zero anaphora detection and resolution that integrates (revised) versions of Denis and Baldridge’s constraints with additional constraints between the values of three distinct classifiers, one of which is a novel one for subject prediction. We demonstrate that treating zero anaphora resolution as a three-way inference problem is succes</context>
<context position="6378" citStr="Roth and Yih (2004)" startWordPosition="1045" endWordPosition="1048"> the following three constraints. Resolve only anaphors: if a pair of mentions (i, j) is coreferent (x�i,j� = 1), then mention j must be anaphoric (yj = 1). x(i,j� ≤ yj ∀(i, j) ∈ P (3) Resolve anaphors: if a mention is anaphoric (yj = 1), it must be coreferent with at least one antecedent. 2 Using ILP for joint anaphoricity and 1: x(i,j) ∀j ∈ M (4) coreference determination yj ≤ i∈Mj Integer Linear Programming (ILP) is a method for constraint-based inference aimed at finding the values for a set of variables that maximize a (linear) objective function while satisfying a number of constraints. Roth and Yih (2004) advocated ILP as a general solution for a number of NLP tasks that require combining multiple classifiers and which the traditional pipeline architecture is not appropriate, such as entity disambiguation and relation extraction. Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. Do not resolve non-anaphors: if a mention is nonanaphoric (yj = 0), it should have no antecedents. yj ≥ 1 |Mj |1:x(i,j) ∀j ∈ M (5) i∈Mj 3 An ILP-based account of zero anaphora detection and resolution In the corpora used in our experiments</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W.-T. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proc. of CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sasano</author>
<author>D Kawahara</author>
<author>S Kurohashi</author>
</authors>
<title>The effect of corpus size on case frame acquisition for discourse analysis.</title>
<date>2009</date>
<booktitle>In Proceedings ofHLT/NAACL,</booktitle>
<pages>521--529</pages>
<contexts>
<context position="2426" citStr="Sasano et al., 2009" startWordPosition="370" endWordPosition="373">iciently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which a</context>
<context position="25608" citStr="Sasano et al., 2009" startWordPosition="4198" endWordPosition="4201">or overall coreference: Japanese (MUC score) ferior to the results obtained with our models. In particular, the effect of introducing the separated model with ILP+BF+SUBJ is more significant when using the system detected mentions; it obtained performance more than 13 points better than I-BART when the model referred to the system detected mentions. 6 Related work We are not aware of any previous machine learning model for zero anaphora in Italian, but there has been quite a lot of work on Japanese zeroanaphora (Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Taira et al., 2010; Sasano et al., 2009). In work such as Taira et al. (2008) and Imamura et al. (2009), zero-anaphora resolution is considered as a sub-task of predicate argument structure analysis, taking the NAIST text corpus as a target data set. Taira et al. (2008) and Taira et al. (2010) applied decision lists and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each argument. In their approach, a ‘null’ argument is explicitly added into the set of c</context>
<context position="27659" citStr="Sasano et al. (2009)" startWordPosition="4542" endWordPosition="4545">– 0.532 0.441 0.482 – – – ILP 0.539 0.321 0.403 0.535 0.316 0.397 0.614 0.369 0.461 0.607 0.384 0.470 ILP +BF 0.471 0.404 0.435 0.483 0.409 0.443 0.545 0.517 0.530 0.563 0.519 0.540 ILP +SUBJ 0.537 0.325 0.405 0.534 0.318 0.399 0.611 0.372 0.463 0.606 0.387 0.473 ILP +BF +SUBJ 0.464 0.410 0.435 0.478 0.418 0.446 0.538 0.527 0.533 0.559 0.536 0.547 R: Recall, P: Precision, F: f-score, BF: best first constraint, SUBJ: subject detection model. Table 5: Results for overall coreference: Italian (MUC score) their model into the ILP formulation proposed here looks like a promising further extension. Sasano et al. (2009) obtained interesting experimental results about the relationship between zeroanaphora resolution and the scale of automatically acquired case frames. In their work, their case frames were acquired from a very large corpus consisting of 100 billion words. They also proposed a probabilistic model to Japanese zero-anaphora in which an argument assignment score is estimated based on the automatically acquired case frames. They concluded that case frames acquired from larger corpora lead to better f-score on zeroanaphora resolution. In contrast to these approaches in Japanese, the participants to </context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2009</marker>
<rawString>R. Sasano, D. Kawahara, and S. Kurohashi. 2009. The effect of corpus size on case frame acquisition for discourse analysis. In Proceedings ofHLT/NAACL, pages 521–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Seki</author>
<author>A Fujii</author>
<author>T Ishikawa</author>
</authors>
<title>A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th COLING,</booktitle>
<pages>911--917</pages>
<contexts>
<context position="2318" citStr="Seki et al., 2002" startWordPosition="350" endWordPosition="353">ain-o ka-tta. 804 The felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora re</context>
</contexts>
<marker>Seki, Fujii, Ishikawa, 2002</marker>
<rawString>K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic method for analyzing Japanese anaphora integrating zero pronoun detection and resolution. In Proceedings of the 19th COLING, pages 911–917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>544</pages>
<contexts>
<context position="15135" citStr="Soon et al. (2001)" startWordPosition="2469" endWordPosition="2472">ecedent identification models; one for intrasentential zero-anaphora, induced using the training instances which a zero pronoun and its candidate antecedent appear in the same sentences, the other for 6http://chasen-legacy.sourceforge.jp/ 7http://sourceforge.jp/projects/naist-jdic/ 807 inter-sentential cases, induced from the remaining training instances. To estimate the feature weights of each classifier, we used MEGAM8, an implementation of the Maximum Entropy model, with default parameter settings. The ILP-based models were compared with the following baselines. PAIRWISE: as in the work by Soon et al. (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. DS-CASCADE: the model first filters out nonanaphoric candidate anaphors using an anaphoricity determination model, then selects an antecedent from a set of candidate antecedents of anaphoric candidate anaphors using an antecedent identification model. 4.3 Features The feature sets for antecedent identification and anaphoricity determination are briefly summarized in Table 2 and Table 3, respectively. The agreement features such as NUM AGREE and GEN AGREE are automatically derived usin</context>
<context position="22399" citStr="Soon et al., 2001" startWordPosition="3666" endWordPosition="3669"> Experiment 2: coreference resolution for all anaphors In a second series of experiments we evaluated the performance of our models together with a full coreference system resolving all anaphors, not just zeros. 5.1 Separating vs combining classifiers Different types of nominal expressions display very different anaphoric behavior: e.g., pronoun resolution involves very different types of information from nominal expression resolution, depending more on syntactic information and on the local context and less on commonsense knowledge. But the most common approach to coreference resolu809 tion (Soon et al., 2001; Ng and Cardie, 2002, etc.) is to use a single classifier to identify antecedents of all anaphoric expressions, relying on the ability of the machine learning algorithm to learn these differences. These models, however, often fail to capture the differences in anaphoric behavior between different types of expressions–one of the reasons being that the amount of training instances is often too small to learn such differences.11 Using different models would appear to be key in the case of zeroanaphora resolution, which differs even more from the rest of anaphora resolution, e.g., in being partic</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521– 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Taira</author>
<author>S Fujita</author>
<author>M Nagata</author>
</authors>
<title>A Japanese predicate argument structure analysis using decision lists.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>523--532</pages>
<contexts>
<context position="2383" citStr="Taira et al., 2008" startWordPosition="362" endWordPosition="365"> depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a v</context>
<context position="13605" citStr="Taira et al. (2008)" startWordPosition="2244" endWordPosition="2247">f.org/ 4http://www-lab25.kuee.kyoto-u.ac.jp/nlresource/corpus.html 5http://chasen.org˜taku/software/cabocha/ as pseudo-mentions all bunsetsu chunks (i.e. base phrases in Japanese) whose head part-of-speech was automatically tagged by the Japanese morphological analyser Chasen6 as either ‘noun’ or ‘unknown word’ according to the NAIST-jdic dictionary.7 For evaluation, articles published from January 1st to January 11th and the editorials from January to August were used for training and articles dated January 14th to 17th and editorials dated October to December are used for testing as done by Taira et al. (2008) and Imamura et al. (2009). Furthermore, in the experiments we only considered subject zero pronouns for a fair comparison to Italian zeroanaphora. 4.2 Models In these first experiments we compared the three ILP-based models discussed in Section 3: the direct reimplementation of the Denis and Baldridge proposal (i.e., using the same constrains), a version replacing Do-Not-Resolve-Not-Anaphors with BestFirst, and a version with Subject Detection as well. As discussed by Iida et al. (2007a) and Imamura et al. (2009), useful features in intra-sentential zeroanaphora are different from ones in int</context>
<context position="25544" citStr="Taira et al., 2008" startWordPosition="4185" endWordPosition="4188">F +SUBJ 0.344 0.450 0.390 0.441 0.415 0.427 Table 6: Results for overall coreference: Japanese (MUC score) ferior to the results obtained with our models. In particular, the effect of introducing the separated model with ILP+BF+SUBJ is more significant when using the system detected mentions; it obtained performance more than 13 points better than I-BART when the model referred to the system detected mentions. 6 Related work We are not aware of any previous machine learning model for zero anaphora in Italian, but there has been quite a lot of work on Japanese zeroanaphora (Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Taira et al., 2010; Sasano et al., 2009). In work such as Taira et al. (2008) and Imamura et al. (2009), zero-anaphora resolution is considered as a sub-task of predicate argument structure analysis, taking the NAIST text corpus as a target data set. Taira et al. (2008) and Taira et al. (2010) applied decision lists and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each argument. In their a</context>
</contexts>
<marker>Taira, Fujita, Nagata, 2008</marker>
<rawString>H. Taira, S. Fujita, and M. Nagata. 2008. A Japanese predicate argument structure analysis using decision lists. In Proceedings of EMNLP, pages 523–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Taira</author>
<author>S Fujita</author>
<author>M Nagata</author>
</authors>
<title>Predicate argument structure analysis using transformation based learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>162--167</pages>
<contexts>
<context position="2447" citStr="Taira et al., 2010" startWordPosition="374" endWordPosition="377">ce this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in competitions such as SEMEVAL 2010 Task 1 on Multilingual Coreference (Recasens et al., 2010), is leading to a renewed interest in zero anaphora resolution, particularly at the light of the mediocre results obtained on zero anaphors by most systems participating in SEMEVAL. Resolving zero anaphora requires the simultaneous decision that one of the arguments of a verb is phonetically unrealized (and which argument exactly–in th</context>
<context position="25586" citStr="Taira et al., 2010" startWordPosition="4194" endWordPosition="4197">7 Table 6: Results for overall coreference: Japanese (MUC score) ferior to the results obtained with our models. In particular, the effect of introducing the separated model with ILP+BF+SUBJ is more significant when using the system detected mentions; it obtained performance more than 13 points better than I-BART when the model referred to the system detected mentions. 6 Related work We are not aware of any previous machine learning model for zero anaphora in Italian, but there has been quite a lot of work on Japanese zeroanaphora (Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Taira et al., 2010; Sasano et al., 2009). In work such as Taira et al. (2008) and Imamura et al. (2009), zero-anaphora resolution is considered as a sub-task of predicate argument structure analysis, taking the NAIST text corpus as a target data set. Taira et al. (2008) and Taira et al. (2010) applied decision lists and transformation-based learning respectively in order to manually analyze which clues are important for each argument assignment. Imamura et al. (2009) also tackled to the same problem setting by applying a pairwise classifier for each argument. In their approach, a ‘null’ argument is explicitly a</context>
</contexts>
<marker>Taira, Fujita, Nagata, 2010</marker>
<rawString>H. Taira, S. Fujita, and M. Nagata. 2010. Predicate argument structure analysis using transformation based learning. In Proceedings of the ACL 2010 Conference Short Papers, pages 162–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>M Iida</author>
<author>S Cote</author>
</authors>
<title>Japanese discourse and the process of centering.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="2019" citStr="Walker et al., 1994" startWordPosition="304" endWordPosition="307">plicitly realized. We call these nonrealized mandatory arguments zero anaphors. (1) a. [EN] [John]i went to visit some friends. On the way, [he]i bought some wine. b. [IT] [Giovanni]i and`o a far visita a degli amici. Per via, φi compr`o del vino. c. [JA] [John]i-wa yujin-o houmon-sita. Tochu-de φi wain-o ka-tta. 804 The felicitousness of zero anaphoric reference depends on the referred entity being sufficiently salient, hence this type of data–particularly in Japanese and Italian–played a key role in early work in coreference resolution, e.g., in the development of Centering (Kameyama, 1985; Walker et al., 1994; Di Eugenio, 1998). This research highlighted both commonalities and differences between the phenomenon in such languages. Zero anaphora resolution has remained a very active area of study for researchers working on Japanese, because of the prevalence of zeros in such languages1 (Seki et al., 2002; Isozaki and Hirao, 2003; Iida et al., 2007a; Taira et al., 2008; Imamura et al., 2009; Sasano et al., 2009; Taira et al., 2010). But now the availability of corpora annotated to study anaphora, including zero anaphora, in languages such as Italian (e.g., Rodriguez et al. (2010)), and their use in c</context>
</contexts>
<marker>Walker, Iida, Cote, 1994</marker>
<rawString>M. A. Walker, M. Iida, and S. Cote. 1994. Japanese discourse and the process of centering. Computational Linguistics, 20(2):193–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
<author>J Su</author>
<author>C L Tan</author>
</authors>
<title>Twin-candidate model for learning-based anaphora resolution.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="30073" citStr="Yang et al. (2008)" startWordPosition="4920" endWordPosition="4923">recall compared to the baselines, that precision is still low. One of the major source of the errors is that zero pronouns are frequently used in Italian and Japanese in contexts in which in English as so-called generic they would be used: “I walked into the hotel and (they) said ..”. In such case, the zero pronoun detection model is often incorrect. We are considering adding a generic they detection component. We also intend to experiment with introducing more sophisticated antecedent identification models in the ILP framework. In this paper, we used a very basic pairwise classifier; however Yang et al. (2008) and Iida et al. (2003) showed that the relative comparison of two candidate antecedents leads to obtaining better accuracy than the pairwise model. However, these approaches do not output absolute probabilities, but relative significance between two candidates, and therefore cannot be directly integrated with the ILP-framework. We plan to examine ways of appropriately estimating an absolute score from a set of relative scores for further refinement. Finally, we would like to test our model with English constructions which closely resemble zero anaphora. One example were studied in the Semeval</context>
</contexts>
<marker>Yang, Su, Tan, 2008</marker>
<rawString>X. Yang, J. Su, and C. L. Tan. 2008. Twin-candidate model for learning-based anaphora resolution. Computational Linguistics, 34(3):327–356.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>