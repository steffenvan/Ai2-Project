<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.925693">
Chinese Comma Disambiguation for Discourse Analysis
</title>
<author confidence="0.98876">
Yaqin Yang
</author>
<affiliation confidence="0.975806">
Brandeis University
</affiliation>
<address confidence="0.9064405">
415 South Street
Waltham, MA 02453, USA
</address>
<email confidence="0.999392">
yaqin@brandeis.edu
</email>
<author confidence="0.916044">
Nianwen Xue
</author>
<affiliation confidence="0.909924">
Brandeis University
</affiliation>
<address confidence="0.888801">
415 South Street
Waltham, MA 02453, USA
</address>
<email confidence="0.999497">
xuen@brandeis.edu
</email>
<sectionHeader confidence="0.996669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999592882352941">
The Chinese comma signals the boundary of
discourse units and also anchors discourse
relations between adjacent text spans. In
this work, we propose a discourse structure-
oriented classification of the comma that can
be automatically extracted from the Chinese
Treebank based on syntactic patterns. We
then experimented with two supervised learn-
ing methods that automatically disambiguate
the Chinese comma based on this classifica-
tion. The first method integrates comma clas-
sification into parsing, and the second method
adopts a “post-processing” approach that ex-
tracts features from automatic parses to train
a classifier. The experimental results show
that the second approach compares favorably
against the first approach.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945632653061">
The Chinese comma, which looks graphically very
similar to its English counterpart, is functionally
quite different. It has attracted a significant amount
of research that studied the problem from the view-
point of natural language processing. For exam-
ple, Jin et al ( 2004) and Li et al ( 2005) view
the disambiguation of the Chinese comma as a way
of breaking up long Chinese sentences into shorter
ones to facilitate parsing. The idea is to split a
long sentence into multiple comma-separated seg-
ments, parse them individually, and reconstruct the
syntactic parse for the original sentence. Although
both studies show a positive impact of this approach,
comma disambiguation is viewed merely as a con-
venient tool to help achieve a more important goal.
Xue and Yang ( 2011) point out that the very rea-
son for the existence of these long Chinese sentences
is because the Chinese comma is ambiguous and in
some context, it identifies the boundary of a sentence
just as a period, a question mark, or an exclamation
mark does. The disambiguation of comma is viewed
as a necessary step to detect sentence boundaries in
Chinese and it can benefit a whole range of down-
stream NLP applications such as syntactic parsing
and Machine Translation. In Machine Translation,
for example, it is very typical for “one” Chinese
sentence to be translated into multiple English sen-
tences, with each comma-separated segment corre-
sponding to one English sentence. In the present
work, we expand this view and propose to look at
the Chinese comma in the context of discourse anal-
ysis. The Chinese comma is viewed as a delimiter
of elementary discourse units (EDUs), in the sense
of the Rhetorical Structure Theory (Carlson et al.,
2002; Mann et al., 1988). It is also considered to
be the anchor of discourse relations, in the sense of
the Penn Discourse Treebank (PDT) (Prasad et al.,
2008). Disambiguating the comma is thus necessary
for the purpose of discourse segmentation, the iden-
tification of EDUs, a first step in building up the dis-
course structure of a Chinese text.
Developing a supervised or semi-supervised
model of discourse segmentation would require
ground truth annotated based on a well-established
representation scheme, but as of right now no such
annotation exists for Chinese to the best of our
knowledge. However, syntactically annotated tree-
banks often contain important clues that can be used
to infer discourse-level information. We present
</bodyText>
<page confidence="0.969653">
786
</page>
<note confidence="0.985862">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 786–794,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99931696875">
a method of automatically deriving a preliminary
form of discourse structure anchored by the Chinese
comma from the Penn Chinese Treebank (CTB)
(Xue et al., 2005), and using this information to
train and test supervised models. This discourse
information is formalized as a classification of the
Chinese comma, with each class representing the
boundary of an elementary discourse unit as well
as the anchor of a coarse-grained discourse rela-
tion between the two discourse units that it delimits.
We then develop two comma classification methods.
In the first method, we replace the part-of-speech
(POS) tag of each comma in the CTB with a de-
rived discourse category and retrain a state-of-the-
art Chinese parser on the relabeled data. We then
evaluate how accurately the commas are classified
in the parsing process. In the second method, we
parse these sentences and extract lexical and syn-
tactic information as features to predict these new
discourse categories. The second approach gives us
more control over what features to extract and our
results show that it compares favorably against the
first approach.
The rest of the paper is organized as follows. In
Section 2, we present our approach to automati-
cally extract discourse information from a syntac-
tically annotated treebank and present our classifi-
cation scheme. In Section 3, we describe our su-
pervised learning methods and the features we ex-
tracted. Section 4 presents our experiment setup and
experimental results. Related work is reviewed in
Section 5. We conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.815837" genericHeader="method">
2 Chinese comma classification
</sectionHeader>
<bodyText confidence="0.999983884615385">
There are many ways to conceptualize the discourse
structure of a text (Mann et al., 1988; Prasad et
al., 2008), but there is more of a consensus among
researchers about the fundamental building blocks
of the discourse structure. For the Rhetorical Dis-
course Theory, the building blocks are Elementary
Discourse Units (EDUs). For the PDT, the build-
ing blocks are abstract objects such as propositions,
facts. Although they are phrased in different ways,
syntactically these discourse units are generally re-
alized as clauses or built on top of clauses. So the
first step in building the discourse structure of a text
is to identify these discourse units.
In Chinese, these elementary discourse units are
generally delimited by the comma, but not all com-
mas mark the boundaries of a discourse unit. In (1),
for example, Comma [1] marks the boundary of a
discourse unit while Comma [2] does not. This is
reflected in its English translation: while the first
comma corresponds to an English comma, the sec-
ond comma is not translated at all, as it marks the
boundary between a subject and its predicate, where
no comma is needed in English. Disambiguating
these two types of commas is thus an important first
step in identifying elementary discourse units and
building up the discourse structure of a text.
</bodyText>
<equation confidence="0.8818775">
Ra:g
challenger
� Ell
DE impression .
</equation>
<bodyText confidence="0.998649947368421">
“Although Wang Xiang is over 50 years old, his
abundant energy and quick thinking leave peo-
ple the impression of a challenger.”
Although to the best of our knowledge, no such
discourse segmented data for Chinese exists in the
public domain, this information can be extracted
from the syntactic annotation of the CTB. In the
syntactic annotation of the sentence, illustrated in
(a), it is clear that while the first comma in the sen-
tence marks the boundary of a clause, the second
one marks the demarcation between the subject NP
and the predicate VP and thus is not an indicator of
a discourse boundary.
In addition to a binary distinction of whether a
comma marks the boundary of a discourse unit,
the CTB annotation also allows the extraction of a
more elaborate classification of commas based on
coordination and subordination relations of comma-
separated clauses. This classification of the Chinese
</bodyText>
<figure confidence="0.999018256410257">
*-ff
50
�
age
f
over
TR
but
,[1]
,
9
although
(1) T-)%
Wang Xiang
�
DE
� YEn
his abundant
)r&amp;quot;j7
energy
�
DE
R
and
W
quick
IP
IP-CND , 1
ADVP NP , 2 VP
(a)
RM
thinking ,
[2] t�
.a
give
A
people
� n
one CL
</figure>
<page confidence="0.99041">
787
</page>
<bodyText confidence="0.998779142857143">
comma can be viewed as a first approximation of the
discourse relations anchored by the comma that can
be refined later via a manual annotation process.
Based on the syntactic annotation in the CTB, we
classify the Chinese comma into seven hierarchi-
cally organized categories, as illustrated in Figure
1. The first distinction is made between commas
that indicate a discourse boundary (RELATION)
and those that do not (OTHER). Commas that in-
dicate discourse boundaries are further divided into
commas that separate coordinated discourse units
(COORD) vs commas that separate discourse units
in a subordination relation (SUBORD). Based on
the levels of embedding and the syntactic category
of the coordinated structures, we define three dif-
ferent types of coordination (SB, IP COORD and
VP COORD). We also define three types of subordi-
nation relations (ADJ, COMP, Sent SBJ), based on
the syntactic structure. As we will show below, each
of the six relations has a clear syntactic pattern that
can be exploited for their automatic detection.
</bodyText>
<equation confidence="0.941341428571429">
ALL
f4* A4 ,[3] ��
science foundation , every year
�� � —17- t
investment at one hundred millioin yuan
t� �_
above
</equation>
<bodyText confidence="0.983251">
“Natural Science Foundation is established in
Guangdong Province. More than one hundred
million yuan is invested every year.”
</bodyText>
<figure confidence="0.918687333333333">
(b) IP-Root
IP , IP
Clause Clause
</figure>
<bodyText confidence="0.942829875">
IP Coordination (IP COORD): Coordinated IPs
that are not the immediate children of the root IP are
also considered to be discourse units and the com-
mas linking them are labeled IP COORD. Different
from the sentence boundary cases, these coordinated
IPs are often embedded in a larger structure. An ex-
ample is given in (3) and its typical syntactic pattern
is illustrated in (c).
</bodyText>
<figure confidence="0.997482464285714">
.
FWÕ
Lu Renfa
411_9E
presentation
(3) �9
According to
,[4]
,
[5]
,
��
overall
ITC
complete
M
exceeding quota
SB COORD_IP COORD_VP ADJ COMP Sent_SBJ
!�Q
the whole country
COORD SUBORD
OTHER RELATION
��
revenue
E
already
11E*
goal
</figure>
<figureCaption confidence="0.993545">
Figure 1: Comma classification �R %Pz AT
situation fairly good.
</figureCaption>
<bodyText confidence="0.999926461538462">
Sentence Boundary (SB): Following (Xue and
Yang, 2011), we consider the loosely coordinated
IPs that are the immediate children of the root IP to
be independent sentences, and the commas separat-
ing them to be delimiters of sentence boundary. This
is illustrated in (2), where a Chinese sentence can be
split into two independent shorter sentences at the
comma. We view this comma to be a marker of the
sentence boundary and it serves the same function as
the unambiguous sentence boundary delimitors (pe-
riods, question marks, exclamation marks) in Chi-
nese. The syntactic pattern that is used to infer this
relation is illustrated in (b).
</bodyText>
<listItem confidence="0.627983">
(2) I_„&apos;&amp; AA
</listItem>
<bodyText confidence="0.99380725">
Guangdong province establish
“According to Lu Renfa, the national revenue
goal is met and exceeded, and the overall situa-
tion is fairly good.”
</bodyText>
<equation confidence="0.515597">
(c) IP
PP
Modifier
</equation>
<bodyText confidence="0.98398975">
VP Coordination (VP COORD): Coordinated
VPs, when separated by the comma, are not seman-
tically different from coordinated IPs. The only dif-
ference is that in the latter case, the coordinated VPs
</bodyText>
<figure confidence="0.70002">
Pr
natural
T
ASP
IP
IP , IP
Conjunct Conjunct
</figure>
<page confidence="0.98328">
788
</page>
<bodyText confidence="0.999921777777778">
share a subject, while coordinated IPs tend to have
different subjects. Maintaining this distinction allow
us to model subject (dis)continuity, which helps re-
cover a subject when it is dropped, a prevalent phe-
nomenon in Chinese. As shown in (4), the VPs in the
text spans separated by Comma [6] have the same
subject, thus the subject in the second VP is dropped.
The syntactic pattern that allows us to extract this
structure is given in (d).
</bodyText>
<figure confidence="0.993289461538462">
14 V X�_! VT
will according to provision excecute
%G IY
compensation .
“If natural disasters within the scope of the in-
surance liability happen in the project, PICC
Property Insurance Company will provide
compensations according to the provisions.”
(4) rPQ
China
VQ)�
four major
Q4
state-owned
A
is
WIT
Bank
WIT
bank
.
(e) IP
,[6]
,
It
also
A
is
O&amp;quot;
commercial
rPQ
China
WIT ±_
bank one of these
gME
foreign exchange
CP/IP-CND
J-1-•
major
M
DE
Main Clause
Subordinate Clause
“Bank of China is one of the four major state-
owned commercial banks, and it is also China’s
major foreign exchange bank.”
(d) IP
NP VP
Subject
VP
, VP
Conjunct Conjunct
</figure>
<bodyText confidence="0.9584235">
Adjunction (ADJ): Adjunction is one of three
types of subordination relations we define. It holds
between a subordinate clause and its main clause.
The subordinate clause is normally introduced by a
subordinating conjunction and it typically provides
the cause, purpose, manner, or condition for the
main clause. In the PDT terms, these subordinate
conjunctions are discourse connectives that anchor
a discourse relation between the subordinate clause
and the main clause. In Chinese, with few excep-
tions, the subordinate clause comes before the main
clause. (5) is an example of this relation.
</bodyText>
<figure confidence="0.990326111111111">
(5) � ZfM &amp;t &apos;r „ RHE MEN
if project happen insurance liability scope
rPa
China Insurance
1f1 &apos;
property
a„
insurance
�JP
company
A
inside
PMN
natural
k!gf ,[7]
disaster ,
M
DE
</figure>
<bodyText confidence="0.9956285">
(e) shows how (5) is represented in the syntac-
tic structure in the CTB. Extracting this relation re-
quires more than just the syntactic configuration be-
tween these two clauses. We also take advantage
of the functional (dash) tags provided in the tree-
bank. The functional tags are attached to the sub-
ordinate clause and they include CND (conditional),
PRP (purpose or reason), MNR (manner), or ADV
(other types of subordinate clauses that are adjuncts
to the main clause).
Complementation (COMP): When a comma
separates a verb governor and its complement
clause, this verb and its subject generally describe
the attribution of the complement clause. Attribu-
tion is an important notion in discourse analysis in
both the RST framework and in the PDT. An exam-
ple of this is given in (6), and the syntactic pattern
used to extract this relation is illustrated in (f).
</bodyText>
<equation confidence="0.5985714">
tL:�FT-1
ninety million
4_:�I
annual output
1z gQ7G
</equation>
<bodyText confidence="0.835354666666667">
three hundred million U.S. dollars .
“According to the the company’s presentation,
they will invest an additional ninety million
</bodyText>
<figure confidence="0.9995061">
A
will
OH
they
��
invest
A
within
T,� 4-:
five year
MAN
additionally
(6) Ak
The
� JP
company
^9E
present
*
future
M
DE
,[8]
,
�
at
gQ7G ,[9] Rit
U.S. dollars , estimate
� �
will reach
</figure>
<page confidence="0.986668">
789
</page>
<bodyText confidence="0.9684325">
U.S. dollars in the next five years, and the esti-
mated annual output will reach $ 300 million.”
</bodyText>
<equation confidence="0.612791">
(f) IP
VV , IP
</equation>
<bodyText confidence="0.9826604">
Sentential Subject (SBJ): This category is for
commas that separate a sentential subject from its
predicate VP. An example is given in (7) and the
syntactic pattern used to extract this relation is il-
lustrated in (g).
</bodyText>
<table confidence="0.49146">
(7) iiiQ ,rAA *-r-I ,[10] )AYIJ M
export rapid grow , become promote
t�VF *-r-I M IR• J3 .
economy growth DE important force
</table>
<bodyText confidence="0.93078119047619">
“The rapid growth of export becomes an impor-
tant force in promoting economic growth.”
(g) IP
Others (OTHER): The remaining cases of
comma receive the OTHER label, indicating they do
not mark the boundary of a discourse segment.
Our proposed comma classification scheme
serves the dual purpose of identifying elementary
discourse units and at the same time detecting
coarse-grained discourse relations anchored by the
comma. The discourse relations identified in this
manner by no means constitute the full discourse
analysis of a text, they are, however, a good first
approximation. The advantage of our approach is
that we do not require manual discourse annotations,
and all the information we need is automatically ex-
tracted from the syntactic annotation of the CTB
and attached to instances of the comma in the cor-
pus. This makes it possible for us to train supervised
models to automatically classify the commas in any
Chinese text.
</bodyText>
<sectionHeader confidence="0.912728" genericHeader="method">
3 Two comma classification methods
</sectionHeader>
<bodyText confidence="0.97173025">
Given the gold standard parses, based on the syntac-
tic patterns described in Section 2, we can map the
POS tag of each comma instance in the CTB to one
of the seven classes described in Section 2. Using
this relabeled data as training data, we experimented
with two automatic comma disambiguation meth-
ods. In the first method, we simply retrained the
Berkeley parser (Petrov and Klein, 2007) on the re-
labeled data and computed how accurately the com-
mas are labeled in a held-out test set. In the second
method, we trained a Maximum Entropy classifier
with the Mallet (McCallum et al., 2002) machine
learning package to classify the commas. The fea-
tures are extracted from the CTB data automatically
parsed with the Berkeley parser. We implemented
features described in (Xue and Yang, 2011), and
also experimented with a set of new features as fol-
lows. In general, these new features are extracted
from the two text spans surrounding the comma.
Given a comma, we define the preceding text span as
i span and the following text span as j span. We also
collected a number of subject-predicate pairs from a
large corpus that doesn’t overlap with the CTB. We
refer to this corpus as the auxiliary corpus.
Subject and Predicate features: We explored
various combinations of the subject (sbj), predicate
(pred) and object (obj) of the two spans. The sub-
ject of i span is represented as sbji, etc.
</bodyText>
<listItem confidence="0.936483153846154">
1. The existence of sbji, sbjj, both, or neither.
2. The lemma of predi, the lemma of predj, the
conjunction of sbji and predj, the conjunction
of predi and sbjj
3. whether the conjunction of sbji and predj oc-
curs more than 2 times in the auxiliary corpus
when j does not have a subject.
4. whether the conjunction of obji and predj oc-
curs more than 2 times in the auxiliary corpus
when j does not have a subject
5. Whether the conjunction of predi and sbjj oc-
curs more than 2 times in the auxiliary corpus
when i does not have a subject.
</listItem>
<bodyText confidence="0.96536725">
Mutual Information features: Mutual informa-
tion is intended to capture the association strength
between the subject of a previous span and the predi-
cate of the current span. We use Mutual Information
</bodyText>
<figure confidence="0.9776086">
....
VP
IP-SBJ
VP
Sentential Subject
</figure>
<page confidence="0.98158">
790
</page>
<bodyText confidence="0.839983666666667">
(Church and Hanks, 1989) as shown in Equation
(1) and the frequency count computed based on the
auxiliary corpus to measure such constraints.
</bodyText>
<note confidence="0.477172">
# co-occur of S and P * corpus size
</note>
<equation confidence="0.975585">
MI = log2 (1)
# S occur * # P occur
</equation>
<listItem confidence="0.995205083333333">
1. The conjunction of sbji and predj when j does
not have a subject if their MIvalue is greater
than -8.0, an empirically established threshold.
2. Whether obji and predj has an MI value
greater than 5.0 if j does not have a subject.
3. Whether the MI value of sbji and predj is
greater than 0.0, and they occur 2 times in the
auxiliary corpus when j doesn’t have a subject.
4. Whether the MI value of obji and predj is
greater than 0.0 and they occur 2 times in the
auxiliary corpus when j doesn’t have a subject.
5. Whether the MI value of predi and sbjj is
</listItem>
<bodyText confidence="0.658781333333333">
greater than 0.0 and they occur more than 2
times in the auxiliary corpus when i does not
have a subject.
Span features: We used span features to cap-
ture syntactic information, e.g. the comma separated
spans are constituents in Tree (b) but not in Tree (d).
</bodyText>
<listItem confidence="0.991190461538462">
1. Whether i forms a single constituent, whether
j forms a single constituent.
2. The conjunction and hierarchical relation of all
constituent labels in i/j, if i/j does not form
a single constituent. The conjunction of all
constituent labels in both spans, if neither span
form a single constituent.
Lexical features:
1. The first word in i if it is an adverb, the first
word in j if it is an adverb.
2. The first word in i span if it is a coordinating
conjunction, the first word in j if it is a coordi-
nating conjunction.
</listItem>
<sectionHeader confidence="0.996262" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.945653">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999961916666667">
We use the CTB 6.0 in our experiments and divide
it into training, development and test sets using the
data split recommended in the CTB 6.0 documenta-
tion, as shown in Table 1. There are 5436 commas
in the test set, including 1327 commas that are sen-
tence boundaries (SB), 539 commas that connect co-
ordinated IPs (IP COORD), 1173 commas that join
coordinated VPs (VP COORD), 379 commas that
delimits a subordinate clause and its main clause
(ADJ), 314 commas that anchor complementation
relations (COMP), and 1625 commas that belong to
the OTHER category.
</bodyText>
<subsectionHeader confidence="0.693435">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.99999625">
As mentioned in Section 3, we experimented with
two comma classification methods. In the first
method, we replace the part-of-speech (POS) tags of
the commas with the seven classes defined in Sec-
tion 2. We then retrain the Berkeley parser (Petrov
and Klein, 2007) using the training set as presented
in Table 1, parse the test set, and evaluate the comma
classification accuracy.
In the second method, we use the relabeled com-
mas as the gold-standard data to train a supervised
classifier to automatically classify the commas. As
shown in the previous section, syntactic structures
are an important source of information for our clas-
sifier. For feature extraction purposes, the entire
CTB6.0 is automatically parsed in a round-robin
fashion. We divided CTB 6.0 into 10 portions,
and parsed each portion with a model trained on
other portions, using the Berkeley parser (Petrov and
Klein, 2007). Measured by the ParsEval metric
(Black et al., 1991), the parsing accuracy on the
CTB test set stands at 83.29% (F-score), with a pre-
cision of 85.18% and a recall of 81.49%.
The results are presented in Table 2, which shows
the overall accuracy of the two methods as well as
the results for each individual category. As should
be clear from Table 2, the results for the two meth-
ods are very comparable, with the second method
performing modestly better than the first method.
</bodyText>
<subsectionHeader confidence="0.698783">
4.2.1 Subject continuity
</subsectionHeader>
<bodyText confidence="0.99800475">
One of the goals for this classification scheme is
to model subject continuity, which answers the ques-
tion of how accurately we can predict whether two
comma-separated text spans have the same subject
or different subjects. When the two spans share
the same subject, the comma belongs to the cate-
gory VP COORD. When they have different sub-
jects, they belong to the categories IP COORD or
</bodyText>
<page confidence="0.993245">
791
</page>
<table confidence="0.999346111111111">
Data Train Dev Test
81-325, 400-454, 500-554 41-80 (1-40,901-931 newswire)
590-596, 600-885, 900 1120-1129 (1018, 1020, 1036, 1044
1001-1017, 1019, 1021-1035 2140-2159 1060-1061,
CTB-6.0 1037-1043, 1045-1059,1062-1071 2280-2294 1072, 1118-1119, 1132
1073-1078, 1100-1117, 1130-1131 2550-2569 1141-1142, 1148 magazine)
1133-1140, 1143-1147, 1149-1151 2775-2799 (2165-2180, 2295-2310
2000-2139, 2160-2164, 2181-2279 3080-3109 2570-2602, 2800-2819
2311-2549, 2603-2774, 2820-3079 3110-3145 broadcast news)
</table>
<tableCaption confidence="0.999974">
Table 1: CTB 6.0 data set division.
</tableCaption>
<bodyText confidence="0.999605375">
SB. When this question is meaningless, e.g., when
one of the span does not even have a subject, the
comma belongs to other categories. To evaluate the
performance of our model on this problem, we re-
computed the results by putting IP COORD and SB
in one category, putting VP COORD in another cat-
egory and the rest of the labels in a third category.
The results are presented in Table 3.
</bodyText>
<subsubsectionHeader confidence="0.559096">
4.2.2 The effect of genre
</subsubsectionHeader>
<bodyText confidence="0.999961181818182">
CTB 6.0 consists of data from three different gen-
res, including newswire, magazine and broadcast
news. Data genres may have very different char-
acteristics. To evaluate how our model works on
different genres, we train a model using training
and development sets, and test the model on differ-
ent genres as described in Table 1. The results on
these three genres are presented in Table 4, and they
shows a significant fluctuation across genres. Our
model works the best on newswire, but not as good
on broadcast news and magazine articles.
</bodyText>
<subsectionHeader confidence="0.892035">
4.2.3 Comparison with prior work
</subsectionHeader>
<bodyText confidence="0.999935230769231">
(Xue and Yang, 2011) presented results on a
binary classification of whether or not a comma
marks a sentence boundary, while the present work
addresses a multi-category classification problem
aimed at identifying discourse segments and prelim-
inary discourse relations anchored by the comma.
However, since we also have a SB category, com-
parison is possible. For comparison purposes, we
retrained our model on their data sets, and computed
the results of SB vs other categories. The results are
shown in Table 5. Our results are very comparable
with (Xue and Yang, 2011) despite that we are per-
forming a multicategory classification.
</bodyText>
<subsectionHeader confidence="0.99659">
4.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.999970740740741">
Even though our feature-based approach can the-
oretically “correct” parsing errors, meaning that a
comma can in theory be classified correctly even if a
sentence is incorrectly parsed, when examining the
system output, errors in automatic parses often lead
to errors in comma classification. A common pars-
ing error is the confusion between Structures (h) and
(i). If the subject of the text span after a comma is
dropped as shown in (h), the parser often produces
a VP coordination structure as shown in (i) and vice
versa. This kind of parsing errors would lead to er-
rors in our syntactic features and thus directly affect
the accuracy of our model.
There is a large body of work on discourse analysis
in the field of Natural Language Processing. Most of
the work, however, are on English. An unsupervised
approach was proposed to recognize discourse rela-
tions in (Marcu and Echihabi, 2002), which extracts
discourse relations that hold between arbitrary spans
of text making use of cue phrases. Like the present
work, a lot of research on discourse analysis is car-
ried out at the sentence level. (Soricut and Marcu,
2003; Sporleder and Lapata, 2005; Polanyi et al.,
2004). (Soricut and Marcu, 2003) and (Polanyi et
al., 2004) implement models to perform discourse
parsing, while (Sporleder and Lapata, 2005) intro-
duces discourse chunking as an alternative to full-
</bodyText>
<sectionHeader confidence="0.635288" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<figure confidence="0.995313625">
(i) IP
NP VP
VP , VP
(h) IP
IP
, IP
VP
NP VP
</figure>
<page confidence="0.972925">
792
</page>
<table confidence="0.999878">
Class Metric Method 1 Method 2
all acc. (%) 71.5 72.9
Prec. (%) 65.6 66.2
SB Rec. (%) 71.7 73.1
F. (%) 68.5 69.5
Prec. (%) 53.3 56.0
IP COORD Rec. (%) 50.5 48.6
F. (%) 52.0 52.0
Prec. (%) 65.6 68.3
VP Coord Rec. (%) 76.3 78.2
F. (%) 70.5 72.9
Prec. (%) 66.9 66.8
ADJ Rec. (%) 29.3 37.7
F. (%) 40.8 48.2
Prec. (%) 88.3 91.2
Comp Rec. (%) 93.9 92.4
F. (%) 91.0 91.8
Prec. (%) 25.0 31.8
SentSBJ Rec. (%) 6 10
F. (%) 9.7 15.6
Prec. (%) 86.9 85.6
Other Rec. (%) 83.4 84.1
F. (%) 85.1 84.8
</table>
<tableCaption confidence="0.987085">
Table 2: Overall accuracy of the two methods as well as
the results for each individual category.
</tableCaption>
<bodyText confidence="0.9871875">
scale discourse parsing.
The emergence of linguistic corpora annotated
with discourse structure such as the RST Discourse
Treebank (Carlson et al., 2002) and PDT (Miltsakaki
et al., 2004; Prasad et al., 2008) have changed the
landscape of discourse analysis. More robust, data-
driven models are starting to emerge.
Compared with English, much less work has
been done in Chinese discourse analysis, presum-
ably due to the lack of discourse resources in Chi-
nese. (Huang and Chen, 2011) constructs a small
corpus following the PDT annotation scheme and
</bodyText>
<table confidence="0.9997085">
Prec. (%) Rec. (%) F. (%)
VP COORD 68.3 78.2 72.9
IP COORD+SB 76.0 78.7 77.3
Other 89.0 80.2 84.4
</table>
<tableCaption confidence="0.9908775">
Table 3: Subject continuity results based on Maximum
Entropy model
</tableCaption>
<table confidence="0.998988">
Genre NW BN MZ
Accuracy. (%) 79.1 73.6 67.7
</table>
<tableCaption confidence="0.969698">
Table 4: Results on different genres based on Maximum
Entropy model
</tableCaption>
<table confidence="0.9987948">
Xue and Yang our model
(%) p r f1 p r f1
Overall 89.2 88.7
EOS 64.7 76.4 70.1 63.0 77.9 69.7
NEOS 95.1 91.7 93.4 95.3 90.8 93.0
</table>
<tableCaption confidence="0.9932395">
Table 5: Comparison of (Xue and Yang, 2011) and the
present work based on Maximum Entropy model
</tableCaption>
<bodyText confidence="0.998413666666667">
trains a statistical classifier to recognize discourse
relations. Their work, however, is only concerned
with discourse relations between adjacent sentences,
thus side-stepping the hard problem of disambiguat-
ing the Chinese comma and analyzing intra-sentence
discourse relations. To the best of our knowledge,
our work is the first in attempting to disambiguating
the Chinese comma as the first step in performing
Chinese discourse analysis.
</bodyText>
<sectionHeader confidence="0.996574" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999973076923077">
We proposed a approach to disambiguate the Chi-
nese comma as a first step toward discourse analy-
sis. Training and testing data are automatically de-
rived from a syntactically annotated corpus. We pre-
sented two automatic comma disambiguation meth-
ods that perform comparably. In the first method,
comma disambiguation is integrated into the parsing
process while in the second method we train a super-
vised classifier to classify the Chinese comma, us-
ing features extracted from automatic parses. Much
needs to be done in the area, but we believe our work
provides insight into the intricacy and complexity of
discourse analysis in Chinese.
</bodyText>
<sectionHeader confidence="0.969989" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.988130666666667">
This work is supported by the IIS Division of Na-
tional Science Foundation via Grant No. 0910532
entitled “Richer Representations for Machine
Translation”. All views expressed in this paper are
those of the authors and do not necessarily represent
the view of the National Science Foundation.
</bodyText>
<page confidence="0.998063">
793
</page>
<sectionHeader confidence="0.993813" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999803044117647">
L Carlson, D Marcu, M E Okurowski. 2002. RST Dis-
course Treebank. Linguistic Data Consortium 2002.
Caroline Sporleder, Mirella Lapata. 2005. Discourse
chunking and its application to sentence compression.
In Proceedings of HLT/EMNLP 2005.
Livia Polanyi, Chris Culy, Martin Van Den Berg, Gian
Lorenzo Thione and David Ahn. 2004. Sentential
structure and discourse parsing. In Proceeedings of
the ACL 2004 Workshop on Discourse Annotation
2004.
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese
Discourse Relation Recognition. In Proceedings of
the 5th International Joint Conference on Natural Lan-
guage Processing 2011,pages1442-1446.
Daniel Marcu and Abdessamad Echihabi. 2002. An Un-
supervised Approach to Recognizing Discourse Rela-
tions. In Proceedings of the ACL, July 6-12, 2002,
Philadelphia, PA, USA.
Radu Soricut and Daniel Marcu. 2003. Sentence Level
Discourse Parsing using Syntactic and Lexical Infor-
mation. In Proceedings of the ACL 2003.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi andBon-
nie Webber. 2004. The Penn Discourse Treebank. In
Proceedings of LREC 2004.
Nianwen Xue and Yaqin Yang. 2011. Chinese sentence
segmentation as comma classification. In Proceedings
of ACL 2011.
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207-238.
Slav Petrov and Dan Klein. 2007. Improved Inferenc-
ing for Unlexicalized Parsing. In Proceedings of HLT-
NAACL 2007.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos, B.
Santorini, and T. Strzalkowski. 1991. A procedure
for quantitively comparing the syntactic coverage of
English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306-
311.
Mann, William C. and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional the-
ory of text organization. Text 8 (3): 243-281.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse Treebank 2.0..
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Meixun Jin, Mi-Young Kim, Dong-Il Kim, and Jong-
Hyeok Lee. 2004. Segmentation of Chinese Long
Sentences Using Commas. In Proceedings of the
SIGHANN Workshop on Chinese Language Process-
ing.
Xing Li, Chengqing Zong, and Rile Hu. 2005. A Hier-
archical Parsing Approach with Punctuation Process-
ing for Long Sentence Sentences. In Proceedings of
the Second International Joint Conference on Natural
Language Processing: Companion Volume including
Posters/Demos and Tutorial Abstracts.
Andrew Kachites McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Church, K., and Hanks, P. 1989. Word Association
Norms, Mutual Information and Lexicography. As-
sociation for Computational Linguistics, Vancouver ,
Canada
</reference>
<page confidence="0.998363">
794
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.079708">
<title confidence="0.991714">Chinese Comma Disambiguation for Discourse Analysis</title>
<author confidence="0.350694">Yaqin</author>
<affiliation confidence="0.32095">Brandeis</affiliation>
<address confidence="0.981214">415 South Waltham, MA 02453,</address>
<email confidence="0.999447">yaqin@brandeis.edu</email>
<note confidence="0.3697645">Nianwen Brandeis</note>
<address confidence="0.976375">415 South Waltham, MA 02453,</address>
<email confidence="0.999828">xuen@brandeis.edu</email>
<abstract confidence="0.999251666666667">The Chinese comma signals the boundary of discourse units and also anchors discourse relations between adjacent text spans. In this work, we propose a discourse structureoriented classification of the comma that can be automatically extracted from the Chinese Treebank based on syntactic patterns. We then experimented with two supervised learning methods that automatically disambiguate the Chinese comma based on this classification. The first method integrates comma classification into parsing, and the second method adopts a “post-processing” approach that extracts features from automatic parses to train a classifier. The experimental results show that the second approach compares favorably against the first approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<date>2002</date>
<booktitle>RST Discourse Treebank. Linguistic Data Consortium</booktitle>
<contexts>
<context position="2697" citStr="Carlson et al., 2002" startWordPosition="423" endWordPosition="426">ntence boundaries in Chinese and it can benefit a whole range of downstream NLP applications such as syntactic parsing and Machine Translation. In Machine Translation, for example, it is very typical for “one” Chinese sentence to be translated into multiple English sentences, with each comma-separated segment corresponding to one English sentence. In the present work, we expand this view and propose to look at the Chinese comma in the context of discourse analysis. The Chinese comma is viewed as a delimiter of elementary discourse units (EDUs), in the sense of the Rhetorical Structure Theory (Carlson et al., 2002; Mann et al., 1988). It is also considered to be the anchor of discourse relations, in the sense of the Penn Discourse Treebank (PDT) (Prasad et al., 2008). Disambiguating the comma is thus necessary for the purpose of discourse segmentation, the identification of EDUs, a first step in building up the discourse structure of a Chinese text. Developing a supervised or semi-supervised model of discourse segmentation would require ground truth annotated based on a well-established representation scheme, but as of right now no such annotation exists for Chinese to the best of our knowledge. Howeve</context>
<context position="25661" citStr="Carlson et al., 2002" startWordPosition="4341" endWordPosition="4344">rec. (%) 53.3 56.0 IP COORD Rec. (%) 50.5 48.6 F. (%) 52.0 52.0 Prec. (%) 65.6 68.3 VP Coord Rec. (%) 76.3 78.2 F. (%) 70.5 72.9 Prec. (%) 66.9 66.8 ADJ Rec. (%) 29.3 37.7 F. (%) 40.8 48.2 Prec. (%) 88.3 91.2 Comp Rec. (%) 93.9 92.4 F. (%) 91.0 91.8 Prec. (%) 25.0 31.8 SentSBJ Rec. (%) 6 10 F. (%) 9.7 15.6 Prec. (%) 86.9 85.6 Other Rec. (%) 83.4 84.1 F. (%) 85.1 84.8 Table 2: Overall accuracy of the two methods as well as the results for each individual category. scale discourse parsing. The emergence of linguistic corpora annotated with discourse structure such as the RST Discourse Treebank (Carlson et al., 2002) and PDT (Miltsakaki et al., 2004; Prasad et al., 2008) have changed the landscape of discourse analysis. More robust, datadriven models are starting to emerge. Compared with English, much less work has been done in Chinese discourse analysis, presumably due to the lack of discourse resources in Chinese. (Huang and Chen, 2011) constructs a small corpus following the PDT annotation scheme and Prec. (%) Rec. (%) F. (%) VP COORD 68.3 78.2 72.9 IP COORD+SB 76.0 78.7 77.3 Other 89.0 80.2 84.4 Table 3: Subject continuity results based on Maximum Entropy model Genre NW BN MZ Accuracy. (%) 79.1 73.6 6</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2002</marker>
<rawString>L Carlson, D Marcu, M E Okurowski. 2002. RST Discourse Treebank. Linguistic Data Consortium 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Mirella Lapata</author>
</authors>
<title>Discourse chunking and its application to sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<contexts>
<context position="24647" citStr="Sporleder and Lapata, 2005" startWordPosition="4146" endWordPosition="4149">This kind of parsing errors would lead to errors in our syntactic features and thus directly affect the accuracy of our model. There is a large body of work on discourse analysis in the field of Natural Language Processing. Most of the work, however, are on English. An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. Like the present work, a lot of research on discourse analysis is carried out at the sentence level. (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Polanyi et al., 2004). (Soricut and Marcu, 2003) and (Polanyi et al., 2004) implement models to perform discourse parsing, while (Sporleder and Lapata, 2005) introduces discourse chunking as an alternative to full5 Related Work (i) IP NP VP VP , VP (h) IP IP , IP VP NP VP 792 Class Metric Method 1 Method 2 all acc. (%) 71.5 72.9 Prec. (%) 65.6 66.2 SB Rec. (%) 71.7 73.1 F. (%) 68.5 69.5 Prec. (%) 53.3 56.0 IP COORD Rec. (%) 50.5 48.6 F. (%) 52.0 52.0 Prec. (%) 65.6 68.3 VP Coord Rec. (%) 76.3 78.2 F. (%) 70.5 72.9 Prec. (%) 66.9 66.8 ADJ Rec. (%) 29.3 37.7 F. (%) 40.8 48.2 Prec. (%) 88.3 91.</context>
</contexts>
<marker>Sporleder, Lapata, 2005</marker>
<rawString>Caroline Sporleder, Mirella Lapata. 2005. Discourse chunking and its application to sentence compression. In Proceedings of HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Chris Culy</author>
<author>Martin Van Den Berg</author>
<author>Gian Lorenzo Thione</author>
<author>David Ahn</author>
</authors>
<title>Sentential structure and discourse parsing.</title>
<date>2004</date>
<booktitle>In Proceeedings of the ACL 2004 Workshop on Discourse Annotation</booktitle>
<marker>Polanyi, Culy, Van Den Berg, Thione, Ahn, 2004</marker>
<rawString>Livia Polanyi, Chris Culy, Martin Van Den Berg, Gian Lorenzo Thione and David Ahn. 2004. Sentential structure and discourse parsing. In Proceeedings of the ACL 2004 Workshop on Discourse Annotation 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hen-Hsen Huang</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Chinese Discourse Relation Recognition.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing</booktitle>
<pages>2011--1442</pages>
<contexts>
<context position="25989" citStr="Huang and Chen, 2011" startWordPosition="4396" endWordPosition="4399">Other Rec. (%) 83.4 84.1 F. (%) 85.1 84.8 Table 2: Overall accuracy of the two methods as well as the results for each individual category. scale discourse parsing. The emergence of linguistic corpora annotated with discourse structure such as the RST Discourse Treebank (Carlson et al., 2002) and PDT (Miltsakaki et al., 2004; Prasad et al., 2008) have changed the landscape of discourse analysis. More robust, datadriven models are starting to emerge. Compared with English, much less work has been done in Chinese discourse analysis, presumably due to the lack of discourse resources in Chinese. (Huang and Chen, 2011) constructs a small corpus following the PDT annotation scheme and Prec. (%) Rec. (%) F. (%) VP COORD 68.3 78.2 72.9 IP COORD+SB 76.0 78.7 77.3 Other 89.0 80.2 84.4 Table 3: Subject continuity results based on Maximum Entropy model Genre NW BN MZ Accuracy. (%) 79.1 73.6 67.7 Table 4: Results on different genres based on Maximum Entropy model Xue and Yang our model (%) p r f1 p r f1 Overall 89.2 88.7 EOS 64.7 76.4 70.1 63.0 77.9 69.7 NEOS 95.1 91.7 93.4 95.3 90.8 93.0 Table 5: Comparison of (Xue and Yang, 2011) and the present work based on Maximum Entropy model trains a statistical classifier </context>
</contexts>
<marker>Huang, Chen, 2011</marker>
<rawString>Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese Discourse Relation Recognition. In Proceedings of the 5th International Joint Conference on Natural Language Processing 2011,pages1442-1446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An Unsupervised Approach to Recognizing Discourse Relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="24388" citStr="Marcu and Echihabi, 2002" startWordPosition="4103" endWordPosition="4106"> in comma classification. A common parsing error is the confusion between Structures (h) and (i). If the subject of the text span after a comma is dropped as shown in (h), the parser often produces a VP coordination structure as shown in (i) and vice versa. This kind of parsing errors would lead to errors in our syntactic features and thus directly affect the accuracy of our model. There is a large body of work on discourse analysis in the field of Natural Language Processing. Most of the work, however, are on English. An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. Like the present work, a lot of research on discourse analysis is carried out at the sentence level. (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Polanyi et al., 2004). (Soricut and Marcu, 2003) and (Polanyi et al., 2004) implement models to perform discourse parsing, while (Sporleder and Lapata, 2005) introduces discourse chunking as an alternative to full5 Related Work (i) IP NP VP VP , VP (h) IP IP , IP VP NP VP 792 Class Metric Method 1 Method 2 all acc. (%) 71.5 72.9 Prec. (%</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An Unsupervised Approach to Recognizing Discourse Relations. In Proceedings of the ACL, July 6-12, 2002, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence Level Discourse Parsing using Syntactic and Lexical Information.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="24619" citStr="Soricut and Marcu, 2003" startWordPosition="4142" endWordPosition="4145">n in (i) and vice versa. This kind of parsing errors would lead to errors in our syntactic features and thus directly affect the accuracy of our model. There is a large body of work on discourse analysis in the field of Natural Language Processing. Most of the work, however, are on English. An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. Like the present work, a lot of research on discourse analysis is carried out at the sentence level. (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Polanyi et al., 2004). (Soricut and Marcu, 2003) and (Polanyi et al., 2004) implement models to perform discourse parsing, while (Sporleder and Lapata, 2005) introduces discourse chunking as an alternative to full5 Related Work (i) IP NP VP VP , VP (h) IP IP , IP VP NP VP 792 Class Metric Method 1 Method 2 all acc. (%) 71.5 72.9 Prec. (%) 65.6 66.2 SB Rec. (%) 71.7 73.1 F. (%) 68.5 69.5 Prec. (%) 53.3 56.0 IP COORD Rec. (%) 50.5 48.6 F. (%) 52.0 52.0 Prec. (%) 65.6 68.3 VP Coord Rec. (%) 76.3 78.2 F. (%) 70.5 72.9 Prec. (%) 66.9 66.8 ADJ Rec. (%) 29.3 37.7 F. (%) </context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence Level Discourse Parsing using Syntactic and Lexical Information. In Proceedings of the ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
</authors>
<title>Rashmi Prasad, Aravind Joshi andBonnie Webber.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Miltsakaki, 2004</marker>
<rawString>Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi andBonnie Webber. 2004. The Penn Discourse Treebank. In Proceedings of LREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Yaqin Yang</author>
</authors>
<title>Chinese sentence segmentation as comma classification.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="9694" citStr="Xue and Yang, 2011" startWordPosition="1586" endWordPosition="1589">idered to be discourse units and the commas linking them are labeled IP COORD. Different from the sentence boundary cases, these coordinated IPs are often embedded in a larger structure. An example is given in (3) and its typical syntactic pattern is illustrated in (c). . FWÕ Lu Renfa 411_9E presentation (3) �9 According to ,[4] , [5] , �� overall ITC complete M exceeding quota SB COORD_IP COORD_VP ADJ COMP Sent_SBJ !�Q the whole country COORD SUBORD OTHER RELATION �� revenue E already 11E* goal Figure 1: Comma classification �R %Pz AT situation fairly good. Sentence Boundary (SB): Following (Xue and Yang, 2011), we consider the loosely coordinated IPs that are the immediate children of the root IP to be independent sentences, and the commas separating them to be delimiters of sentence boundary. This is illustrated in (2), where a Chinese sentence can be split into two independent shorter sentences at the comma. We view this comma to be a marker of the sentence boundary and it serves the same function as the unambiguous sentence boundary delimitors (periods, question marks, exclamation marks) in Chinese. The syntactic pattern that is used to infer this relation is illustrated in (b). (2) I_„&apos;&amp; AA Gua</context>
<context position="16004" citStr="Xue and Yang, 2011" startWordPosition="2664" endWordPosition="2667">es described in Section 2. Using this relabeled data as training data, we experimented with two automatic comma disambiguation methods. In the first method, we simply retrained the Berkeley parser (Petrov and Klein, 2007) on the relabeled data and computed how accurately the commas are labeled in a held-out test set. In the second method, we trained a Maximum Entropy classifier with the Mallet (McCallum et al., 2002) machine learning package to classify the commas. The features are extracted from the CTB data automatically parsed with the Berkeley parser. We implemented features described in (Xue and Yang, 2011), and also experimented with a set of new features as follows. In general, these new features are extracted from the two text spans surrounding the comma. Given a comma, we define the preceding text span as i span and the following text span as j span. We also collected a number of subject-predicate pairs from a large corpus that doesn’t overlap with the CTB. We refer to this corpus as the auxiliary corpus. Subject and Predicate features: We explored various combinations of the subject (sbj), predicate (pred) and object (obj) of the two spans. The subject of i span is represented as sbji, etc.</context>
<context position="22866" citStr="Xue and Yang, 2011" startWordPosition="3852" endWordPosition="3855">. 4.2.2 The effect of genre CTB 6.0 consists of data from three different genres, including newswire, magazine and broadcast news. Data genres may have very different characteristics. To evaluate how our model works on different genres, we train a model using training and development sets, and test the model on different genres as described in Table 1. The results on these three genres are presented in Table 4, and they shows a significant fluctuation across genres. Our model works the best on newswire, but not as good on broadcast news and magazine articles. 4.2.3 Comparison with prior work (Xue and Yang, 2011) presented results on a binary classification of whether or not a comma marks a sentence boundary, while the present work addresses a multi-category classification problem aimed at identifying discourse segments and preliminary discourse relations anchored by the comma. However, since we also have a SB category, comparison is possible. For comparison purposes, we retrained our model on their data sets, and computed the results of SB vs other categories. The results are shown in Table 5. Our results are very comparable with (Xue and Yang, 2011) despite that we are performing a multicategory cla</context>
<context position="26504" citStr="Xue and Yang, 2011" startWordPosition="4493" endWordPosition="4496">discourse analysis, presumably due to the lack of discourse resources in Chinese. (Huang and Chen, 2011) constructs a small corpus following the PDT annotation scheme and Prec. (%) Rec. (%) F. (%) VP COORD 68.3 78.2 72.9 IP COORD+SB 76.0 78.7 77.3 Other 89.0 80.2 84.4 Table 3: Subject continuity results based on Maximum Entropy model Genre NW BN MZ Accuracy. (%) 79.1 73.6 67.7 Table 4: Results on different genres based on Maximum Entropy model Xue and Yang our model (%) p r f1 p r f1 Overall 89.2 88.7 EOS 64.7 76.4 70.1 63.0 77.9 69.7 NEOS 95.1 91.7 93.4 95.3 90.8 93.0 Table 5: Comparison of (Xue and Yang, 2011) and the present work based on Maximum Entropy model trains a statistical classifier to recognize discourse relations. Their work, however, is only concerned with discourse relations between adjacent sentences, thus side-stepping the hard problem of disambiguating the Chinese comma and analyzing intra-sentence discourse relations. To the best of our knowledge, our work is the first in attempting to disambiguating the Chinese comma as the first step in performing Chinese discourse analysis. 6 Conclusions and future work We proposed a approach to disambiguate the Chinese comma as a first step to</context>
</contexts>
<marker>Xue, Yang, 2011</marker>
<rawString>Nianwen Xue and Yaqin Yang. 2011. Chinese sentence segmentation as comma classification. In Proceedings of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering,</title>
<date>2005</date>
<pages>11--2</pages>
<contexts>
<context position="3790" citStr="Xue et al., 2005" startWordPosition="591" endWordPosition="594">lished representation scheme, but as of right now no such annotation exists for Chinese to the best of our knowledge. However, syntactically annotated treebanks often contain important clues that can be used to infer discourse-level information. We present 786 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 786–794, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics a method of automatically deriving a preliminary form of discourse structure anchored by the Chinese comma from the Penn Chinese Treebank (CTB) (Xue et al., 2005), and using this information to train and test supervised models. This discourse information is formalized as a classification of the Chinese comma, with each class representing the boundary of an elementary discourse unit as well as the anchor of a coarse-grained discourse relation between the two discourse units that it delimits. We then develop two comma classification methods. In the first method, we replace the part-of-speech (POS) tag of each comma in the CTB with a derived discourse category and retrain a state-of-theart Chinese parser on the relabeled data. We then evaluate how accurat</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 11(2):207-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inferencing for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL</booktitle>
<contexts>
<context position="15606" citStr="Petrov and Klein, 2007" startWordPosition="2597" endWordPosition="2600">actic annotation of the CTB and attached to instances of the comma in the corpus. This makes it possible for us to train supervised models to automatically classify the commas in any Chinese text. 3 Two comma classification methods Given the gold standard parses, based on the syntactic patterns described in Section 2, we can map the POS tag of each comma instance in the CTB to one of the seven classes described in Section 2. Using this relabeled data as training data, we experimented with two automatic comma disambiguation methods. In the first method, we simply retrained the Berkeley parser (Petrov and Klein, 2007) on the relabeled data and computed how accurately the commas are labeled in a held-out test set. In the second method, we trained a Maximum Entropy classifier with the Mallet (McCallum et al., 2002) machine learning package to classify the commas. The features are extracted from the CTB data automatically parsed with the Berkeley parser. We implemented features described in (Xue and Yang, 2011), and also experimented with a set of new features as follows. In general, these new features are extracted from the two text spans surrounding the comma. Given a comma, we define the preceding text spa</context>
<context position="19799" citStr="Petrov and Klein, 2007" startWordPosition="3356" endWordPosition="3359">st set, including 1327 commas that are sentence boundaries (SB), 539 commas that connect coordinated IPs (IP COORD), 1173 commas that join coordinated VPs (VP COORD), 379 commas that delimits a subordinate clause and its main clause (ADJ), 314 commas that anchor complementation relations (COMP), and 1625 commas that belong to the OTHER category. 4.2 Results As mentioned in Section 3, we experimented with two comma classification methods. In the first method, we replace the part-of-speech (POS) tags of the commas with the seven classes defined in Section 2. We then retrain the Berkeley parser (Petrov and Klein, 2007) using the training set as presented in Table 1, parse the test set, and evaluate the comma classification accuracy. In the second method, we use the relabeled commas as the gold-standard data to train a supervised classifier to automatically classify the commas. As shown in the previous section, syntactic structures are an important source of information for our classifier. For feature extraction purposes, the entire CTB6.0 is automatically parsed in a round-robin fashion. We divided CTB 6.0 into 10 portions, and parsed each portion with a model trained on other portions, using the Berkeley p</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inferencing for Unlexicalized Parsing. In Proceedings of HLTNAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickinger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="20483" citStr="Black et al., 1991" startWordPosition="3466" endWordPosition="3469">et, and evaluate the comma classification accuracy. In the second method, we use the relabeled commas as the gold-standard data to train a supervised classifier to automatically classify the commas. As shown in the previous section, syntactic structures are an important source of information for our classifier. For feature extraction purposes, the entire CTB6.0 is automatically parsed in a round-robin fashion. We divided CTB 6.0 into 10 portions, and parsed each portion with a model trained on other portions, using the Berkeley parser (Petrov and Klein, 2007). Measured by the ParsEval metric (Black et al., 1991), the parsing accuracy on the CTB test set stands at 83.29% (F-score), with a precision of 85.18% and a recall of 81.49%. The results are presented in Table 2, which shows the overall accuracy of the two methods as well as the results for each individual category. As should be clear from Table 2, the results for the two methods are very comparable, with the second method performing modestly better than the first method. 4.2.1 Subject continuity One of the goals for this classification scheme is to model subject continuity, which answers the question of how accurately we can predict whether two</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitively comparing the syntactic coverage of English grammars. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 306-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text</journal>
<volume>8</volume>
<issue>3</issue>
<pages>243--281</pages>
<marker>Mann, Thompson, 1988</marker>
<rawString>Mann, William C. and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Toward a functional theory of text organization. Text 8 (3): 243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0..</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="2853" citStr="Prasad et al., 2008" startWordPosition="451" endWordPosition="454">ranslation, for example, it is very typical for “one” Chinese sentence to be translated into multiple English sentences, with each comma-separated segment corresponding to one English sentence. In the present work, we expand this view and propose to look at the Chinese comma in the context of discourse analysis. The Chinese comma is viewed as a delimiter of elementary discourse units (EDUs), in the sense of the Rhetorical Structure Theory (Carlson et al., 2002; Mann et al., 1988). It is also considered to be the anchor of discourse relations, in the sense of the Penn Discourse Treebank (PDT) (Prasad et al., 2008). Disambiguating the comma is thus necessary for the purpose of discourse segmentation, the identification of EDUs, a first step in building up the discourse structure of a Chinese text. Developing a supervised or semi-supervised model of discourse segmentation would require ground truth annotated based on a well-established representation scheme, but as of right now no such annotation exists for Chinese to the best of our knowledge. However, syntactically annotated treebanks often contain important clues that can be used to infer discourse-level information. We present 786 Proceedings of the </context>
<context position="5313" citStr="Prasad et al., 2008" startWordPosition="838" endWordPosition="841">compares favorably against the first approach. The rest of the paper is organized as follows. In Section 2, we present our approach to automatically extract discourse information from a syntactically annotated treebank and present our classification scheme. In Section 3, we describe our supervised learning methods and the features we extracted. Section 4 presents our experiment setup and experimental results. Related work is reviewed in Section 5. We conclude in Section 6. 2 Chinese comma classification There are many ways to conceptualize the discourse structure of a text (Mann et al., 1988; Prasad et al., 2008), but there is more of a consensus among researchers about the fundamental building blocks of the discourse structure. For the Rhetorical Discourse Theory, the building blocks are Elementary Discourse Units (EDUs). For the PDT, the building blocks are abstract objects such as propositions, facts. Although they are phrased in different ways, syntactically these discourse units are generally realized as clauses or built on top of clauses. So the first step in building the discourse structure of a text is to identify these discourse units. In Chinese, these elementary discourse units are generall</context>
<context position="25716" citStr="Prasad et al., 2008" startWordPosition="4351" endWordPosition="4354">.0 52.0 Prec. (%) 65.6 68.3 VP Coord Rec. (%) 76.3 78.2 F. (%) 70.5 72.9 Prec. (%) 66.9 66.8 ADJ Rec. (%) 29.3 37.7 F. (%) 40.8 48.2 Prec. (%) 88.3 91.2 Comp Rec. (%) 93.9 92.4 F. (%) 91.0 91.8 Prec. (%) 25.0 31.8 SentSBJ Rec. (%) 6 10 F. (%) 9.7 15.6 Prec. (%) 86.9 85.6 Other Rec. (%) 83.4 84.1 F. (%) 85.1 84.8 Table 2: Overall accuracy of the two methods as well as the results for each individual category. scale discourse parsing. The emergence of linguistic corpora annotated with discourse structure such as the RST Discourse Treebank (Carlson et al., 2002) and PDT (Miltsakaki et al., 2004; Prasad et al., 2008) have changed the landscape of discourse analysis. More robust, datadriven models are starting to emerge. Compared with English, much less work has been done in Chinese discourse analysis, presumably due to the lack of discourse resources in Chinese. (Huang and Chen, 2011) constructs a small corpus following the PDT annotation scheme and Prec. (%) Rec. (%) F. (%) VP COORD 68.3 78.2 72.9 IP COORD+SB 76.0 78.7 77.3 Other 89.0 80.2 84.4 Table 3: Subject continuity results based on Maximum Entropy model Genre NW BN MZ Accuracy. (%) 79.1 73.6 67.7 Table 4: Results on different genres based on Maxim</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0.. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meixun Jin</author>
<author>Mi-Young Kim</author>
<author>Dong-Il Kim</author>
<author>JongHyeok Lee</author>
</authors>
<title>Segmentation of Chinese Long Sentences Using Commas.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGHANN Workshop on Chinese Language Processing.</booktitle>
<marker>Jin, Kim, Kim, Lee, 2004</marker>
<rawString>Meixun Jin, Mi-Young Kim, Dong-Il Kim, and JongHyeok Lee. 2004. Segmentation of Chinese Long Sentences Using Commas. In Proceedings of the SIGHANN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Li</author>
<author>Chengqing Zong</author>
<author>Rile Hu</author>
</authors>
<title>A Hierarchical Parsing Approach with Punctuation Processing for Long Sentence Sentences.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Joint Conference on Natural Language Processing: Companion Volume including Posters/Demos and Tutorial Abstracts.</booktitle>
<marker>Li, Zong, Hu, 2005</marker>
<rawString>Xing Li, Chengqing Zong, and Rile Hu. 2005. A Hierarchical Parsing Approach with Punctuation Processing for Long Sentence Sentences. In Proceedings of the Second International Joint Conference on Natural Language Processing: Companion Volume including Posters/Demos and Tutorial Abstracts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information and Lexicography. Association for Computational Linguistics,</title>
<date>1989</date>
<location>Vancouver , Canada</location>
<contexts>
<context position="17409" citStr="Church and Hanks, 1989" startWordPosition="2918" endWordPosition="2921">unction of sbji and predj occurs more than 2 times in the auxiliary corpus when j does not have a subject. 4. whether the conjunction of obji and predj occurs more than 2 times in the auxiliary corpus when j does not have a subject 5. Whether the conjunction of predi and sbjj occurs more than 2 times in the auxiliary corpus when i does not have a subject. Mutual Information features: Mutual information is intended to capture the association strength between the subject of a previous span and the predicate of the current span. We use Mutual Information .... VP IP-SBJ VP Sentential Subject 790 (Church and Hanks, 1989) as shown in Equation (1) and the frequency count computed based on the auxiliary corpus to measure such constraints. # co-occur of S and P * corpus size MI = log2 (1) # S occur * # P occur 1. The conjunction of sbji and predj when j does not have a subject if their MIvalue is greater than -8.0, an empirically established threshold. 2. Whether obji and predj has an MI value greater than 5.0 if j does not have a subject. 3. Whether the MI value of sbji and predj is greater than 0.0, and they occur 2 times in the auxiliary corpus when j doesn’t have a subject. 4. Whether the MI value of obji and</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Church, K., and Hanks, P. 1989. Word Association Norms, Mutual Information and Lexicography. Association for Computational Linguistics, Vancouver , Canada</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>