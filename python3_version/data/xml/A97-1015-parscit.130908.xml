<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.991085">
The Domain Dependence of Parsing
</title>
<author confidence="0.984106">
Satoshi Sekine
</author>
<affiliation confidence="0.984359">
New York University
Computer Science Department
</affiliation>
<address confidence="0.9961065">
715 Broadway, Room 709
New York, NY 10003, USA
</address>
<email confidence="0.906033">
sekine@cs.nyu.edu
http://cs.nyu.edu/cs/projects/proteus/sekine
</email>
<sectionHeader confidence="0.994639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998757">
A major concern in corpus based ap-
proaches is that the applicability of the ac-
quired knowledge may be limited by some
feature of the corpus, in particular, the no-
tion of text &apos;domain&apos;. In order to examine
the domain dependence of parsing, in this
paper, we report 1) Comparison of struc-
ture distributions across domains; 2) Ex-
amples of domain specific structures; and
3) Parsing experiment using some domain
dependent grammars. The observations
using the Brown corpus demonstrate do-
main dependence and idiosyncrasy of syn-
tactic structure. The parsing results show
that the best accuracy is obtained using
the grammar acquired from the same do-
main or the same class (fiction or non-
fiction). We will also discuss the relation-
ship between parsing accuracy and the size
of training corpus.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971029411765">
A major concern in corpus based approaches is that
the applicability of the acquired knowledge may be
limited by some feature of the corpus. In particular,
the notion of text &apos;domain&apos; has been seen as a ma-
jor constraint on the applicability of the knowledge.
This is a crucial issue for most application systems,
since most systems operate within a specific domain
and we are generally limited in the corpora available
in that domain.
There has been considerable research in this area
(Kittredge and Hirschman, 1983) (Grishman and
Kittredge, 1986). For example, the domain depen-
dence of lexical semantics is widely known. It is
easy to observe that usage of the word &apos;bank&apos; is dif-
ferent between the &apos;economic document&apos; domain and
the &apos;geographic&apos; domain. Also, there are surveys of
domain dependencies concerning syntax or syntax-
related features (Slocum, 1986) (Biber, 1993) (Karl-
gren, 1994). It is intuitively conceivable that there
are syntactic differences between &apos;telegraphic mes-
sages&apos; and &apos;press report&apos;, or between &apos;weather fore-
cast sentences&apos; and &apos;romance and love story&apos;. But,
how about the difference between &apos;press report&apos; and
&apos;romance and love story&apos;? Is there a general and sim-
ple method to compare domains? More importantly,
shall we prepare different knowledge for these two
domain sets?
In this paper, we describe two observations and
an experiment which suggest an answer to the ques-
tions. Among the several types of linguistic knowl-
edge, we are interested in parsing, the essential com-
ponent of many NLP systems, and hence domain de-
pendencies of syntactic knowledge. The observations
and an experiment are the following:
</bodyText>
<listItem confidence="0.99324">
• Comparison of structure distributions across
domains
• Examples of domain specific structures
• Parsing experiment using some domain depen-
dent grammars
</listItem>
<sectionHeader confidence="0.806101" genericHeader="introduction">
2 Data and Tools
</sectionHeader>
<bodyText confidence="0.999662076923077">
The definition of domain will dominate the perfor-
mance of our experiments, so it is very important to
choose a proper corpus. However, for practical rea-
sons (availability and time constraint), we decided
to use an existing multi-domain corpus which has
naturally acceptable domain definition. In order to
acquire grammar rules in our experiment, we need a
syntactically tagged corpus consisting of different do-
mains, and the tagging has to be uniform throughout
the corpus. To meet these requirements, the Brown
Corpus (Francis and Kucera, 1964) on the distribu-
tion of PennTreeBank version 1 (Marcus et.al., 1995)
is used in our experiments. The corpus consists of 15
</bodyText>
<page confidence="0.984555">
96
</page>
<bodyText confidence="0.841326891891892">
domains as shown in Appendix A; in the rest of the
paper, we use the letters from the list to represent
the domains. Each sample consists of about the same
size of text in terms of the number of words (2000
words), although a part of the data is discarded be-
cause of erroneous data format.
For the parsing experiment, we use &apos;Apple Pie
Parser&apos; (Sekine, 1995) (Sekine, 1996). It is a
probabilistic, bottom-up, best-first search, chart
parser and its grammar can be obtained from a
syntactically-tagged corpus. We acquire two-non-
terminal grammars from corpus. Here, `two-non-
terminal grammar&apos; means a grammar which uses
only &apos;S&apos; (sentence) and `1\IP&apos; (noun phrase) as ac-
tual non-terminals in the grammar and other gram-
matical nodes, like &apos;VP&apos; or &apos;PP&apos;, are embedded into a
rule. In other words, all rules can only have either &apos;S&apos;
or `NP&apos; as their left hand-side symbol. This strat-
egy is useful to produce better accuracy compared
to all non-terminal grammar. See (Sekine, 1995) for
details.
In this experiment, grammars are acquired from
the corpus of a single domain, or from some combina-
tion of domains. In order to avoid the unknown word
problem, we used a general dictionary to supplement
the dictionary acquired from corpus. Then, we ap-
ply each of the grammars to some texts of different
domains. We use only 8 domains (A,B,E,J,K,L,N
and P) for this experiment, because we want to fix
the corpus size for each domain, and we want to
have the same number of domains for the non-fiction
and the fiction domains. The main objective is to
observe the parsing performance based on the gram-
mar acquired from the same domain compared with
the performance based on grammars of different do-
mains, or combined domains. Also, the issue of the
size of training corpus will be discussed.
</bodyText>
<sectionHeader confidence="0.988623" genericHeader="method">
3 Domain Dependence of Structures
</sectionHeader>
<bodyText confidence="0.917228">
First, we investigate the syntactic structure of each
domain of the Brown corpus and compare these for
different domains. In order to represent the syntactic
structure of each domain, the distribution of partial
trees of syntactic structure is used. A partial tree is
a part of syntactic tree with depth of one, and it cor-
responds to a production rule. Note that this partial
tree definition is not the same as the structure defini-
tion used in the parsing experiments described later.
We accumulate these partial trees for each domain
and compute the distribution of partial trees based
on their frequency divided by the total number of
partial trees in the domain. For example, Figure
1 shows the five most frequent partial trees (in the
format of production rule) in domain A (Press: Re-
</bodyText>
<table confidence="0.911715625">
domain A domain P
PP -&gt; IN NP 8.40% NP -&gt; PRP 9.52%
NP -&gt; NNPX 6.42% PP -&gt; IN NP 5.797,
S -&gt; S 5.06% S -&gt; NP VP 5.777,
S -&gt; NP VP 4.287. S -&gt; S 5.377.
NP -&gt; DT NNX 3.817. NP -&gt; DT NNX 3.907.
Figure 1: Partial Trees
T\MABEJKLNP
A 5.13 6.35 5.41 5.45 5.51 6.52 5.53 5.55
B 5.47 5.19 5.50 5.51 5.55 5.58 5.60 5.60
E 5.50 5.48 5.20 5.48 5.58 5.59 5.58 5.61
J 5.39 5.37 5.35 5.15 5.52 5.57 5.58 5.59
K 5.32 5.25 5.31 5.41 4.95 5.14 5.15 5.17
L 5.32 6.26 6.32 5.45 5.12 4.91 5.09 5.13
N 5.29 5.25 5.28 5.43 5.10 5.06 4.89 5.12
P 5.43 5.36 5.40 5.56 5.23 5.21 5.21 5.00
</table>
<figureCaption confidence="0.644047615384615">
Figure 2: Cross Entropy of grammar across domains
portage) and domain P (Romance and love story).
For each domain, we compute the probabilities of
partial trees like this. Then, for each pair of domains,
cross entropy is computed using the probability data.
Figure 2 shows a part of the cross entropy data. For
example, 5.41 in column A, row E shows the cross
entropy of modeling by domain E and testing on do-
main A. From the matrix, we can tell that some pairs
of domains have lower cross entropy than others. It
means that there are difference in similarity among
domains. In particular, the differences among fiction
domains are relatively small.
</figureCaption>
<bodyText confidence="0.9983474">
In order to make the observation easier, we clus-
tered the domains based on the cross entropy data.
The distance between two domains is calculated as
the average of the two cross-entropies in both direc-
tions. We use non-overlapping and average-distance
clustering. Figure 3 shows the clustering result based
on grammar cross entropy data. From the results,
we can clearly see that fiction domains, in particular
domains K, L, and N are close which is intuitively
understandable.
</bodyText>
<sectionHeader confidence="0.966915" genericHeader="method">
4 Domain Specific Structures
</sectionHeader>
<bodyText confidence="0.997290166666667">
Secondly, in contrast to the global analysis reported
in the previous section, we investigate the structural
idiosyncrasies of each domain in the Brown corpus.
For each domain, the list of partial trees which are
relatively frequent in that domain is created. We
select the partial trees which satisfy the following
</bodyText>
<page confidence="0.99893">
97
</page>
<table confidence="0.897418363636364">
ADFGKLNPRM JEB CH
1111111111 I I 1 1
5.08 111111- -111 I I 1 1
5.13 11111-- 111 I I I 1
5.17 11111 1 1 1 1 I
5.26 11111 1 I 1 1
5.27 11111 1 I I 1
5.30 I I I-- 1 I I I 1
5.30 1 1 1 1 1 1
5.33 1 1 I I I 1
5.33 1 1-- I 1 1
</table>
<figure confidence="0.8080508">
5.34 I I
5.37 1
5.38 II
5.42
5.50
</figure>
<figureCaption confidence="0.878812">
Figure 3: Clustering result
two conditions:
</figureCaption>
<listItem confidence="0.990242333333333">
1. Frequency of the partial tree in a domain should
be 5 times greater than that in the entire corpus
2. It occurs more than 5 times in the domain
</listItem>
<bodyText confidence="0.999889916666667">
The second condition is used to delete noise, because
low frequency partial trees which satisfy the first con-
dition have very low frequency in the entire corpus.
The list is too large to show in this paper; a part
of the list is shown in Appendix B. It obviously
demonstrates that each domain has many idiosyn-
cratic structures. Many of them are interesting to
see and can be easily explained by our linguistic in-
tuition. (Some examples are listed under the cor-
responding partial tree) This supports the idea of
domain dependent grammar, because these idiosyn-
cratic structures are useful only in that domain.
</bodyText>
<sectionHeader confidence="0.886343" genericHeader="method">
5 Parsing Results
</sectionHeader>
<bodyText confidence="0.999937">
In this section, the parsing experiments are de-
scribed. There are two subsections. The first is the
individual experiment, where texts from 8 domains
are parsed with 4 different types of grammars. These
are grammars acquired from the same size corpus of
the same domain, all domains, non-fiction domains
and fiction domains.
The other parsing experiment is the intensive ex-
periment, where we try to find the best suitable
grammar for some particular domain of text and to
see the relationship of the size of the training corpus.
We use the domains of &apos;Press Reportage&apos; and &apos;Ro-
mance and Love Story&apos; in this intensive experiment.
</bodyText>
<table confidence="0.898653555555556">
Text Same domain All non-fiction fiction
A 66.82/84.14 64.39/61.45 65.57/62.40 62.23/59.32
B 67.65/62.55 64.67/61.78 65.73/62.69 63.03/60.36
E 64.05/60.79 65.25/61.51 65.28/62.18 62.87/59.04
J 87.80/85.59 65.87/63.90 65.57/64.58 63.04/60.77
K 70.99/68.54 71.00/68.04 70.04/66.64 71.79/68.95
L 67.59/65.02 68.08/66.22 67.32/64.31 88.89/86.55
N 73.09/71.38 72.97/70.27 70.51/67.90 74.29/72.28
P 88.44/65.51 64.52/63.95 62.37/61.55 64.69/64.50
</table>
<figureCaption confidence="0.998501">
Figure 4: Parsing accuracy for individual section
</figureCaption>
<bodyText confidence="0.997928">
In order to measure the accuracy of parsing, recall
and precision measures are used (Black et.al., 1991).
</bodyText>
<subsectionHeader confidence="0.929188">
5.1 Individual Experiment
</subsectionHeader>
<bodyText confidence="0.999942513513514">
Figure 4 shows the parsing performance for domain
A, B, E, J, K, L, N and P with four types of gram-
mars. In the table, results are shown in the form of
&apos;recall/precision&apos;. Each grammar is acquired from
roughly the same size (24 samples except L with 21
samples) of corpus. For example, the grammar of all
domains is created using corpus of 3 samples each
from the 8 domains. The grammar of non-fiction and
fiction domains are created from corpus of 6 samples
each from 4 domains. Then text of each domain is
parsed by the four types of grammar. There is no
overlap between training corpus and test corpus.
We can see that the result is always the best when
the grammar acquired from either the same domain
or the same class (fiction or non-fiction) is used. We
will call the division into fiction and non-fiction as
&apos;class&apos;. It is interesting to see that the grammar ac-
quired from all domains is not the best grammar in
any tests. In other words, if the size of the training
corpus is the same, using a training corpus drawn
from a wide variety of domains does not help to
achieve better parsing performance.
For non-fiction domain texts (A, B, E and J),
the performance of the fiction grammar is notably
worse than that of the same domain grammar or the
same class grammar. In contrast, the performance
on some fiction domain texts (K and L) with the
non-fiction grammar is not so different from that of
the same domain. Here, we can find a relationship
between these results and the cross entropy obser-
vations. The cross entropies where any of the fic-
tion domains are models and any of the non-fiction
domains are test are the highest figures in the ta-
ble. This means that the fiction domains are not
suitable for modeling the syntactic structure of the
non-fiction domains. On the other hand, the cross
entropies where any of the non-fiction domains are
</bodyText>
<page confidence="0.988487">
98
</page>
<bodyText confidence="0.999072133333333">
models and any of the non-fiction domains (except
P) are test have some lower figures. Except for the
case of N with the non-fiction grammar, these ob-
servations explains the result of parsing very nicely.
The higher the cross entropy, the worse the parsing
performance.
It is not easy to argue why, for some domains, the
result is better with the grammar of the same class
rather than the same domain. One rationale we can
think of is based on the comparison observation de-
scribed in section 3. For example, in the cross com-
parison experiment, we have seen that domains K, L
and N are very close. So it may be plausible to say
that the grammar of the fiction domains is mainly
representing K, L and N and, because it covers wide
syntactic structure, it gives better performance for
each of these domains. This could be the explana-
tion that the grammar of fiction domains are superior
to the own grammar for the three domains. In other
words, it is a small sampling problem, which can be
seen in the next experiment, too. Because only 24
samples are used, a single domain grammar tends to
covers relatively small part of the language phenom-
ena. On the other hands, a corpus of similar domains
could provide wider coverage for the grammar. The
assumption that the fiction domain grammar repre-
sents domains of K, L and M may explain that the
parsing result of domain P strongly favors the gram-
mar of the same domain compared to that of the 50
fiction class domains.
</bodyText>
<page confidence="0.596313">
45
</page>
<subsectionHeader confidence="0.934031">
5.2 Intensive Experiments
</subsectionHeader>
<bodyText confidence="0.81433705">
In this section, the parsing experiments on texts of
two domains are reported. The texts of the two do-
mains are parsed with several grammars, e.g. gram-
mars acquired from different domains or classes, and
different sizes of the training corpus. The size of the
training corpus is an interesting and important issue.
We can easily imagine that the smaller the training
corpus, the poorer the parsing performance. How-
ever, we don&apos;t know which of the following two types
of grammar produce better performance: a grammar
trained on a smaller corpus of the same domain, or
a grammar trained on a larger corpus including dif-
ferent domains.
Figure 5 and Figure 6 shows recall and precision of
the parsing result for the Press Reportage text. The
same text is parsed with 5 different types of gram-
mars of several variations of training corpus size. Be-
cause of corpus availability, we can not make single
domain grammars of large size training corpus, as
you can find it in the figures.
</bodyText>
<figureCaption confidence="0.998197">
Figure 7 and Figure 8 shows recall and precision
of the parsing result for the Romance and Love Story
</figureCaption>
<figure confidence="0.997806090909091">
65
60
55
o ALL
• fiction
* non-fiction
o press report
C? romance/love
0 20 40 60 80 100
Number of Samples
recall
70
65
60
o ALL
• fiction
* non-fiction
o press report
1:7 romance/love
45
0 20 40 60 80 100
Number of Samples
</figure>
<figureCaption confidence="0.998081">
Figure 5: Size and Recall (Press Report)
</figureCaption>
<figure confidence="0.98955475">
55
50
precision
70
</figure>
<figureCaption confidence="0.999157">
Figure 6: Size and Precision (Press Report)
</figureCaption>
<figure confidence="0.991306666666667">
o ALL
• fiction
* non-fiction
o press report
C2 romance/love
20 40 60 80 100
Number of Samples
recall
70
</figure>
<page confidence="0.9208725">
65
60
55
50
45
0
</page>
<figureCaption confidence="0.999251">
Figure 7: Size and Recall (Romance/Love)
</figureCaption>
<page confidence="0.881921">
99
</page>
<figure confidence="0.997019222222222">
precision
70
65
60
55
50
45
0 20 40 60 80 100
Number of Samples
</figure>
<figureCaption confidence="0.999985">
Figure 8: Size and Precision (Romance/Love)
</figureCaption>
<bodyText confidence="0.99953295945946">
text. This text is also parsed with 5 different types
of grammars.
The graph between the size of training corpus and
accuracy is generally an increasing curve with the
slope gradually flattening as the size of the corpus in-
creases. Note that the small declines of some graphs
at large number of samples are mainly due to the
memory limitation for parsing. Parsing is carried
out with the same memory size, but when the train-
ing corpus grows and the grammar becomes large,
some long sentences can&apos;t be parsed because of data
area limitation. When the data area is exhausted
during the parsing, a fitted parsing technique is used
to build the most plausible parse tree from the par-
tially parsed trees. These are generally worse than
the trees completely parsed.
It is very interesting to see that the saturation
point of any graph is about 10 to 30 samples. That
is about 20,000 to 60,000 words, or about 1,000 to
3,000 sentences. In the romance and love story do-
main, the precision of the grammar acquired from 8
samples of the same domain is only about 2% lower
than the precision of the grammar trained on 26 sam-
ples of the same domain. We believe that the reason
why the performance in this domain saturates with
such a small corpus is that there is relatively little
variety in the syntactical structure of this domain.
The order of the performance is generally the fol-
lowing: the same domain (best), the same class,
all domains, the other class and the other domain
(worst). The performance of the last two grammars
are very close in many cases. In the romance and
love story domain, the grammar acquired from the
same domain made the solo best performance. The
difference of the accuracy of the grammars of the
same domain and the other domain is quite large.
The results for the press reportage is not so obvious,
but the same tendencies can be observed.
In terms of the relationship between the size of
training corpus and domain dependency, we will
compare the performance of the grammar acquired
from 24 samples of the same domain (we will call
it &apos;baseline grammar&apos;), and that of the other gram-
mars. In the press reportage domain, one needs a
three to four times bigger corpus of all domains or
non-fiction domains to catch up to the performance
of the baseline grammar. It should be noticed that
a quarter of the non-fiction domain corpus and one
eighth of the all domain corpus consists of the press
report domain corpus. In other words, the fact that
the performance of the baseline grammar is about
the same as that of 92 samples of the non-fiction do-
mains means that in the latter grammar, the rest of
the corpus does not improve or is not harmful for
the parsing performance. In the romance and love
story domain, the wide variety grammar, in particu-
lar the fiction domain grammar quickly catch up to
the performance of the baseline grammar. It needs
only less than twice size of fiction domain corpus to
achieve the performance of the baseline grammar.
These two results and the evidence that fiction do-
mains are close in terms of structure indicate that if
you have a corpus consisting of similar domains, it is
worthwhile to include the corpus in grammar acqui-
sition, otherwise not so useful. We need to further
quantify these trade-offs in terms of the syntactic di-
versity of individual domains and the difference be-
tween domains.
We also find the small sampling problem in this
experiment. In the press reportage experiment, the
grammar acquired from the same domain does not
make the best performance when the size of the train-
ing corpus is small. We observed the same phenom-
ena in the previous experiment.
</bodyText>
<sectionHeader confidence="0.998608" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999829785714286">
One of our basic claims is the following. When
we try to parse a text in a particular domain, we
should prepare a grammar which suits that domain.
This idea naturally contrasts to the idea of robust
broad-coverage parsing (Carroll and Briscoe, 1996),
in which a single grammar should be prepared for
parsing of any kind of text. Obviously, the latter
idea has a great advantage that you do not have to
create a number of grammars for different domains
and also do not need to consider which grammar
should be used for a given text. On the other hand,
it is plausible that a domain specific grammar can
produce better results than a domain independent
grammar. Practically, the increasing availability of
</bodyText>
<figure confidence="0.9899034">
o ALL
• fiction
* non-fiction
o press report
C7 romance/love
</figure>
<page confidence="0.94248">
100
</page>
<bodyText confidence="0.9966386">
corpora provides the possibilities of creating domain
dependent grammars. Also, it should be noted that
we don&apos;t need a very large corpus to achieve a rela-
tively good quality of parsing.
To summarize our observations and experiments:
</bodyText>
<listItem confidence="0.998219523809524">
• There are domain dependencies on syntactic
structure distribution.
• Fiction domains in the Brown corpus are very
similar in terms of syntactic structure.
• We found many idiosyncratic structures from
each domain by a simple method.
• For 8 different domains, domain dependent
grammar or the grammar of the same class pro-
vide the best performance, if the size of the
training corpus is the same.
• The parsing performance is saturated at very
small size of training corpus. This is the case,
in particular, for the romance and love story do-
main.
• The order of the parsing performance is gener-
ally the following; the same domain (best), the
same class, all domain, the other class and the
other domain (worst).
• Sometime, training corpus in similar domains is
useful for grammar acquisition.
• It may not be so useful to use different domain
</listItem>
<bodyText confidence="0.951695909090909">
corpus even if the size of the corpus is relatively
large.
Undoubtedly these conclusions depend on the
parser, the corpus and the evaluation methods. Also
our experiments don&apos;t cover all domains and possi-
ble combinations. However, the observations and the
experiment suggest the significance of the notion of
domain in parsing. The results would be useful for
deciding what strategy should be taken in developing
a grammar on a &apos;domain dependent&apos; NLP application
systems.
</bodyText>
<sectionHeader confidence="0.998042" genericHeader="method">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.989085333333333">
The work reported here was supported under con-
tract 95-F145800-000 from the Office of Research
and Development. We would like to thank our
colleagues, in particular Prof.Ralph Grishman and
Ms.Sarah Taylor for valuable discussions and sug-
gestions.
</bodyText>
<sectionHeader confidence="0.979533" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.999383386363636">
Douglas Biber: 1993. Using Register-Diversified
Corpora for General Language Studies. Journal
of Computer Linguistics Vol.19, Num 2, pp219-
241.
Ezra Black, et.al: 1991. A procedure for Quanti-
tatively Comparing the Syntactic Coverage of En-
glish Grammars. Proc. of Fourth DARPA Speech
and Natural Language Workshop
John Carroll and Ted Briscoe: 1996. Apportioning
development effort in a probabilistic LR parsing
system through evaluation. Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing.
W. Nelson Francis and Henry Kucera: 1964/1979.
Manual of information to accompany A Standard
Corpus of Present-Day Edited American English.
Brown University, Department of Linguistics
Ralph Grishman and Richard Kittredge: 1986. An-
alyzing Language in Restricted Domains: Sublan-
guage Description and Processing. Lawrence Erl-
baum Associates, Publishers
Jussi Karlgren and Douglass Cutting: 1994. Rec-
ognizing Text Genres with Simple Metrics Us-
ing Discriminant Analysis. The 15th Interna-
tional Conference on Computational Linguistics,
pp1071-1075.
Richard Kittredge, Lynette Hirschman: 1983. Sub-
language: Studies of Language in Restricted Se-
mantic domains. Series of Foundations of Com-
munications, Walter de Gruyter, Berlin
Mitchell P. Marcus, Beatrice Santorini and Mary A
Marcinkiewicz: 1993. Building a Large Anno-
tated Corpus of English: The Penn TreeBank.
Computational Linguistics, 19.1, pp313-330.
Satoshi Sekine: 1996. Apple Pie Parser homepage.
http://cs.nyu.edu/cs/projects/proteus/app
Satoshi Sekine and Ralph Grishman: 1995. A
Corpus-based Probabilistic Grammar with Only
Two Non-terminals. International Workshop on
Parsing Technologies, pp216-223.
Johathan Slocum: 1986. How One Might Automat-
ically Identify and Adapt to a Sublanguage: An
Initial Exploration. Analyzing Language in Re-
stricted Domains, pp195-210.
</reference>
<page confidence="0.998035">
101
</page>
<sectionHeader confidence="0.731903" genericHeader="references">
APPENDIX
</sectionHeader>
<figure confidence="0.6949801875">
A Categories in Brown corpus
I. Informative Prose (374 samples)
A. Press: Reportage (44)
B. Press: Editorial (27)
C. Press: Reviews (17)
D. Religion (17)
E. Skills and Hobbies (36)
F. Popular Lore (48)
G. Letters,Bibliography,Memories, (75)
H. Miscellaneous (30)
J. Learned (80)
II. Imaginative Prose (126 Samples)
K. General Fiction (29)
L. Mystery and Detective Fiction (24)
M. Science Fiction ( 6)
N. Adventure and Western Fiction (29)
P. Romance and Love Story (29)
R. Humor ( 9)
B Sample of Relatively Frequent
Partial Trees
SYM. DOMAIN (num.of type;total freq. of
qualified partial trees)
ratio frequency rule (Example)
(domain/corpus)
A. Press: Reportage (30;507)
9.40 11 / 14 NP -&gt; NNPX NNX NP
9.30 7 / 9 NP -&gt; NP POS JJ NNPX
8.70 8 / 11 S -&gt; NP VBX VP NP PP
8.44 12 / 17 NP -&gt; DT $ CD NNX
&apos;The $40,000,000 budget&apos;
&apos;a 12,500 payment&apos;
8.30 77 / 111 NP -&gt; NNPX NP
</figure>
<reference confidence="0.926637451612903">
&apos;Vice President L.B. Johnson&apos;
&apos;First Lady Jacqueline Kennedy&apos;
B. Press: Editorial (20;255)
18.57 34 / 34 S -&gt; PP :
&apos;To the editor:&apos;
&apos;To the editor of New York Times:&apos;
11.14 6 / 10 NP -&gt; DT &amp;quot; ADJP &amp;quot; NNX
&apos;an &amp;quot;autistic&apos; child&apos;
&apos;a &amp;quot;stair-step&apos; plan&apos;
C. Press: Reviews (19;267)
26.27 8 / 9 WHADVP -&gt; NNPX
25.33 12 / 14 NP -&gt; NP POS &amp;quot; NNPX &amp;quot;
D. Religion (8;87)
26.83 26 / 28 S -&gt; NP -RRB- S
25.28 14 / 16 NP -&gt; NNPX CD : CD
&apos;St. Peter 1:4&apos;
&apos;St. John 3:8&apos;
E. Skills and Hobbies (17;219)
10.58 22 / 22 NP -&gt; CD NNX &amp;quot;
10.21 27 / 28 S -&gt; SBAR :
&apos;How to feed :&apos;
&apos;What it does :&apos;
F. Popular Lore (12;86)
10.58 8 / 8 NP -&gt; DT NP POS NNPX
10.58 6 / 6 NP -&gt; NNX DT NNX PP
G. Letters,Bibliography,Memories,etc (12;125)
6.59 8 / 8 WHPP -&gt; TO SBAR
&apos;to what they mean by the concept&apos;
&apos;to what may happen next&apos;
6.04 22 / 24 WHPP -&gt; OOF SBAR
&apos;of what it is all about&apos;
&apos;of what he had to show his country&apos;
H. Miscellaneous (69;2607)
16.82 70 / 70 S -&gt; NP . S
16.82 17 / 17 S -&gt; -LRB- VP . -RRB-
J. Learned (22;295)
6.51 28 / 28 NP -&gt; CD : CD
6.51 20 / 20 NP -&gt; NNX :
6.22 44 / 46 S -&gt; S -LRB- NP -RRB-
Sentence and name and year in bracket
Sentence and figure name in bracket
K. General Fiction (14;148)
11.58 7 / 10 NP -&gt; PRP S
11.03 6 / 9 S -&gt; ADVP S : : S
10.75 13 / 20 S -&gt; PP S , CC S
L. Mystery and Detective Fiction (19;229)
14.28 8 / 11 SQ -&gt; S , SQ
Tag questions
M. Science Fiction (6;57)
17.89 7 / 32 S -&gt; S , SINV
&amp;quot;&amp;quot;Forgive me, Sandalphon&amp;quot;, said Hal&amp;quot;
&amp;quot;&amp;quot;sentence&amp;quot;, remarked Helva&amp;quot;
10.22 8 / 64 S -&gt; SBARQ
N. Adventure and Western Fiction (24;422)
14.59 45 / 50 VP -&gt; VBX RB
12.97 8 / 10 VP -&gt; VBX RB PP
P. Romance and Love Story (31;556)
15.99 7 / 7 S -&gt; CC SBARQ
15.99 6 / 6 S -&gt; &amp;quot; NP VP , NP &amp;quot;
12.23 13 / 17 S -&gt; SQ S
11.99 6 / 8 S -&gt; &amp;quot; VP , NP &amp;quot;
R. Humor (3;20)
</reference>
<footnote confidence="0.90977825">
6.92 6 / 47 NP -&gt; DT ADJP NP
6.78 7 / 56 NP -&gt; PRP ODLQ
5.67 7 / 67 PP -&gt; IN &amp;quot; NP &amp;quot;
&apos;as &amp;quot;off-Broadway&apos;&amp;quot;
</footnote>
<page confidence="0.995119">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935024">
<title confidence="0.999877">The Domain Dependence of Parsing</title>
<author confidence="0.947174">Satoshi Sekine</author>
<affiliation confidence="0.994885">New York University Computer Science Department</affiliation>
<address confidence="0.998954">715 Broadway, Room 709 New York, NY 10003, USA</address>
<email confidence="0.999881">sekine@cs.nyu.edu</email>
<web confidence="0.998986">http://cs.nyu.edu/cs/projects/proteus/sekine</web>
<abstract confidence="0.999718">concern in corpus based approaches is that the applicability of the acquired knowledge may be limited by some feature of the corpus, in particular, the notion of text &apos;domain&apos;. In order to examine the domain dependence of parsing, in this paper, we report 1) Comparison of structure distributions across domains; 2) Examples of domain specific structures; and 3) Parsing experiment using some domain dependent grammars. The observations using the Brown corpus demonstrate domain dependence and idiosyncrasy of syntactic structure. The parsing results show that the best accuracy is obtained using the grammar acquired from the same domain or the same class (fiction or nonfiction). We will also discuss the relationship between parsing accuracy and the size of training corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Using Register-Diversified Corpora for General Language Studies.</title>
<date>1993</date>
<journal>Journal of Computer Linguistics Vol.19, Num</journal>
<volume>2</volume>
<pages>219</pages>
<contexts>
<context position="1890" citStr="Biber, 1993" startWordPosition="298" endWordPosition="299">s is a crucial issue for most application systems, since most systems operate within a specific domain and we are generally limited in the corpora available in that domain. There has been considerable research in this area (Kittredge and Hirschman, 1983) (Grishman and Kittredge, 1986). For example, the domain dependence of lexical semantics is widely known. It is easy to observe that usage of the word &apos;bank&apos; is different between the &apos;economic document&apos; domain and the &apos;geographic&apos; domain. Also, there are surveys of domain dependencies concerning syntax or syntaxrelated features (Slocum, 1986) (Biber, 1993) (Karlgren, 1994). It is intuitively conceivable that there are syntactic differences between &apos;telegraphic messages&apos; and &apos;press report&apos;, or between &apos;weather forecast sentences&apos; and &apos;romance and love story&apos;. But, how about the difference between &apos;press report&apos; and &apos;romance and love story&apos;? Is there a general and simple method to compare domains? More importantly, shall we prepare different knowledge for these two domain sets? In this paper, we describe two observations and an experiment which suggest an answer to the questions. Among the several types of linguistic knowledge, we are interested </context>
</contexts>
<marker>Biber, 1993</marker>
<rawString> Douglas Biber: 1993. Using Register-Diversified Corpora for General Language Studies. Journal of Computer Linguistics Vol.19, Num 2, pp219-</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ezra Black</author>
<author>et al</author>
</authors>
<title>A procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<booktitle>Proc. of Fourth DARPA Speech and Natural Language Workshop</booktitle>
<volume>19</volume>
<pages>1071--1075</pages>
<publisher>W. Nelson</publisher>
<institution>Brown University, Department of Linguistics</institution>
<location>Richard Kittredge, Lynette Hirschman:</location>
<marker>241.</marker>
<rawString> Ezra Black, et.al: 1991. A procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars. Proc. of Fourth DARPA Speech and Natural Language Workshop John Carroll and Ted Briscoe: 1996. Apportioning development effort in a probabilistic LR parsing system through evaluation. Proceedings of Conference on Empirical Methods in Natural Language Processing. W. Nelson Francis and Henry Kucera: 1964/1979. Manual of information to accompany A Standard Corpus of Present-Day Edited American English. Brown University, Department of Linguistics Ralph Grishman and Richard Kittredge: 1986. Analyzing Language in Restricted Domains: Sublanguage Description and Processing. Lawrence Erlbaum Associates, Publishers Jussi Karlgren and Douglass Cutting: 1994. Recognizing Text Genres with Simple Metrics Using Discriminant Analysis. The 15th International Conference on Computational Linguistics, pp1071-1075. Richard Kittredge, Lynette Hirschman: 1983. Sublanguage: Studies of Language in Restricted Semantic domains. Series of Foundations of Communications, Walter de Gruyter, Berlin Mitchell P. Marcus, Beatrice Santorini and Mary A Marcinkiewicz: 1993. Building a Large Annotated Corpus of English: The Penn TreeBank. Computational Linguistics, 19.1, pp313-330. Satoshi Sekine: 1996. Apple Pie Parser homepage. http://cs.nyu.edu/cs/projects/proteus/app Satoshi Sekine and Ralph Grishman: 1995. A Corpus-based Probabilistic Grammar with Only Two Non-terminals. International Workshop on Parsing Technologies, pp216-223. Johathan Slocum: 1986. How One Might Automatically Identify and Adapt to a Sublanguage: An Initial Exploration. Analyzing Language in Restricted Domains, pp195-210. &apos;Vice President L.B. Johnson&apos; &apos;First Lady Jacqueline Kennedy&apos; B. Press: Editorial (20;255)</rawString>
</citation>
<citation valid="false">
<title>To the editor:&apos; &apos;To the editor of</title>
<volume>57</volume>
<editor>S -&gt; PP :</editor>
<location>New York Times:&apos;</location>
<marker>18.</marker>
<rawString>57 34 / 34 S -&gt; PP : &apos;To the editor:&apos; &apos;To the editor of New York Times:&apos;</rawString>
</citation>
<citation valid="true">
<title>NP -&gt; DT &amp;quot; ADJP &amp;quot; NNX &apos;an &amp;quot;autistic&apos; child&apos; &apos;a &amp;quot;stair-step&apos; plan&apos; C. Press: Reviews</title>
<date></date>
<volume>14</volume>
<marker>11.</marker>
<rawString>14 6 / 10 NP -&gt; DT &amp;quot; ADJP &amp;quot; NNX &apos;an &amp;quot;autistic&apos; child&apos; &apos;a &amp;quot;stair-step&apos; plan&apos; C. Press: Reviews (19;267)</rawString>
</citation>
<citation valid="false">
<journal>WHADVP -&gt; NNPX</journal>
<volume>27</volume>
<marker>26.</marker>
<rawString>27 8 / 9 WHADVP -&gt; NNPX</rawString>
</citation>
<citation valid="false">
<journal>NP -&gt; NP POS &amp;quot; NNPX &amp;quot; D. Religion</journal>
<volume>33</volume>
<marker>25.</marker>
<rawString>33 12 / 14 NP -&gt; NP POS &amp;quot; NNPX &amp;quot; D. Religion (8;87)</rawString>
</citation>
<citation valid="false">
<journal>83 26 / 28 S -&gt; NP -RRB- S</journal>
<marker>26.</marker>
<rawString>83 26 / 28 S -&gt; NP -RRB- S</rawString>
</citation>
<citation valid="true">
<title>3:8&apos; E. Skills and Hobbies</title>
<date></date>
<journal>NP -&gt; NNPX CD : CD &apos;St. Peter</journal>
<volume>28</volume>
<publisher>St. John</publisher>
<marker>25.</marker>
<rawString>28 14 / 16 NP -&gt; NNPX CD : CD &apos;St. Peter 1:4&apos; &apos;St. John 3:8&apos; E. Skills and Hobbies (17;219)</rawString>
</citation>
<citation valid="false">
<journal>22 NP -&gt; CD NNX &amp;quot;</journal>
<volume>58</volume>
<marker>10.</marker>
<rawString>58 22 / 22 NP -&gt; CD NNX &amp;quot;</rawString>
</citation>
<citation valid="true">
<title>S -&gt; SBAR : &apos;How to feed :&apos; &apos;What it does :&apos; F. Popular Lore</title>
<date></date>
<volume>21</volume>
<marker>10.</marker>
<rawString>21 27 / 28 S -&gt; SBAR : &apos;How to feed :&apos; &apos;What it does :&apos; F. Popular Lore (12;86)</rawString>
</citation>
<citation valid="false">
<journal>NP -&gt; DT NP POS NNPX</journal>
<volume>58</volume>
<marker>10.</marker>
<rawString>58 8 / 8 NP -&gt; DT NP POS NNPX</rawString>
</citation>
<citation valid="true">
<date></date>
<journal>NP -&gt; NNX DT NNX PP G. Letters,Bibliography,Memories,etc</journal>
<volume>58</volume>
<marker>10.</marker>
<rawString>58 6 / 6 NP -&gt; NNX DT NNX PP G. Letters,Bibliography,Memories,etc (12;125)</rawString>
</citation>
<citation valid="false">
<title>WHPP -&gt; TO SBAR &apos;to what they mean by the concept&apos; &apos;to what may happen next&apos;</title>
<volume>59</volume>
<marker>6.</marker>
<rawString>59 8 / 8 WHPP -&gt; TO SBAR &apos;to what they mean by the concept&apos; &apos;to what may happen next&apos;</rawString>
</citation>
<citation valid="false">
<title>WHPP -&gt; OOF SBAR &apos;of what it is all about&apos; &apos;of what he had to show his country&apos; H. Miscellaneous (69;2607)</title>
<volume>04</volume>
<marker>6.</marker>
<rawString>04 22 / 24 WHPP -&gt; OOF SBAR &apos;of what it is all about&apos; &apos;of what he had to show his country&apos; H. Miscellaneous (69;2607)</rawString>
</citation>
<citation valid="false">
<journal>S -&gt; NP . S</journal>
<volume>82</volume>
<marker>16.</marker>
<rawString>82 70 / 70 S -&gt; NP . S</rawString>
</citation>
<citation valid="false">
<journal>S -&gt; -LRB- VP . -RRBJ. Learned</journal>
<volume>82</volume>
<marker>16.</marker>
<rawString>82 17 / 17 S -&gt; -LRB- VP . -RRBJ. Learned (22;295)</rawString>
</citation>
<citation valid="false">
<journal>28 NP -&gt; CD : CD</journal>
<volume>51</volume>
<marker>6.</marker>
<rawString>51 28 / 28 NP -&gt; CD : CD</rawString>
</citation>
<citation valid="false">
<journal>20 NP -&gt; NNX :</journal>
<volume>51</volume>
<marker>6.</marker>
<rawString>51 20 / 20 NP -&gt; NNX :</rawString>
</citation>
<citation valid="true">
<title>S -&gt; S -LRB- NP -RRBSentence and name and year in bracket Sentence and figure name in bracket K. General Fiction</title>
<date></date>
<volume>22</volume>
<marker>6.</marker>
<rawString>22 44 / 46 S -&gt; S -LRB- NP -RRBSentence and name and year in bracket Sentence and figure name in bracket K. General Fiction (14;148)</rawString>
</citation>
<citation valid="false">
<journal>10 NP -&gt; PRP S</journal>
<volume>58</volume>
<marker>11.</marker>
<rawString>58 7 / 10 NP -&gt; PRP S</rawString>
</citation>
<citation valid="false">
<journal>S -&gt; ADVP S : : S</journal>
<volume>03</volume>
<marker>11.</marker>
<rawString>03 6 / 9 S -&gt; ADVP S : : S</rawString>
</citation>
<citation valid="true">
<date></date>
<journal>75 13 / 20 S -&gt; PP S , CC S L. Mystery and Detective Fiction</journal>
<marker>10.</marker>
<rawString>75 13 / 20 S -&gt; PP S , CC S L. Mystery and Detective Fiction (19;229)</rawString>
</citation>
<citation valid="false">
<journal>11 SQ -&gt; S , SQ Tag questions M. Science Fiction</journal>
<volume>28</volume>
<marker>14.</marker>
<rawString>28 8 / 11 SQ -&gt; S , SQ Tag questions M. Science Fiction (6;57)</rawString>
</citation>
<citation valid="false">
<title>Forgive me, Sandalphon&amp;quot;, said Hal&amp;quot; &amp;quot;&amp;quot;sentence&amp;quot;, remarked Helva&amp;quot;</title>
<journal>32 S -&gt; S , SINV</journal>
<volume>89</volume>
<marker>17.</marker>
<rawString>89 7 / 32 S -&gt; S , SINV &amp;quot;&amp;quot;Forgive me, Sandalphon&amp;quot;, said Hal&amp;quot; &amp;quot;&amp;quot;sentence&amp;quot;, remarked Helva&amp;quot;</rawString>
</citation>
<citation valid="false">
<journal>64 S -&gt; SBARQ N. Adventure and Western Fiction</journal>
<volume>22</volume>
<pages>24--422</pages>
<marker>10.</marker>
<rawString>22 8 / 64 S -&gt; SBARQ N. Adventure and Western Fiction (24;422)</rawString>
</citation>
<citation valid="false">
<journal>59 45 / 50 VP -&gt; VBX RB</journal>
<marker>14.</marker>
<rawString>59 45 / 50 VP -&gt; VBX RB</rawString>
</citation>
<citation valid="false">
<journal>10 VP -&gt; VBX RB PP P. Romance and Love Story</journal>
<volume>97</volume>
<pages>31--556</pages>
<marker>12.</marker>
<rawString>97 8 / 10 VP -&gt; VBX RB PP P. Romance and Love Story (31;556)</rawString>
</citation>
<citation valid="false">
<journal>S -&gt; CC SBARQ</journal>
<volume>99</volume>
<marker>15.</marker>
<rawString>99 7 / 7 S -&gt; CC SBARQ</rawString>
</citation>
<citation valid="false">
<journal>S -&gt; &amp;quot; NP VP , NP &amp;quot;</journal>
<volume>99</volume>
<marker>15.</marker>
<rawString>99 6 / 6 S -&gt; &amp;quot; NP VP , NP &amp;quot;</rawString>
</citation>
<citation valid="false">
<journal>S -&gt; SQ S</journal>
<volume>23</volume>
<marker>12.</marker>
<rawString>23 13 / 17 S -&gt; SQ S</rawString>
</citation>
<citation valid="false">
<journal>S -&gt; &amp;quot; VP , NP &amp;quot; R. Humor</journal>
<volume>99</volume>
<marker>11.</marker>
<rawString>99 6 / 8 S -&gt; &amp;quot; VP , NP &amp;quot; R. Humor (3;20)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>