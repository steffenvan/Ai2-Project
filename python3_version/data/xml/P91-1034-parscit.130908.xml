<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005933">
<sectionHeader confidence="0.966136" genericHeader="abstract">
WORD-SENSE DISAMBIGUATION USING STATISTICAL
METHODS
</sectionHeader>
<bodyText confidence="0.874072">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer
IBM Thomas J. Watson Research Center
P.O. Box 704
Yorktown Heights, NY 10598
</bodyText>
<sectionHeader confidence="0.973859" genericHeader="keywords">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9998738">
We describe a statistical technique for assign-
ing senses to words. An instance of a word is as-
signed a sense by asking a question about the con-
text in which the word appears. The question is
constructed to have high mutual information with
the translation of that instance in another lan-
guage. When we incorporated this method of as-
signing senses into our statistical machine transla-
tion system, the error rate of the system decreased
by thirteen percent.
</bodyText>
<sectionHeader confidence="0.998074" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.997117224489796">
An alluring aspect of the statistical ap-
proach to machine translation rejuvenated by
Brown et al. [Brown et at., 1988, Brown et al.,
1990] is the systematic framework it provides
for attacking the problem of lexical disam-
biguation. For example, the system they de-
scribe translates the French sentence Je vais
prendre la decision as I will make the decision,
correctly interpreting prendre as make. The
statistical translation model, which supplies
English translations of French words, prefers
the more common translation take, but the
trigram language model recognizes that the
three-word sequence make the decision is much
more probable than take the decision..
The system is not always so successful. It
incorrectly renders Je vais prendre ma propre
decision as / will take my own decision. The
language model does not realize that take my
own decision is improbable because take and
decision no longer fall within a single trigram.
Errors such as this are common because
the statistical models only capture local phe-
nomena; if the context necessary to determine
a translation falls outside the scope of the
models, the word is likely to be translated in-
correctly. However, if the relevant context is
encoded locally, the word should be translated
correctly. We can achieve this within the tra-
ditional paradigm of analysis, transfer, and
synthesis by incorporating into the analysis
phase a sense-disambiguation component that
assigns sense labels to French words. If pren-
dre is labeled with one sense in the context
of decision but with a different sense in other
contexts, then the translation model will learn
from training data that the first sense usually
translates to make, whereas the other sense
usually translates to take.
Previous efforts at algorithmic disambigua-
tion of word senses [Lesk, 1986, White, 1988,
He and â€¢Veronis, 1990] have concentrated on
information that can be extracted from elec-
tronic dictionaries, and focus, therefore, on
senses as determined by those dictionaries.
Here, in contrast, we present a procedure for
constructing a sense-disambiguation compo-
nent that labels words so as to elucidate their
translations in another language. We are con-
</bodyText>
<page confidence="0.998635">
264
</page>
<figureCaption confidence="0.999886">
Figure 1: Alignment Example
</figureCaption>
<bodyText confidence="0.9818601">
The proposal will not now be implemented
Les propositions ne seront pas mises en application maintenant
cerned about senses as they occur in a dic-
tionary only to the extent that those senses
are translated differently. The French noun
interet, for example, is translated into Ger-
man as either Zins or Interesse according to
its sense, but both of these senses are trans-
lated into English as interest, and so we make
no attempt to distinguish them.
</bodyText>
<sectionHeader confidence="0.909507" genericHeader="method">
STATISTICAL TRANSLATION
</sectionHeader>
<bodyText confidence="0.990942">
Following Brown et al. [Brown et al., 1994
we choose as the translation of a French sen-
tence F that sentence E for which Pr (EIF)
is greatest. By Bayes&apos; rule,
</bodyText>
<equation confidence="0.977435333333333">
Pr (EI F)
Pr (E) Pr (FIE). (i)
pr (F)
</equation>
<bodyText confidence="0.994035181818182">
Since the denominator does not depend on
E, the sentence for which Pr (EIF) is great-
est is also the sentence for which the product
Pr (E) Pr (FIE) is greatest. The first factor
in this product is a statistical characteriza-
tion of the English language and the second
factor is a statistical characterization of the
process by which English sentences are trans-
lated into French. We can compute neither
factors precisely. Rather, in statistical trans-
lation, we employ models from which we can
obtain estimates of these values. We call the
model from which we compute Pr (E) the lan-
guage model and that from which we compute
Pr (FIE) the translation model.
The translation model used by Brown et al.
[Brown et al., 1990] incorporates the concept
of an alignment in which each word in E acts
independently to produce some of the words
in F. If we denote a typical alignment by A,
then we can write the probability of F given
E as a sum over all possible alignments:
</bodyText>
<equation confidence="0.848417">
Pr (FIE) = E Pr (F, A1E) . (2)
A
</equation>
<bodyText confidence="0.989670611111111">
Although the number of possible alignments is
a very rapidly growing function of the lengths
of the French and English sentences, only a
tiny fraction of the alignments contributes sub-
stantially to the sum, and of these few, one
makes the greatest contribution. We call this
most probable alignment the Viterbi align-
ment between E and F.
The identity of the Viterbi alignment for
a pair of sentences depends on the details of
the translation model, but once the model is
known, probable alignments can be discovered
algorithmically [Brown et al., 1991]. Brown
et al. [Brown et al., 1990], show an example
of such au automatically derived alignment in
their Figure 3. (For the reader&apos;s convenience,
we have reproduced that figure here as Figure
1.)
</bodyText>
<page confidence="0.996828">
265
</page>
<bodyText confidence="0.999828083333333">
In a Viterbi alignment, a French word that
is connected by a line to an English word is
said to be aligned with that English word.
Thus, in Figure 1, Les is aligned with The,
propositions with proposal, and so on. We call
a pair of aligned words obtained in this wa.y a
connection.
From the Viterbi alignments for 1,002,165
pairs of short French and English sentences
from the Canadian Hansard data [Brown et al.,
1990], we have extracted a set of 12,028,485
connections. Let p(e, f) be the probability
that a connection chosen at random from this
set will connect the English word e to the
French word f. Because each French word
gives rise to exactly one connection, the right
marginal of this distribution is identical to
the distribution of French words in these sen-
tences. The left marginal, however, is not
the same as the distribution of English words:
English words that tend to produce several
French words at a time are overrepresented
while those that tend to produce no French
words are underrepresented.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="method">
SENSES BASED ON BINARY
QUESTIONS
</sectionHeader>
<bodyText confidence="0.998801910447761">
Using p(e, f) we can compute the mutual
information between a French word and its
English mate in a connection. In this section,
we discuss a method for labelling a word with
a sense that depends on the context in which
it appears in such a way as to increase the
mutual information between the members of
a connection.
In the sentence Je vais prendre ma pro-
pre decision, the French verb prendre should
be translated as make because the object of
prendre is decision. If we replace decision by
voiture, then prendre should be translated as
take to yield I will take my own car. In these
examples, one can imagine assigning a sense
to prendre by asking whether the first noun to
the right of prendre is decision OT voiture. We
say that the noun to the right is the informant
for prendre.
In Il doute que les notres gagnent, which.
means He doubts that we will win, the French
word il should be translated as he. On the
other hand, in Il faut que les notres gagnent,
which means it is necessary that we win, il
should be translated as it. Here, we can de-
termine which sense to assign to il by asking
about the identity of the first verb to its right.
Even though we cannot hope to determine the
translation of il from this informant unam-
biguously, we can hope to obtain a significant
amount of information about the translation.
As a -final example, consider the English
word is. In the sentence I think it is a prob-
lem, it is best to translate is as est as in Je
pense que c&apos;est un ,probleme. However, this is
certainly not true in the sentence I think there
is a problem, which translates as Je pense qu&apos;ll
y a un probleme. Here we can reduce the en-
tropy of the distribution of the translation of
is by asking if the word to the left is there. If
so, then is is less likely to be translated as est
than if not.
Motivated by examples like these, we in-
vestigated a simple method of assigning two
senses to a word w by asking a single binary
question about one word of the context in
which w appears. One does not know before-
hand whether the informant will be the first
noun to the right, the first verb to the right,
or some other word. in the context of w. How-
ever, one can construct a question for each of
a number of candidate informant sites, and
then choose the most informative question.
Given a potential informant such as the
first 1101111 to the right, we can construct a
question that has high mutual information with
the translation of w by using the flip-flop algo-
rithm devised by Nadas, Nahamoo, Picheny,
and Powell [Nadas et al., 1991]. To under-
stand their algorithm, first imagine that w is a
French word and that English words which are
possible translations of w have been divided
into two classes. Consider the problem of con-
structing a binary question about the poten-
tial informant that provides maximal informa-
tion about these two English word classes. If
the French vocabulary is of size V, then there
</bodyText>
<page confidence="0.990338">
266
</page>
<bodyText confidence="0.99981744">
are 2&amp;quot; possible questions. However, using the
splitting theorem of Breiman, Friedman, 01-
shen, and Stone [Breiman et aL, 1984], it is
possible to find the most informative of these
2v questions in time which is linear in V.
The flip-flop algorithm begins by making
an initial assignment of the English transla-
tions into two classes, and then uses the split-
ting theorem to find the best question about
the potential informant. This question divides
the French vocabulary into two sets. One can
then use the splitting theorem to find a di-
vision of the English translations of w into
two sets which has maximal mutual informa-
tion with the French sets. In the flip-flop al-
gorithm, one alternates between splitting the
French vocabulary into two sets and the En-
glish translations of w into two sets. After
each such split, the mutual information be-
tween the French and English sets is at least
as great as before the split. Since the mutual
information is bounded by one bit, the process
converges to a partition of the French vocab-
ulary that has high mutual information with
the translation of w.
</bodyText>
<sectionHeader confidence="0.983089" genericHeader="method">
A PILOT EXPERIMENT
</sectionHeader>
<bodyText confidence="0.999555761904762">
We used the flip-flop algorithm in a pilot
experiment in which we assigned two senses to
each of the 500 most common English words
and two senses to each of the 200 most com-
mon French words.
For a French word, we considered ques-
tions about seven informants: the word to the
left, the word to the right, the first noun to
the left, the first noun to the right, the first
verb to the left, the first verb to the right,
and the tense of either the current word, if it
is a verb, or of the first verb to the left of the
current word. For an English word, we only
considered questions about the the word to
the left and the word two to the left. We re-
stricted the English questions to the previous
two words so that we could easily use them
in our translation system which produces an
English sentence from left to right. When
a. potential informant did not exist, because,
say there wa,s no noun to the left of some
</bodyText>
<table confidence="0.969215535714286">
Word: prendre
Informant: Right noun
Information: .381 bits
Sense 1 Sense 2
TERM_WORD
mesure
note
exemple
temps
initiative
part
decision
parole
connaissance
engagement
fin
retraite
Common informant values for each sense
Pr (English I Sense 1) Pr (English (Sense 2)
to_take .433 to_make .186
to_ma,ke .061 to_speak .105
to_do .051 to_rise .066
to_be .045 to_take .066
to_be .058
decision .036
to_get .025
to_have .021
Probabilities of English translations
</table>
<figureCaption confidence="0.989737">
Figure 2: Senses for the French word prendre
</figureCaption>
<bodyText confidence="0.979146352941176">
word in a particular sentence, we used the spe-
cial word, TERM...WORD. To find the nouns
and verbs in our French sentences, we used
the tagging algorithm described by Merialdo
[Merialdo, 1990].
Figure 2 shows the question that was con-
structed for the verb prendre. The noun to
the right yielded the most information, .381
bits, about the English translation of prendre.
The box in the top of the figure shows the
words which most frequently occupy that site,
that is, the nouns which appear to the right
of prendre with a probability greater than one
part in fifty. An instance of prendre is assigned
the first or second sense depending on whether
the first noun to the right appears in the left-
hand or the right-hand column. So, for ex-
</bodyText>
<page confidence="0.984138">
267
</page>
<figure confidence="0.971377181818182">
Word: vouloir Word : depu is
Informant: Verb tense Informant: Word to the right
Information: .349 bits Information: .738 bits
Common informant values for each sense Common informant values for each sense
Sense 1
3rd p sing present
1st p sing present
3rd p plur present
1st p Our present
2nd p plur present
3rd p sing imperfect
1st p sing imperfect
3rd p sing future
Sense 2
1st p sing conditional
3rd p sing conditional
3rd p plur conditional
3rd p plur subjunctive
1st p plur conditional
Sense 1 Sense 2
longtemps
de
un
quelques
deux
1
plus
trois
le
la
ce
les
1968
</figure>
<table confidence="0.966346777777778">
Pr (English I Sense 1)
Pr (English I Sense 2)
Probabilities of English translations
for .432 since .772
last .123 from .040
long .102
past .078
over .027
in .022
overdue .021
Pr (English I Sense 1) Pr (English I Sense 2)
to_want .484 .391
to_mean .056 to_want .169
to_be .056 to_have .083
to_wish .033 to_wish .066
to_refer .022 me .029
to_like .020
Probabilities of English translations
</table>
<figureCaption confidence="0.989278">
Figure 3: Senses for the French word vouloir
</figureCaption>
<bodyText confidence="0.991552111111111">
ample, if the noun to the right of prendre is
decision, parole, or connaissance, then pren-
dre is assigned the second sense. The box at
the bottom of the figure shows the most prob-
able translations of each of the two senses.
Notice that the English verb to_make is three
times as likely when prendre has the second
sense as when it has the first sense. People
make decisions, speeches, and acquaintances,
they do not take them.
Figure 3 shows our results for the verb
vouloir. Here, the best informant is the tense
of vouloir. The first sense is three times more
likely than the second sense to translate as
to_want, but twelve times less likely to trans-
late as to like. In polite English, one says I
would like so and so more commonly than I
would want so and so.
</bodyText>
<figureCaption confidence="0.995472">
Figure 4: Senses for the French word depuis
</figureCaption>
<bodyText confidence="0.9999676875">
The question in Figure 4 reduces the en-
tropy of the translation of the French prepo-
sition depuis by .738 bits. When depuis is fol-
lowed by an article, it translates with proba-
bility .772 to since, and otherwise only with
probability .016.
Finally, consider the English word cent. In
our text, it is either a denomination of cur-
rency, in which case it is usually preceded by
a number and translated as c., or it is the
second half of per cent, in which case it is pre-
ceded by per and translated along with per as
Z. The results in Figure 5 show that the al-
gorithm has discovered this, and in so doing
has reduced the entropy of the translation of
cent by .378 bits.
</bodyText>
<page confidence="0.985571">
268
</page>
<figure confidence="0.976604153846154">
Word: cent
Informant: Word to the left
Information: .378 bits
Sense 1 Sense 2
per
8
5
2
a
one
4
7
Common informant values for each sense
</figure>
<bodyText confidence="0.9997974">
to a word, and thus can extract no more than
one bit of information about the translation of
that, word. Since the entropy of the transla-
tion of a. common word can be as high as five
bits, there is reason to hope that using more
senses will further improve the performance of
our system. Our method asks a. single ques-
tion about a single word of context. We can
think of this as the first question in a deci-
sion tree which can be extended to additional
levels [Lucassen, 1983, Lucassen and Mercer,
1984, Breiman et al., 1984, Bahl et a/., 1989].
We are working on these and other improve-
ments and hope to report better results in the
future.
</bodyText>
<sectionHeader confidence="0.963188" genericHeader="conclusions">
REFERENCES
</sectionHeader>
<figure confidence="0.7606892">
Pr (French I Sense 1) Pr (French. I Sense 2)
.891 C. .592
cent .239
sou .046
.022
</figure>
<figureCaption confidence="0.842514">
Probabilities of French translations
Figure 5: Senses for the English word cent
</figureCaption>
<bodyText confidence="0.9999220625">
Pleased with these results, we incorporated
sense-assignment questions for the 500 most
common English words and 200 most com-
mon French words into our translation sys-
tem. This system is an enhanced version of
the one described by Brown et al. [Brown
et al., 1990] in that it uses a trigram lan-
guage model, and has a French vocabulary of
57,802 words, a.nd an English vocabulary of
40,809 words. We translated 100 randomly
selected Hansard sentences each of which is
10 words or less in length. We judged 45
of the resultant translations as acceptable as
compared with 37 acceptable translations pro-
duced by the same system running without
sense-disambiguation questions.
</bodyText>
<sectionHeader confidence="0.992631" genericHeader="references">
FUTURE WORK
</sectionHeader>
<bodyText confidence="0.991194">
Although our results are promising, this
particular method of assigning senses to words
is quite limited. It assigns at most two senses
[Bahl et al., 1989] Baltl, L., Brown,
P., de Souza, P., and Mercer, R. (1989).
A tree-based statistical language model for
natural language speech recognition. IEEE
</bodyText>
<reference confidence="0.967694074074074">
Transactions on Acoustics, Speech and Sig-
nal Processing, 37:1001-1008.
[Breima,n et al., 1984] Breiman, L., Fried-
man, J. H., Olshen, R. A., and Stone,
C. J. (1984). Classification and Regres-
sion Trees. Wadsworth &amp; Brooks/Cole Ad-
vanced Books /tz Software, Monterey, Cali-
fornia.
[Brown et al., 1990] Brown, P. F., Cocke, J.,
DellaPietra, S. A., DellaPietra, V. J., Je-
linek, F., Lafferty, J. D., Mercer, R. L.,
and Roossin, P. S. (1990). A statistical ap-
proach to machine translation. Computa-
tional Linguistics, 16(2):79-85.
[Brown et al., 1988] Brown, P. F., Cocke, J.,
DellaPietra, S. A., DellaPietra, V. J., Je-
linek, F., Mercer, R. L., and Roossin, P. S.
(1988). A statistical approach to language
translation. In Proceedings of the 12th In-
ternational Conference on Computational
Linguistics, Budapest, Hungary.
[Brown et al., 1991] Blown, P. F., DellaPi-
etra, S. A., Della.Pietra, V. J., and Mercer,
H. L. (1991). Parameter estimation for ma-
chine translation. In preparation.
[Edo and Veronis, 1990] Ide, N. and Veronis,
J. (1990). Mapping dictionaires: A spread-
</reference>
<page confidence="0.98182">
269
</page>
<reference confidence="0.946535179487179">
ing activation approach. Ti Proceedings of
the Sixth Annual Conferenre of the 1111.
Centre for the New Oxford English Dictio-
nary and Text Research, pages 52-64, Wa-
terloo, Canada.
[Lesk, 1986) Lesk, M. E. (1986). Auto-
mated sense disambiguation using machine-
readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceed-
ings of the SIGDOC Conference.
[Lucassen, 1983] Lucassen, J. M. (1983). Dis-
covering phonemic baseforms automati-
cally: an information theoretic approach.
Technical Report RC 9833, IBM Research
Division.
[Lucassen and Mercer, 19841 Lucassen, J. M.
and Mercer, R. L. (1984). An information
theoretic approach to automatic determi-
nation of phonemic baseforms. In Proceed-
ings of the IEEE International Conference
on Acoustics, Speech and Signal Processing,
pages 42.5.1-42.5.4, San Diego, California.
[Merialdo, 19901 Merialdo, B. (1990). Tag-
ging text with a probabilistic model. In
Proceedings of the IBM Natural Language
ITL, pages 161-172, Paris, France.
[Nadas et al., 1991] Nadas, A., Nahamoo,
D., Picheny, M. A., and Powell, J. (1991).
An iterative &amp;quot;flip-flop&amp;quot; approximation of
the most informative split in the construc-
tion of decision trees. In Proceedings of the
IEEE International Conference on Acous-
tics, Speech and Signal Processing, Toronto,
Canada.
[White, 1988] White, J. S. (1988). Deter-
mination of lexical-semantic relations for
multi-lingual terminology structures. In
Relational Models of the Lexicon. Cam-
bridge University Press, Cambridge, UK.
</reference>
<page confidence="0.996834">
270
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960669">
<title confidence="0.997061">WORD-SENSE DISAMBIGUATION USING STATISTICAL METHODS</title>
<author confidence="0.999741">Peter F Brown</author>
<author confidence="0.999741">Stephen A Della Pietra</author>
<author confidence="0.999741">Vincent J Della Pietra</author>
<author confidence="0.999741">Robert L Mercer</author>
<affiliation confidence="0.99964">IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.9925005">P.O. Box 704 Yorktown Heights, NY 10598</address>
<abstract confidence="0.998318272727273">We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Transactions on Acoustics, Speech and Signal Processing,</booktitle>
<pages>37--1001</pages>
<marker></marker>
<rawString>Transactions on Acoustics, Speech and Signal Processing, 37:1001-1008.</rawString>
</citation>
<citation valid="true">
<title>Classification and Regression Trees.</title>
<date>1984</date>
<booktitle>Wadsworth &amp; Brooks/Cole Advanced Books /tz Software,</booktitle>
<location>Monterey, California.</location>
<marker>1984</marker>
<rawString>[Breima,n et al., 1984] Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984). Classification and Regression Trees. Wadsworth &amp; Brooks/Cole Advanced Books /tz Software, Monterey, California.</rawString>
</citation>
<citation valid="true">
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<marker>1990</marker>
<rawString>[Brown et al., 1990] Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra, V. J., Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. (1990). A statistical approach to machine translation. Computational Linguistics, 16(2):79-85.</rawString>
</citation>
<citation valid="true">
<title>A statistical approach to language translation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics,</booktitle>
<location>Budapest, Hungary.</location>
<marker>1988</marker>
<rawString>[Brown et al., 1988] Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra, V. J., Jelinek, F., Mercer, R. L., and Roossin, P. S. (1988). A statistical approach to language translation. In Proceedings of the 12th International Conference on Computational Linguistics, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<title>Parameter estimation for machine translation. In preparation.</title>
<date>1991</date>
<marker>1991</marker>
<rawString>[Brown et al., 1991] Blown, P. F., DellaPietra, S. A., Della.Pietra, V. J., and Mercer, H. L. (1991). Parameter estimation for machine translation. In preparation.</rawString>
</citation>
<citation valid="true">
<title>Mapping dictionaires: A spreading activation approach.</title>
<date>1990</date>
<booktitle>Ti Proceedings of the Sixth Annual Conferenre of the 1111. Centre for the New Oxford English Dictionary and Text Research,</booktitle>
<pages>52--64</pages>
<location>Waterloo, Canada.</location>
<marker>1990</marker>
<rawString>[Edo and Veronis, 1990] Ide, N. and Veronis, J. (1990). Mapping dictionaires: A spreading activation approach. Ti Proceedings of the Sixth Annual Conferenre of the 1111. Centre for the New Oxford English Dictionary and Text Research, pages 52-64, Waterloo, Canada.</rawString>
</citation>
<citation valid="true">
<title>Automated sense disambiguation using machinereadable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference.</booktitle>
<marker>1986</marker>
<rawString>[Lesk, 1986) Lesk, M. E. (1986). Automated sense disambiguation using machinereadable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference.</rawString>
</citation>
<citation valid="true">
<title>Discovering phonemic baseforms automatically: an information theoretic approach.</title>
<date>1983</date>
<tech>Technical Report RC 9833,</tech>
<institution>IBM Research Division.</institution>
<marker>1983</marker>
<rawString>[Lucassen, 1983] Lucassen, J. M. (1983). Discovering phonemic baseforms automatically: an information theoretic approach. Technical Report RC 9833, IBM Research Division.</rawString>
</citation>
<citation valid="true">
<title>An information theoretic approach to automatic determination of phonemic baseforms.</title>
<date>1984</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>42--5</pages>
<location>San Diego, California.</location>
<marker>1984</marker>
<rawString>[Lucassen and Mercer, 19841 Lucassen, J. M. and Mercer, R. L. (1984). An information theoretic approach to automatic determination of phonemic baseforms. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 42.5.1-42.5.4, San Diego, California.</rawString>
</citation>
<citation valid="true">
<title>Tagging text with a probabilistic model.</title>
<date>1990</date>
<booktitle>In Proceedings of the IBM Natural Language ITL,</booktitle>
<pages>161--172</pages>
<location>Paris, France.</location>
<marker>1990</marker>
<rawString>[Merialdo, 19901 Merialdo, B. (1990). Tagging text with a probabilistic model. In Proceedings of the IBM Natural Language ITL, pages 161-172, Paris, France.</rawString>
</citation>
<citation valid="true">
<title>An iterative &amp;quot;flip-flop&amp;quot; approximation of the most informative split in the construction of decision trees.</title>
<date>1991</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<location>Toronto, Canada.</location>
<marker>1991</marker>
<rawString>[Nadas et al., 1991] Nadas, A., Nahamoo, D., Picheny, M. A., and Powell, J. (1991). An iterative &amp;quot;flip-flop&amp;quot; approximation of the most informative split in the construction of decision trees. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<title>Determination of lexical-semantic relations for multi-lingual terminology structures.</title>
<date>1988</date>
<booktitle>In Relational Models of the Lexicon.</booktitle>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>1988</marker>
<rawString>[White, 1988] White, J. S. (1988). Determination of lexical-semantic relations for multi-lingual terminology structures. In Relational Models of the Lexicon. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>