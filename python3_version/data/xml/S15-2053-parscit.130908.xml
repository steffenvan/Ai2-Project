<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000146">
<title confidence="0.984734">
SemEval-2015 Task 15: A Corpus Pattern Analysis
Dictionary-Entry-Building Task
</title>
<author confidence="0.981568">
Vit Baisa
</author>
<affiliation confidence="0.979575">
Masaryk University
</affiliation>
<email confidence="0.963856">
xbaisa@fi.muni.cz
</email>
<author confidence="0.99345">
Ismail El Maarouf
</author>
<affiliation confidence="0.998337">
University of Wolverhampton
</affiliation>
<email confidence="0.994263">
i.el-maarouf@wlv.ac.uk
</email>
<author confidence="0.990326">
Jane Bradbury
</author>
<affiliation confidence="0.996359">
University of Wolverhampton
</affiliation>
<email confidence="0.940874">
J.Bradbury3@wlv.ac.uk
</email>
<author confidence="0.949528">
Adam Kilgarriff
</author>
<affiliation confidence="0.898185">
Lexical Computing Ltd
</affiliation>
<email confidence="0.990453">
adam@sketchengine.co.uk
</email>
<author confidence="0.920942">
Silvie Cinkov´a
</author>
<affiliation confidence="0.918762">
Charles University
</affiliation>
<email confidence="0.9536">
cinkova@ufal.mff.cuni.cz
</email>
<author confidence="0.94609">
Octavian Popescu
</author>
<affiliation confidence="0.903623">
IBM Research Center
</affiliation>
<email confidence="0.992824">
o.popescu@us.ibm.com
</email>
<sectionHeader confidence="0.997347" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999615866666667">
This paper describes the first SemEval task to
explore the use of Natural Language Process-
ing systems for building dictionary entries,
in the framework of Corpus Pattern Analysis.
CPA is a corpus-driven technique which pro-
vides tools and resources to identify and rep-
resent unambiguously the main semantic pat-
terns in which words are used. Task 15 draws
on the Pattern Dictionary of English Verbs
(www.pdev.org.uk), for the targeted lexi-
cal entries, and on the British National Corpus
for the input text.
Dictionary entry building is split into three
subtasks which all start from the same con-
cordance sample: 1) CPA parsing, where ar-
guments and their syntactic and semantic cate-
gories have to be identified, 2) CPA clustering,
in which sentences with similar patterns have
to be clustered and 3) CPA automatic lexicog-
raphy where the structure of patterns have to
be constructed automatically.
Subtask 1 attracted 3 teams, though none
could beat the baseline (rule-based system).
Subtask 2 attracted 2 teams, one of which beat
the baseline (majority-class classifier). Sub-
task 3 did not attract any participant.
The task has produced a major semantic multi-
dataset resource which includes data for 121
verbs and about 17,000 annotated sentences,
and which is freely accessible.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995891">
It is a central vision of NLP to represent the mean-
ings of texts in a formalised way, amenable to au-
tomated reasoning. Since its birth, SEMEVAL (or
SENSEVAL as it was then; (Kilgarriff and Palmer,
2000)) has been part of the programme of enrich-
ing NLP analyses of text so they get ever closer
to a ’meaning representation’. In relation to lexi-
cal information, this meant finding a lexical resource
which
</bodyText>
<listItem confidence="0.982551333333333">
• identified the different meanings of words in
a way that made high-quality disambiguation
possible,
• represented those meanings in ways that were
useful for the next steps of building meaning
representations.
</listItem>
<bodyText confidence="0.99920985">
Most lexical resources explored to date have
had only limited success, on either front. The
most obvious candidates—published dictionaries
and WordNets—look like they might support the
first task, but are very limited in what they offer to
the second.
FrameNet moved the game forward a stage. Here
was a framework with a convincing account of how
the lexical entry might contribute to building the
meaning of the sentence, and with enough meat in
the lexical entries (e.g. the verb frames) so that
it might support disambiguation. Papers such as
(Gildea and Jurafsky, 2002) looked promising, and
in 2007 there was a SEMEVAL task on Frame Se-
mantic Structure Extraction (Baker et al., 2007) and
in 2010, one on Linking Events and Their Partici-
pants (Ruppenhofer et al., 2010).
While there has been a substantial amount of
follow-up work, there are some aspects of FrameNet
that make it a hard target.
</bodyText>
<listItem confidence="0.9421105">
• It is organised around frames, rather than
words, so inevitably its priority is to give a co-
</listItem>
<page confidence="0.987893">
315
</page>
<note confidence="0.952931">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 315–324,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.98906975">
herent account of the different verb senses in
a frame, rather than the different senses of an
individual verb. This will tend to make it less
good for supporting disambiguation.
</bodyText>
<listItem confidence="0.872274636363636">
• Frames are not ‘data-driven’: they are the work
of a theorist (Fillmore) doing his best to make
sense of the data for a set of verbs. The
prospects of data-driven frame discovery are,
correspondingly, slim.
• While FrameNet has worked hard at being sys-
tematic in its use of corpus data, FrameNetters
looked only for examples showing the verb be-
ing used in the relevant sense. From the point
of view of a process that could possibly be au-
tomated, this is problematic.
</listItem>
<bodyText confidence="0.999317">
An approach which bears many similarities to
FrameNet, but which starts from the verb rather than
the frame, and is more thoroughgoing in its empiri-
cism, is Hanks’s Corpus Pattern Analysis (Hanks
and Pustejovsky, 2005; Hanks, 2012; Hanks, 2013).
</bodyText>
<sectionHeader confidence="0.938745" genericHeader="method">
2 Corpus Pattern Analysis
</sectionHeader>
<bodyText confidence="0.994547761904762">
Corpus Pattern Analysis (CPA) is a new technique
of language analysis, which produces the main pat-
terns of use of words in text. Figure 1 is a sample
lexical entry from the main output of CPA, the Pat-
tern Dictionary of English Verbs1 (PDEV).
This tells us that, for the verb abolish, three pat-
terns were found. For each pattern it tells us the per-
centage of the data that it accounted for, its gram-
matical structure and the semantic type (drawn from
a shallow ontology of 225 semantic types2) of each
of the arguments in this structure. For instance,
pattern 1 means: i) that the subject is preferably
a word referring to [[Human]] or [[Institution]] (se-
mantic alternation), and ii) that the object is prefer-
ably [[Action]], [[Rule]] or [[Privilege]].
It also tells us the implicature (which is simi-
lar to a “definition” in a traditional dictionary) of
a sentence exemplifying the pattern: that is, if
we have a sentence of the pattern [[Institution  |Hu-
man]] abolish [[Action=Punishment  |Rule  |Privilege]],
then we know that [[Institution  |Human]] formally
</bodyText>
<footnote confidence="0.998446">
1http://pdev.org.uk
2http://pdev.org.uk/#onto
</footnote>
<bodyText confidence="0.997270634146341">
declares that [[Action=Punishment  |Rule  |Privi-
lege]] is no longer legal or operative. Abol-
ish has only one sense. For many verbs, there will
be multiple senses, each with one or more pattern.
There are currently full CPA entries for more than
1,000 verbs with a total of over 4,000 patterns. For
each verb a random sample of (by default) 250 cor-
pus instances was examined, used to build the lex-
ical entry, and tagged with the senses and patterns
they represented. For commoner verbs, more cor-
pus lines were examined. The corpus instances were
drawn from the written part of the British National
Corpus3 (BNC).
PDEV has been studied from different NLP per-
spectives, all mainly involved with Word Sense Dis-
ambiguation and semantic analysis (Cinkov´a et al.,
2012a; Holub et al., 2012; El Maarouf et al., ;
El Maarouf and Baisa, 2013; Kawahara et al., ;
Popescu, 2013; Popescu et al., ; Pustejovsky et al.,
2004; Rumshisky et al., ). For example, (Popescu,
2013) described experiments in modeling finite state
automata on a set of 721 verbs taken from PDEV.
The author reports an accuracy of over 70% in pat-
tern disambiguation. (Holub et al., 2012) trained
several statistical classifiers on a modified subset
of 30 PDEV entries (Cinkov´a et al., 2012c) using
morpho-syntactic as well as semantic features, and
obtained over 80% accuracy. On a smaller set of 20
high frequency verbs (El Maarouf and Baisa, 2013)
reached a similar 0.81 overall F1 score with a super-
vised SVM classifier based on dependency parsing
and named entity recognition features.
The goal of Task 15 at SemEval 2015 are i) to ex-
plore in more depth the mechanics of corpus-based
semantic analysis and ii) to provide a high-quality
standard dataset as well as baselines for the advance-
ment of semantic processing. Given the complexity
and wealth of PDEV, a major issue was to select rel-
evant subtasks and subsets. The task was eventually
split into three essential steps in building a CPA lex-
ical entry, that systems could tackle separately:
</bodyText>
<listItem confidence="0.99008475">
1. CPA parsing: all sentences in the dataset to be
syntactically and semantically parsed.
2. CPA clustering: all sentences in the dataset to
be grouped according to their similarities.
</listItem>
<footnote confidence="0.962581">
3http://www.natcorp.ox.ac.uk/
</footnote>
<page confidence="0.997286">
316
</page>
<table confidence="0.995089428571429">
1 Pattern Institution or Human abolishes Action or Rule or Privilege 58.8%
Implicature Institution or Human formally declares that Action = Punishment or Rule or Privilege
is no longer legal or operative
2 Pattern Institution 1 or Human abolishes Institution 2 or Human Role 24.4%
Implicature Institution 1 or Human formally puts an end to Institution 2 or Human Role
3 Pattern Process abolishes State of Affairs 14.4%
Implicature Process brings State of Affairs to an end
</table>
<figureCaption confidence="0.680771">
Figure 1: PDEV Entry for abolish.
</figureCaption>
<table confidence="0.99795825">
Tag Definition
subj Subject
obj Object
iobj Indirect Object
advprep Adverbial Preposition or other
Adverbial/Verbal Link
acomp Adverbial or Verb Complement
scomp Noun or Adjective complement
</table>
<tableCaption confidence="0.999845">
Table 1: Syntactic tagset used for subtask 1.
</tableCaption>
<listItem confidence="0.619524">
3. CPA lexicography: all verb patterns found in
the dataset to be described in terms of their syn-
tactic and semantic properties.
</listItem>
<sectionHeader confidence="0.989626" genericHeader="method">
3 Task Description
</sectionHeader>
<bodyText confidence="0.999989052631579">
In order to encourage participants to design sys-
tems which could successfully tackle all three sub-
tasks, all tasks were to be evaluated on the same
set of verbs. As opposed to previous experiments
on PDEV, it was decided that the set of verbs from
the test dataset would be different from the set of
verbs given in the training set. This was meant to
avoid limiting tasks to supervised approaches and to
encourage innovative approaches, maybe using pat-
terns learnt in an unsupervised manner from very
large corpora and other resources. This also im-
plied that the dataset would be constructed so as to
make it possible for systems to generalize from the
behaviour and description of one set of verbs to a
set of unseen verbs used in similar structures, as hu-
man language learners do. Although this obviously
makes the task harder, it was hoped that this would
put us in a better position to evaluate current limits
of automatic semantic analysis.
</bodyText>
<subsectionHeader confidence="0.998579">
3.1 Subtask 1: CPA Parsing
</subsectionHeader>
<bodyText confidence="0.9999692">
The CPA parsing subtask focuses on the detection
and classification (syntactic and semantic) of the
arguments of the verb. The subtask is similar to
Semantic Role Labelling (Carreras and Marquez,
2004) that arguments will be identified in the de-
pendency parsing paradigm (Buchholz and Marsi,
2006), using head words instead of phrases.
The syntactic tagset was designed specially for
this subtask and kept to a minimum, and the seman-
tic tagset was based on the CPA Semantic Ontology.
In Example (1), this would mean identify-
ing government as subject of abolish, from the
[[Institution]] type, and tax as object belonging to
[[Rule]]. The expected output is represented in XML
format in Example (2).
</bodyText>
<listItem confidence="0.998501333333333">
(1) In 1981 the Conservative government abol-
ished capital transfer tax capital transfer tax and
replaced it with inheritance tax.
(2) In 1981 the Conservative &lt;entity syn=‘subj’
sem=‘Institution’&gt; government &lt;/entity&gt; &lt;entity
syn=‘v’ sem=‘-’&gt; abolished &lt;/entity&gt; capital
transfer &lt;entity synt=‘obj’ sem=‘Rule’&gt; tax
&lt;/entity&gt; capital transfer tax and replaced it with
inheritance tax
</listItem>
<bodyText confidence="0.9999848">
The only dependency relations shown are those
involving the node verb. Thus, for example, the de-
pendency relation between Conservative and gov-
ernment is not shown. Also only the relations in
Table 1 are shown. The relation between abolished
and replaced is not shown as it is not one of the tar-
geted dependency relations. The input text consisted
of individual sentences one word per line with both
ID and FORM fields, and in which only the target
verb token was pre-tagged.
</bodyText>
<subsectionHeader confidence="0.999056">
3.2 Subtask 2: CPA Clustering
</subsectionHeader>
<bodyText confidence="0.99829">
The CPA clustering subtask is similar to a Word
Sense Discrimination task in which systems have to
</bodyText>
<page confidence="0.994915">
317
</page>
<table confidence="0.999816142857143">
Layer Annotator dataset observations categories Kappa (Cohen) F-score
Annotator 1 both 3,662 5 0.898 0.924
Syn Annotator 2 train 4,106 5 0.752 0.789
Annotator 3 test 1,518 5 0.931 0.942
Annotator 1 both 3,662 108 0.649 0.693
Sem Annotator 2 train 4,106 113 0.444 0.498
Annotator 3 test 1,518 75 0.765 0.782
</table>
<tableCaption confidence="0.990319">
Table 2: Inter-annotator figures where annotators are compared to the expert (annotator 4) who reviewed all the
annotations (Microcheck Task 1).
</tableCaption>
<bodyText confidence="0.999640833333333">
predict which pattern a verb instance belongs to.
With respect to abolish (Figure 1), it would in-
volve identifying all sentences containing the verb
abolish which belonged to the same pattern (one of
the patterns in Figure 1) and tagging them with the
same number.
</bodyText>
<subsectionHeader confidence="0.998714">
3.3 Subtask 3: CPA Automatic Lexicography
</subsectionHeader>
<bodyText confidence="0.999888461538461">
The CPA automatic lexicography subtask aims to
evaluate how systems can approach the design of a
lexicographical entry within CPA’s framework.
The input was, as for the other tasks, plain text
with node verb identified. The output format was
a variant of that shown in Figure 1, simplified to
a form which would be more tractable by systems
while still being a relevant representation from the
lexicographical perspective.
Specifically, contextual roles were discarded and
semantic alternations were decomposed into seman-
tic strings4 so that pattern 1 in Figure 1 would give
rise to six strings (with V for the verb, here abolish):
</bodyText>
<figure confidence="0.938535">
[[Human]] V [[Action]]
[[Human]] V [[Rule]]
[[Human]] V [[Privilege]]
[[Institution]] V [[Action]]
[[Institution]] V [[Rule]]
[[Institution]] V [[Privilege]]
</figure>
<bodyText confidence="0.947918666666667">
This transformation from the PDEV format as in
Figure 1 was done automatically and checked man-
ually. These strings are different to (and generally
more numerous than) the patterns evaluated in sub-
task 2. The goal of this subtask was to general-
ize sentence examples for each verb and create a
list of possible semantic strings. This subtask was
autonomous with respect to other subtasks in that
participants did not have to return the set of sen-
</bodyText>
<footnote confidence="0.856653">
4See (Bradbury and El Maarouf, 2013).
</footnote>
<bodyText confidence="0.9255545">
tences which matched their candidate patterns, pat-
terns were evaluated independently.
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="method">
4 Task Data
</sectionHeader>
<subsectionHeader confidence="0.99979">
4.1 The Microcheck and Wingspread Datasets
</subsectionHeader>
<bodyText confidence="0.9999802">
All subtasks (except the first) include two setups and
their associated datasets: the number of patterns for
each verb is disclosed in the first dataset but not in
the second. This setup was created to see whether it
would influence the results.
The two datasets were also created in the hope
that system development would start on the first
small and carefully crafted dataset (Microcheck) and
only then be tested on a larger and more varied sub-
set of verbs (Wingspread)5.
</bodyText>
<subsectionHeader confidence="0.957681">
4.2 Annotation Process
</subsectionHeader>
<bodyText confidence="0.999946294117647">
Both Microcheck and Wingspread start from data
extracted from PDEV and the manually pattern-
tagged BNC. We took only verbs declared as com-
plete and started by the same lexicographer, so that
each verb had been checked twice: once by the
lexicographer who compiled the entry and once by
the editor-in-chief. Some tagging errors may have
slipped in but the tagging is generally of high quality
(Cinkov´a et al., 2012a; Cinkov´a et al., 2012b). Addi-
tional checks have been performed on Microcheck,
since this was the dataset chosen for subtask 1, for
which data had to be created. This section describes
the annotation process.
PDEV contains only one kind of link between a
given pattern and a given corpus instance: each verb
token found in the sample is tagged with a pattern
identifier, and the pattern then specifies syntactic
</bodyText>
<footnote confidence="0.9978865">
5The datasets as well as the systems’ outputs will soon be
made publicly available on the task website.
</footnote>
<page confidence="0.974463">
318
</page>
<table confidence="0.999949333333333">
V P I IMP %MP V P I IMP %MP
boo 2 36 27 0.769 ascertain 2 7 4 0.676
teeter 2 28 23 0.828 totter 2 19 12 0.697
begrudge 2 19 11 0.678 tense 3 37 23 0.628
avert 2 240 230 0.958 belch 3 24 14 0.612
breeze 2 12 7 0.679 attain 3 240 200 0.833
wing 2 22 19 0.867 avoid 3 242 176 0.728
brag 2 29 18 0.692 adapt 4 182 98 0.583
sue 2 247 242 0.980 advise 8 230 84 0.391
bluff 2 25 14 0.673 ask 9 573 299 0.518
afflict 2 179 172 0.961 SUM 59 2,423 1,689 —
bludgeon 2 32 16 0.667 AVERAGE 2.95 121.15 84.45 0.721
</table>
<tableCaption confidence="0.9941925">
Table 3: Statistics on the Wingspread test dataset with V standing for verb, P for patterns, I for instances, IMP for
instances of majority pattern, and %MP for proportion of the majority pattern.
</tableCaption>
<table confidence="0.999915166666667">
V P I IMP %MP V P I IMP %MP
appreciate 2 160 215 0.765 apprehend 3 77 123 0.652
crush 5 62 170 0.413 decline 3 135 201 0.690
continue 7 71 203 0.401
undertake 2 204 228 0.896 SUM 30 749 1,280 —
operate 8 40 140 0.300 AVERAGE 4.286 107 182.857 0.588
</table>
<tableCaption confidence="0.999848">
Table 4: Statistics on the Microcheck test dataset; abbreviations as for previous table.
</tableCaption>
<bodyText confidence="0.999625090909091">
roles and their semantic types. The job in subtask 1
annotation consists of tagging the arguments of each
token in the sample, both syntactically and semanti-
cally (see Table 1 for tagsets of each layer). The
syntactic information was the same as for subtask 3
except that category names were shortened and pairs
of categories were merged in two places.6
The annotation was carried out by 4 annotators,
with 3 for the training data and 3 for test data, and
2 annotators annotating both training and test data,
one of them being an expert PDEV annotator. An-
notators could ask for feedback on the task at any
moment, and any doubts were cleared by the ex-
pert annotator. Each pair of annotators annotated
one share of the dataset, and their annotation was
double-checked by the expert annotator. The agree-
ment was not very high (e.g. Annotator 2, see Ta-
ble 2) in some cases so the double-check by the ex-
pert annotator was crucial. Table 2 reports the agree-
ment in terms of F-score and Cohen’s Kappa (Co-
hen, 1960) between each annotator and the expert
annotator.7
</bodyText>
<footnote confidence="0.997701333333333">
6See http://alt.qcri.org/semeval2015/
task15/index.php?id=appendices
7The expert did not start from scratch, but from other anno-
</footnote>
<subsectionHeader confidence="0.982474">
4.3 Statistics on the Data
</subsectionHeader>
<bodyText confidence="0.996793">
Strict rules were implemented to develop a high-
quality and consistent dataset:
</bodyText>
<listItem confidence="0.978163222222222">
1. PDEV patterns discriminate exploited8 uses of
a pattern using a different tag; these were left
aside for the CPA task.
2. For the test set, when patterns contained at
least one semantic type or grammatical cate-
gory which was not covered in the training set,
they were discarded.
3. Only patterns which contained more than 3 ex-
amples were kept in the final dataset.
</listItem>
<bodyText confidence="0.9264185">
Applying these filters led to the Microcheck
dataset, containing 28 verbs (train: 21; test: 7),
378 patterns (train: 306; test: 72) with 4,529 anno-
tated sentences (train: 3,249; test: 1,280) and to the
Wingspread dataset set containing 93 verbs (train:
tators’ work. Since his target was the conformity of the tagging
with guidelines as well as with CPA’s principles, we maintain
that the expert would have produced a very similar output had
he not started from the product of other annotators, who them-
selves used the output of a system to speed up their work.
8An exploitation corresponds to an anomalous use of a pat-
tern, as in a figurative use.
</bodyText>
<page confidence="0.998494">
319
</page>
<bodyText confidence="0.552053714285714">
that systems make over the 9 slots: errors of Inser-
tion, Substitution, Deletion. Equation 4 formulates
how Precision and Recall are computed.
73; test: 20), 856 patterns (train: 652; test: 204),
and 12,440 annotated sentences (train: 10,017; test:
2,423). More detailed figures for the test datasets are
provided in Tables 3 and 4.
</bodyText>
<figure confidence="0.994562714285714">
Correct + Subst + Ins
Correct
Recall =
Precision =
Correct
Correct + Subst + Del
(4)
</figure>
<subsectionHeader confidence="0.988814">
4.4 Metrics
</subsectionHeader>
<bodyText confidence="0.99847">
The final score for all subtasks is the average of F-
scores over all verbs (Eq. 1). What varies across sub-
tasks is the way Precision and Recall are defined.
</bodyText>
<table confidence="0.929795">
F1verb = 2 × Precisionverb × Recallverb
Precisionverb + Recallverb (1)
nverb F 1
Task =
Score �i=1 verbi
nverb
</table>
<figureCaption confidence="0.6570695">
Subtask 1. Equation 2 illustrates that Precision
and Recall are computed on all tags, both syntactic
and semantic. To count as correct, tags had to be set
on the same token as in the gold standard.
</figureCaption>
<figure confidence="0.993520666666667">
Correct tags
Precision =
Retrieved tags
Recall =
Correct tags
Reference tags
</figure>
<bodyText confidence="0.927094846153846">
Subtask 2. Clustering is known to be difficult
to evaluate. Subtask 2 used the B-cubed definition
of Precision and Recall, first used for coreference
(Bagga and Baldwin, 1999) and later extended to
cluster evaluation (Amig´o et al., 2009). Both mea-
sures are averages of the precision and recall over
all instances. To calculate the precision of each in-
stance we count all correct pairs associated with this
instance and divide by the number of actual pairs
in the candidate cluster that the instance belongs to.
Recall is computed by interchanging Gold and Can-
didate clusterings (Eq. 3).
Pairsi in Candidate found in Gold
</bodyText>
<equation confidence="0.9871066">
Precisioni =
Pairsi in Candidate
Pairsi in Gold found in Candidate
Recalli =
Pairsi in Gold
</equation>
<bodyText confidence="0.948184277777778">
(3)
Subtask 3. This task was evaluated as a slot-filling
exercise (Makhoul et al., 1999), so the scores were
computed by taking into account the kinds of errors
In order not to penalize systems, the best match
was computed for each Candidate pattern, and one
candidate pattern could match more than one Gold
pattern. When a given slot was filled both in the
Gold data and the Candidate data, this counted as a
“match”. When not, it was a Deletion. If a slot was
filled in the run but not in the gold, it was counted
as an Insertion. When a match (aligned slots) was
also a semantic type match, it was Correct (1 point).
When not, it was a Substitution; the CPA ontology
was used to allow for partial matches, allowing hy-
pernyms and hyponyms. For that particular task, the
maximum number of Candidate patterns was limited
to 150% with respect to the number in the Gold set.
</bodyText>
<sectionHeader confidence="0.998893" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999973705882353">
The evaluation was split into 2 phases (one week for
each): a feedback phase and a validation phase. The
reason for this was to allow for the detection of un-
foreseen issues in the output of participants’ systems
so as to prepare for any major problem. However,
this was not put to use by participants since only one
team submitted their output in the first phase which
also happened to be their final submission.
5 teams9 participated in the task, but none partic-
ipated in more than one subtask. Subtask 1 attracted
3 teams and subtask 2 attracted 2, while subtask 3
did not receive any submissions. Systems were al-
lowed 3 runs on each subtask and each dataset, and
were asked to indicate which would be the official
one. The following subsections report in brief on
the main features of their systems (for more details
see relevant papers in SemEval proceedings).
</bodyText>
<subsectionHeader confidence="0.98686">
5.1 Subtask 1
</subsectionHeader>
<bodyText confidence="0.98862725">
All systems for this subtask used syntactic depen-
dencies and named entities as features. Since the
9Unfortunately, teams BOB90 and FANTASY did not sub-
mit articles, so it is difficult to analyze their results.
</bodyText>
<figure confidence="0.447292">
(2)
</figure>
<page confidence="0.986579">
320
</page>
<table confidence="0.999863454545455">
Category #Gold CMILLS FANTASY BLCUNLP baseline
subj 1,008 0.564 0.694 0.739 0.815
obj 777 0.659 0.792 0.777 0.783
Human 580 0.593 0.770 0.691 0.724
Activity 438 0.450 0.479 0.393 0.408
acomp 308 0.545 0.418 0.702 0.729
LexicalItem 303 0.668 0.830 0.771 0.811
advprep 289 0.621 0.517 0.736 0.845
State Of Affairs 192 0.410 0.276 0.373 0.211
Institution 182 0.441 0.531 0.483 0.461
Action 115 0.421 0.594 0.526 0.506
</table>
<tableCaption confidence="0.896519">
Table 6: Detailed scores for subtask 1 (10 most frequent categories).
</tableCaption>
<table confidence="0.999464">
Team Score
baseline 0.624
FANTASY 0.589
BLCUNLP 0.530
CMILLS 0.516
</table>
<tableCaption confidence="0.999511">
Table 5: Official scores for subtask 1.
</tableCaption>
<bodyText confidence="0.999243818181818">
subtask allowed it, some systems used external re-
sources such as Wordnet or larger corpora.
BLCUNLP (Feng et al., 2015) used the Stanford
CoreNLP package10 to get POS, NE and basic de-
pendency features. These features were used to pre-
dict both syntax and semantic information. The
method did not involve the use of a statistical classi-
fier.
CMILLS (Mills and Levow, 2015) used three
models to solve the task: one for argument detec-
tion, and the other two for each layer. Argument de-
tection and syntactic tagging were performed using
a MaxEnt supervised classifier, while the last was
based on heuristics. CMILLS also reported the use
of an external resource, the enTentTen12 (Jakubiˇcek
et al., 2013) corpus available in Sketch Engine (Kil-
garriff et al., 2014).
FANTASY approached the subtask in a super-
vised setting to predict first the syntactic tags, and
then the semantic tags. The team used features from
the MST parser11, as well as Stanford CoreNLP for
NE, Wordnet12, they also applied word embedding
</bodyText>
<footnote confidence="0.9826274">
10http://nlp.stanford.edu/software/
corenlp.shtml
11http://www.seas.upenn.edu/˜strctlrn/
MSTParser/MSTParser.html
12http://wordnet.princeton.edu/
</footnote>
<bodyText confidence="0.99664625">
representations to predict the output of each layer.
The baseline system was a rule-based system tak-
ing as input the output of the BLLIP parser (Char-
niak and Johnson, 2005), and mapping heads of rel-
evant dependency relations to the most probable tags
from subtask 1 tagset. The semantic tags were only
then added to those headwords based on the most
frequent semantic category found in the training set.
</bodyText>
<subsectionHeader confidence="0.997029">
5.2 Subtask 2
</subsectionHeader>
<bodyText confidence="0.999906">
As opposed to subtask 1, systems in subtask 2 used
very few semantic and syntactic resources.
BOB90 used a supervised approach to tackle the
clustering problem. The main features used were
preposition analyses.
DULUTH (Pedersen, 2015) used an unsupervised
approach and focused on lexical similarity (both
first and second order representations) based on uni-
grams and bigrams (see SenseClusters13). The num-
ber of clusters was predicted on the basis of the
best value for the clustering criterion function. The
team also performed some corpus pre-processing,
like conversion to lower case and conversion of all
numeric values to a string.
The baseline system clusters everything together,
so its score depends on the distribution of patterns:
the more a pattern covers all instances of the data
(majority class), the higher the baseline score.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.99733">
6.1 Subtask 1
</subsectionHeader>
<bodyText confidence="0.999474">
As previously noted, subtask 1 provided only one
dataset, Microcheck. The results on the test set are
</bodyText>
<footnote confidence="0.84331">
13http://senseclusters.sourceforge.net
</footnote>
<page confidence="0.997623">
321
</page>
<bodyText confidence="0.9999404">
described in Table 5: FANTASY is the best system
with 0.589 average F1 score, but does not beat the
baseline (0.624).
It is worth noting that, on the same set of verbs,
BLCUNLP and FANTASY are almost on a par, but
since the former did not submit one verb file, the
score gap is more significant. FANTASY is a more
precise system while BLCUNLP has higher recall.
To get a better picture of the results, Table 6 pro-
vides the F-scores for the ten most frequent cate-
gories in the test set. We can see that FANTASY
has the best semantic model since it gets the high-
est scores on most semantic categories (except for
State Of Affairs) and systematically beats the base-
line, which assigns a word the most frequent seman-
tic category in the training set. The baseline and
BCUNLP however get higher scores on most syn-
tactic relations except on obj, where the difference
is low. The gap is much more significant on ad-
vprep and acomp, which suggests that FANTASY
does not properly handle prepositional complements
correctly (and/or causal complements). This could
be due to the choice of parser or to model param-
eters. Overall, it seems that progress can still be
made, since systems can benefit from one another.
</bodyText>
<subsectionHeader confidence="0.997902">
6.2 Subtask 2
</subsectionHeader>
<bodyText confidence="0.999487842105263">
Subtask 2 was evaluated on both datasets. BOB90
only submitted one run while DULUTH submit-
ted three. The results are displayed on Table 7.
For this task, only BOB90 beat the baseline with a
higher amplitude on Microcheck (+0.153) than on
Wingspread (+0.071). This high score welcomes
a more detailed evaluation of the system, since it
would seem that, as also found for subtask 1, prepo-
sitions play a substantial role in CPA patterns and
semantic similarity.
It can also be observed that overall results are bet-
ter on Wingspread. This seems to be mainly due
to the higher number of verbs with a large majority
class in Wingspread (see Table 3), since the base-
line system scores 0.72 on Wingspread, and 0.588
on Microcheck. This shows that when the distri-
bution of patterns is highly skewed, the evaluation
of systems is difficult, and tends to underrate poten-
tially useful systems.
</bodyText>
<table confidence="0.999217285714286">
Team Scores
Microcheck Wingspread
BOB90 0.741 0.791
baseline 0.588 0.720
DULUTH-1 (off) 0.525 0.604
DULUTH-2 0.439 0.581
DULUTH-3 0.439 0.615
</table>
<tableCaption confidence="0.998213">
Table 7: Official scores for subtask 2.
</tableCaption>
<sectionHeader confidence="0.997302" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999977558823529">
This paper introduces a new SemEval task to explore
the use of Natural Language Processing systems for
building dictionary entries, in the framework of Cor-
pus Pattern Analysis. Dictionary entry building is
split into three subtasks: 1) CPA parsing, where ar-
guments and their syntactic and semantic categories
have to be identified, 2) CPA clustering, in which
sentences with similar patterns have to be clustered
and 3) CPA automatic lexicography where the struc-
ture of patterns have to be constructed automatically.
Drawing from the Pattern Dictionary of English
Verbs, we have produced a high-quality resource for
the advancement of semantic processing: it contains
121 verbs connected to a corpus of 17,000 sentences.
This resource will be made freely accessible from
the task website for more in depth future research.
Task 15 has attracted 5 participants, 3 on subtask 1
and 2 on subtask 2. Subtask 1 proved to be more
difficult for participants than expected, since no sys-
tem beat the baseline. We however show that the
submissions possess interesting features that should
be put to use in future experiments on the dataset.
Subtask 2’s baseline was beaten by one of the par-
ticipants on a large margin, despite the fact that the
baseline is very competitive.
It seems that splitting the task into 3 subtasks
has had the benefit of attracting different approaches
(supervised and unsupervised) towards the common
target of the task, which is to build a dictionary entry.
Lexicography is such a complex task that it needs
major efforts from the NLP community to support
it. We hope that this task will stimulate more re-
search and the development of new approaches to
the automatic creation of lexical resources.
</bodyText>
<page confidence="0.996329">
322
</page>
<sectionHeader confidence="0.999212" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996975">
We are very grateful for feedback on the task from
Ken Litkowski as well as from participants who
greatly contributed to the overall quality of the task.
We would also like to thank SemEval’s organizers
for their support. The work was also supported
by the UK’s AHRC grant [DVC, AH/J005940/1,
2012-2015], by the Ministry of Education of
Czech Republic within the LINDAT-Clarin project
LM2010013, by the Czech-Norwegian Research
Programme within the HaBiT Project 7F14047 and
by the Czech Science Foundation grant [GA15-
20031S].
</bodyText>
<sectionHeader confidence="0.999403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99983808988764">
Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A Comparison of Extrinsic Cluster-
ing Evaluation Metrics Based on Formal Constraints.
Information Retrieval, 12(4):461–486.
Amit Bagga and Breck Baldwin. 1999. Cross-document
event coreference: Annotations, experiments, and ob-
servations. In Proceedings of the Workshop on Coref-
erence and its Applications, pages 1–8.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval-2007 Task 19: Frame Semantic Structure
Extraction. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 99–104, Prague, Czech Republic.
Jane Bradbury and Ismail El Maarouf. 2013. An empir-
ical classification of verbs based on Semantic Types:
the case of the ’poison’ verbs. In Proceedings of
JSSP2013, pages 70–74, Trento,Italy.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, New York, USA.
Xavier Carreras and Lluis Marquez. 2004. Introduction
to the CoNLL-2004 shared task: Semantic role label-
ing. In Proceedings of CoNLL, Boston, USA.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173–
180.
Silvie Cinkov´a, Martin Holub, and Vincent Kr´ı&amp;quot;z. 2012a.
Managing Uncertainty in Semantic Tagging. In Pro-
ceedings of 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 840–850, Avignon, France.
Silvie Cinkov´a, Martin Holub, and Vincent Krf&amp;quot;z. 2012b.
Optimizing semantic granularity for NLP - report on
a lexicographic experiment. In Proceedings of the
15th EURALEX International Congress, pages 523–
531, Oslo, Norway.
Silvie Cinkov´a, Martin Holub, Adam Rambousek, and
Lenka Smejkalov´a. 2012c. A database of semantic
clusters of verb usages. In Proceedings of the 8th In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3176–3183, Istanbul,
Turkey.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37–46.
Isma¨ıl El Maarouf and Vft Baisa. 2013. Automatic clas-
sification of semantic patterns from the Pattern Dictio-
nary of English Verbs. In Proceedings of JSSP2013,
pages 95–99, Trento, Italy.
Isma¨ıl El Maarouf, Jane Bradbury, Vft Baisa, and Patrick
Hanks. Disambiguating Verbs by Collocation: Corpus
Lexicography meets Natural Language Processing. In
Proceedings of LREC, pages 1001–1006, Reykjavik,
Iceland.
Yukun Feng, Qiao Deng, and Dong Yu. 2015. BL-
CUNLP: Corpus Pattern Analysis for Verbs Based on
Dependency Chain. In Proceedings of SemEval 2015,
Denver, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic La-
beling of Semantic Roles. Computational Linguistics,
28(3):245–288, September.
Patrick Hanks and James Pustejovsky. 2005. A Pattern
Dictionary for Natural Language Processing. Revue
Franc¸aise de linguistique applique, 10:2.
Patrick Hanks. 2012. How people use words to make
meanings: Semantic types meet valencies. In A. Boul-
ton and J. Thomas, editors, Input, Process and Prod-
uct: Developments in Teaching and Language Cor-
pora, pages 54–69. Brno.
Patrick Hanks. 2013. Lexical Analysis: Norms and Ex-
ploitations. MIT Press, Cambridge, MA.
Martin Holub, Vincent Krfz&amp;quot;, Silvie Cinkov´a, and Eck-
hard Bick. 2012. Tailored Feature Extraction for Lex-
ical Disambiguation of English Verbs Based on Cor-
pus Pattern Analysis. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics
(Coling 2012), pages 1195–1209, Mumbai, India.
Milo&amp;quot;s Jakubf&amp;quot;cek, Adam Kilgarriff, Vojt&amp;quot;ech Kov´a&amp;quot;r, Pavel
Rychl´y, and Vft Suchomel. 2013. The TenTen Corpus
Family. In Proceedings of the International Confer-
ence on Corpus Linguistics.
Daisuke Kawahara, Daniel W Peterson, Octavian
Popescu, and Martha Palmer. Inducing Example-
based Semantic Frames from a Massive Amount of
Verb Uses. In Proceedings of the 14th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 58–67.
</reference>
<page confidence="0.990755">
323
</page>
<reference confidence="0.999204694444444">
Adam Kilgarriff and Martha Palmer. 2000. Introduction
to the Special Issue on SENSEVAL. Computers and
the Humanities, 34:1–2.
Adam Kilgarriff, V´ıt Baisa, Jan Bu&amp;quot;sta, Milos&amp;quot; Jakub´ı&amp;quot;cek,
Vojt&amp;quot;ech Kov´a&amp;quot;r, Jan Michelfeit, Pavel Rychl´y, and V´ıt
Suchomel. 2014. The Sketch Engine: ten years on.
Lexicography, 1(1):7–36.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance Measures For
Information Extraction. In Proceedings of DARPA
Broadcast News Workshop, pages 249–252.
Chad Mills and Gina-Anne Levow. 2015. CMILLS:
Adapting SRL Features to Dependency Parsing. In
Proceedings of SemEval 2015, Denver, USA.
Ted Pedersen. 2015. Duluth: Word Sense Discrimina-
tion in the Service of Lexicography. In Proceedings of
SemEval 2015, Denver, USA.
Octavian Popescu, Martha Palmer, and Patrick Hanks.
Mapping CPA onto OntoNotes Senses. In Proceed-
ings of LREC, pages 882–889, Reykjavik, Iceland.
Octavian Popescu. 2013. Learning corpus patterns using
finite state automata. In Proceedings of the 10th In-
ternational Conference on Computational Semantics,
pages 191–203, Potsdam, Germany.
James Pustejovsky, Patrick Hanks, and Anna Rumshisky.
2004. Automated Induction of Sense in Context. In
Proceedings of COLING, Geneva, Switzerland.
Anna Rumshisky, Patrick Hanks, Catherine Havasi, and
James Pustejovsky. Constructing a corpus-based on-
tology using model bias. In Proceedings of FLAIRS,
pages 327–332, Melbourne, FL.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In 5th International Workshop on Se-
mantic Evaluation, pages 45–50, Uppsala, Sweden.
</reference>
<page confidence="0.999023">
324
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.144860">
<title confidence="0.961184">SemEval-2015 Task 15: A Corpus Pattern Dictionary-Entry-Building Task</title>
<author confidence="0.956266">Vit Baisa</author>
<affiliation confidence="0.9985">Masaryk University</affiliation>
<email confidence="0.822999">xbaisa@fi.muni.cz</email>
<author confidence="0.95579">Ismail El</author>
<affiliation confidence="0.999918">University of Wolverhampton</affiliation>
<email confidence="0.981269">i.el-maarouf@wlv.ac.uk</email>
<author confidence="0.939354">Jane</author>
<affiliation confidence="0.999861">University of Wolverhampton</affiliation>
<email confidence="0.988199">J.Bradbury3@wlv.ac.uk</email>
<author confidence="0.801576">Adam</author>
<affiliation confidence="0.992948">Lexical Computing Ltd</affiliation>
<email confidence="0.911168">adam@sketchengine.co.uk</email>
<author confidence="0.47915">Silvie</author>
<affiliation confidence="0.977135">Charles University</affiliation>
<email confidence="0.872905">cinkova@ufal.mff.cuni.cz</email>
<author confidence="0.748841">Octavian</author>
<affiliation confidence="0.999131">IBM Research Center</affiliation>
<email confidence="0.999654">o.popescu@us.ibm.com</email>
<abstract confidence="0.998443161290323">This paper describes the first SemEval task to explore the use of Natural Language Processing systems for building dictionary entries, in the framework of Corpus Pattern Analysis. CPA is a corpus-driven technique which provides tools and resources to identify and represent unambiguously the main semantic patterns in which words are used. Task 15 draws on the Pattern Dictionary of English Verbs for the targeted lexical entries, and on the British National Corpus for the input text. Dictionary entry building is split into three subtasks which all start from the same concordance sample: 1) CPA parsing, where arguments and their syntactic and semantic categories have to be identified, 2) CPA clustering, in which sentences with similar patterns have to be clustered and 3) CPA automatic lexicography where the structure of patterns have to be constructed automatically. Subtask 1 attracted 3 teams, though none could beat the baseline (rule-based system). Subtask 2 attracted 2 teams, one of which beat the baseline (majority-class classifier). Subtask 3 did not attract any participant. The task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences, and which is freely accessible.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Javier Artiles</author>
<author>Felisa Verdejo</author>
</authors>
<title>A Comparison of Extrinsic Clustering Evaluation Metrics Based on Formal Constraints.</title>
<date>2009</date>
<journal>Information Retrieval,</journal>
<volume>12</volume>
<issue>4</issue>
<marker>Amig´o, Gonzalo, Artiles, Verdejo, 2009</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A Comparison of Extrinsic Clustering Evaluation Metrics Based on Formal Constraints. Information Retrieval, 12(4):461–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Cross-document event coreference: Annotations, experiments, and observations.</title>
<date>1999</date>
<booktitle>In Proceedings of the Workshop on Coreference and its Applications,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="19442" citStr="Bagga and Baldwin, 1999" startWordPosition="3237" endWordPosition="3240">s across subtasks is the way Precision and Recall are defined. F1verb = 2 × Precisionverb × Recallverb Precisionverb + Recallverb (1) nverb F 1 Task = Score �i=1 verbi nverb Subtask 1. Equation 2 illustrates that Precision and Recall are computed on all tags, both syntactic and semantic. To count as correct, tags had to be set on the same token as in the gold standard. Correct tags Precision = Retrieved tags Recall = Correct tags Reference tags Subtask 2. Clustering is known to be difficult to evaluate. Subtask 2 used the B-cubed definition of Precision and Recall, first used for coreference (Bagga and Baldwin, 1999) and later extended to cluster evaluation (Amig´o et al., 2009). Both measures are averages of the precision and recall over all instances. To calculate the precision of each instance we count all correct pairs associated with this instance and divide by the number of actual pairs in the candidate cluster that the instance belongs to. Recall is computed by interchanging Gold and Candidate clusterings (Eq. 3). Pairsi in Candidate found in Gold Precisioni = Pairsi in Candidate Pairsi in Gold found in Candidate Recalli = Pairsi in Gold (3) Subtask 3. This task was evaluated as a slot-filling exer</context>
</contexts>
<marker>Bagga, Baldwin, 1999</marker>
<rawString>Amit Bagga and Breck Baldwin. 1999. Cross-document event coreference: Annotations, experiments, and observations. In Proceedings of the Workshop on Coreference and its Applications, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>SemEval-2007 Task 19: Frame Semantic Structure Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>99--104</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3044" citStr="Baker et al., 2007" startWordPosition="465" endWordPosition="468">, on either front. The most obvious candidates—published dictionaries and WordNets—look like they might support the first task, but are very limited in what they offer to the second. FrameNet moved the game forward a stage. Here was a framework with a convincing account of how the lexical entry might contribute to building the meaning of the sentence, and with enough meat in the lexical entries (e.g. the verb frames) so that it might support disambiguation. Papers such as (Gildea and Jurafsky, 2002) looked promising, and in 2007 there was a SEMEVAL task on Frame Semantic Structure Extraction (Baker et al., 2007) and in 2010, one on Linking Events and Their Participants (Ruppenhofer et al., 2010). While there has been a substantial amount of follow-up work, there are some aspects of FrameNet that make it a hard target. • It is organised around frames, rather than words, so inevitably its priority is to give a co315 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 315–324, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics herent account of the different verb senses in a frame, rather than the different senses of an individual ve</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin Baker, Michael Ellsworth, and Katrin Erk. 2007. SemEval-2007 Task 19: Frame Semantic Structure Extraction. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 99–104, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Bradbury</author>
<author>Ismail El Maarouf</author>
</authors>
<title>An empirical classification of verbs based on Semantic Types: the case of the ’poison’ verbs.</title>
<date>2013</date>
<booktitle>In Proceedings of JSSP2013,</booktitle>
<pages>70--74</pages>
<marker>Bradbury, El Maarouf, 2013</marker>
<rawString>Jane Bradbury and Ismail El Maarouf. 2013. An empirical classification of verbs based on Semantic Types: the case of the ’poison’ verbs. In Proceedings of JSSP2013, pages 70–74, Trento,Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="9949" citStr="Buchholz and Marsi, 2006" startWordPosition="1611" endWordPosition="1614">rom the behaviour and description of one set of verbs to a set of unseen verbs used in similar structures, as human language learners do. Although this obviously makes the task harder, it was hoped that this would put us in a better position to evaluate current limits of automatic semantic analysis. 3.1 Subtask 1: CPA Parsing The CPA parsing subtask focuses on the detection and classification (syntactic and semantic) of the arguments of the verb. The subtask is similar to Semantic Role Labelling (Carreras and Marquez, 2004) that arguments will be identified in the dependency parsing paradigm (Buchholz and Marsi, 2006), using head words instead of phrases. The syntactic tagset was designed specially for this subtask and kept to a minimum, and the semantic tagset was based on the CPA Semantic Ontology. In Example (1), this would mean identifying government as subject of abolish, from the [[Institution]] type, and tax as object belonging to [[Rule]]. The expected output is represented in XML format in Example (2). (1) In 1981 the Conservative government abolished capital transfer tax capital transfer tax and replaced it with inheritance tax. (2) In 1981 the Conservative &lt;entity syn=‘subj’ sem=‘Institution’&gt; g</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="9853" citStr="Carreras and Marquez, 2004" startWordPosition="1596" endWordPosition="1599">mplied that the dataset would be constructed so as to make it possible for systems to generalize from the behaviour and description of one set of verbs to a set of unseen verbs used in similar structures, as human language learners do. Although this obviously makes the task harder, it was hoped that this would put us in a better position to evaluate current limits of automatic semantic analysis. 3.1 Subtask 1: CPA Parsing The CPA parsing subtask focuses on the detection and classification (syntactic and semantic) of the arguments of the verb. The subtask is similar to Semantic Role Labelling (Carreras and Marquez, 2004) that arguments will be identified in the dependency parsing paradigm (Buchholz and Marsi, 2006), using head words instead of phrases. The syntactic tagset was designed specially for this subtask and kept to a minimum, and the semantic tagset was based on the CPA Semantic Ontology. In Example (1), this would mean identifying government as subject of abolish, from the [[Institution]] type, and tax as object belonging to [[Rule]]. The expected output is represented in XML format in Example (2). (1) In 1981 the Conservative government abolished capital transfer tax capital transfer tax and replac</context>
</contexts>
<marker>Carreras, Marquez, 2004</marker>
<rawString>Xavier Carreras and Lluis Marquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of CoNLL, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="23880" citStr="Charniak and Johnson, 2005" startWordPosition="3978" endWordPosition="3982">) corpus available in Sketch Engine (Kilgarriff et al., 2014). FANTASY approached the subtask in a supervised setting to predict first the syntactic tags, and then the semantic tags. The team used features from the MST parser11, as well as Stanford CoreNLP for NE, Wordnet12, they also applied word embedding 10http://nlp.stanford.edu/software/ corenlp.shtml 11http://www.seas.upenn.edu/˜strctlrn/ MSTParser/MSTParser.html 12http://wordnet.princeton.edu/ representations to predict the output of each layer. The baseline system was a rule-based system taking as input the output of the BLLIP parser (Charniak and Johnson, 2005), and mapping heads of relevant dependency relations to the most probable tags from subtask 1 tagset. The semantic tags were only then added to those headwords based on the most frequent semantic category found in the training set. 5.2 Subtask 2 As opposed to subtask 1, systems in subtask 2 used very few semantic and syntactic resources. BOB90 used a supervised approach to tackle the clustering problem. The main features used were preposition analyses. DULUTH (Pedersen, 2015) used an unsupervised approach and focused on lexical similarity (both first and second order representations) based on </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinkov´a</author>
<author>Martin Holub</author>
<author>Vincent Kr´ız</author>
</authors>
<title>Managing Uncertainty in Semantic Tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>840--850</pages>
<location>Avignon, France.</location>
<marker>Cinkov´a, Holub, Kr´ız, 2012</marker>
<rawString>Silvie Cinkov´a, Martin Holub, and Vincent Kr´ı&amp;quot;z. 2012a. Managing Uncertainty in Semantic Tagging. In Proceedings of 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 840–850, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinkov´a</author>
<author>Martin Holub</author>
<author>Vincent Krfz</author>
</authors>
<title>Optimizing semantic granularity for NLP - report on a lexicographic experiment.</title>
<date>2012</date>
<booktitle>In Proceedings of the 15th EURALEX International Congress,</booktitle>
<pages>523--531</pages>
<location>Oslo, Norway.</location>
<marker>Cinkov´a, Holub, Krfz, 2012</marker>
<rawString>Silvie Cinkov´a, Martin Holub, and Vincent Krf&amp;quot;z. 2012b. Optimizing semantic granularity for NLP - report on a lexicographic experiment. In Proceedings of the 15th EURALEX International Congress, pages 523– 531, Oslo, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinkov´a</author>
<author>Martin Holub</author>
<author>Adam Rambousek</author>
<author>Lenka Smejkalov´a</author>
</authors>
<title>A database of semantic clusters of verb usages.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012),</booktitle>
<pages>3176--3183</pages>
<location>Istanbul, Turkey.</location>
<marker>Cinkov´a, Holub, Rambousek, Smejkalov´a, 2012</marker>
<rawString>Silvie Cinkov´a, Martin Holub, Adam Rambousek, and Lenka Smejkalov´a. 2012c. A database of semantic clusters of verb usages. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012), pages 3176–3183, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement,</title>
<date>1960</date>
<contexts>
<context position="16990" citStr="Cohen, 1960" startWordPosition="2825" endWordPosition="2827">tors, with 3 for the training data and 3 for test data, and 2 annotators annotating both training and test data, one of them being an expert PDEV annotator. Annotators could ask for feedback on the task at any moment, and any doubts were cleared by the expert annotator. Each pair of annotators annotated one share of the dataset, and their annotation was double-checked by the expert annotator. The agreement was not very high (e.g. Annotator 2, see Table 2) in some cases so the double-check by the expert annotator was crucial. Table 2 reports the agreement in terms of F-score and Cohen’s Kappa (Cohen, 1960) between each annotator and the expert annotator.7 6See http://alt.qcri.org/semeval2015/ task15/index.php?id=appendices 7The expert did not start from scratch, but from other anno4.3 Statistics on the Data Strict rules were implemented to develop a highquality and consistent dataset: 1. PDEV patterns discriminate exploited8 uses of a pattern using a different tag; these were left aside for the CPA task. 2. For the test set, when patterns contained at least one semantic type or grammatical category which was not covered in the training set, they were discarded. 3. Only patterns which contained </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isma¨ıl El Maarouf</author>
<author>Vft Baisa</author>
</authors>
<title>Automatic classification of semantic patterns from the Pattern Dictionary of English Verbs.</title>
<date>2013</date>
<booktitle>In Proceedings of JSSP2013,</booktitle>
<pages>95--99</pages>
<location>Trento, Italy.</location>
<marker>El Maarouf, Baisa, 2013</marker>
<rawString>Isma¨ıl El Maarouf and Vft Baisa. 2013. Automatic classification of semantic patterns from the Pattern Dictionary of English Verbs. In Proceedings of JSSP2013, pages 95–99, Trento, Italy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Isma¨ıl El Maarouf</author>
<author>Jane Bradbury</author>
<author>Vft Baisa</author>
<author>Patrick Hanks</author>
</authors>
<title>Disambiguating Verbs by Collocation: Corpus Lexicography meets Natural Language Processing.</title>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1001--1006</pages>
<location>Reykjavik, Iceland.</location>
<marker>El Maarouf, Bradbury, Baisa, Hanks, </marker>
<rawString>Isma¨ıl El Maarouf, Jane Bradbury, Vft Baisa, and Patrick Hanks. Disambiguating Verbs by Collocation: Corpus Lexicography meets Natural Language Processing. In Proceedings of LREC, pages 1001–1006, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yukun Feng</author>
<author>Qiao Deng</author>
<author>Dong Yu</author>
</authors>
<title>BLCUNLP: Corpus Pattern Analysis for Verbs Based on Dependency Chain.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval 2015,</booktitle>
<location>Denver, USA.</location>
<contexts>
<context position="22674" citStr="Feng et al., 2015" startWordPosition="3796" endWordPosition="3799">59 0.792 0.777 0.783 Human 580 0.593 0.770 0.691 0.724 Activity 438 0.450 0.479 0.393 0.408 acomp 308 0.545 0.418 0.702 0.729 LexicalItem 303 0.668 0.830 0.771 0.811 advprep 289 0.621 0.517 0.736 0.845 State Of Affairs 192 0.410 0.276 0.373 0.211 Institution 182 0.441 0.531 0.483 0.461 Action 115 0.421 0.594 0.526 0.506 Table 6: Detailed scores for subtask 1 (10 most frequent categories). Team Score baseline 0.624 FANTASY 0.589 BLCUNLP 0.530 CMILLS 0.516 Table 5: Official scores for subtask 1. subtask allowed it, some systems used external resources such as Wordnet or larger corpora. BLCUNLP (Feng et al., 2015) used the Stanford CoreNLP package10 to get POS, NE and basic dependency features. These features were used to predict both syntax and semantic information. The method did not involve the use of a statistical classifier. CMILLS (Mills and Levow, 2015) used three models to solve the task: one for argument detection, and the other two for each layer. Argument detection and syntactic tagging were performed using a MaxEnt supervised classifier, while the last was based on heuristics. CMILLS also reported the use of an external resource, the enTentTen12 (Jakubiˇcek et al., 2013) corpus available in</context>
</contexts>
<marker>Feng, Deng, Yu, 2015</marker>
<rawString>Yukun Feng, Qiao Deng, and Dong Yu. 2015. BLCUNLP: Corpus Pattern Analysis for Verbs Based on Dependency Chain. In Proceedings of SemEval 2015, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="2929" citStr="Gildea and Jurafsky, 2002" startWordPosition="445" endWordPosition="448"> the next steps of building meaning representations. Most lexical resources explored to date have had only limited success, on either front. The most obvious candidates—published dictionaries and WordNets—look like they might support the first task, but are very limited in what they offer to the second. FrameNet moved the game forward a stage. Here was a framework with a convincing account of how the lexical entry might contribute to building the meaning of the sentence, and with enough meat in the lexical entries (e.g. the verb frames) so that it might support disambiguation. Papers such as (Gildea and Jurafsky, 2002) looked promising, and in 2007 there was a SEMEVAL task on Frame Semantic Structure Extraction (Baker et al., 2007) and in 2010, one on Linking Events and Their Participants (Ruppenhofer et al., 2010). While there has been a substantial amount of follow-up work, there are some aspects of FrameNet that make it a hard target. • It is organised around frames, rather than words, so inevitably its priority is to give a co315 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 315–324, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Lingu</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28(3):245–288, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
<author>James Pustejovsky</author>
</authors>
<title>A Pattern Dictionary for Natural Language Processing. Revue Franc¸aise de linguistique applique,</title>
<date>2005</date>
<pages>10--2</pages>
<contexts>
<context position="4401" citStr="Hanks and Pustejovsky, 2005" startWordPosition="693" endWordPosition="696">rist (Fillmore) doing his best to make sense of the data for a set of verbs. The prospects of data-driven frame discovery are, correspondingly, slim. • While FrameNet has worked hard at being systematic in its use of corpus data, FrameNetters looked only for examples showing the verb being used in the relevant sense. From the point of view of a process that could possibly be automated, this is problematic. An approach which bears many similarities to FrameNet, but which starts from the verb rather than the frame, and is more thoroughgoing in its empiricism, is Hanks’s Corpus Pattern Analysis (Hanks and Pustejovsky, 2005; Hanks, 2012; Hanks, 2013). 2 Corpus Pattern Analysis Corpus Pattern Analysis (CPA) is a new technique of language analysis, which produces the main patterns of use of words in text. Figure 1 is a sample lexical entry from the main output of CPA, the Pattern Dictionary of English Verbs1 (PDEV). This tells us that, for the verb abolish, three patterns were found. For each pattern it tells us the percentage of the data that it accounted for, its grammatical structure and the semantic type (drawn from a shallow ontology of 225 semantic types2) of each of the arguments in this structure. For inst</context>
</contexts>
<marker>Hanks, Pustejovsky, 2005</marker>
<rawString>Patrick Hanks and James Pustejovsky. 2005. A Pattern Dictionary for Natural Language Processing. Revue Franc¸aise de linguistique applique, 10:2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>How people use words to make meanings: Semantic types meet valencies.</title>
<date>2012</date>
<booktitle>Input, Process and Product: Developments in Teaching and Language Corpora,</booktitle>
<pages>54--69</pages>
<editor>In A. Boulton and J. Thomas, editors,</editor>
<location>Brno.</location>
<contexts>
<context position="4414" citStr="Hanks, 2012" startWordPosition="697" endWordPosition="698">t to make sense of the data for a set of verbs. The prospects of data-driven frame discovery are, correspondingly, slim. • While FrameNet has worked hard at being systematic in its use of corpus data, FrameNetters looked only for examples showing the verb being used in the relevant sense. From the point of view of a process that could possibly be automated, this is problematic. An approach which bears many similarities to FrameNet, but which starts from the verb rather than the frame, and is more thoroughgoing in its empiricism, is Hanks’s Corpus Pattern Analysis (Hanks and Pustejovsky, 2005; Hanks, 2012; Hanks, 2013). 2 Corpus Pattern Analysis Corpus Pattern Analysis (CPA) is a new technique of language analysis, which produces the main patterns of use of words in text. Figure 1 is a sample lexical entry from the main output of CPA, the Pattern Dictionary of English Verbs1 (PDEV). This tells us that, for the verb abolish, three patterns were found. For each pattern it tells us the percentage of the data that it accounted for, its grammatical structure and the semantic type (drawn from a shallow ontology of 225 semantic types2) of each of the arguments in this structure. For instance, pattern</context>
</contexts>
<marker>Hanks, 2012</marker>
<rawString>Patrick Hanks. 2012. How people use words to make meanings: Semantic types meet valencies. In A. Boulton and J. Thomas, editors, Input, Process and Product: Developments in Teaching and Language Corpora, pages 54–69. Brno.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>Lexical Analysis: Norms and Exploitations.</title>
<date>2013</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4428" citStr="Hanks, 2013" startWordPosition="699" endWordPosition="700">se of the data for a set of verbs. The prospects of data-driven frame discovery are, correspondingly, slim. • While FrameNet has worked hard at being systematic in its use of corpus data, FrameNetters looked only for examples showing the verb being used in the relevant sense. From the point of view of a process that could possibly be automated, this is problematic. An approach which bears many similarities to FrameNet, but which starts from the verb rather than the frame, and is more thoroughgoing in its empiricism, is Hanks’s Corpus Pattern Analysis (Hanks and Pustejovsky, 2005; Hanks, 2012; Hanks, 2013). 2 Corpus Pattern Analysis Corpus Pattern Analysis (CPA) is a new technique of language analysis, which produces the main patterns of use of words in text. Figure 1 is a sample lexical entry from the main output of CPA, the Pattern Dictionary of English Verbs1 (PDEV). This tells us that, for the verb abolish, three patterns were found. For each pattern it tells us the percentage of the data that it accounted for, its grammatical structure and the semantic type (drawn from a shallow ontology of 225 semantic types2) of each of the arguments in this structure. For instance, pattern 1 means: i) t</context>
</contexts>
<marker>Hanks, 2013</marker>
<rawString>Patrick Hanks. 2013. Lexical Analysis: Norms and Exploitations. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Holub</author>
<author>Vincent Krfz</author>
<author>Silvie Cinkov´a</author>
<author>Eckhard Bick</author>
</authors>
<title>Tailored Feature Extraction for Lexical Disambiguation of English Verbs Based on Corpus Pattern Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (Coling</booktitle>
<pages>1195--1209</pages>
<location>Mumbai, India.</location>
<marker>Holub, Krfz, Cinkov´a, Bick, 2012</marker>
<rawString>Martin Holub, Vincent Krfz&amp;quot;, Silvie Cinkov´a, and Eckhard Bick. 2012. Tailored Feature Extraction for Lexical Disambiguation of English Verbs Based on Corpus Pattern Analysis. In Proceedings of the 24th International Conference on Computational Linguistics (Coling 2012), pages 1195–1209, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milos Jakubfcek</author>
<author>Adam Kilgarriff</author>
<author>Vojtech Kov´ar</author>
<author>Pavel Rychl´y</author>
<author>Vft Suchomel</author>
</authors>
<title>The TenTen Corpus Family.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Corpus Linguistics.</booktitle>
<marker>Jakubfcek, Kilgarriff, Kov´ar, Rychl´y, Suchomel, 2013</marker>
<rawString>Milo&amp;quot;s Jakubf&amp;quot;cek, Adam Kilgarriff, Vojt&amp;quot;ech Kov´a&amp;quot;r, Pavel Rychl´y, and Vft Suchomel. 2013. The TenTen Corpus Family. In Proceedings of the International Conference on Corpus Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daisuke Kawahara</author>
<author>Daniel W Peterson</author>
<author>Octavian Popescu</author>
<author>Martha Palmer</author>
</authors>
<title>Inducing Examplebased Semantic Frames from a Massive Amount of Verb Uses.</title>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>58--67</pages>
<marker>Kawahara, Peterson, Popescu, Palmer, </marker>
<rawString>Daisuke Kawahara, Daniel W Peterson, Octavian Popescu, and Martha Palmer. Inducing Examplebased Semantic Frames from a Massive Amount of Verb Uses. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 58–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<title>Introduction to the Special</title>
<date>2000</date>
<booktitle>Issue on SENSEVAL. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1942" citStr="Kilgarriff and Palmer, 2000" startWordPosition="285" endWordPosition="288"> constructed automatically. Subtask 1 attracted 3 teams, though none could beat the baseline (rule-based system). Subtask 2 attracted 2 teams, one of which beat the baseline (majority-class classifier). Subtask 3 did not attract any participant. The task has produced a major semantic multidataset resource which includes data for 121 verbs and about 17,000 annotated sentences, and which is freely accessible. 1 Introduction It is a central vision of NLP to represent the meanings of texts in a formalised way, amenable to automated reasoning. Since its birth, SEMEVAL (or SENSEVAL as it was then; (Kilgarriff and Palmer, 2000)) has been part of the programme of enriching NLP analyses of text so they get ever closer to a ’meaning representation’. In relation to lexical information, this meant finding a lexical resource which • identified the different meanings of words in a way that made high-quality disambiguation possible, • represented those meanings in ways that were useful for the next steps of building meaning representations. Most lexical resources explored to date have had only limited success, on either front. The most obvious candidates—published dictionaries and WordNets—look like they might support the f</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>Adam Kilgarriff and Martha Palmer. 2000. Introduction to the Special Issue on SENSEVAL. Computers and the Humanities, 34:1–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>V´ıt Baisa</author>
<author>Jan Busta</author>
<author>Milos Jakub´ıcek</author>
<author>Vojtech Kov´ar</author>
<author>Jan Michelfeit</author>
<author>Pavel Rychl´y</author>
<author>V´ıt Suchomel</author>
</authors>
<title>The Sketch Engine: ten years on.</title>
<date>2014</date>
<journal>Lexicography,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Kilgarriff, Baisa, Busta, Jakub´ıcek, Kov´ar, Michelfeit, Rychl´y, Suchomel, 2014</marker>
<rawString>Adam Kilgarriff, V´ıt Baisa, Jan Bu&amp;quot;sta, Milos&amp;quot; Jakub´ı&amp;quot;cek, Vojt&amp;quot;ech Kov´a&amp;quot;r, Jan Michelfeit, Pavel Rychl´y, and V´ıt Suchomel. 2014. The Sketch Engine: ten years on. Lexicography, 1(1):7–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Makhoul</author>
<author>Francis Kubala</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Performance Measures For Information Extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of DARPA Broadcast News Workshop,</booktitle>
<pages>249--252</pages>
<contexts>
<context position="20069" citStr="Makhoul et al., 1999" startWordPosition="3343" endWordPosition="3346">ater extended to cluster evaluation (Amig´o et al., 2009). Both measures are averages of the precision and recall over all instances. To calculate the precision of each instance we count all correct pairs associated with this instance and divide by the number of actual pairs in the candidate cluster that the instance belongs to. Recall is computed by interchanging Gold and Candidate clusterings (Eq. 3). Pairsi in Candidate found in Gold Precisioni = Pairsi in Candidate Pairsi in Gold found in Candidate Recalli = Pairsi in Gold (3) Subtask 3. This task was evaluated as a slot-filling exercise (Makhoul et al., 1999), so the scores were computed by taking into account the kinds of errors In order not to penalize systems, the best match was computed for each Candidate pattern, and one candidate pattern could match more than one Gold pattern. When a given slot was filled both in the Gold data and the Candidate data, this counted as a “match”. When not, it was a Deletion. If a slot was filled in the run but not in the gold, it was counted as an Insertion. When a match (aligned slots) was also a semantic type match, it was Correct (1 point). When not, it was a Substitution; the CPA ontology was used to allow </context>
</contexts>
<marker>Makhoul, Kubala, Schwartz, Weischedel, 1999</marker>
<rawString>John Makhoul, Francis Kubala, Richard Schwartz, and Ralph Weischedel. 1999. Performance Measures For Information Extraction. In Proceedings of DARPA Broadcast News Workshop, pages 249–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Mills</author>
<author>Gina-Anne Levow</author>
</authors>
<title>CMILLS: Adapting SRL Features to Dependency Parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval 2015,</booktitle>
<location>Denver, USA.</location>
<contexts>
<context position="22925" citStr="Mills and Levow, 2015" startWordPosition="3839" endWordPosition="3842">Institution 182 0.441 0.531 0.483 0.461 Action 115 0.421 0.594 0.526 0.506 Table 6: Detailed scores for subtask 1 (10 most frequent categories). Team Score baseline 0.624 FANTASY 0.589 BLCUNLP 0.530 CMILLS 0.516 Table 5: Official scores for subtask 1. subtask allowed it, some systems used external resources such as Wordnet or larger corpora. BLCUNLP (Feng et al., 2015) used the Stanford CoreNLP package10 to get POS, NE and basic dependency features. These features were used to predict both syntax and semantic information. The method did not involve the use of a statistical classifier. CMILLS (Mills and Levow, 2015) used three models to solve the task: one for argument detection, and the other two for each layer. Argument detection and syntactic tagging were performed using a MaxEnt supervised classifier, while the last was based on heuristics. CMILLS also reported the use of an external resource, the enTentTen12 (Jakubiˇcek et al., 2013) corpus available in Sketch Engine (Kilgarriff et al., 2014). FANTASY approached the subtask in a supervised setting to predict first the syntactic tags, and then the semantic tags. The team used features from the MST parser11, as well as Stanford CoreNLP for NE, Wordnet</context>
</contexts>
<marker>Mills, Levow, 2015</marker>
<rawString>Chad Mills and Gina-Anne Levow. 2015. CMILLS: Adapting SRL Features to Dependency Parsing. In Proceedings of SemEval 2015, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Duluth: Word Sense Discrimination in the Service of Lexicography.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval 2015,</booktitle>
<location>Denver, USA.</location>
<contexts>
<context position="24360" citStr="Pedersen, 2015" startWordPosition="4059" endWordPosition="4060"> of each layer. The baseline system was a rule-based system taking as input the output of the BLLIP parser (Charniak and Johnson, 2005), and mapping heads of relevant dependency relations to the most probable tags from subtask 1 tagset. The semantic tags were only then added to those headwords based on the most frequent semantic category found in the training set. 5.2 Subtask 2 As opposed to subtask 1, systems in subtask 2 used very few semantic and syntactic resources. BOB90 used a supervised approach to tackle the clustering problem. The main features used were preposition analyses. DULUTH (Pedersen, 2015) used an unsupervised approach and focused on lexical similarity (both first and second order representations) based on unigrams and bigrams (see SenseClusters13). The number of clusters was predicted on the basis of the best value for the clustering criterion function. The team also performed some corpus pre-processing, like conversion to lower case and conversion of all numeric values to a string. The baseline system clusters everything together, so its score depends on the distribution of patterns: the more a pattern covers all instances of the data (majority class), the higher the baseline</context>
</contexts>
<marker>Pedersen, 2015</marker>
<rawString>Ted Pedersen. 2015. Duluth: Word Sense Discrimination in the Service of Lexicography. In Proceedings of SemEval 2015, Denver, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Octavian Popescu</author>
<author>Martha Palmer</author>
<author>Patrick Hanks</author>
</authors>
<title>Mapping CPA onto OntoNotes Senses.</title>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>882--889</pages>
<location>Reykjavik, Iceland.</location>
<marker>Popescu, Palmer, Hanks, </marker>
<rawString>Octavian Popescu, Martha Palmer, and Patrick Hanks. Mapping CPA onto OntoNotes Senses. In Proceedings of LREC, pages 882–889, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Octavian Popescu</author>
</authors>
<title>Learning corpus patterns using finite state automata.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics,</booktitle>
<pages>191--203</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="6423" citStr="Popescu, 2013" startWordPosition="1032" endWordPosition="1033">rbs with a total of over 4,000 patterns. For each verb a random sample of (by default) 250 corpus instances was examined, used to build the lexical entry, and tagged with the senses and patterns they represented. For commoner verbs, more corpus lines were examined. The corpus instances were drawn from the written part of the British National Corpus3 (BNC). PDEV has been studied from different NLP perspectives, all mainly involved with Word Sense Disambiguation and semantic analysis (Cinkov´a et al., 2012a; Holub et al., 2012; El Maarouf et al., ; El Maarouf and Baisa, 2013; Kawahara et al., ; Popescu, 2013; Popescu et al., ; Pustejovsky et al., 2004; Rumshisky et al., ). For example, (Popescu, 2013) described experiments in modeling finite state automata on a set of 721 verbs taken from PDEV. The author reports an accuracy of over 70% in pattern disambiguation. (Holub et al., 2012) trained several statistical classifiers on a modified subset of 30 PDEV entries (Cinkov´a et al., 2012c) using morpho-syntactic as well as semantic features, and obtained over 80% accuracy. On a smaller set of 20 high frequency verbs (El Maarouf and Baisa, 2013) reached a similar 0.81 overall F1 score with a supervis</context>
</contexts>
<marker>Popescu, 2013</marker>
<rawString>Octavian Popescu. 2013. Learning corpus patterns using finite state automata. In Proceedings of the 10th International Conference on Computational Semantics, pages 191–203, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Anna Rumshisky</author>
</authors>
<title>Automated Induction of Sense in Context.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="6467" citStr="Pustejovsky et al., 2004" startWordPosition="1038" endWordPosition="1041">tterns. For each verb a random sample of (by default) 250 corpus instances was examined, used to build the lexical entry, and tagged with the senses and patterns they represented. For commoner verbs, more corpus lines were examined. The corpus instances were drawn from the written part of the British National Corpus3 (BNC). PDEV has been studied from different NLP perspectives, all mainly involved with Word Sense Disambiguation and semantic analysis (Cinkov´a et al., 2012a; Holub et al., 2012; El Maarouf et al., ; El Maarouf and Baisa, 2013; Kawahara et al., ; Popescu, 2013; Popescu et al., ; Pustejovsky et al., 2004; Rumshisky et al., ). For example, (Popescu, 2013) described experiments in modeling finite state automata on a set of 721 verbs taken from PDEV. The author reports an accuracy of over 70% in pattern disambiguation. (Holub et al., 2012) trained several statistical classifiers on a modified subset of 30 PDEV entries (Cinkov´a et al., 2012c) using morpho-syntactic as well as semantic features, and obtained over 80% accuracy. On a smaller set of 20 high frequency verbs (El Maarouf and Baisa, 2013) reached a similar 0.81 overall F1 score with a supervised SVM classifier based on dependency parsin</context>
</contexts>
<marker>Pustejovsky, Hanks, Rumshisky, 2004</marker>
<rawString>James Pustejovsky, Patrick Hanks, and Anna Rumshisky. 2004. Automated Induction of Sense in Context. In Proceedings of COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anna Rumshisky</author>
<author>Patrick Hanks</author>
<author>Catherine Havasi</author>
<author>James Pustejovsky</author>
</authors>
<title>Constructing a corpus-based ontology using model bias.</title>
<booktitle>In Proceedings of FLAIRS,</booktitle>
<pages>327--332</pages>
<location>Melbourne, FL.</location>
<marker>Rumshisky, Hanks, Havasi, Pustejovsky, </marker>
<rawString>Anna Rumshisky, Patrick Hanks, Catherine Havasi, and James Pustejovsky. Constructing a corpus-based ontology using model bias. In Proceedings of FLAIRS, pages 327–332, Melbourne, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<date>2010</date>
<booktitle>SemEval2010 Task 10: Linking Events and Their Participants in Discourse. In 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>45--50</pages>
<location>Uppsala,</location>
<contexts>
<context position="3129" citStr="Ruppenhofer et al., 2010" startWordPosition="480" endWordPosition="483">ets—look like they might support the first task, but are very limited in what they offer to the second. FrameNet moved the game forward a stage. Here was a framework with a convincing account of how the lexical entry might contribute to building the meaning of the sentence, and with enough meat in the lexical entries (e.g. the verb frames) so that it might support disambiguation. Papers such as (Gildea and Jurafsky, 2002) looked promising, and in 2007 there was a SEMEVAL task on Frame Semantic Structure Extraction (Baker et al., 2007) and in 2010, one on Linking Events and Their Participants (Ruppenhofer et al., 2010). While there has been a substantial amount of follow-up work, there are some aspects of FrameNet that make it a hard target. • It is organised around frames, rather than words, so inevitably its priority is to give a co315 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 315–324, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics herent account of the different verb senses in a frame, rather than the different senses of an individual verb. This will tend to make it less good for supporting disambiguation. • Frames are n</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. SemEval2010 Task 10: Linking Events and Their Participants in Discourse. In 5th International Workshop on Semantic Evaluation, pages 45–50, Uppsala, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>