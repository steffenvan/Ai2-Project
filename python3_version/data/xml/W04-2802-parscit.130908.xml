<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.971144">
Towards Measuring Scalability in Natural Language Understanding Tasks
</title>
<author confidence="0.889294">
Robert Porzel Rainer Malaka
</author>
<affiliation confidence="0.753053">
European Media Laboratory
</affiliation>
<address confidence="0.8552545">
Schloss-Wolfsbrunnenweg 33
D-69118 Heidelberg, Germany
</address>
<email confidence="0.892031">
{robert.porzel,rainer.malaka@eml-d.villa-bosch.de}
</email>
<sectionHeader confidence="0.990451" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999434">
In this paper we present a discussion of existing
metrics for evaluation the performance of indi-
vidual natural language understanding systems
and components as well as the commonly em-
ployed metrics for measuring the specific task
difficulties. We extend and generalize the com-
mon majority class baseline metric and intro-
duce an general entropy-based metric for mea-
suring the task difficulty of arbitrary language
understanding tasks. Finally, we show an em-
pirical study evaluating this metric followed by
a discussion of its role in measuring the scal-
ability of language understanding systems and
components.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918137931035">
Current evaluation frameworks for uni- or multi-modal
dialogue systems (Walker et al., 2000; Beringer et al.,
2002) that allow for spoken language input do not include
metrics for measuring the accuracy of the involved inten-
tion recognition systems, simply because such informa-
tion is hard to extract automatically from log files. Fur-
thermore no general computational method or framework
for measuring the difficulty of natural language under-
standing tasks have been proposed so far. We are, there-
fore, faced with a lack of methods for measuring the dif-
ficulties of the individual tasks involved in the language
understanding process. Such generally applicable meth-
ods, however, are needed for measuring the scalability of
natural language understanding systems and components.
In this paper we first discuss existing metrics for mea-
suring task-specific performances and the correspond-
ing baseline metrics in natural language understanding
in Section 2. We, then, propose a generalized baseline-
based metric in Section 4.1 as well as a general entropy-
based metric in Section 4.2. Both methods can be em-
ployed for measuring the difficulties of various under-
standing tasks and, consequently, for evaluating natural
language understanding components involved in the in-
tention recognition process. Section 5 provides a case
study evaluation of the proposed methods. In Section 6
we discuss how an analysis of a specific system on tasks
differing in their difficulty can yield a first approach for
measuring the scalability of a natural language under-
standing systems and its components.
</bodyText>
<sectionHeader confidence="0.954188" genericHeader="method">
2 Evaluating Dialogue-, Speech- and
Discourse Understanding Systems
</sectionHeader>
<bodyText confidence="0.9984186">
In this section we will briefly sketch out the most fre-
quently used metrics for evaluating the performances of
the relevant components and systems at hand.
Evaluation of the Dialogue Systems Performance:
For evaluation of the overall performance of a dia-
logue system as a whole frameworks such as PAR-
ADISE (Walker et al., 2000) for unimodal and PROMISE
(Beringer et al., 2002) for multimodal systems have set
a de facto standard. These frameworks differentiate be-
tween:
</bodyText>
<listItem confidence="0.999335571428572">
• dialogue efficiency metrics, i.e. elapsed time,
system- and user turns
• dialogue quality metrics, mean recognition score
and absolute number as well as percentages of time-
outs, rejections, helps, cancels, and barge-ins,
• task success metrics, task completion (per survey)
• user satisfaction metrics (per survey)
</listItem>
<bodyText confidence="0.99960155">
These metrics are crucial for evaluating the aggregate
performance of the individual components, they cannot,
however, determine the amount of understanding versus
misunderstanding or the system-specific a priori diffi-
culty of the understanding task. Their importance, how-
ever, will remain undiminished, as ways of determining
such global parameters are vital to determining the aggre-
gate usefulness and felicity of a system as a whole. At the
same time individual components and ensembles thereof
- such as the performance of the uni- or multi-modal in-
put understanding system - need to be evaluated as well
to determine bottlenecks and weak links in the discourse
understanding processing chain.
Evaluation of the Automatic Speech Recognition Per-
formance: The commonly used word error rate (WER)
can be calculated by aligning any two sets word se-
quences and adding the number of substitutions S, dele-
tions D and insertions I. The WER is then given by the
following formula where N is the total number of words
in the test set.
</bodyText>
<equation confidence="0.769216">
WER = S + N+ I × 100
</equation>
<bodyText confidence="0.999476647058824">
Another measure of accuracy that is frequently used is
the so called Out Of Vocabulary (OOV) measure, which
represents the percentage of words that was not recog-
nized despite their lexical coverage. WER and OOV
are commonly intertwined together with the combined
acoustic- and language-model confidence scores, which
are constituted by the posterior probabilities of the hidden
Markov chains and n-gram frequencies. Together these
scores enable evaluators to measure the absolute perfor-
mance of a given speech recognition system. In order
to arrive at a measure that is relative to the given task-
difficulty, this difficulty must also be calculated, which
can be done by means of measuring the perplexity of the
task (see Section 3).
Evaluation of the Natural Language Understanding
Performance: A measure for understanding rates -
called concept error rate has been proposed for example
by Chotimongcol and Rudnicky (2001) and is designed
in analogy to word error rates employed in automatic
speech recognition that are combined with keyword spot-
ting systems. Chotimongcol and Rudnicky (2001) pro-
pose to differentiate whether the erroneous concept oc-
curs in a non-concept slot that contains information that
is captured in the grammar but not considered relevant for
selecting a system action (e.g., politeness markers, such
as please), in a value-insensitive slot whose identity, suf-
fices to produce a system action (e.g., affirmatives such
as yes), or in a value-sensitive slot for which both the
occurrence and the value of the slot are important (e.g.,
a goal object, such as Heidelberg). An alternative pro-
posal for concept error rates is embedded into the speech
recognition and intention spotting system by Lumenvox1,
wherein two types of errors and two types of non-errors
for concept transcriptions are proposed:
</bodyText>
<listItem confidence="0.9975375">
• A match when the application returned the correct
concept and an out of grammar match when the ap-
</listItem>
<footnote confidence="0.877024">
1www.lomunevox.com/support/tunerhelp/Tuning/Concept
Transcription.htm
</footnote>
<bodyText confidence="0.999396">
plication returned no concepts, or discarded the re-
turned concepts because the user failed to say any
concept covered by the grammar.
</bodyText>
<listItem confidence="0.8506925">
• A grammar mismatch when the application returned
the incorrect concept, but the user said a concept
covered by the grammar and an out ofgrammar mis-
match when the application returned a concept, and
chose that concept as a correct interpretation, but the
user did not say a concept covered by the grammar.
</listItem>
<bodyText confidence="0.999922">
Neither of these measures are suitable for our pur-
poses as they are known to be feasible only for context-
insensitive applications that do not include discourse
models, implicit domain-specific information and other
contextual knowledge as discussed in (Porzel et al.,
2004). Therefore this measure has also been called key-
word recognition rate for single utterance systems. In our
minds another crucial shortcoming is the lack of compa-
rability, as these measures do not take the general dif-
ficulty of the understanding tasks into account. Again,
this has been realized in the automatic speech recogni-
tion community and led to the so called perplexity mea-
surements for a given speech recognition task. We will,
therefore, sketch out the commonly employed perplexity
measurements in Section 3.
The most detailed evaluation scheme for discourse
comprehension, introduced by Higashinaka et al. (2002)
and also extended by Higashinaka et al. (2003), features
the metrics given in Table 2.
</bodyText>
<listItem confidence="0.989478615384615">
1. slot accuracy
2. insertion error rate
3. deletion error rate
4. substitution error rate
5. slot error rate
6. update precision
7. update insertion error rate
8. update deletion error rate
9. update substitution error rate
10. speech understanding rate
11. slot accuracy for filled slots
12. deletion error rate for filled slots
13. substitution error rate for filled slots
</listItem>
<tableCaption confidence="0.908822">
Table 1: Discourse Comprehension Measurements
</tableCaption>
<bodyText confidence="0.993457464788733">
These metrics are combined by means of combin-
ing the results of an m5 multiple linear regression al-
gorithm and a support vector regression approach. The
resulting weighted sum is compared to human intuitions
and PARADISE-like metrics concerning task completion
rates and -times. While this promising approach manages
to combine factors related to speech recognition, interpre-
tation and discourse modeling, there are some shortcom-
ings that stem from the fact that this schema was devel-
oped for single-domain systems that employ frame-based
attribute value pairs for representing the user’s intent.
Recent advances in dialogue management and multi-
domain systems enable approaches that are more flexi-
ble than slot-filling, e.g. using discourse pegs, dialogue
games and overlay operations for handling multiple tasks
and cross-modal references (LuperFoy, 1992; L¨ockelt et
al., 2002; Pfleger et al., 2002; Alexandersson and Becker,
2003). More importantly - for the topic of this paper - no
means of measuring the a priori discourse understanding
difficulty is given.
Measuring Precision, Recall and F-Measures: In the
realm of semantic analyses the task of word sense dis-
ambiguation is usually regarded to be among the difficult
ones. This means it can only be solved after all other
problems involved in language understanding have been
resolved as well. The hierarchical nature and interdepen-
dencies of the various tasks are mirrored in the results
of the corresponding competitive evaluation tracts - e.g.
the message understanding conference (MUC) or SEN-
SEVAL competition. It becomes obvious that the un-
graceful degradation of f-measure scores (shown in Ta-
ble 2 is due to the fact that each higher-level task inherits
the imprecisions and omissions of the previous ones, e.g.
errors in the named entity recognition (NE) task cause re-
call and precision declines in the template element task
(TE), which, in turn, thwart successful template relation
task performance (TR) as well as the most difficult sce-
nario template (ST) and co-reference task (CO). This de-
cline can be seen in Table 2 (Marsh and Perzanowski,
1999).
NE CO TE TR ST
f &lt; .94 f &lt; .62 f &lt; .87 f &lt; .76 f &lt; .51
Table 2: F-measure-based (a = 0.5) evaluation results of
the best performing systems of the 7th Message Under-
standing Conference
Despite several problems stemming from the prerequi-
site to craft costly gold standards, e.g. tree banks or an-
notated test corpora, precision and recall and their weigh-
able combinations in the corresponding f-measures (such
as given in Table 2), have become a de facto standard for
measuring the performance of classification and retrieval
tasks (Van Rijsbergen, 1979). Precision p states the per-
centage of correctly tagged (or classified) entities of all
tagged/classified entities, whereas recallr states the pos-
itive percentage of entities tagged/classified as compared
to the normative amount, i.e. those that ought to have
been tagged or classified. Together these are combinable
to an overall f-measure score, defined as:
Herein a can be set to reflect the respective importance
of p versus r, if a = 0.5 then both are weighted equally.
These measures are commonly employed for evaluating
part-of-speech tagging, shallow parsing, reference reso-
lution tasks and information retrieval tasks and sub-tasks.
An additional problem with this method is that most
natural language understanding systems that perform
deeper semantic analyses produce representations often
based on individual grammar formalisms and mark-up
languages for which no gold standards exist. For evaluat-
ing discourse understanding systems, however, such gold
standards and annotated training corpora will continue to
be needed.
</bodyText>
<sectionHeader confidence="0.609124" genericHeader="method">
3 Measuring Perplexity and Baselines
</sectionHeader>
<bodyText confidence="0.989761181818182">
In this section we will describe the most frequently used
metrics for estimating the complexity of the tasks per-
formed by the relevant components and systems at hand.
Measuring Perplexity in Automatic Speech Recog-
nition: Perplexity is a measure of the probability
weighted average number of words that may follow af-
ter a given word (Hirschman and Thompson, 1997). In
order to calculate the perplexity B, the entropy H needs
to be given - i.e., the probability of the word sequences in
the specific language of the systemW. The perplexity is
then defined as:
</bodyText>
<equation confidence="0.999946666666667">
H = − � P(W)log2P(W)
VW
B = 2H
</equation>
<bodyText confidence="0.991977909090909">
Improvements of specific ASR systems can then con-
sequently be measured by keeping the perplexity constant
and measuring WER and OOV performance for recogni-
tion quality and confidence scores for hypothesis verifi-
cation and selection.
Measuring Task-specific Baselines: Baselines for
classification or tagging tasks are commonly defined
based on chance performance, on an a posteriori com-
puted majority class performance or on the performance
of an established baseline classification method such as
naive bayes, tf*idf or k-means . That means:
</bodyText>
<listItem confidence="0.934699">
• what is the corresponding f-measure, if the evalu-
ated component guesses randomly - for chance per-
formance metrics,
</listItem>
<equation confidence="0.87482575">
1
a 1+ (1 − a)1
p r
F =
</equation>
<listItem confidence="0.9967286">
• what is the corresponding f-measure if the evaluated
component always chooses the most frequent solu-
tion - for majority class performance metrics,
• what is the corresponding f-measure of the estab-
lisched baseline classification method.
</listItem>
<bodyText confidence="0.999688894736842">
Much like kappa statistics proposed by
Carletta (1996), existing employments of majority
class baselines assume an equal set of identical poten-
tial mark-ups, i.e. attributes and their values, for all
markables. Therefore, they cannot be used in a straight
forward manner for many tasks that involve disjunct
sets of attributes and values in terms of the type and
number of attributes and their values involved in the
classification task. This, however, is exactly what we
find in natural language understanding tasks, such as
semantic tagging or word sense disambiguation tasks
(Stevenson, 2003). Additionally, baseline computed on
other methods cannot serve as a means for measuring
scalability, because of the circularity involved: as one
would need a way of measuring the baseline method’s
scalability factor in the first place. Table 3 provides an
overview of the existing ways of measuring performance
and task difficulty in automatic speech recognition and
understanding.
</bodyText>
<table confidence="0.9998414375">
Domain Performance Complexity
automatic WER/OVV Perplexity
speech
recognition
natural CER none
language
understanding
MUC tasks f-measure baselines
(NE, TE, TR,
ST, CO)
unimodal PARADISE none
dialogue
system
mulitmodal PARADISE none
dialogue
system
</table>
<tableCaption confidence="0.999129">
Table 3: Summary of Measurements
</tableCaption>
<sectionHeader confidence="0.987825" genericHeader="method">
4 Measuring Task Difficulty
</sectionHeader>
<subsectionHeader confidence="0.98784">
4.1 Proportional Baseline Rates
</subsectionHeader>
<bodyText confidence="0.95385114">
As a precursor step before this we need a clear defini-
tion of a natural language understanding task. For this
we propose to assume a MATE-like annotation point of
view, which provides a set of disjunct levels of annota-
tions for the individual discriminatory decisions that can
be performed on spoken dialogue data, ranging from an-
notating referring expressions, e.g. named entities and
their relations, anaphora and their antecedents, to word
senses and dialogue acts. Each task must, therefore, have
a clearly defined set of markables, attributes and values
for each corpus of spoken dialogue data.
As a first step we will propose a uniform and generic
method for computing task-specific majority class base-
lines for a given task T,,, from the entire set of task, i.e.
T = {T1,..., Tz} and T,,, E T.
A gold standard annotation of a task features a finite set
of markable tokens C = {c1, ... , cn} for task T,,,, e.g.
if n = 2 in a corpus containing only the two ambiguous
lexemes bank and run as markables, i.e. c1 and c2 respec-
tively. For a member ci of the set C we can now define
the number of values for the tagging attribute of sense
as: Ai = {bi1, ... , bin,}. For example, for three senses of
the markable bank as c1 we get the corresponding value
set A1 = {building, institution, shore} and
for run as c2 the value set A2 = {motion, storm}.
Note that the value sets have markable-dependent sizes.
For our toy example containing the two markables c1 for
bank and c2 for run they are:
bi j bi1 bi2 bi3
A1 building institution shore
A2 motion storm
For computing the proportional majority classes we
need to compute the occurrences of a value j for a mark-
able i in a given gold standard test data set. We call this
Vij. Now we can determine the most frequently given
value and its number for each markable ci as:
Vmax
i = maxijE{1,...,b,}Vij
For example, given a marked-up toy corpus containing
our ambiguous lexemes as shown below as task T1:
The runstorm on the bankbuilding on Mon-
day caused the bankinstitution to collapse early
this week. It employees can therefore now
enjoy a leisurely runmotion on the bankshore
of the Hudson river. It is uncertain if the
bankinstitution can be saved so that they can
runmotion back to their desks and resume their
work.
This results in the list of the value occurrences shown
below with Vmax iset in bold face:
</bodyText>
<table confidence="0.741927">
Vij bi bi bi
c1 1 2 3
c2 1 2 1
2 1
</table>
<bodyText confidence="0.99756">
We define the total number of values for a markableci as:
</bodyText>
<equation confidence="0.959151307692308">
Vij
V S
i =
n 1
E
j=1
With V max
i we define the majority classe baseline as:
V max
i
Bi =
V S
i
</equation>
<bodyText confidence="0.998686833333333">
If we always choose the most frequent attribute for mark-
able ci, the percentage of correct guesses correspomds to
Bi. We can now calculate the total number of values as:
Based on this we can compute the task-specific propor-
tional baseline for task Tw, i.e., BT., over the entire test
set as:
</bodyText>
<equation confidence="0.929802">
�n
V S
i Bi = V1S ·
i=1
</equation>
<bodyText confidence="0.999891875">
Thus, BT. calculates the average of correct guesses
for the majority baseline. Returning to our toy example
for c1 we get V1S = 4 and for c2 we get V2S = 3. Ad-
ditionally, we also get different individual majority class
baselines for each markable, i.e., for c1 we get B1 = 12,
and for c2 we get B2 = 23. We also get a total number of
values given for C (c1 and c2), i.e., VS = 7. Now we can
compute the overall baseline BT, as:
</bodyText>
<equation confidence="0.9968975">
1 1 24
7 · ((4 · 2) + (3 · 3)) = 7  0.57
</equation>
<bodyText confidence="0.999235">
If we extend the corpus by an additional ambiguity, i.e.
that of the spatial and temporal readings of the lexeme on,
to yield an annotated corpus such as given below as task
T2:
The runstorm on the bankbvilding ontemporal
Monday caused the bankinstitvtion to collapse
early this week. It employees can therefore
now enjoy a leisurely runmotion onspatial the
bankshore of the Hudson river. It is uncertain
if the bankinstitvtion can be saved so that they
can runmotion back to their desks and resume
their work.
We get the list of the value occurrences shown below:
</bodyText>
<equation confidence="0.98302875">
C bi1 bi2 bi3
c1 1 2 1
c2 2 1
c3 1 1
</equation>
<bodyText confidence="0.9243465">
Now we can compute the overall baseline BT, again
as:
</bodyText>
<equation confidence="0.9903325">
1 1 2 15
9 · ((4 · 2) + (3 · 3) + (2 · 2)) = 9  0.55
</equation>
<bodyText confidence="0.999943916666667">
The reduction by .02 points, in this case, indicates that
a method that for each markable always chooses the most
frequently occurring one would perform slightly worse
on the second corpus as compared to the first. Note that
this proportional baseline measure is able to compute the
performance of such a majority class-based method on
any data set for any task. It does as such provide a pic-
ture depicting a problem’s or task’s inherent difficulty, but
only if the distribution of values for the markables at hand
is fairly homogeneous. However, if we assume distribu-
tions of markable values such as shown below, we get
identical values for BT,, and BT4.
</bodyText>
<equation confidence="0.948929142857143">
T3 bi1 bi2 bi3 bi 4 bi5
c3 16 16 0 0 0
T4 bi1 bi2 bi3 bi4 bi5
c4 16 4 4 4 4
That is, we get: · (32 1 1 = 0.5
1 · 2) = 2
32
</equation>
<bodyText confidence="0.9998465">
for both task baselines BT,, and BT4 with T3 featuring the
task distribution depicted as c3 and T4 that of c4, despite
the fact task T2 was undoubtedly the more difficult one.
To create a more applicable measure for task difficulty -
i.e. one that also applies for cases of heterogeneous value
distributions - we need do need to calculate an entropy
metric that takes the individual value distributions into
account.
</bodyText>
<subsectionHeader confidence="0.997149">
4.2 Measuring Markable-specific Entropy
</subsectionHeader>
<bodyText confidence="0.999671">
As a means of illustrating such a markable-specific en-
tropy metric we can look at the value space for each
markable and define a minimal amount of binary deci-
sions that are on average necessary for solving the prob-
lem and compute what part of the problem is solved by
them. For example, looking at the markable c4 from
above we find that the problem can be solved by means of
the following decisions: With one decision we can parti-
tion the space between b4 1 and the rest (b42 through b45)
thereby assigning 16 times the value b41 to c4. This deci-
sion already solves 50% of the problem. Next we need a
second decision for partitioning the value space between
b42  b43 and b44  b45 and a third for cutting between b42
and b43 as well as b44 and b45 respectively. Therefore, three
decisions are needed for assigning the value 4 to b42 and
solving 12.5% of the problem. In the case of c4 the same
holds for b43, b44 and b45, giving us the following decision
and solution table with d(bji ) standing for the average
amount of binary decisions necessary for solving bji :
</bodyText>
<table confidence="0.9054552">
T4 b4 b4 b4 b4 b4
1 2 3 4 5
c4 16 4 4 4 4
d(b4i) 1 3 3 3 3
solved 50% 12.5% 12.5% 12.5% 12.5%
</table>
<bodyText confidence="0.9184685">
Looking at the markable c3 we find that the problem
can be solved as shown below:
</bodyText>
<equation confidence="0.9748205625">
V S
i
V S =
�n
i=1
�n
1
BT. = V S ·
i=1
V.max
2
T3 b3 b3 b3 3 b3 4 b3
1 2 5
c4 16 16 0 0 0
d(b3i) 1 1 0 0 0
solved 50% 50% 0% 0% 0%
</equation>
<bodyText confidence="0.914454428571429">
As an illustrative approximation of a task’s entropy
we can now compute the aggregate amount of decisions
weighted by their contribution to the overall solution
(given as its probability - i.e. 50% = .5). For c4 this
yields:
2 = (1·.5)+(3·.125)+(3·.125)+(3·.125)+(3·.125)
And for T3 we get:
</bodyText>
<equation confidence="0.99488">
1 = (1 ·.5) + (1 ·.5)
</equation>
<bodyText confidence="0.999778545454545">
In these cases we can now say that solving the markable-
specific value distribution of c4 is more difficult than
solving that of c3, indicated by the increase of 1 point
in this quasi-entropy measure. Note that if we had a
binary decision procedure that solves f% of the cases
correctly than we get an average error rate for T4f2 of
0.9 · 0.9 = 0, 81 whereas for T3 only 0.9.
After this approximate illustration of measuring task
difficulty via the notion of its entropy, we can now com-
pute a corresponding markable-specific entropy measure
Hci based on the standard formula:
</bodyText>
<equation confidence="0.811688">
P(Vij)log2P(bij)
</equation>
<bodyText confidence="0.999865">
This computation yields Hc3 = 1 and Hc4 = 2, which
also reflects the difference in difficulty of T3 (consisting
of the sole markable c3) versus T4 (consisting of the sole
markable c4).
</bodyText>
<subsectionHeader confidence="0.999947">
4.3 Combing Markable-specific Entropies
</subsectionHeader>
<bodyText confidence="0.999725166666667">
We propose to apply an analogous way of combining the
individual markable-specific entropies, by a weighted av-
erage, whereby the markable-specific weights are deter-
mined by V S
i and the averaging based on V S. As an
example we return to our sample tasks T1 and T2.
</bodyText>
<table confidence="0.9699462">
Vij bi1 bi2 bi Hci V S Hci · V S
3 i i
c1 1 2 1 1.5 4 6
c2 2 1  0.92 3  2.76
c3 1 1 1 2 2
</table>
<bodyText confidence="0.999619090909091">
Correspondingly, we get for task T1 consisting of
markables c1 and c2 a value HT1  1.25 and for task T2
consisting of markables c1, c2 and c3 a value HT2  1.35.
In much the same way as the proportional baseline rate
- only more generally applicable - this increase of 0.1
points in task entropy reflects the increase in task diffi-
culty from T1 to T2. Now, that we have a clearly de-
fined way of measuring task-specific difficulties - based
on their markable-specific entropies - we can evaluate our
approach by means of a larger experiment described be-
low.
</bodyText>
<sectionHeader confidence="0.90966" genericHeader="method">
5 Evaluating the Metrics
</sectionHeader>
<bodyText confidence="0.999566333333333">
In the following we will report on the results of a corpus
study to evaluate the task-specific entropy measurement
proposed above. In our mind such a study can be per-
formed in the following way: Given a marked up corpus
as an evaluation gold standard we can alternate the cor-
pus’ difficulty in three potential ways:
</bodyText>
<listItem confidence="0.984308222222222">
• eliminate parts of the corpus so that the number of
values of the individual markables is decreased, we
will call this vertical pruning;
• eliminate parts of the corpus so that the number of
the individual values is decreased, we will call this
horizontal pruning;
• eliminate parts of the corpus so that both the number
of markables and their respective values is reduced,
we will call this diagonal pruning.
</listItem>
<bodyText confidence="0.9962163">
Since each of these procedures can increase and reduce
the overall task difficulty, we can use them to test if our
proposed task entropy measure is able to reflect that in
a non toy-world example. For our study we employ the
SMARTKOM (Wahlster, 2003) sense-tagged corpus em-
ployed in the word sense disambiguation study reported
by (Loos and Porzel, 2004). An overview of the mark-
ables and their value distributions is given in Appendix
1.
We can now compute the entropy for the whole task as:
</bodyText>
<equation confidence="0.825689">
1966.18083
HTwhole =  0.94
2100
</equation>
<bodyText confidence="0.999478428571429">
For the horizontal pruning we removed all markables
were a single value assumed more than 90% of the entire
set. Intuitively that makes the task harder because we
took out the easy cases which amounted to about 20% of
the entire corpus.
We can now compute the entropy for the horizontally
eased task as:
</bodyText>
<page confidence="0.863638">
1718.07959
</page>
<bodyText confidence="0.8772415">
HThorizontal = 1548 1.11
For the vertical pruning we removed all values of bi3
of the entire set. Intuitively that makes the task easier
Based on this we can defineHTw as follows:
</bodyText>
<equation confidence="0.4558385">
H Tw = Eni=1(Hci · VS)
V S
ni
E
j=1
Hci = −
</equation>
<bodyText confidence="0.495265">
because less decisions are necessary to solve those mark-
ables that had values in bi3.
</bodyText>
<equation confidence="0.407755">
1894.89386
HTvertical = �0.91
2073
</equation>
<bodyText confidence="0.97461025">
For the diagonal pruning we again removed all values
of bi3 of the entire set making the task easier and removed
horizontally all markable where the majority class was
under 60%, i.e. the hardest cases.
</bodyText>
<equation confidence="0.5944465">
1729.4254
HTdiagonal = 1936 � 0.89
</equation>
<sectionHeader confidence="0.988038" genericHeader="method">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993366666667">
We have discussed various measures for evaluating per-
formance of individual components and systems and for
estimating the corresponding task complexities. Addi-
tionally, we demonstrated the feasibility to employ an
entropy-based metric for tasks that are heterogeneously
structured in terms of their markable/attribute setup as
well as attribute/value distribution. That means it can be
applied to any corpora even if they feature disjunct at-
tributes with different values of their set sizes. In a first
study, on such a heterogeneous task, we have shown that
the results of this generally applicable entropy-based met-
ric line up correspondingly to increases and decreases in
task difficulty.
In our minds this metric for measuring task difficulty
can now be employed to approach the question of measur-
ing scalability. Since it is now feasible to manipulate task
sizes and difficulties in a controlled and measurable fash-
ion, future experiments and studies can be performed that
do almost exactly the opposite from current evaluations
of systems and components. That is, instead of keep-
ing the task - test corpus - identical and measuring the
performance of different methods, we can now keep the
method identical and measure its performance on tasks
differing in their difficulty. Hereby, some open question
still have to be solved, such as evaluating and determining
suitable performance measures and formalizing the spe-
cific dimensions of scalability that can be measured using
this approach, e.g. scalability in terms of performance on
problems that are equally difficult but vary in sizeversus
problems that vary in size and difficulty to name a few.
</bodyText>
<sectionHeader confidence="0.99895" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.998637698630137">
Jan Alexandersson and Tilman Becker. 2003. The For-
mal Foundations Underlying Overlay. In Proceedings
of the Fifth International Workshop on Computational
Semantics (IWCS-5), Tilburg, The Netherlands, Febru-
ary.
Nicole Beringer, Ute Kartal, Katerina Louka, Florian
Schiel, and Uli T¨urk. 2002. PROMISE: A Procedure
for Multimodal Interactive System Evaluation. In Pro-
ceedings of the Workshop ’Multimodal Resources and
Multimodal Systems Evaluation, Las Palmas, Spain.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249–254.
Ananlada Chotimongcol and Alexander Rudnicky. 2001.
N-best speech hypotheses reordering using linear re-
gression. In Proceedings of Eurospeech, pages 1829–
1832, Aalborg, Denmark.
Ryuichiro Higashinaka, Noboru Miyazaki, Mikio
Nakano, and Kiyoaki Aikawa. 2002. A method
for evaluating incremental utterance understanding
in spoken dialogue systems. In Proceedings of the
International Conference on Speech and Language
Processing 2002, pages 829–833, Denver, USA.
Ryuichiro Higashinaka, Noboru Miyazaki, Mikio
Nakano, and Kiyoaki Aikawa. 2003. Evaluating
discourse understanding in spoken dialogue systems.
In Proceedings of Eurospeech, pages 1941–1944,
Geneva, Switzerland.
Lynette Hirschman and Henry Thompson. 1997.
Overview of evaluation in speech and natural language.
In R Cole, editor, Survey of the State of the Art in
Human Language Technology. Cambridge University
Press, Cambridge.
Markus L¨ockelt, Tilman Becker, Norbert Pfleger, and Jan
Alexandersson. 2002. Making sense of partial. In
Proceedings of the sixth workshop on the semantics
and pragmatics of dialogue (EDILOG 2002), pages
101–107, Edinburgh, UK, September.
Berenike Loos and Robert Porzel. 2004. Resolution
of lexical ambiguities in spoken dialogue systems. In
Proceedings of the 5th SIGdial Workshop on Discourse
and Dialogue, Boston, USA. Sumitted.
Susann LuperFoy. 1992. The representation of multi-
modal user interface dialogues using discourse pegs.
In Proceedings of the 30th Annual Meeting ofthe Asso-
ciation for Computational Linguistics, Newark, Del.,
28 June – 2 July 1992, pages 22–31.
Elaine Marsh and Dennis Perzanowski. 1999. MUC-7
evaluation of IE technology: Overview of results. In
Proceedings of the 7th Message Understanding Con-
ference. Morgan Kaufman Publishers.
Norbert Pfleger, Jan Alexandersson, and Tilman Becker.
2002. Scoring functions for overlay and their ap-
plication in discourse processing. In KONVENS-02,
Saarbr¨ucken, September – October.
Robert Porzel, Iryna Gurevych, and Rainer Malaka.
2004. In context: Integrating domain- and situation-
specific knowledge. In W. Wahlster, editor,SmartKom
- Foundations of Multimodal Dialogue Systems.
Sprigner, Berlin.
Mark Stevenson. 2003. Word Sense Disambiguation:
The Case for Combining Knowldge Sources. CSLI.
C. J. Van Rijsbergen. 1979. Information Retrieval, 2nd
edition. Dept. of Computer Science, University of
Glasgow.
Wolfgang Wahlster. 2003. SmartKom: Symmetric mul-
timodality in an adaptive an reusable dialog shell. In
Proceedings of the Human Computer Interaction Sta-
tus Conference, Berlin, Germany.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards developing general model of us-
ability with PARADISE. Natural Language Engeneer-
ing, 6.
</reference>
<sectionHeader confidence="0.450395" genericHeader="method">
Appendix 1
</sectionHeader>
<table confidence="0.943591692307692">
C V S b� b� b3 b4
� 1 2
altstadt 7 42.86 0 0 57.14
am 29 24.14 24.14 0 51.72
an 27 0 7.41 0 92.59
auf 43 0 6.98 9.30 83.72
aus 27 0 14.81 0 85.19
bin 37 56.76 10.81 0 32.43
bis 10 40.00 50.00 0 10.00
ein 87 0 1.15 0 98.85
ersten 7 0 14.29 0 85.71
geben 15 0 93.33 0 6.67
gibt 111 91.89 0 0 8.11
</table>
<bodyText confidence="0.90089325">
kirche 8 12.50 0 62.50 25.00
htte 25 0 92.00 0 8.00
in 214 70.56 12.15 0 17.29
ins 74 94.59 0 0 5.41
is 177 22.03 0.56 0 77.40
ist 55 18.18 1.81 0 80.00
kann 98 0 69.39 0 30.61
kino 226 39.82 44.69 0 15.49
kirche 6 16.67 66.67 0 16.67
kommen 6 50.00 16.67 0 33.33
kommt 31 0 74.19 0 25.81
laufen 16 6.25 81.25 0 12.50
luft 49 0 95.92 0 4.08
mchte 149 97.99 0 0 2.01
nach 38 36.84 23.68 0 39.47
nehmen 12 0 41.67 0 58.33
schlo 61 21.31 27.87 26.23 24.59
schlsser 2 0 50 0 50
sind 28 17.86 0 0 82.14
um 50 76.00 0 0 24.00
vom 26 26.92 0 0 73.08
von 82 2.44 30.49 0 67.07
vor 4 0 50.00 0 50.00
war 14 21.43 0 0 78.57
welch 3 0 33.33 0 66.67
welche 25 0 88.00 0 12.00
will 21 90.48 0 0 9.52
zeig 26 73.08 0 0 26.92
zeige 7 85.71 0 0 14.29
zeigen 27 59.26 14.81 0 25.93
zu 85 0 10.59 0 89.41
zum 55 47.27 0 0 52.72
2Horizontal single lines indicate removed markables in hor-
izontal pruning, vertical single lines indicate removed values in
vertical and diagonal pruning and horizontal double lines indi-
cate removed markables in diagonal pruning.
</bodyText>
<page confidence="0.784353">
2
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.716492">
<title confidence="0.999976">Towards Measuring Scalability in Natural Language Understanding Tasks</title>
<author confidence="0.999143">Robert Porzel Rainer Malaka</author>
<affiliation confidence="0.898073">European Media Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.977583">D-69118 Heidelberg,</address>
<abstract confidence="0.993282466666667">In this paper we present a discussion of existing metrics for evaluation the performance of individual natural language understanding systems and components as well as the commonly employed metrics for measuring the specific task difficulties. We extend and generalize the common majority class baseline metric and introduce an general entropy-based metric for measuring the task difficulty of arbitrary language understanding tasks. Finally, we show an empirical study evaluating this metric followed by a discussion of its role in measuring the scalability of language understanding systems and components.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan Alexandersson</author>
<author>Tilman Becker</author>
</authors>
<title>The Formal Foundations Underlying Overlay.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5),</booktitle>
<location>Tilburg, The Netherlands,</location>
<contexts>
<context position="9121" citStr="Alexandersson and Becker, 2003" startWordPosition="1403" endWordPosition="1406">proach manages to combine factors related to speech recognition, interpretation and discourse modeling, there are some shortcomings that stem from the fact that this schema was developed for single-domain systems that employ frame-based attribute value pairs for representing the user’s intent. Recent advances in dialogue management and multidomain systems enable approaches that are more flexible than slot-filling, e.g. using discourse pegs, dialogue games and overlay operations for handling multiple tasks and cross-modal references (LuperFoy, 1992; L¨ockelt et al., 2002; Pfleger et al., 2002; Alexandersson and Becker, 2003). More importantly - for the topic of this paper - no means of measuring the a priori discourse understanding difficulty is given. Measuring Precision, Recall and F-Measures: In the realm of semantic analyses the task of word sense disambiguation is usually regarded to be among the difficult ones. This means it can only be solved after all other problems involved in language understanding have been resolved as well. The hierarchical nature and interdependencies of the various tasks are mirrored in the results of the corresponding competitive evaluation tracts - e.g. the message understanding c</context>
</contexts>
<marker>Alexandersson, Becker, 2003</marker>
<rawString>Jan Alexandersson and Tilman Becker. 2003. The Formal Foundations Underlying Overlay. In Proceedings of the Fifth International Workshop on Computational Semantics (IWCS-5), Tilburg, The Netherlands, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicole Beringer</author>
<author>Ute Kartal</author>
<author>Katerina Louka</author>
<author>Florian Schiel</author>
<author>Uli T¨urk</author>
</authors>
<title>PROMISE: A Procedure for Multimodal Interactive System Evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop ’Multimodal Resources and Multimodal Systems Evaluation,</booktitle>
<location>Las Palmas,</location>
<marker>Beringer, Kartal, Louka, Schiel, T¨urk, 2002</marker>
<rawString>Nicole Beringer, Ute Kartal, Katerina Louka, Florian Schiel, and Uli T¨urk. 2002. PROMISE: A Procedure for Multimodal Interactive System Evaluation. In Proceedings of the Workshop ’Multimodal Resources and Multimodal Systems Evaluation, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="13494" citStr="Carletta (1996)" startWordPosition="2107" endWordPosition="2108">, on an a posteriori computed majority class performance or on the performance of an established baseline classification method such as naive bayes, tf*idf or k-means . That means: • what is the corresponding f-measure, if the evaluated component guesses randomly - for chance performance metrics, 1 a 1+ (1 − a)1 p r F = • what is the corresponding f-measure if the evaluated component always chooses the most frequent solution - for majority class performance metrics, • what is the corresponding f-measure of the establisched baseline classification method. Much like kappa statistics proposed by Carletta (1996), existing employments of majority class baselines assume an equal set of identical potential mark-ups, i.e. attributes and their values, for all markables. Therefore, they cannot be used in a straight forward manner for many tasks that involve disjunct sets of attributes and values in terms of the type and number of attributes and their values involved in the classification task. This, however, is exactly what we find in natural language understanding tasks, such as semantic tagging or word sense disambiguation tasks (Stevenson, 2003). Additionally, baseline computed on other methods cannot s</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananlada Chotimongcol</author>
<author>Alexander Rudnicky</author>
</authors>
<title>N-best speech hypotheses reordering using linear regression.</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>1829--1832</pages>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="5272" citStr="Chotimongcol and Rudnicky (2001)" startWordPosition="807" endWordPosition="810">l confidence scores, which are constituted by the posterior probabilities of the hidden Markov chains and n-gram frequencies. Together these scores enable evaluators to measure the absolute performance of a given speech recognition system. In order to arrive at a measure that is relative to the given taskdifficulty, this difficulty must also be calculated, which can be done by means of measuring the perplexity of the task (see Section 3). Evaluation of the Natural Language Understanding Performance: A measure for understanding rates - called concept error rate has been proposed for example by Chotimongcol and Rudnicky (2001) and is designed in analogy to word error rates employed in automatic speech recognition that are combined with keyword spotting systems. Chotimongcol and Rudnicky (2001) propose to differentiate whether the erroneous concept occurs in a non-concept slot that contains information that is captured in the grammar but not considered relevant for selecting a system action (e.g., politeness markers, such as please), in a value-insensitive slot whose identity, suffices to produce a system action (e.g., affirmatives such as yes), or in a value-sensitive slot for which both the occurrence and the valu</context>
</contexts>
<marker>Chotimongcol, Rudnicky, 2001</marker>
<rawString>Ananlada Chotimongcol and Alexander Rudnicky. 2001. N-best speech hypotheses reordering using linear regression. In Proceedings of Eurospeech, pages 1829– 1832, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Noboru Miyazaki</author>
<author>Mikio Nakano</author>
<author>Kiyoaki Aikawa</author>
</authors>
<title>A method for evaluating incremental utterance understanding in spoken dialogue systems.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Speech and Language Processing</booktitle>
<pages>829--833</pages>
<location>Denver, USA.</location>
<contexts>
<context position="7678" citStr="Higashinaka et al. (2002)" startWordPosition="1184" endWordPosition="1187">4). Therefore this measure has also been called keyword recognition rate for single utterance systems. In our minds another crucial shortcoming is the lack of comparability, as these measures do not take the general difficulty of the understanding tasks into account. Again, this has been realized in the automatic speech recognition community and led to the so called perplexity measurements for a given speech recognition task. We will, therefore, sketch out the commonly employed perplexity measurements in Section 3. The most detailed evaluation scheme for discourse comprehension, introduced by Higashinaka et al. (2002) and also extended by Higashinaka et al. (2003), features the metrics given in Table 2. 1. slot accuracy 2. insertion error rate 3. deletion error rate 4. substitution error rate 5. slot error rate 6. update precision 7. update insertion error rate 8. update deletion error rate 9. update substitution error rate 10. speech understanding rate 11. slot accuracy for filled slots 12. deletion error rate for filled slots 13. substitution error rate for filled slots Table 1: Discourse Comprehension Measurements These metrics are combined by means of combining the results of an m5 multiple linear regr</context>
</contexts>
<marker>Higashinaka, Miyazaki, Nakano, Aikawa, 2002</marker>
<rawString>Ryuichiro Higashinaka, Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa. 2002. A method for evaluating incremental utterance understanding in spoken dialogue systems. In Proceedings of the International Conference on Speech and Language Processing 2002, pages 829–833, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Noboru Miyazaki</author>
<author>Mikio Nakano</author>
<author>Kiyoaki Aikawa</author>
</authors>
<title>Evaluating discourse understanding in spoken dialogue systems.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech,</booktitle>
<pages>1941--1944</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="7725" citStr="Higashinaka et al. (2003)" startWordPosition="1192" endWordPosition="1195"> keyword recognition rate for single utterance systems. In our minds another crucial shortcoming is the lack of comparability, as these measures do not take the general difficulty of the understanding tasks into account. Again, this has been realized in the automatic speech recognition community and led to the so called perplexity measurements for a given speech recognition task. We will, therefore, sketch out the commonly employed perplexity measurements in Section 3. The most detailed evaluation scheme for discourse comprehension, introduced by Higashinaka et al. (2002) and also extended by Higashinaka et al. (2003), features the metrics given in Table 2. 1. slot accuracy 2. insertion error rate 3. deletion error rate 4. substitution error rate 5. slot error rate 6. update precision 7. update insertion error rate 8. update deletion error rate 9. update substitution error rate 10. speech understanding rate 11. slot accuracy for filled slots 12. deletion error rate for filled slots 13. substitution error rate for filled slots Table 1: Discourse Comprehension Measurements These metrics are combined by means of combining the results of an m5 multiple linear regression algorithm and a support vector regressio</context>
</contexts>
<marker>Higashinaka, Miyazaki, Nakano, Aikawa, 2003</marker>
<rawString>Ryuichiro Higashinaka, Noboru Miyazaki, Mikio Nakano, and Kiyoaki Aikawa. 2003. Evaluating discourse understanding in spoken dialogue systems. In Proceedings of Eurospeech, pages 1941–1944, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Henry Thompson</author>
</authors>
<title>Overview of evaluation in speech and natural language. In</title>
<date>1997</date>
<booktitle>Survey of the State of the Art in Human Language Technology.</booktitle>
<editor>R Cole, editor,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="12290" citStr="Hirschman and Thompson, 1997" startWordPosition="1908" endWordPosition="1911">sed on individual grammar formalisms and mark-up languages for which no gold standards exist. For evaluating discourse understanding systems, however, such gold standards and annotated training corpora will continue to be needed. 3 Measuring Perplexity and Baselines In this section we will describe the most frequently used metrics for estimating the complexity of the tasks performed by the relevant components and systems at hand. Measuring Perplexity in Automatic Speech Recognition: Perplexity is a measure of the probability weighted average number of words that may follow after a given word (Hirschman and Thompson, 1997). In order to calculate the perplexity B, the entropy H needs to be given - i.e., the probability of the word sequences in the specific language of the systemW. The perplexity is then defined as: H = − � P(W)log2P(W) VW B = 2H Improvements of specific ASR systems can then consequently be measured by keeping the perplexity constant and measuring WER and OOV performance for recognition quality and confidence scores for hypothesis verification and selection. Measuring Task-specific Baselines: Baselines for classification or tagging tasks are commonly defined based on chance performance, on an a p</context>
</contexts>
<marker>Hirschman, Thompson, 1997</marker>
<rawString>Lynette Hirschman and Henry Thompson. 1997. Overview of evaluation in speech and natural language. In R Cole, editor, Survey of the State of the Art in Human Language Technology. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus L¨ockelt</author>
<author>Tilman Becker</author>
<author>Norbert Pfleger</author>
<author>Jan Alexandersson</author>
</authors>
<title>Making sense of partial.</title>
<date>2002</date>
<booktitle>In Proceedings of the sixth workshop on the</booktitle>
<pages>101--107</pages>
<location>Edinburgh, UK,</location>
<marker>L¨ockelt, Becker, Pfleger, Alexandersson, 2002</marker>
<rawString>Markus L¨ockelt, Tilman Becker, Norbert Pfleger, and Jan Alexandersson. 2002. Making sense of partial. In Proceedings of the sixth workshop on the semantics and pragmatics of dialogue (EDILOG 2002), pages 101–107, Edinburgh, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berenike Loos</author>
<author>Robert Porzel</author>
</authors>
<title>Resolution of lexical ambiguities in spoken dialogue systems.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<publisher>Sumitted.</publisher>
<location>Boston, USA.</location>
<contexts>
<context position="24571" citStr="Loos and Porzel, 2004" startWordPosition="4164" endWordPosition="4167">minate parts of the corpus so that the number of the individual values is decreased, we will call this horizontal pruning; • eliminate parts of the corpus so that both the number of markables and their respective values is reduced, we will call this diagonal pruning. Since each of these procedures can increase and reduce the overall task difficulty, we can use them to test if our proposed task entropy measure is able to reflect that in a non toy-world example. For our study we employ the SMARTKOM (Wahlster, 2003) sense-tagged corpus employed in the word sense disambiguation study reported by (Loos and Porzel, 2004). An overview of the markables and their value distributions is given in Appendix 1. We can now compute the entropy for the whole task as: 1966.18083 HTwhole =  0.94 2100 For the horizontal pruning we removed all markables were a single value assumed more than 90% of the entire set. Intuitively that makes the task harder because we took out the easy cases which amounted to about 20% of the entire corpus. We can now compute the entropy for the horizontally eased task as: 1718.07959 HThorizontal = 1548 1.11 For the vertical pruning we removed all values of bi3 of the entire set. Intuitively th</context>
</contexts>
<marker>Loos, Porzel, 2004</marker>
<rawString>Berenike Loos and Robert Porzel. 2004. Resolution of lexical ambiguities in spoken dialogue systems. In Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue, Boston, USA. Sumitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susann LuperFoy</author>
</authors>
<title>The representation of multimodal user interface dialogues using discourse pegs.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>22--31</pages>
<location>Newark, Del.,</location>
<contexts>
<context position="9043" citStr="LuperFoy, 1992" startWordPosition="1393" endWordPosition="1394">ing task completion rates and -times. While this promising approach manages to combine factors related to speech recognition, interpretation and discourse modeling, there are some shortcomings that stem from the fact that this schema was developed for single-domain systems that employ frame-based attribute value pairs for representing the user’s intent. Recent advances in dialogue management and multidomain systems enable approaches that are more flexible than slot-filling, e.g. using discourse pegs, dialogue games and overlay operations for handling multiple tasks and cross-modal references (LuperFoy, 1992; L¨ockelt et al., 2002; Pfleger et al., 2002; Alexandersson and Becker, 2003). More importantly - for the topic of this paper - no means of measuring the a priori discourse understanding difficulty is given. Measuring Precision, Recall and F-Measures: In the realm of semantic analyses the task of word sense disambiguation is usually regarded to be among the difficult ones. This means it can only be solved after all other problems involved in language understanding have been resolved as well. The hierarchical nature and interdependencies of the various tasks are mirrored in the results of the </context>
</contexts>
<marker>LuperFoy, 1992</marker>
<rawString>Susann LuperFoy. 1992. The representation of multimodal user interface dialogues using discourse pegs. In Proceedings of the 30th Annual Meeting ofthe Association for Computational Linguistics, Newark, Del., 28 June – 2 July 1992, pages 22–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elaine Marsh</author>
<author>Dennis Perzanowski</author>
</authors>
<title>MUC-7 evaluation of IE technology: Overview of results.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference.</booktitle>
<publisher>Morgan Kaufman Publishers.</publisher>
<contexts>
<context position="10305" citStr="Marsh and Perzanowski, 1999" startWordPosition="1596" endWordPosition="1599">on tracts - e.g. the message understanding conference (MUC) or SENSEVAL competition. It becomes obvious that the ungraceful degradation of f-measure scores (shown in Table 2 is due to the fact that each higher-level task inherits the imprecisions and omissions of the previous ones, e.g. errors in the named entity recognition (NE) task cause recall and precision declines in the template element task (TE), which, in turn, thwart successful template relation task performance (TR) as well as the most difficult scenario template (ST) and co-reference task (CO). This decline can be seen in Table 2 (Marsh and Perzanowski, 1999). NE CO TE TR ST f &lt; .94 f &lt; .62 f &lt; .87 f &lt; .76 f &lt; .51 Table 2: F-measure-based (a = 0.5) evaluation results of the best performing systems of the 7th Message Understanding Conference Despite several problems stemming from the prerequisite to craft costly gold standards, e.g. tree banks or annotated test corpora, precision and recall and their weighable combinations in the corresponding f-measures (such as given in Table 2), have become a de facto standard for measuring the performance of classification and retrieval tasks (Van Rijsbergen, 1979). Precision p states the percentage of correctl</context>
</contexts>
<marker>Marsh, Perzanowski, 1999</marker>
<rawString>Elaine Marsh and Dennis Perzanowski. 1999. MUC-7 evaluation of IE technology: Overview of results. In Proceedings of the 7th Message Understanding Conference. Morgan Kaufman Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Pfleger</author>
<author>Jan Alexandersson</author>
<author>Tilman Becker</author>
</authors>
<title>Scoring functions for overlay and their application in discourse processing.</title>
<date>2002</date>
<booktitle>In KONVENS-02, Saarbr¨ucken, September –</booktitle>
<contexts>
<context position="9088" citStr="Pfleger et al., 2002" startWordPosition="1399" endWordPosition="1402">hile this promising approach manages to combine factors related to speech recognition, interpretation and discourse modeling, there are some shortcomings that stem from the fact that this schema was developed for single-domain systems that employ frame-based attribute value pairs for representing the user’s intent. Recent advances in dialogue management and multidomain systems enable approaches that are more flexible than slot-filling, e.g. using discourse pegs, dialogue games and overlay operations for handling multiple tasks and cross-modal references (LuperFoy, 1992; L¨ockelt et al., 2002; Pfleger et al., 2002; Alexandersson and Becker, 2003). More importantly - for the topic of this paper - no means of measuring the a priori discourse understanding difficulty is given. Measuring Precision, Recall and F-Measures: In the realm of semantic analyses the task of word sense disambiguation is usually regarded to be among the difficult ones. This means it can only be solved after all other problems involved in language understanding have been resolved as well. The hierarchical nature and interdependencies of the various tasks are mirrored in the results of the corresponding competitive evaluation tracts -</context>
</contexts>
<marker>Pfleger, Alexandersson, Becker, 2002</marker>
<rawString>Norbert Pfleger, Jan Alexandersson, and Tilman Becker. 2002. Scoring functions for overlay and their application in discourse processing. In KONVENS-02, Saarbr¨ucken, September – October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Porzel</author>
<author>Iryna Gurevych</author>
<author>Rainer Malaka</author>
</authors>
<title>In context: Integrating domain- and situationspecific knowledge.</title>
<date>2004</date>
<booktitle>In W. Wahlster, editor,SmartKom - Foundations of Multimodal Dialogue Systems.</booktitle>
<publisher>Sprigner,</publisher>
<location>Berlin.</location>
<contexts>
<context position="7055" citStr="Porzel et al., 2004" startWordPosition="1087" endWordPosition="1090">say any concept covered by the grammar. • A grammar mismatch when the application returned the incorrect concept, but the user said a concept covered by the grammar and an out ofgrammar mismatch when the application returned a concept, and chose that concept as a correct interpretation, but the user did not say a concept covered by the grammar. Neither of these measures are suitable for our purposes as they are known to be feasible only for contextinsensitive applications that do not include discourse models, implicit domain-specific information and other contextual knowledge as discussed in (Porzel et al., 2004). Therefore this measure has also been called keyword recognition rate for single utterance systems. In our minds another crucial shortcoming is the lack of comparability, as these measures do not take the general difficulty of the understanding tasks into account. Again, this has been realized in the automatic speech recognition community and led to the so called perplexity measurements for a given speech recognition task. We will, therefore, sketch out the commonly employed perplexity measurements in Section 3. The most detailed evaluation scheme for discourse comprehension, introduced by Hi</context>
</contexts>
<marker>Porzel, Gurevych, Malaka, 2004</marker>
<rawString>Robert Porzel, Iryna Gurevych, and Rainer Malaka. 2004. In context: Integrating domain- and situationspecific knowledge. In W. Wahlster, editor,SmartKom - Foundations of Multimodal Dialogue Systems. Sprigner, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
</authors>
<title>Word Sense Disambiguation: The Case for Combining Knowldge Sources.</title>
<date>2003</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="14035" citStr="Stevenson, 2003" startWordPosition="2191" endWordPosition="2192">assification method. Much like kappa statistics proposed by Carletta (1996), existing employments of majority class baselines assume an equal set of identical potential mark-ups, i.e. attributes and their values, for all markables. Therefore, they cannot be used in a straight forward manner for many tasks that involve disjunct sets of attributes and values in terms of the type and number of attributes and their values involved in the classification task. This, however, is exactly what we find in natural language understanding tasks, such as semantic tagging or word sense disambiguation tasks (Stevenson, 2003). Additionally, baseline computed on other methods cannot serve as a means for measuring scalability, because of the circularity involved: as one would need a way of measuring the baseline method’s scalability factor in the first place. Table 3 provides an overview of the existing ways of measuring performance and task difficulty in automatic speech recognition and understanding. Domain Performance Complexity automatic WER/OVV Perplexity speech recognition natural CER none language understanding MUC tasks f-measure baselines (NE, TE, TR, ST, CO) unimodal PARADISE none dialogue system mulitmoda</context>
</contexts>
<marker>Stevenson, 2003</marker>
<rawString>Mark Stevenson. 2003. Word Sense Disambiguation: The Case for Combining Knowldge Sources. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval, 2nd edition.</title>
<date>1979</date>
<institution>Dept. of Computer Science, University of Glasgow.</institution>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C. J. Van Rijsbergen. 1979. Information Retrieval, 2nd edition. Dept. of Computer Science, University of Glasgow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>SmartKom: Symmetric multimodality in an adaptive an reusable dialog shell.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Computer Interaction Status Conference,</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="24467" citStr="Wahlster, 2003" startWordPosition="4150" endWordPosition="4151">ber of values of the individual markables is decreased, we will call this vertical pruning; • eliminate parts of the corpus so that the number of the individual values is decreased, we will call this horizontal pruning; • eliminate parts of the corpus so that both the number of markables and their respective values is reduced, we will call this diagonal pruning. Since each of these procedures can increase and reduce the overall task difficulty, we can use them to test if our proposed task entropy measure is able to reflect that in a non toy-world example. For our study we employ the SMARTKOM (Wahlster, 2003) sense-tagged corpus employed in the word sense disambiguation study reported by (Loos and Porzel, 2004). An overview of the markables and their value distributions is given in Appendix 1. We can now compute the entropy for the whole task as: 1966.18083 HTwhole =  0.94 2100 For the horizontal pruning we removed all markables were a single value assumed more than 90% of the entire set. Intuitively that makes the task harder because we took out the easy cases which amounted to about 20% of the entire corpus. We can now compute the entropy for the horizontally eased task as: 1718.07959 HThorizon</context>
</contexts>
<marker>Wahlster, 2003</marker>
<rawString>Wolfgang Wahlster. 2003. SmartKom: Symmetric multimodality in an adaptive an reusable dialog shell. In Proceedings of the Human Computer Interaction Status Conference, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards developing general model of usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engeneering,</journal>
<volume>6</volume>
<contexts>
<context position="954" citStr="Walker et al., 2000" startWordPosition="127" endWordPosition="130">vidual natural language understanding systems and components as well as the commonly employed metrics for measuring the specific task difficulties. We extend and generalize the common majority class baseline metric and introduce an general entropy-based metric for measuring the task difficulty of arbitrary language understanding tasks. Finally, we show an empirical study evaluating this metric followed by a discussion of its role in measuring the scalability of language understanding systems and components. 1 Introduction Current evaluation frameworks for uni- or multi-modal dialogue systems (Walker et al., 2000; Beringer et al., 2002) that allow for spoken language input do not include metrics for measuring the accuracy of the involved intention recognition systems, simply because such information is hard to extract automatically from log files. Furthermore no general computational method or framework for measuring the difficulty of natural language understanding tasks have been proposed so far. We are, therefore, faced with a lack of methods for measuring the difficulties of the individual tasks involved in the language understanding process. Such generally applicable methods, however, are needed f</context>
<context position="2840" citStr="Walker et al., 2000" startWordPosition="421" endWordPosition="424">ds. In Section 6 we discuss how an analysis of a specific system on tasks differing in their difficulty can yield a first approach for measuring the scalability of a natural language understanding systems and its components. 2 Evaluating Dialogue-, Speech- and Discourse Understanding Systems In this section we will briefly sketch out the most frequently used metrics for evaluating the performances of the relevant components and systems at hand. Evaluation of the Dialogue Systems Performance: For evaluation of the overall performance of a dialogue system as a whole frameworks such as PARADISE (Walker et al., 2000) for unimodal and PROMISE (Beringer et al., 2002) for multimodal systems have set a de facto standard. These frameworks differentiate between: • dialogue efficiency metrics, i.e. elapsed time, system- and user turns • dialogue quality metrics, mean recognition score and absolute number as well as percentages of timeouts, rejections, helps, cancels, and barge-ins, • task success metrics, task completion (per survey) • user satisfaction metrics (per survey) These metrics are crucial for evaluating the aggregate performance of the individual components, they cannot, however, determine the amount </context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn A. Walker, Candace A. Kamm, and Diane J. Litman. 2000. Towards developing general model of usability with PARADISE. Natural Language Engeneering, 6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>