<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.998012">
Joint Training and Decoding Using Virtual Nodes for Cascaded
Segmentation and Tagging Tasks
</title>
<author confidence="0.999357">
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide Wu
</author>
<affiliation confidence="0.99936">
School of Computer Science, Fudan University
</affiliation>
<address confidence="0.961998">
825 Zhangheng Road, Shanghai, P.R.China
</address>
<email confidence="0.987183">
{qianxian, qz, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn
</email>
<sectionHeader confidence="0.982465" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995395">
Many sequence labeling tasks in NLP require
solving a cascade of segmentation and tag-
ging subtasks, such as Chinese POS tagging,
named entity recognition, and so on. Tradi-
tional pipeline approaches usually suffer from
error propagation. Joint training/decoding in
the cross-product state space could cause too
many parameters and high inference complex-
ity. In this paper, we present a novel method
which integrates graph structures of two sub-
tasks into one using virtual nodes, and per-
forms joint training and decoding in the fac-
torized state space. Experimental evaluations
on CoNLL 2000 shallow parsing data set and
Fourth SIGHAN Bakeoff CTB POS tagging
data set demonstrate the superiority of our
method over cross-product, pipeline and can-
didate reranking approaches.
</bodyText>
<sectionHeader confidence="0.995152" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981673076923">
There is a typical class of sequence labeling tasks
in many natural language processing (NLP) applica-
tions, which require solving a cascade of segmenta-
tion and tagging subtasks. For example, many Asian
languages such as Japanese and Chinese which
do not contain explicitly marked word boundaries,
word segmentation is the preliminary step for solv-
ing part-of-speech (POS) tagging problem. Sen-
tences are firstly segmented into words, then each
word is assigned with a part-of-speech tag. Both
syntactic parsing and dependency parsing usually
start with a textual input that is tokenized, and POS
tagged.
The most commonly approach solves cascaded
subtasks in a pipeline, which is very simple to im-
plement and allows for a modular approach. While,
the key disadvantage of such method is that er-
rors propagate between stages, significantly affect-
ing the quality of the final results. To cope with this
problem, Shi and Wang (2007) proposed a rerank-
ing framework in which N-best segment candidates
generated in the first stage are passed to the tag-
ging model, and the final output is the one with the
highest overall segmentation and tagging probabil-
ity score. The main drawback of this method is that
the interaction between tagging and segmentation is
restricted by the number of candidate segmentation
outputs. Razvan C. Bunescu (2008) presented an
improved pipeline model in which upstream subtask
outputs are regarded as hidden variables, together
with their probabilities are used as probabilistic fea-
tures in the downstream subtasks. One shortcom-
ing of this method is that calculation of marginal
probabilities of features may be inefficient and some
approximations are required for fast computation.
Another disadvantage of these two methods is that
they employ separate training and the segmentation
model could not take advantages of tagging infor-
mation in the training procedure.
On the other hand, joint learning and decoding
using cross-product of segmentation states and tag-
ging states does not suffer from error propagation
problem and achieves higher accuracy on both sub-
tasks (Ng and Low, 2004). However, two problems
arises due to the large state space, one is that the
amount of parameters increases rapidly, which is apt
to overfit on the training corpus, the other is that
the inference by dynamic programming could be in-
efficient. Sutton (2004) proposed Dynamic Con-
ditional Random Fields (DCRFs) to perform joint
training/decoding of subtasks using much fewer pa-
rameters than the cross-product approach. How-
</bodyText>
<note confidence="0.816788333333333">
187
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187–195,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999836770833333">
ever, DCRFs do not guarantee non-violation of hard-
constraints that nodes within the same segment get
a single consistent tagging label. Another draw-
back of DCRFs is that exact inference is generally
time consuming, some approximations are required
to make it tractable.
Recently, perceptron based learning framework
has been well studied for incorporating node level
and segment level features together (Kazama and
Torisawa, 2007; Zhang and Clark, 2008). The main
shortcoming is that exact inference is intractable
for those dynamically generated segment level fea-
tures, so candidate based searching algorithm is
used for approximation. On the other hand, Jiang
(2008) proposed a cascaded linear model which has
a two layer structure, the inside-layer model uses
node level features to generate candidates with their
weights as inputs of the outside layer model which
captures non-local features. As pipeline models, er-
ror propagation problem exists for such method.
In this paper, we present a novel graph structure
that exploits joint training and decoding in the fac-
torized state space. Our method does not suffer
from error propagation, and guards against viola-
tions of those hard-constraints imposed by segmen-
tation subtask. The motivation is to integrate two
Markov chains for segmentation and tagging sub-
tasks into a single chain, which contains two types of
nodes, then standard dynamic programming based
exact inference is employed on the hybrid struc-
ture. Experiments are conducted on two different
tasks, CoNLL 2000 shallow parsing and SIGHAN
2008 Chinese word segmentation and POS tagging.
Evaluation results of shallow parsing task show
the superiority of our proposed method over tradi-
tional joint training/decoding approach using cross-
product state space, and achieves the best reported
results when no additional resources at hand. For
Chinese word segmentation and POS tagging task, a
strong baseline pipeline model is built, experimental
results show that the proposed method yields a more
substantial improvement over the baseline than can-
didate reranking approach.
The rest of this paper is organized as follows: In
Section 2, we describe our novel graph structure. In
Section 3, we analyze complexity of our proposed
method. Experimental results are shown in Section
4. We conclude the work in Section 5.
</bodyText>
<sectionHeader confidence="0.960619" genericHeader="method">
2 Multi-chain integration using Virtual
Nodes
</sectionHeader>
<subsectionHeader confidence="0.999175">
2.1 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.999921">
We begin with a brief review of the Conditional Ran-
dom Fields(CRFs). Let x = x1x2 ... xl denote the
observed sequence, where xi is the ith node in the
sequence, l is sequence length, y = y1y2 ... yl is a
label sequence over x that we wish to predict. CRFs
(Lafferty et al., 2001) are undirected graphic mod-
els that use Markov network distribution to learn the
conditional probability. For sequence labeling task,
linear chain CRFs are very popular, in which a first
order Markov assumption is made on the labels:
</bodyText>
<equation confidence="0.99783">
p(y|x) = Z1x)ri φ(x, y, i) i
,where
� �
φ(x, y, i) = exp wT f(x, yi−1, yi, i)
Z(x) = E ri φ(x, y, i)
Y i
f(x, yi−1, yi, i) =
</equation>
<bodyText confidence="0.996978608695652">
[f1(x, yi−1, yi, i), ...,fm(x, yi−1, yi, i)]T, each ele-
ment fj(x, yi−1, yi, i) is a real valued feature func-
tion, here we simplify the notation of state feature
by writing fj(x, yi, i) = fj(x, yi−1, yi, i), m is the
cardinality of feature set {fj}. w = [w1, . . . , wm]T
is a weight vector to be learned from the training
set. Z(x) is the normalization factor over all label
sequences for x.
In the traditional joint training/decoding approach
for cascaded segmentation and tagging task, each
label yi has the form si-ti, which consists of seg-
mentation label si and tagging label ti. Let s =
s1s2 ... sl be the segmentation label sequence over
x. There are several commonly used label sets such
as BI, BIO, IOE, BIES, etc. To facilitate our dis-
cussion, in later sections we will use BIES label set,
where B,I,E represents Beginning, Inside and End of
a multi-node segment respectively, S denotes a sin-
gle node segment. Let t = t1t2 ... tl be the tagging
label sequence over x. For example, in named entity
recognition task, ti ∈ {PER, LOC, ORG, MISC,
O} represents an entity type (person name, loca-
tion name, organization name, miscellaneous entity
</bodyText>
<figure confidence="0.5851115">
188
Hendrix ’s girlfriend Kathy Etchingham
</figure>
<figureCaption confidence="0.9893135">
Figure 1: Graphical representation of linear chain CRFs
for traditional joint learning/decoding
</figureCaption>
<bodyText confidence="0.99766575">
name and other). Graphical representation of lin-
ear chain CRFs is shown in Figure 1, where tagging
label “P” is the simplification of “PER”. For nodes
that are labeled as other, we define si =S, ti =O.
</bodyText>
<subsectionHeader confidence="0.811422">
2.2 Hybrid structure for cascaded labeling
tasks
</subsectionHeader>
<bodyText confidence="0.999968315789474">
Different from traditional joint approach, our
method integrates two linear markov chains for seg-
mentation and tagging subtasks into one that con-
tains two types of nodes. Specifically, we first
regard segmentation and tagging as two indepen-
dent sequence labeling tasks, corresponding chain
structures are built, as shown in the top and mid-
dle sub-figures of Figure 2. Then a chain of twice
length of the observed sequence is built, where
nodes x1, ... , xl on the even positions are original
observed nodes, while nodes v1, ... , vl on the odd
positions are virtual nodes that have no content in-
formation. For original nodes xi, the state space is
the tagging label set, while for virtual nodes, their
states are segmentation labels. The label sequence
of the hybrid chain is y = y1 ... y2l = s1t1 ... sltl,
where combination of consecutive labels siti repre-
sents the full label for node xi.
Then we let si be connected with si−1 and si+1
, so that first order Markov assumption is made
on segmentation states. Similarly, ti is connected
with ti−1 and ti+1. Then neighboring tagging and
segmentation states are connected as shown in the
bottom sub-figure of Figure 2. Non-violation of
hard-constraints that nodes within the same seg-
ment get a single consistent tagging label is guar-
anteed by introducing second order transition fea-
tures f(ti−1, si, ti, i) that are true if ti−1 =� ti and
si E {I,E}. For example, fj(ti−1, si, ti, i) is de-
fined as true if ti−1 =PER, si =I and ti =LOC.
In other words, it is true, if a segment is partially
tagging as PER, and partially tagged as LOC. Since
such features are always false in the training corpus,
their corresponding weights will be very low so that
inconsistent label assignments impossibly appear in
decoding procedure. The hybrid graph structure can
be regarded as a special case of second order Markov
chain.
</bodyText>
<note confidence="0.616429">
Hendrix ’S girlfriend Kathy Etchingham
</note>
<figureCaption confidence="0.993301">
Figure 2: Multi-chain integration using Virtual Nodes
</figureCaption>
<subsectionHeader confidence="0.9954">
2.3 Factorized features
</subsectionHeader>
<bodyText confidence="0.998881583333333">
Compared with traditional joint model that exploits
cross-product state space, our hybrid structure uses
factorized states, hence could handle more flexible
features. Any state feature g(x, yi, i) defined in
the cross-product state space can be replaced by a
first order transition feature in the factorized space:
f(x, si, ti, i). As for the transition features, we
use f(si−1, ti−1, si, i) and f(ti−1, si, ti, i) instead
of g(yi−1, yi, i) in the conventional joint model.
Features in cross-product state space require that
segmentation label and tagging label take on partic-
ular values simultaneously, however, sometimes we
</bodyText>
<figure confidence="0.965108592592593">
S-P S-O
x1
s
1
t
×
1
x2
s
t2
×
2
S-O
x3
s3
t3
×
B-P
x4
s4
t4
×
E-P
x5
s5
t5
×
</figure>
<equation confidence="0.870132636363636">
x1 x2 x3 x4 x5
S1 S2 S3 S4 S5
S S S B E
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
Integrate
V1 x1 V2 x2 V3 x3 V4 x4 V5 x5
S1 t1 S2 t2 S3 t3 S4 t4 S5 t5
S P S O S O B P E P
189
</equation>
<bodyText confidence="0.999787266666667">
want to specify requirement on only segmentation or
tagging label. For example, “Smith” may be an end
of a person name, “Speaker: John Smith”; or a sin-
gle word person name “Professor Smith will... ”. In
such case, our observation is that “Smith” is likely a
(part of) person name, we do not care about its seg-
mentation label. So we could define state feature
f(x, ti, i) = true, if xi is “Smith” with tagging la-
bel ti=PER.
Further more, we could define features like
f(x, ti−1, ti, i), f(x, si−1, si, i), f(x, ti−1, si, i),
etc. The hybrid structure facilitates us to use
varieties of features. In the remainder of the
paper, we use notations f(x, ti−1, si, ti, i) and
f(x, si−1, ti−1, si, i) for simplicity.
</bodyText>
<subsectionHeader confidence="0.990746">
2.4 Hybrid CRFs
</subsectionHeader>
<bodyText confidence="0.999837333333333">
A hybrid CRFs is a conditional distribution that fac-
torizes according to the hybrid graphical model, and
is defined as:
</bodyText>
<equation confidence="0.9526355">
1 Y Y
p(s, t|x) = φ(x, s, t, i) ψ(x, s, t, i)
Z(x) i i
Where
3 ´
φ(x, s, t, i) = exp w� 1 f(x, si−1, ti−1I IS
ψ(x, s, t, i) = exp3w2 f (x,
ti−1,si, ti)´
Z(x) = XÃY φ(x, s, t, i) Y ψ(x, s, t, i)!
Where w1, w2 are weight vectors.
</equation>
<bodyText confidence="0.999712142857143">
Luckily, unlike DCRFs, in which graph structure
can be very complex, and the cross-product state
space can be very large, in our cascaded labeling
task, the segmentation label set is often small, so
far as we known, the most complicated segmenta-
tion label set has only 6 labels (Huang and Zhao,
2007). So exact dynamic programming based algo-
rithms can be efficiently performed.
In the training stage, we use second order forward
backward algorithm to compute the marginal proba-
bilities p(x, si−1, ti−1, si) and p(x, ti−1, si, ti), and
the normalization factor Z(x). In decoding stage,
we use second order Viterbi algorithm to find the
best label sequence. The Viterbi decoding can be
</bodyText>
<tableCaption confidence="0.995361">
Table 1: Time Complexity
</tableCaption>
<table confidence="0.9977068">
Method Training Decoding
Pipeline (|S|2c. + |T|2ct)L (|S|2 + |T|2)U
Cross-Product (|S||T|)2cL (|S||T|)2U
Reranking (|S|2c. + |T|2ct)L (|S|2 + |T|2)NU
Hybrid (|S |+ |T|)|S||T|cL (|S |+ |T|)|S||T|U
</table>
<bodyText confidence="0.8357315">
used to label a new sequence, and marginal compu-
tation is used for parameter estimation.
</bodyText>
<sectionHeader confidence="0.981864" genericHeader="method">
3 Complexity Analysis
</sectionHeader>
<bodyText confidence="0.99953675">
The time complexity of the hybrid CRFs train-
ing and decoding procedures is higher than that of
pipeline methods, but lower than traditional cross-
product methods. Let
</bodyText>
<listItem confidence="0.990606916666667">
• |S |= size of the segmentation label set.
• |T |= size of the tagging label set.
• L = total number of nodes in the training data
set.
• U = total number of nodes in the testing data
set.
• c = number of joint training iterations.
• cs = number of segmentation training itera-
tions.
• ct = number of tagging training iterations.
• N = number of candidates in candidate rerank-
ing approach.
</listItem>
<bodyText confidence="0.999763416666667">
Time requirements for pipeline, cross-product, can-
didate reranking and hybrid CRFs are summarized
in Table 1. For Hybrid CRFs, original node xi has
features {fj(ti−1, si, ti)}, accessing all label subse-
quences ti−1siti takes |S||T|2 time, while virtual
node vi has features {fj(si−1, ti−1, si)1, accessing
all label subsequences si−1ti−1si takes |S|2|T |time,
so the final complexity is (|S |+ |T|)|S||T|cL.
In real applications, |S |is small, |T |could be
very large, we assume that |T |&gt;&gt; |S|, so for
each iteration, hybrid CRFs is about |S |times slower
than pipeline and |S |times faster than cross-product
</bodyText>
<figure confidence="0.5185595">
s,t i i
190
</figure>
<tableCaption confidence="0.929185">
Table 2: Feature templates for shallow parsing task
</tableCaption>
<table confidence="0.998296894736842">
Cross Product CRFs Hybrid CRFs
wi−2yi, wi−1yi, wiyi wi−1si, wisi, wi+1si
wi+1yi, wi+2yi
wi−2ti, wi−1ti, witi, wi+1ti, wi+2ti
wi−1wiyi, wiwi+1yi wi−1wisi, wiwi+1si
wi−1witi, wiwi+1ti
pi−2yi, pi−1yi, piyi pi−1si, pisi, pi+1si
pi+1yi, pi+2yi
pi−2ti, pi−1ti, pi+1ti, pi+2ti
pi−2pi−1yi, pi−1piyi, pipi+1yi, pi−2pi−1si, pi−1pisi, pipi+1si, pi+1pi+2si
pi+1pi+2yi
pi−3pi−2ti, pi−2pi−1ti, pi−1piti, pipi+1ti,
pi+1pi+2ti, pi+2pi+3ti, pi−1pi+1ti
pi−2pi−1piyi, pi−1pipi+1yi, pi−2pi−1pisi, pi−1pipi+1si, pipi+1pi+2si
pipi+1pi+2yi
wipiti
wisi−1si
wi−1ti−1ti, witi−1ti, pi−1ti−1ti, piti−1ti
yi−1yi si−1ti−1si, ti−1siti
</table>
<bodyText confidence="0.989277666666667">
method. When decoding, candidate reranking ap-
proach requires more time if candidate number N &gt;
ISI.
Though the space complexity could not be com-
pared directly among some of these methods, hybrid
CRFs require less parameters than cross-product
CRFs due to the factorized state space. This is sim-
ilar with factorized CRFs (FCRFs) (Sutton et al.,
2004).
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995078">
4.1 Shallow Parsing
</subsectionHeader>
<bodyText confidence="0.999670076923077">
Our first experiment is the shallow parsing task. We
use corpus from CoNLL 2000 shared task, which
contains 8936 sentences for training and 2012 sen-
tences for testing. There are 11 tagging labels: noun
phrase(NP), verb phrase(VP) , ... and other (O), the
segmentation state space we used is BIES label set,
since we find that it yields a little improvement over
BIO set.
We use the standard evaluation metrics, which are
precision P (percentage of output phrases that ex-
actly match the reference phrases), recall R (percent-
age of reference phrases returned by our system),
and their harmonic mean, the F1 score F1 = 2� �
</bodyText>
<equation confidence="0.691686">
� +�
</equation>
<bodyText confidence="0.997286107142857">
(which we call F score in what follows).
We compare our approach with traditional cross-
product method. To find good feature templates,
development data are required. Since CoNLL2000
does not provide development data set, we divide
the training data into 10 folds, of which 9 folds for
training and 1 fold for developing. After selecting
feature templates by cross validation, we extract fea-
tures and learn their weights on the whole training
data set. Feature templates are summarized in Table
2, where wi denotes the i1h word, pi denotes the i1h
POS tag.
Notice that in the second row, feature templates
of the hybrid CRFs does not contain wi−2si, wi+2si,
since we find that these two templates degrade per-
formance in cross validation. However, wi−2ti,
wi+2ti are useful, which implies that the proper con-
text window size for segmentation is smaller than
tagging. Similarly, for hybrid CRFs, the window
size of POS bigram features for segmentation is 5
(from pi−2 to pi+2, see the eighth row in the sec-
ond column); while for tagging, the size is 7 (from
pi−3 to pi+3, see the ninth row in the second col-
umn). However for cross-product method, their win-
dow sizes must be consistent.
For traditional cross-product CRFs and our hybrid
CRFs, we use fixed gaussian prior σ = 1.0 for both
methods, we find that this parameter does not signifi-
</bodyText>
<page confidence="0.944922">
191
</page>
<tableCaption confidence="0.915899333333333">
Table 3: Results for shallow parsing task, Hybrid CRFs
significantly outperform Cross-Product CRFs (McNe-
mar’s test; p &lt; 0.01)
</tableCaption>
<table confidence="0.996687285714286">
Method Cross-Product Hybrid
CRFs CRFs
Training Time 11.6 hours 6.3 hours
Feature Num- 13 million 10 mil-
ber lion
Iterations 118 141
F1 93.88 94.31
</table>
<bodyText confidence="0.999572333333333">
cantly affect the results when it varies between 1 and
10. LBFGS(Nocedal and Wright, 1999) method is
employed for numerical optimization. Experimen-
tal results are shown in Table 3. Our proposed CRFs
achieve a performance gain of 0.43 points in F-score
over cross-product CRFs that use state space while
require less training time.
For comparison, we also listed the results of pre-
vious top systems, as shown in Table 4. Our pro-
posed method outperforms other systems when no
additional resources at hand. Though recently semi-
supervised learning that incorporates large mounts
of unlabeled data has been shown great improve-
ment over traditional supervised methods, such as
the last row in Table 4, supervised learning is funda-
mental. We believe that combination of our method
and semi-supervised learning will achieve further
improvement.
</bodyText>
<subsectionHeader confidence="0.7969125">
4.2 Chinese word segmentation and POS
tagging
</subsectionHeader>
<bodyText confidence="0.999750533333333">
Our second experiment is the Chinese word seg-
mentation and POS tagging task. To facilitate com-
parison, we focus only on the closed test, which
means that the system is trained only with a des-
ignated training corpus, any extra knowledge is not
allowed, including Chinese and Arabic numbers, let-
ters and so on. We use the Chinese Treebank (CTB)
POS corpus from the Fourth International SIGHAN
Bakeoff data sets (Jin and Chen, 2008). The train-
ing data consist of 23444 sentences, 642246 Chinese
words, 1.05M Chinese characters and testing data
consist of 2079 sentences, 59955 Chinese words,
0.1M Chinese characters.
We compare our hybrid CRFs with pipeline and
candidate reranking methods (Shi and Wang, 2007)
</bodyText>
<tableCaption confidence="0.990439">
Table 4: Comparison with other systems on shallow pars-
ing task
</tableCaption>
<table confidence="0.9989034">
Method F1 Additional Re-
sources
Cross-Product CRFs 93.88
Hybrid CRFs 94.31
SVM combination 93.91
(Kudo and Mat-
sumoto, 2001)
Voted Perceptrons 93.74 none
(Carreras and Mar-
quez, 2003)
ETL (Milidiu et al., 92.79
2008)
(Wu et al., 2006) 94.21 Extended features
such as token fea-
tures, affixes
HySOL 94.36 17M words unla-
beled
(Suzuki et al., 2007) data
ASO-semi 94.39 15M words unla-
beled
(Ando and Zhang, data
2005)
(Zhang et al., 2002) 94.17 full parser output
(Suzuki and Isozaki, 95.15 1G words unla-
2008) beled data
</table>
<bodyText confidence="0.999582523809524">
using the same evaluation metrics as shallow pars-
ing. We do not compare with cross-product CRFs
due to large amounts of parameters.
For pipeline method, we built our word segmenter
based on the work of Huang and Zhao (2007),
which uses 6 label representation, 7 feature tem-
plates (listed in Table 5, where cz denotes the ith
Chinese character in the sentence) and CRFs for pa-
rameter learning. We compare our segmentor with
other top systems using SIGHAN CTB corpus and
evaluation metrics. Comparison results are shown
in Table 6, our segmenter achieved 95.12 F-score,
which is ranked 4th of 26 official runs. Except for
the first system which uses extra unlabeled data, dif-
ferences between rest systems are not significant.
Our POS tagging system is based on linear chain
CRFs. Since SIGHAN dose not provide develop-
ment data, we use the 10 fold cross validation de-
scribed in the previous experiment to turning feature
templates and Gaussian prior. Feature templates are
listed in Table 5, where wz denotes the ith word in
</bodyText>
<page confidence="0.844995">
192
</page>
<tableCaption confidence="0.995301">
Table 5: Feature templates for Chinese word segmentation and POS tagging task
</tableCaption>
<figure confidence="0.736356368421053">
Segmentation feature templates
(1.1) ci−2si, ci−1si, cisi, ci+1si, ci+2si
(1.2) ci−1cisi, cici+1si, ci−1ci+1si
(1.3) si−1si
POS tagging feature templates
(2.1) wi−2ti, wi−1ti, witi, wi+1ti, wi+2ti
(2.2) wi−2wi−1ti, wi−1witi, wiwi+1ti, wi+1wi+2ti, wi−1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c−2(wi)ti, c−1(wi)ti
(2.4) c1(wi)c2(wi)ti, c−2(wi)c−1(wi)ti
(2.5) l(wi)ti
(2.6) ti−1ti
Joint segmentation and POS tagging feature templates
(3.1) ci−2si, ci−1si, cisi, ci+1si, ci+2si
(3.2) ci−1cisi, cici+1si, ci−1ci+1si
(3.3) ci−3ti, ci−2ti, ci−1ti, citi, ci+1ti, ci+2ti, ci+3ti
(3.4) ci−3ci−2ti, ci−2ci−1ti, ci−1citi, cici+1ti ci+1ci+2ti, ci+2ci+3ti, ci−2citi, cici+2ti
(3.5) cisiti
(3.6) citi−1ti
(3.7) si−1ti−1si, ti−1siti
</figure>
<tableCaption confidence="0.8916325">
Table 6: Word segmentation results on Fourth SIGHAN
Bakeoff CTB corpus
</tableCaption>
<table confidence="0.984716125">
Rank F1 Description
1/26 95.89∗ official best, using extra un-
labeled data (Zhao and Kit,
2008)
2/26 95.33 official second
3/26 95.17 official third
4/26 95.12 segmentor in pipeline sys-
tem
</table>
<tableCaption confidence="0.918833">
Table 7: POS results on Fourth SIGHAN Bakeoff CTB
corpus
</tableCaption>
<table confidence="0.944831666666667">
Rank Accuracy Description
1/7 94.29 POS tagger in pipeline sys-
tem
2/7 94.28 official best
3/7 94.01 official second
4/7 93.24 official third
</table>
<bodyText confidence="0.998990375">
the sentence, cj(wi), j &gt; 0 denotes the jIh Chinese
character of word wi, cj(wi), j &lt; 0 denotes the j1h
last Chinese character, l(wi) denotes the word length
of wi. We compare our POS tagger with other top
systems on Bakeoff CTB POS corpus where sen-
tences are perfectly segmented into words, our POS
tagger achieved 94.29 accuracy, which is the best of
7 official runs. Comparison results are shown in Ta-
ble 7.
For reranking method, we varied candidate num-
bers n among n E 110, 20, 50, 1001. For hybrid
CRFs, we use the same segmentation label set as
the segmentor in pipeline. Feature templates are
listed in Table 5. Experimental results are shown
in Figure 3. The gain of hybrid CRFs over the
baseline pipeline model is 0.48 points in F-score,
about 3 times higher than 100-best reranking ap-
proach which achieves 0.13 points improvement.
Though larger candidate number can achieve higher
performance, such improvement becomes trivial for
n &gt; 20.
Table 8 shows the comparison between our work
and other relevant work. Notice that, such com-
parison is indirect due to different data sets and re-
</bodyText>
<figure confidence="0.854579">
193
0 20 40 60 80 100
candidate number
</figure>
<figureCaption confidence="0.980915">
Figure 3: Results for Chinese word segmentation and
</figureCaption>
<tableCaption confidence="0.8740358">
POS tagging task, Hybrid CRFs significantly outperform
100-Best Reranking (McNemar’s test; p &lt; 0.01)
Table 8: Comparison of word segmentation and POS tag-
ging, such comparison is indirect due to different data
sets and resources.
</tableCaption>
<table confidence="0.919664333333333">
Model F1
Pipeline (ours) 90.40
100-Best Reranking (ours) 90.53
Hybrid CRFs (ours) 90.88
Pipeline (Shi and Wang, 2007) 91.67
20-Best Reranking (Shi and Wang, 91.86
2007)
Pipeline (Zhang and Clark, 2008) 90.33
Joint Perceptron (Zhang and Clark, 91.34
2008)
Perceptron Only (Jiang et al., 2008) 92.5
Cascaded Linear (Jiang et al., 2008) 93.4
</table>
<bodyText confidence="0.919877">
sources. One common conclusion is that joint mod-
els generally outperform pipeline models.
</bodyText>
<sectionHeader confidence="0.99747" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979555555556">
We introduced a framework to integrate graph struc-
tures for segmentation and tagging subtasks into one
using virtual nodes, and performs joint training and
decoding in the factorized state space. Our approach
does not suffer from error propagation, and guards
against violations of those hard-constraints imposed
by segmentation subtask. Experiments on shal-
low parsing and Chinese word segmentation tasks
demonstrate our technique.
</bodyText>
<sectionHeader confidence="0.998465" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.970029454545454">
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
The National High Technology Research and De-
velopment Program of China (2009AA01A346),
Shanghai Leading Academic Discipline Project
(B114), Doctoral Fund of Ministry of Education of
China (200802460066), National Natural Science
Funds for Distinguished Young Scholar of China
(61003092), and Shanghai Science and Technology
Development Funds (08511500302).
</bodyText>
<sectionHeader confidence="0.984082" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999569371428571">
R. Ando and T. Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Pro-
ceedings of ACL, pages 1–9.
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In Proceedings
of EMNLP, Waikiki, Honolulu, Hawaii.
X Carreras and L Marquez. 2003. Phrase recognition by
filtering and ranking with perceptrons. In Proceedings
of RANLP.
Changning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21:8–19.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL, Columbus, Ohio, USA.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing, India.
Junichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP, pages 315–
324, Prague, June.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
Ruy L. Milidiu, Cicero Nogueira dos Santos, and Julio C.
Duarte. 2008. Phrase chunking using entropy guided
transformation learning. In Proceedings ofACL, pages
647–655.
</reference>
<figure confidence="0.9612546">
90.9
90.8
90.7
90.6
90.5
90.4
90.3
candidate reranking
Hybrid CRFs
194
</figure>
<reference confidence="0.99819747368421">
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
ofspeech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs
based joint decoding method for cascaded segmenta-
tion and labeling tasks. In Proceedings of IJCAI, pages
1707–1712, Hyderabad, India.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. In Proceedings of ICML.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL, pages
665–673.
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structured output learning based on
a hybrid generative and discriminative approach. In
Proceedings of EMNLP, Prague.
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006.
A general and multi-lingual phrase chunking model
based on masking method. In Proceedings of Intel-
ligent Text Processing and Computational Linguistics,
pages 144–155.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of ACL, Columbus, Ohio, USA.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. ma-
chine learning research. Machine Learning Research,
2:615–637.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106–111.
</reference>
<page confidence="0.952735">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.976775">
<title confidence="0.9993605">Joint Training and Decoding Using Virtual Nodes for Segmentation and Tagging Tasks</title>
<author confidence="0.999607">Xian Qian</author>
<author confidence="0.999607">Qi Zhang</author>
<author confidence="0.999607">Yaqian Zhou</author>
<author confidence="0.999607">Xuanjing Huang</author>
<author confidence="0.999607">Lide</author>
<affiliation confidence="0.999735">School of Computer Science, Fudan</affiliation>
<address confidence="0.999258">825 Zhangheng Road, Shanghai,</address>
<email confidence="0.989502">qz,zhouyaqian,xjhuang,</email>
<abstract confidence="0.999452631578947">Many sequence labeling tasks in NLP require solving a cascade of segmentation and tagging subtasks, such as Chinese POS tagging, named entity recognition, and so on. Traditional pipeline approaches usually suffer from error propagation. Joint training/decoding in the cross-product state space could cause too many parameters and high inference complexity. In this paper, we present a novel method which integrates graph structures of two subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space. Experimental evaluations on CoNLL 2000 shallow parsing data set and Fourth SIGHAN Bakeoff CTB POS tagging data set demonstrate the superiority of our method over cross-product, pipeline and candidate reranking approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Ando</author>
<author>T Zhang</author>
</authors>
<title>A high-performance semisupervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1--9</pages>
<marker>Ando, Zhang, 2005</marker>
<rawString>R. Ando and T. Zhang. 2005. A high-performance semisupervised learning method for text chunking. In Proceedings of ACL, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
</authors>
<title>Learning with probabilistic features for improved pipeline models.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Waikiki, Honolulu, Hawaii.</location>
<contexts>
<context position="2420" citStr="Bunescu (2008)" startWordPosition="373" endWordPosition="374"> modular approach. While, the key disadvantage of such method is that errors propagate between stages, significantly affecting the quality of the final results. To cope with this problem, Shi and Wang (2007) proposed a reranking framework in which N-best segment candidates generated in the first stage are passed to the tagging model, and the final output is the one with the highest overall segmentation and tagging probability score. The main drawback of this method is that the interaction between tagging and segmentation is restricted by the number of candidate segmentation outputs. Razvan C. Bunescu (2008) presented an improved pipeline model in which upstream subtask outputs are regarded as hidden variables, together with their probabilities are used as probabilistic features in the downstream subtasks. One shortcoming of this method is that calculation of marginal probabilities of features may be inefficient and some approximations are required for fast computation. Another disadvantage of these two methods is that they employ separate training and the segmentation model could not take advantages of tagging information in the training procedure. On the other hand, joint learning and decoding </context>
</contexts>
<marker>Bunescu, 2008</marker>
<rawString>Razvan C. Bunescu. 2008. Learning with probabilistic features for improved pipeline models. In Proceedings of EMNLP, Waikiki, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Marquez</author>
</authors>
<title>Phrase recognition by filtering and ranking with perceptrons.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="19628" citStr="Carreras and Marquez, 2003" startWordPosition="3269" endWordPosition="3273">ese Treebank (CTB) POS corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008). The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) Table 4: Comparison with other systems on shallow parsing task Method F1 Additional Resources Cross-Product CRFs 93.88 Hybrid CRFs 94.31 SVM combination 93.91 (Kudo and Matsumoto, 2001) Voted Perceptrons 93.74 none (Carreras and Marquez, 2003) ETL (Milidiu et al., 92.79 2008) (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, data 2005) (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 95.15 1G words unla2008) beled data using the same evaluation metrics as shallow parsing. We do not compare with cross-product CRFs due to large amounts of parameters. For pipeline method, we built our word segmenter based on the work of Huang and Zhao (2007), which uses 6 label representation, 7 featu</context>
</contexts>
<marker>Carreras, Marquez, 2003</marker>
<rawString>X Carreras and L Marquez. 2003. Phrase recognition by filtering and ranking with perceptrons. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changning Huang</author>
<author>Hai Zhao</author>
</authors>
<title>Chinese word segmentation: A decade review.</title>
<date>2007</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>21--8</pages>
<contexts>
<context position="12619" citStr="Huang and Zhao, 2007" startWordPosition="2140" endWordPosition="2143">nal distribution that factorizes according to the hybrid graphical model, and is defined as: 1 Y Y p(s, t|x) = φ(x, s, t, i) ψ(x, s, t, i) Z(x) i i Where 3 ´ φ(x, s, t, i) = exp w� 1 f(x, si−1, ti−1I IS ψ(x, s, t, i) = exp3w2 f (x, ti−1,si, ti)´ Z(x) = XÃY φ(x, s, t, i) Y ψ(x, s, t, i)! Where w1, w2 are weight vectors. Luckily, unlike DCRFs, in which graph structure can be very complex, and the cross-product state space can be very large, in our cascaded labeling task, the segmentation label set is often small, so far as we known, the most complicated segmentation label set has only 6 labels (Huang and Zhao, 2007). So exact dynamic programming based algorithms can be efficiently performed. In the training stage, we use second order forward backward algorithm to compute the marginal probabilities p(x, si−1, ti−1, si) and p(x, ti−1, si, ti), and the normalization factor Z(x). In decoding stage, we use second order Viterbi algorithm to find the best label sequence. The Viterbi decoding can be Table 1: Time Complexity Method Training Decoding Pipeline (|S|2c. + |T|2ct)L (|S|2 + |T|2)U Cross-Product (|S||T|)2cL (|S||T|)2U Reranking (|S|2c. + |T|2ct)L (|S|2 + |T|2)NU Hybrid (|S |+ |T|)|S||T|cL (|S |+ |T|)|S|</context>
<context position="20184" citStr="Huang and Zhao (2007)" startWordPosition="3368" endWordPosition="3371">, 2001) Voted Perceptrons 93.74 none (Carreras and Marquez, 2003) ETL (Milidiu et al., 92.79 2008) (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, data 2005) (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 95.15 1G words unla2008) beled data using the same evaluation metrics as shallow parsing. We do not compare with cross-product CRFs due to large amounts of parameters. For pipeline method, we built our word segmenter based on the work of Huang and Zhao (2007), which uses 6 label representation, 7 feature templates (listed in Table 5, where cz denotes the ith Chinese character in the sentence) and CRFs for parameter learning. We compare our segmentor with other top systems using SIGHAN CTB corpus and evaluation metrics. Comparison results are shown in Table 6, our segmenter achieved 95.12 F-score, which is ranked 4th of 26 official runs. Except for the first system which uses extra unlabeled data, differences between rest systems are not significant. Our POS tagging system is based on linear chain CRFs. Since SIGHAN dose not provide development dat</context>
</contexts>
<marker>Huang, Zhao, 2007</marker>
<rawString>Changning Huang and Hai Zhao. 2007. Chinese word segmentation: A decade review. Journal of Chinese Information Processing, 21:8–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Lu</author>
</authors>
<title>A cascaded linear model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="23947" citStr="Jiang et al., 2008" startWordPosition="3963" endWordPosition="3966">s and re193 0 20 40 60 80 100 candidate number Figure 3: Results for Chinese word segmentation and POS tagging task, Hybrid CRFs significantly outperform 100-Best Reranking (McNemar’s test; p &lt; 0.01) Table 8: Comparison of word segmentation and POS tagging, such comparison is indirect due to different data sets and resources. Model F1 Pipeline (ours) 90.40 100-Best Reranking (ours) 90.53 Hybrid CRFs (ours) 90.88 Pipeline (Shi and Wang, 2007) 91.67 20-Best Reranking (Shi and Wang, 91.86 2007) Pipeline (Zhang and Clark, 2008) 90.33 Joint Perceptron (Zhang and Clark, 91.34 2008) Perceptron Only (Jiang et al., 2008) 92.5 Cascaded Linear (Jiang et al., 2008) 93.4 sources. One common conclusion is that joint models generally outperform pipeline models. 5 Conclusion We introduced a framework to integrate graph structures for segmentation and tagging subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space. Our approach does not suffer from error propagation, and guards against violations of those hard-constraints imposed by segmentation subtask. Experiments on shallow parsing and Chinese word segmentation tasks demonstrate our technique. 6 Acknowledgement</context>
</contexts>
<marker>Jiang, Huang, Liu, Lu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu. 2008. A cascaded linear model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of ACL, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangjin Jin</author>
<author>Xiao Chen</author>
</authors>
<title>The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<contexts>
<context position="19106" citStr="Jin and Chen, 2008" startWordPosition="3189" endWordPosition="3192">4, supervised learning is fundamental. We believe that combination of our method and semi-supervised learning will achieve further improvement. 4.2 Chinese word segmentation and POS tagging Our second experiment is the Chinese word segmentation and POS tagging task. To facilitate comparison, we focus only on the closed test, which means that the system is trained only with a designated training corpus, any extra knowledge is not allowed, including Chinese and Arabic numbers, letters and so on. We use the Chinese Treebank (CTB) POS corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008). The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) Table 4: Comparison with other systems on shallow parsing task Method F1 Additional Resources Cross-Product CRFs 93.88 Hybrid CRFs 94.31 SVM combination 93.91 (Kudo and Matsumoto, 2001) Voted Perceptrons 93.74 none (Carreras and Marquez, 2003) ETL (Milidiu et al., 92.79 2008) (Wu et al., 2006) 94.21 Extended features su</context>
</contexts>
<marker>Jin, Chen, 2008</marker>
<rawString>Guangjin Jin and Xiao Chen. 2008. The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging. In Proceedings of Sixth SIGHAN Workshop on Chinese Language Processing, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>A new perceptron algorithm for sequence labeling with nonlocal features.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>315--324</pages>
<location>Prague,</location>
<contexts>
<context position="4240" citStr="Kazama and Torisawa, 2007" startWordPosition="645" endWordPosition="648">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187–195, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics ever, DCRFs do not guarantee non-violation of hardconstraints that nodes within the same segment get a single consistent tagging label. Another drawback of DCRFs is that exact inference is generally time consuming, some approximations are required to make it tractable. Recently, perceptron based learning framework has been well studied for incorporating node level and segment level features together (Kazama and Torisawa, 2007; Zhang and Clark, 2008). The main shortcoming is that exact inference is intractable for those dynamically generated segment level features, so candidate based searching algorithm is used for approximation. On the other hand, Jiang (2008) proposed a cascaded linear model which has a two layer structure, the inside-layer model uses node level features to generate candidates with their weights as inputs of the outside layer model which captures non-local features. As pipeline models, error propagation problem exists for such method. In this paper, we present a novel graph structure that exploit</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Junichi Kazama and Kentaro Torisawa. 2007. A new perceptron algorithm for sequence labeling with nonlocal features. In Proceedings of EMNLP, pages 315– 324, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="19570" citStr="Kudo and Matsumoto, 2001" startWordPosition="3260" endWordPosition="3264">e and Arabic numbers, letters and so on. We use the Chinese Treebank (CTB) POS corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008). The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) Table 4: Comparison with other systems on shallow parsing task Method F1 Additional Resources Cross-Product CRFs 93.88 Hybrid CRFs 94.31 SVM combination 93.91 (Kudo and Matsumoto, 2001) Voted Perceptrons 93.74 none (Carreras and Marquez, 2003) ETL (Milidiu et al., 92.79 2008) (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, data 2005) (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 95.15 1G words unla2008) beled data using the same evaluation metrics as shallow parsing. We do not compare with cross-product CRFs due to large amounts of parameters. For pipeline method, we built our word segmenter based on the work of Huang a</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="6492" citStr="Lafferty et al., 2001" startWordPosition="1006" endWordPosition="1009">idate reranking approach. The rest of this paper is organized as follows: In Section 2, we describe our novel graph structure. In Section 3, we analyze complexity of our proposed method. Experimental results are shown in Section 4. We conclude the work in Section 5. 2 Multi-chain integration using Virtual Nodes 2.1 Conditional Random Fields We begin with a brief review of the Conditional Random Fields(CRFs). Let x = x1x2 ... xl denote the observed sequence, where xi is the ith node in the sequence, l is sequence length, y = y1y2 ... yl is a label sequence over x that we wish to predict. CRFs (Lafferty et al., 2001) are undirected graphic models that use Markov network distribution to learn the conditional probability. For sequence labeling task, linear chain CRFs are very popular, in which a first order Markov assumption is made on the labels: p(y|x) = Z1x)ri φ(x, y, i) i ,where � � φ(x, y, i) = exp wT f(x, yi−1, yi, i) Z(x) = E ri φ(x, y, i) Y i f(x, yi−1, yi, i) = [f1(x, yi−1, yi, i), ...,fm(x, yi−1, yi, i)]T, each element fj(x, yi−1, yi, i) is a real valued feature function, here we simplify the notation of state feature by writing fj(x, yi, i) = fj(x, yi−1, yi, i), m is the cardinality of feature se</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruy L Milidiu</author>
<author>Cicero Nogueira dos Santos</author>
<author>Julio C Duarte</author>
</authors>
<title>Phrase chunking using entropy guided transformation learning.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>647--655</pages>
<marker>Milidiu, Santos, Duarte, 2008</marker>
<rawString>Ruy L. Milidiu, Cicero Nogueira dos Santos, and Julio C. Duarte. 2008. Phrase chunking using entropy guided transformation learning. In Proceedings ofACL, pages 647–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese partofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3193" citStr="Ng and Low, 2004" startWordPosition="489" endWordPosition="492">abilistic features in the downstream subtasks. One shortcoming of this method is that calculation of marginal probabilities of features may be inefficient and some approximations are required for fast computation. Another disadvantage of these two methods is that they employ separate training and the segmentation model could not take advantages of tagging information in the training procedure. On the other hand, joint learning and decoding using cross-product of segmentation states and tagging states does not suffer from error propagation problem and achieves higher accuracy on both subtasks (Ng and Low, 2004). However, two problems arises due to the large state space, one is that the amount of parameters increases rapidly, which is apt to overfit on the training corpus, the other is that the inference by dynamic programming could be inefficient. Sutton (2004) proposed Dynamic Conditional Random Fields (DCRFs) to perform joint training/decoding of subtasks using much fewer parameters than the cross-product approach. How187 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187–195, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computat</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese partofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>1999</date>
<publisher>Springer.</publisher>
<contexts>
<context position="17884" citStr="Nocedal and Wright, 1999" startWordPosition="2991" endWordPosition="2994">inth row in the second column). However for cross-product method, their window sizes must be consistent. For traditional cross-product CRFs and our hybrid CRFs, we use fixed gaussian prior σ = 1.0 for both methods, we find that this parameter does not signifi191 Table 3: Results for shallow parsing task, Hybrid CRFs significantly outperform Cross-Product CRFs (McNemar’s test; p &lt; 0.01) Method Cross-Product Hybrid CRFs CRFs Training Time 11.6 hours 6.3 hours Feature Num- 13 million 10 milber lion Iterations 118 141 F1 93.88 94.31 cantly affect the results when it varies between 1 and 10. LBFGS(Nocedal and Wright, 1999) method is employed for numerical optimization. Experimental results are shown in Table 3. Our proposed CRFs achieve a performance gain of 0.43 points in F-score over cross-product CRFs that use state space while require less training time. For comparison, we also listed the results of previous top systems, as shown in Table 4. Our proposed method outperforms other systems when no additional resources at hand. Though recently semisupervised learning that incorporates large mounts of unlabeled data has been shown great improvement over traditional supervised methods, such as the last row in Tab</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>J. Nocedal and S. J. Wright. 1999. Numerical Optimization. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanxin Shi</author>
<author>Mengqiu Wang</author>
</authors>
<title>A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1707--1712</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="2013" citStr="Shi and Wang (2007)" startWordPosition="305" endWordPosition="308">tation is the preliminary step for solving part-of-speech (POS) tagging problem. Sentences are firstly segmented into words, then each word is assigned with a part-of-speech tag. Both syntactic parsing and dependency parsing usually start with a textual input that is tokenized, and POS tagged. The most commonly approach solves cascaded subtasks in a pipeline, which is very simple to implement and allows for a modular approach. While, the key disadvantage of such method is that errors propagate between stages, significantly affecting the quality of the final results. To cope with this problem, Shi and Wang (2007) proposed a reranking framework in which N-best segment candidates generated in the first stage are passed to the tagging model, and the final output is the one with the highest overall segmentation and tagging probability score. The main drawback of this method is that the interaction between tagging and segmentation is restricted by the number of candidate segmentation outputs. Razvan C. Bunescu (2008) presented an improved pipeline model in which upstream subtask outputs are regarded as hidden variables, together with their probabilities are used as probabilistic features in the downstream </context>
<context position="19384" citStr="Shi and Wang, 2007" startWordPosition="3231" endWordPosition="3234">te comparison, we focus only on the closed test, which means that the system is trained only with a designated training corpus, any extra knowledge is not allowed, including Chinese and Arabic numbers, letters and so on. We use the Chinese Treebank (CTB) POS corpus from the Fourth International SIGHAN Bakeoff data sets (Jin and Chen, 2008). The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) Table 4: Comparison with other systems on shallow parsing task Method F1 Additional Resources Cross-Product CRFs 93.88 Hybrid CRFs 94.31 SVM combination 93.91 (Kudo and Matsumoto, 2001) Voted Perceptrons 93.74 none (Carreras and Marquez, 2003) ETL (Milidiu et al., 92.79 2008) (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, data 2005) (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 95.15 1G words unla2008) beled data using the same evaluati</context>
<context position="23773" citStr="Shi and Wang, 2007" startWordPosition="3936" endWordPosition="3939">rovement becomes trivial for n &gt; 20. Table 8 shows the comparison between our work and other relevant work. Notice that, such comparison is indirect due to different data sets and re193 0 20 40 60 80 100 candidate number Figure 3: Results for Chinese word segmentation and POS tagging task, Hybrid CRFs significantly outperform 100-Best Reranking (McNemar’s test; p &lt; 0.01) Table 8: Comparison of word segmentation and POS tagging, such comparison is indirect due to different data sets and resources. Model F1 Pipeline (ours) 90.40 100-Best Reranking (ours) 90.53 Hybrid CRFs (ours) 90.88 Pipeline (Shi and Wang, 2007) 91.67 20-Best Reranking (Shi and Wang, 91.86 2007) Pipeline (Zhang and Clark, 2008) 90.33 Joint Perceptron (Zhang and Clark, 91.34 2008) Perceptron Only (Jiang et al., 2008) 92.5 Cascaded Linear (Jiang et al., 2008) 93.4 sources. One common conclusion is that joint models generally outperform pipeline models. 5 Conclusion We introduced a framework to integrate graph structures for segmentation and tagging subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space. Our approach does not suffer from error propagation, and guards against violati</context>
</contexts>
<marker>Shi, Wang, 2007</marker>
<rawString>Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs based joint decoding method for cascaded segmentation and labeling tasks. In Proceedings of IJCAI, pages 1707–1712, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>K Rohanimanesh</author>
<author>A McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="15519" citStr="Sutton et al., 2004" startWordPosition="2588" endWordPosition="2591"> pi+1pi+2si pi+1pi+2yi pi−3pi−2ti, pi−2pi−1ti, pi−1piti, pipi+1ti, pi+1pi+2ti, pi+2pi+3ti, pi−1pi+1ti pi−2pi−1piyi, pi−1pipi+1yi, pi−2pi−1pisi, pi−1pipi+1si, pipi+1pi+2si pipi+1pi+2yi wipiti wisi−1si wi−1ti−1ti, witi−1ti, pi−1ti−1ti, piti−1ti yi−1yi si−1ti−1si, ti−1siti method. When decoding, candidate reranking approach requires more time if candidate number N &gt; ISI. Though the space complexity could not be compared directly among some of these methods, hybrid CRFs require less parameters than cross-product CRFs due to the factorized state space. This is similar with factorized CRFs (FCRFs) (Sutton et al., 2004). 4 Experiments 4.1 Shallow Parsing Our first experiment is the shallow parsing task. We use corpus from CoNLL 2000 shared task, which contains 8936 sentences for training and 2012 sentences for testing. There are 11 tagging labels: noun phrase(NP), verb phrase(VP) , ... and other (O), the segmentation state space we used is BIES label set, since we find that it yields a little improvement over BIO set. We use the standard evaluation metrics, which are precision P (percentage of output phrases that exactly match the reference phrases), recall R (percentage of reference phrases returned by our </context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>665--673</pages>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In Proceedings of ACL, pages 665–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Akinori Fujino</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised structured output learning based on a hybrid generative and discriminative approach.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Prague.</location>
<contexts>
<context position="19789" citStr="Suzuki et al., 2007" startWordPosition="3299" endWordPosition="3302">words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) Table 4: Comparison with other systems on shallow parsing task Method F1 Additional Resources Cross-Product CRFs 93.88 Hybrid CRFs 94.31 SVM combination 93.91 (Kudo and Matsumoto, 2001) Voted Perceptrons 93.74 none (Carreras and Marquez, 2003) ETL (Milidiu et al., 92.79 2008) (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, data 2005) (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 95.15 1G words unla2008) beled data using the same evaluation metrics as shallow parsing. We do not compare with cross-product CRFs due to large amounts of parameters. For pipeline method, we built our word segmenter based on the work of Huang and Zhao (2007), which uses 6 label representation, 7 feature templates (listed in Table 5, where cz denotes the ith Chinese character in the sentence) and CRFs for parameter learning. We compare our segmentor with other</context>
</contexts>
<marker>Suzuki, Fujino, Isozaki, 2007</marker>
<rawString>Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007. Semi-supervised structured output learning based on a hybrid generative and discriminative approach. In Proceedings of EMNLP, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Chia-Hui Chang</author>
<author>Yue-Shi Lee</author>
</authors>
<title>A general and multi-lingual phrase chunking model based on masking method.</title>
<date>2006</date>
<booktitle>In Proceedings of Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>144--155</pages>
<contexts>
<context position="19679" citStr="Wu et al., 2006" startWordPosition="3280" endWordPosition="3283">GHAN Bakeoff data sets (Jin and Chen, 2008). The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) Table 4: Comparison with other systems on shallow parsing task Method F1 Additional Resources Cross-Product CRFs 93.88 Hybrid CRFs 94.31 SVM combination 93.91 (Kudo and Matsumoto, 2001) Voted Perceptrons 93.74 none (Carreras and Marquez, 2003) ETL (Milidiu et al., 92.79 2008) (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, data 2005) (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 95.15 1G words unla2008) beled data using the same evaluation metrics as shallow parsing. We do not compare with cross-product CRFs due to large amounts of parameters. For pipeline method, we built our word segmenter based on the work of Huang and Zhao (2007), which uses 6 label representation, 7 feature templates (listed in Table 5, where cz denotes t</context>
</contexts>
<marker>Wu, Chang, Lee, 2006</marker>
<rawString>Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006. A general and multi-lingual phrase chunking model based on masking method. In Proceedings of Intelligent Text Processing and Computational Linguistics, pages 144–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and pos tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="4264" citStr="Zhang and Clark, 2008" startWordPosition="649" endWordPosition="652">ference on Empirical Methods in Natural Language Processing, pages 187–195, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics ever, DCRFs do not guarantee non-violation of hardconstraints that nodes within the same segment get a single consistent tagging label. Another drawback of DCRFs is that exact inference is generally time consuming, some approximations are required to make it tractable. Recently, perceptron based learning framework has been well studied for incorporating node level and segment level features together (Kazama and Torisawa, 2007; Zhang and Clark, 2008). The main shortcoming is that exact inference is intractable for those dynamically generated segment level features, so candidate based searching algorithm is used for approximation. On the other hand, Jiang (2008) proposed a cascaded linear model which has a two layer structure, the inside-layer model uses node level features to generate candidates with their weights as inputs of the outside layer model which captures non-local features. As pipeline models, error propagation problem exists for such method. In this paper, we present a novel graph structure that exploits joint training and dec</context>
<context position="23857" citStr="Zhang and Clark, 2008" startWordPosition="3949" endWordPosition="3952">k and other relevant work. Notice that, such comparison is indirect due to different data sets and re193 0 20 40 60 80 100 candidate number Figure 3: Results for Chinese word segmentation and POS tagging task, Hybrid CRFs significantly outperform 100-Best Reranking (McNemar’s test; p &lt; 0.01) Table 8: Comparison of word segmentation and POS tagging, such comparison is indirect due to different data sets and resources. Model F1 Pipeline (ours) 90.40 100-Best Reranking (ours) 90.53 Hybrid CRFs (ours) 90.88 Pipeline (Shi and Wang, 2007) 91.67 20-Best Reranking (Shi and Wang, 91.86 2007) Pipeline (Zhang and Clark, 2008) 90.33 Joint Perceptron (Zhang and Clark, 91.34 2008) Perceptron Only (Jiang et al., 2008) 92.5 Cascaded Linear (Jiang et al., 2008) 93.4 sources. One common conclusion is that joint models generally outperform pipeline models. 5 Conclusion We introduced a framework to integrate graph structures for segmentation and tagging subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space. Our approach does not suffer from error propagation, and guards against violations of those hard-constraints imposed by segmentation subtask. Experiments on shallo</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and pos tagging using a single perceptron. In Proceedings of ACL, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F Damerau</author>
<author>D Johnson</author>
</authors>
<title>Text chunking based on a generalization of winnow. machine learning research.</title>
<date>2002</date>
<booktitle>Machine Learning Research,</booktitle>
<pages>2--615</pages>
<contexts>
<context position="19878" citStr="Zhang et al., 2002" startWordPosition="3315" endWordPosition="3318">words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) Table 4: Comparison with other systems on shallow parsing task Method F1 Additional Resources Cross-Product CRFs 93.88 Hybrid CRFs 94.31 SVM combination 93.91 (Kudo and Matsumoto, 2001) Voted Perceptrons 93.74 none (Carreras and Marquez, 2003) ETL (Milidiu et al., 92.79 2008) (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, data 2005) (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 95.15 1G words unla2008) beled data using the same evaluation metrics as shallow parsing. We do not compare with cross-product CRFs due to large amounts of parameters. For pipeline method, we built our word segmenter based on the work of Huang and Zhao (2007), which uses 6 label representation, 7 feature templates (listed in Table 5, where cz denotes the ith Chinese character in the sentence) and CRFs for parameter learning. We compare our segmentor with other top systems using SIGHAN CTB corpus and evaluation metrics. Comparison results are shown</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>T. Zhang, F. Damerau, and D. Johnson. 2002. Text chunking based on a generalization of winnow. machine learning research. Machine Learning Research, 2:615–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging forword segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>106--111</pages>
<contexts>
<context position="21950" citStr="Zhao and Kit, 2008" startWordPosition="3627" endWordPosition="3630">(wi)ti, c2(wi)ti, c3(wi)ti, c−2(wi)ti, c−1(wi)ti (2.4) c1(wi)c2(wi)ti, c−2(wi)c−1(wi)ti (2.5) l(wi)ti (2.6) ti−1ti Joint segmentation and POS tagging feature templates (3.1) ci−2si, ci−1si, cisi, ci+1si, ci+2si (3.2) ci−1cisi, cici+1si, ci−1ci+1si (3.3) ci−3ti, ci−2ti, ci−1ti, citi, ci+1ti, ci+2ti, ci+3ti (3.4) ci−3ci−2ti, ci−2ci−1ti, ci−1citi, cici+1ti ci+1ci+2ti, ci+2ci+3ti, ci−2citi, cici+2ti (3.5) cisiti (3.6) citi−1ti (3.7) si−1ti−1si, ti−1siti Table 6: Word segmentation results on Fourth SIGHAN Bakeoff CTB corpus Rank F1 Description 1/26 95.89∗ official best, using extra unlabeled data (Zhao and Kit, 2008) 2/26 95.33 official second 3/26 95.17 official third 4/26 95.12 segmentor in pipeline system Table 7: POS results on Fourth SIGHAN Bakeoff CTB corpus Rank Accuracy Description 1/7 94.29 POS tagger in pipeline system 2/7 94.28 official best 3/7 94.01 official second 4/7 93.24 official third the sentence, cj(wi), j &gt; 0 denotes the jIh Chinese character of word wi, cj(wi), j &lt; 0 denotes the j1h last Chinese character, l(wi) denotes the word length of wi. We compare our POS tagger with other top systems on Bakeoff CTB POS corpus where sentences are perfectly segmented into words, our POS tagger a</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging forword segmentation and named entity recognition. In Proceedings of Sixth SIGHAN Workshop on Chinese Language Processing, pages 106–111.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>