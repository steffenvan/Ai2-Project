<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.877907333333333">
Specifying a shallow grammatical representation
for parsing purposes
Atro Voutilainen and Timo Jarvinen
</title>
<author confidence="0.5669285">
Research Unit for Multilingual Language Technology
P.O. Box 4
</author>
<affiliation confidence="0.777114">
FIN-00014 University of Helsinki
Finland
</affiliation>
<email confidence="0.500953">
Atro.Voutilainen,Timo.Jarvinen0Helsinki.FI
</email>
<sectionHeader confidence="0.993445" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999891380952381">
Is it possible to specify a grammatical
representation (descriptors and their ap-
plication guidelines) to such a degree
that it can be consistently applied by
different grammarians e.g. for producing
a benchmark corpus for parser evalua-
tion? Arguments for and against have
been given, but very little empirical ev-
idence. In this article we report on a
double-blind experiment with a surface-
oriented morphosyntactic grammatical
representation used in a large-scale En-
glish parser. We argue that a consis-
tently applicable representation for mor-
phology and also shallow syntax can be
specified. A grammatical representation
with a near-100% coverage of running
text can be specified with a reasonable
effort, especially if the representation is
based on structural distinctions (i.e. it is
structurally resolvable).
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999887036363637">
The central task of a parser is to assign gram-
matical descriptions onto input sentences. Eval-
uating a parser&apos;s output (as well as designing a
computational lexicon and grammar) presupposes
a predefined, parser-independent specification of
the grammatical representation.
Perhaps surprisingly, the possibility of specify-
ing a workable grammatical representation is a
matter of controversy, even at lower levels of anal-
ysis, e.g. morphology (incl. parts of speech).
Consider the following setting (the double-blind
experiment). Two linguists trained to apply a tag
set to running text according to application guide-
lines (a &amp;quot;style sheet&amp;quot;) are to analyse a given data
individually. The results are then automatically
compared, and the differences are jointly exam-
ined by these linguists to see whether the differ-
ences are due to inattention, or whether they are
intentional (i.e. there is a genuine difference in
analysis). — How many percentage points of all
words in running text are retain a different anal-
ysis after the differences due to inattention have
been omitted? The higher this percentage, the
more susceptible seems the possibility of specify-
ing a workable grammatical representation.
According to a pessimistic view (e.g. Church
1992), the part of speech of several percentage
points of words in running text is impossible to
agree on by different judges, even after negotia-
tions. A more optimistic view can be found in
(Leech and Eyes 1993, p. 39; Marcus et al. 1993,
p. 328); they argue that a near-100% interjudge
agreement is possible, provided the part-of-speech
annotation is done carefully by experts. Unfortu-
nately, they give very little empirical evidence for
their position e.g. in terms of double-blind exper-
iments.
Supposing defining these lower levels of gram-
matical representation is so problematic, the more
distinctive levels should be even more difficult. If
specifying the task of the parser — what the parser
is supposed to do — turns out to be so problematic,
one could even question the rationality of natu-
ral language parser design as a whole. In other
words, the controversy regarding the specifiability
of a grammatical representation is a fundamental
issue.
In this article we report on a double-blind ex-
periment with a surface-oriented morphosyntactic
grammatical representation used by a large-scale
English parser. We show that defining a gram-
matical representation is possible, even relatively
straightforward. We present results from part-of-
speech annotation and shallow syntactic analysis.
Our three main findings are:
</bodyText>
<listItem confidence="0.9988897">
1. A practically 100% interjudge agreement can
be reached at the level of morphological (incl.
part-of-speech) analysis provided that (i) the
grammatical representation is based on struc-
tural distinctions and (ii) the individual de-
scriptive practices of the most frequent &apos;prob-
lem cases&apos; are properly documented.
2. A shallow dependency-oriented functional
syntax can be defined, very much like a mor-
phological representation. The only substan-
</listItem>
<page confidence="0.99804">
210
</page>
<bodyText confidence="0.951964764705882">
tial difference seems to be that somewhat
more effort for documenting the individual
solution is needed at the level of syntax.
3. A grammatical representation (morphosyn-
tactic descriptors and their application guide-
lines) can be specified with a reasonable ef-
fort. In addition to general descriptive prin-
ciples, only a few dozen construction-specific
entries seem necessary for reaching a high
coverage of running text.
In short: In this paper we give empirical evi-
dence for the possibility of specifying a grammati-
cal representation in enough detail to make it (al-
most) consistently applicable. What we are less
specific about here is the exact formal properties
that make a representation easy to specify; this
topic remains open for future investigation.
</bodyText>
<sectionHeader confidence="0.590064" genericHeader="method">
2 Grammatical representation in
English Constraint Gr;,&apos;unuruar
</sectionHeader>
<bodyText confidence="0.9985608">
In the experiment to be reported in Section 3, we
employed the grammatical representation that de-
fines the descriptive task of the English Constraint
Grammar Parser ENGCG (Karlsson et al. (eds.)
1995).1
</bodyText>
<subsectionHeader confidence="0.96132">
2.1 Morphology
</subsectionHeader>
<bodyText confidence="0.999959625">
The morpholexical component in ENGCG em-
ploys 139 morphological tags for part of speech,
inflection, derivation and certain syntactic prop-
erties (e.g. verb classification). Each morpholog-
ical analysis usually consists of several tags, and
many words get several analyses as alternatives.
The following analysis of the sentence That round
table might collapse is a rather extreme example:
</bodyText>
<equation confidence="0.631014833333333">
&amp;quot;&lt;*that&gt;&amp;quot;
&amp;quot;that&amp;quot; &lt;*&gt; &lt;**CLB&gt; CS
&amp;quot;that&amp;quot; &lt;*&gt; DET CENTRAL DEM SG
&amp;quot;that&amp;quot; &lt;*&gt; ADV
&amp;quot;that&amp;quot; &lt;*&gt; PRON DEM SG
&amp;quot;that&amp;quot; &lt;*&gt; &lt;**CLB&gt; &lt;Rel&gt; PRON SG/PL
&amp;quot;&lt;round&gt;&amp;quot;
&amp;quot;round&amp;quot; &lt;SVO&gt; V SUBJUNCTIVE VFIN
&amp;quot;round&amp;quot; &lt;SVO&gt; V IMP VFIN
&amp;quot;round&amp;quot; &lt;SVO&gt; V INF
&amp;quot;round&amp;quot; &lt;SVO&gt; V PRES -SG3 VFIN
&amp;quot;round&amp;quot; PREP
&amp;quot;round&amp;quot; N NOM SG
&amp;quot;round&amp;quot; A ABS
&amp;quot;round&amp;quot; ADV
&amp;quot;&lt;table&gt;&amp;quot;
&amp;quot;table&amp;quot; N NOM SG
&amp;quot;table&amp;quot; &lt;SVO&gt; V SUBJUNCTIVE VFIN
</equation>
<bodyText confidence="0.469314">
1A list of the ENGCG tags can be retrieved via
e-mail by sending an empty mail message to engcg-
info@ling.helsinki.fi. The returned document will also
tell how to analyse own samples using the ENGCG
server,
</bodyText>
<figure confidence="0.847414538461538">
&amp;quot;table&amp;quot; &lt;SVO&gt; V IMP VFIN
&amp;quot;table&amp;quot; &lt;SVO&gt; V INF
&amp;quot;table&amp;quot; &lt;SVO&gt; V PRES -SG3 VFIN
&amp;quot;&lt;might&gt;&amp;quot;
&amp;quot;might&amp;quot; N NOM SG
&amp;quot;might&amp;quot; V AUXMOD VFIN
&amp;quot;&lt;collapse&gt;&amp;quot;
&amp;quot;collapse&amp;quot; N NOM SG
&amp;quot;collapse&amp;quot; &lt;SVO&gt; V SUBJUNCTIVE VFIN
&amp;quot;collapse&amp;quot; &lt;SVO&gt; V IMP VFIN
&amp;quot;collapse&amp;quot; &lt;SVO&gt; V INF
&amp;quot;collapse&amp;quot; &lt;SVO&gt; V PRES -SG3 VFIN
&gt;&amp;quot;
</figure>
<bodyText confidence="0.999618578947368">
The morphological analyser produces about
180 different tag combinations. To compare the
ENGCG morphological description with another
well-known tag set, the Brown Corpus tag set:
ENGCG is more distinctive in that the part of
speech distinction is spelled out in the description
of determiner-pronoun, preposition-conjunction,
and determiner-adverb-pronoun homographs, as
well as uninflected verb forms, which are repre-
sented as ambiguous due to the subjunctive, im-
perative, infinitive and present tense readings. On
the other hand, ENGCG does not spell out part-
of-speech ambiguity in the description of -ing and
nonfinite -ed forms, noun-adjective homographs
when the core meanings of the adjective and noun
readings are similar, nor abbreviations vs. proper
vs. common nouns. Generally, the ENGCG mor-
phological tag set avoids the introduction of struc-
turally unjustified distinctions.
</bodyText>
<subsectionHeader confidence="0.994156">
2.2 Syntax
</subsectionHeader>
<bodyText confidence="0.999690461538462">
ENGCG syntax employs 30 dependency-oriented
functional tags that indicate the surface-syntactic
roles of nominal heads (subject, object, preposi-
tion complement, apposition, etc.) and modifiers
(premodifiers, postmodifiers). The shallow struc-
ture of verb chains is also given - the tag set distin-
guishes between auxiliaries and main verbs, finite
and nonfinite. Also the structure of adverbials
as well as prepositional and adjective phrases is
given, though some of the attachments of adver-
bials is left underspecified.
Finally, a disambiguated sample analysis of the
above sample sentence:
</bodyText>
<figure confidence="0.554040454545454">
&amp;quot;&lt;*that&gt;&amp;quot;
&amp;quot;that&amp;quot; &lt;*&gt; DET CENTRAL DEM SG ODN&gt;
&amp;quot;&lt;round&gt;&amp;quot;
&amp;quot;round&amp;quot; A ABS CAN&gt;
&amp;quot;&lt;table&gt;&amp;quot;
&amp;quot;table&amp;quot; N NOM SG OSUBJ
&amp;quot;&lt;might&gt;&amp;quot;
&amp;quot;might&amp;quot; V AUXMOD VFIN 0+FAUXV
&amp;quot;&lt;collapse&gt;&amp;quot;
&amp;quot;collapse&amp;quot; &lt;SVO&gt; V INF O-FMAINV
“&lt;$.&gt;“
</figure>
<page confidence="0.989697">
211
</page>
<bodyText confidence="0.996544">
Syntactic tags are flanked with the @-sign;2
morphological tags and the base form are given
to the left of the syntactic tags.
</bodyText>
<sectionHeader confidence="0.943401" genericHeader="method">
3 The experiment
</sectionHeader>
<bodyText confidence="0.9995534">
This section reports on an experiment on part-
of-speech and syntactic disambiguation by human
experts (the authors of this article). Three 2,000-
word texts were successively used: a software
manual, a scientific magazine, and a newspaper.
</bodyText>
<subsectionHeader confidence="0.998271">
3.1 Setting
</subsectionHeader>
<bodyText confidence="0.99324">
The experiment was conducted as follows.
</bodyText>
<listItem confidence="0.9753735">
1. The text was morphologically analysed us-
ing the ENGCG morphological analyser. For
the analysis of unrecognised words, we used
a rule-based heuristic component that assigns
morphological analyses, one or more, to each
word not represented in the lexicon of the sys-
tem.
2. Two experts in the ENGCG grammatical rep-
resentation independently marked the correct
alternative analyses in the ambiguous input,
using mainly structural, but in some struc-
turally unresolvable cases also higher-level,
</listItem>
<bodyText confidence="0.949264666666667">
information. The corpora consisted of con-
tinuous text rather than isolated sentences;
this made the use of textual knowledge possi-
ble in the selection of the correct alternative.
In the rare cases where two analyses were re-
garded as equally legitimate, both could be
marked. The judges were encouraged to con-
sult the documentation of the grammatical
representation.
</bodyText>
<listItem confidence="0.9410473125">
3. These tagged versions were compared to each
other using the Unix sdiff program.
4. The differences were jointly examined by the
judges in order to see whether they were due
to (i) inattention, (ii) incomplete specifica-
tion of the grammatical representation or (iii)
an undecidable analysis.
5. A &apos;consensus&apos; version of the tagged corpus
was prepared. Usually only a unique analysis
was given. However, there were three situa-
tions where a multiple analysis was accepted:
• When the judges disagree about the cor-
rect analysis even after negotiations. In
this case, comments were added to dis-
tinguish it from the other two types.
• Neutralisation: both analyses were re-
garded as equivalent. (This often indi-
cates a redundancy in the lexicon.)
&amp;quot;ODN&gt;&amp;quot; represents determiners; &amp;quot;@AN&gt;&amp;quot; rep-
resents premodifying adjectives; &amp;quot;@SUBJ&amp;quot; represents
subjects; &amp;quot;@+FAUXV&amp;quot; represents finite auxiliaries;
and &amp;quot;©-FMAINV&amp;quot; represents nonfinite main verbs.
• Global ambiguity: the sentence was
agreed to be globally ambiguous.
6. Whenever an undefined construction was de-
tected during the joint examination, the
grammar definition manual was updated.
7. The preparation of the syntactic version was
the next main step. For each contextually
appropriate morphological reading, all syn-
tactic tags were introduced with a mapping
program. An example:3
</listItem>
<figure confidence="0.990690357142857">
&amp;quot;&lt;*that&gt;&amp;quot;
&amp;quot;that&amp;quot; &lt;*&gt; DET CENTRAL DEM SG ON&gt;
&amp;quot;&lt;round&gt;&amp;quot;
&amp;quot;round&amp;quot; A ABS @AN&gt;
&amp;quot;&lt;table&gt;&amp;quot;
&amp;quot;table&amp;quot; N NOM SG ONPRR OSUBJ @OBJ
@I-OBJ @PCOMPL-S OPCOMPL-0
CAPP (INN&gt; O&lt;P 00-ADVL
&amp;quot;&lt;might&gt;&amp;quot;
&amp;quot;might&amp;quot; V AUXMOD VFIN C+FAUXV
&amp;quot;&lt;collapse&gt;&amp;quot;
&amp;quot;collapse&amp;quot; &lt;SVO&gt; V INF e-FMAINV
e&lt;P-FMAINV el&lt;NOM-FMAINV
&amp;quot;&lt;$.&gt;&amp;quot;
</figure>
<bodyText confidence="0.850749875">
8. Steps 2-6 were applied to these syntactic am-
biguities.
This procedure was successively applied to the
three texts to see how much previous updates of
the grammar definition manual decreased the need
for further updates and how much the interjudge
agreement might increase even after the first me-
chanical comparison (cf. Step 3).
</bodyText>
<subsectionHeader confidence="0.804938">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999209">
The results are given in Figure 1 (next page).
Some comments are in order, first about mor-
phology.
</bodyText>
<listItem confidence="0.830491785714286">
• The initial consistency rate was constantly
above 99%.
• After negotiations, the judges agreed about
the correct analysis or analyses in all cases.
The vast majority of the initial differences
were due to inattention, and the remaining
few to incomplete specification of the mor-
phological representation. Some representa-
tive examples about these jointly examined
3 &amp;quot;ONPIIR&amp;quot; represents stray nominal heads;
&amp;quot;OB J&amp;quot; represents objects; &amp;quot;@I-OBJ&amp;quot; represents
indirect objects; &amp;quot;@PCOMPL-S&amp;quot; represents sub-
ject complements; &amp;quot;OPCOMPL-0&amp;quot; represents ob-
ject complements; &amp;quot;@APP&amp;quot; represents appositions;
</listItem>
<bodyText confidence="0.840276428571429">
&amp;quot;ONN&gt;&amp;quot; represents premodifying nouns (and non-
final noun parts in compounds); &amp;quot;O&lt;P&amp;quot; represents
nominal preposition complements; &amp;quot;@0-ADVL&amp;quot; rep-
resents nominal adverbials; &amp;quot;©&lt;P-FMAINV&amp;quot; repre-
sents nonfinite main verbs as preposition comple-
ments; and &amp;quot;O&lt;NOM-FMAINV&amp;quot; represents post-
modifying nonfinite main verbs.
</bodyText>
<page confidence="0.994015">
212
</page>
<table confidence="0.999000571428571">
text news technical magazine . total
words 1999 1999 2073 6071
morph.tags/word in input 1.78 1.95 1.72 1.82
morphologically ambiguous words 41.1% 45.4% 36.8% 41.0%
agreement after mechanical comparison 99.3% 99.3% 99.1% 99.2%
updates to morphology manual 1 1 1 3
agreement after negotiations 100% 100% 100% 100%
morph.tags/word in consensus corpus 1.00 (+2) 1.00 (+0) 1.00 (+1) 1.00 (+3)
syn.tags/word in input 3.50 3.53 3.36 3.46
syntactically ambiguous words 42.0% 41.9% 44.9% 42.9%
agreement after mechanical comparison 95.8% 97.0% 97.4% 96.8%
updates to syntax manual 5 1 1 7
agreement after negotiations 100% 100% 100% 100%
syn.tags/word in consensus corpus 1.01 (+18) 1.01 (+11) 1.00 (+3) 1.01 (+32)
</table>
<figureCaption confidence="0.998703">
Figure 1: Results of a tagging test.
</figureCaption>
<bodyText confidence="0.8988415">
differences are in order. (Words followed by
an expression of the form (X/Y) were ini-
tially tagged differently by the judges. After
joint examination, Y was agreed to be the
correct alternative in all cases but (5), where
X and Y were regarded as equally possible.r
</bodyText>
<listItem confidence="0.9984948">
1. As we go(V INF / V PRES) to(INF-
MARK / PREP) press(V INF / N),
George Bush&apos;s decision not to sign the
Biodiversity Convention, and Britain&apos;s
apparent intention to follow suit, are
grievous blows..
2. .. they were circulating a letter ex-
pressing concern that(PRON REL / CS)
it would give the developing countries
a blank cheque to demand money from
donors to finance sustainable develop-
ment.
3. That(PRON DEM / CS) there was no
outburst of protest over the new policy
suggests that public anxiety over genetic
engineering has ebbed in recent years.
.4. The value-added information is the
kind(A / N) we want ourselves.&amp;quot;
5. .. they had not seen before at one(NUM
/ PROM of the busiest times of the
school year.
6. I don&apos;t think people get(V INF / V
PRES) a great deal from bald figures.
7. She had to ask because some of the
six-year-olds from other schools who at-
tend(V INF / V PRES) her classes know
the names of as(PREP / AD-A&gt;) many
hard drugs as she does.
• Only three updates were needed to the mor-
phological part of the manual.
</listItem>
<bodyText confidence="0.734737416666667">
&apos;Before an &amp;quot;of&amp;quot; phrase, the pronoun/numeral dis-
tinction of &amp;quot;one&amp;quot; was regarded as neutralised. This
observation was also added to the morphology manual.
• Though multiple analyses were considered ac-
ceptable in the case of (even semantically) un-
decidable situations, very few were actually
needed: only 3 words out of 6,071 received
two analyses (for example, it was agreed that
more could be analysed both as an adverb
and as a pronoun in .. free trade will mean
you destroy more.).
Next, some observations about syntax.
</bodyText>
<listItem confidence="0.94388475">
• At the level of syntax, most of the initial dif-
ferences were identified as obvious mistakes,
e.g.:
- He was (@+FMAINV / @-i-FAUXV) ad-
dressing his hosts ..
• Sometimes, however, there was a need to dis-
cuss the descriptive policies. Consider the fol-
lowing sentence fragment:5
- that managers&apos;(@GN&gt;) keeping
(@-FMAINV / @SUBJ) in(@ADVL /
@&lt;NOM) touch with employees en-
hances communication ..
</listItem>
<bodyText confidence="0.938376764705882">
In principle, managers&apos; could be described as
a subject in a nonfinite clause, and keeping
accordingly as a nonfinite main verb. How-
ever, the ENGCG syntactic representation
does not recognise the subject category in
nonfinite clauses; therefore, in the name of
consistency, keeping in the above example
should be assigned a nominal rather than a
verbal function - finite clause subject, in this
case.
• Initially, the syntactic representation was less
completely specified than the morphological
representation. The grammar definition man-
ual initially comprised twelve entries for syn-
tactic functions; seven additions were made
5&amp;quot;©GN&gt;&amp;quot; represents genitival premodifiers, and
&amp;quot;©&lt;NOM&amp;quot; represents postmodifiers.
</bodyText>
<page confidence="0.997427">
213
</page>
<bodyText confidence="0.911225">
during the experiment. This had a positive
effect: the initial disagreements decreased
from 4.2% to 2.6% during the three rounds.
• The entries in the syntax manual can be clas-
sified into three types:
</bodyText>
<listItem confidence="0.884096222222222">
1. Two or more alternatives are struc-
turally plausible, but one is to be con-
sistently preferred; e.g. A number that
occurs after a proper noun and is sur-
rounded by commas is a postmodifier
(rather than an apposition)
2. Elimination of a distinction in certain
contexts; e.g. Premodifying -ing forms
are to be analysed as adjectives (rather
than as nouns)
3. An unorthodox policy is adopted; e.g. In
sentences with a formal subject, what is
usually regarded as a notional subject is
here analysed as a subject complement.
• Multiple analyses were given to 32 words
(0.5% of all words). PP attachment, in par-
ticular the distinction between clause level
(@ADVL) and postmodifying (@&lt;NOM)
</listItem>
<bodyText confidence="0.982698117647059">
functions, proved to be the most difficult syn-
tactic phenomenon to define uniquely; often
the analyses remained somewhat indetermi-
nate. With the first sample, 5.7% of all
prepositions were initially annotated differ-
ently; even with the last sample, 4.5% of the
initial analyses differed. Unsurprisingly, a fre-
quent agreement in the analysis of these cases
was to accept both alternatives as legitimate.
• Some further examples are in order. These
examples show some possible structural am-
biguities in which text-level semantic infor-
mation was needed to decide upon the pre-
ferred analysis Y over less plausible alterna-
tive X. Note that the adopted decision often
determines the correct analysis of one or more
subsequent items (the &amp;quot;domino effect&amp;quot;).
</bodyText>
<listItem confidence="0.9985144">
1. .. his priority was (@+FAUXV
/ @+FMAINV) keeping his country&apos;s
biotechnology industry free ..
2. Germany wants the heads of European
governments and perhaps Japan (OBJ
/ @&lt;P) to issue a &apos;declaration of like-
minded parties&amp;quot;.
3. We were (@-FMAINV / @+FAUXV)
pleased (@PCOMPL-S / @-FMAINV)
with (@&lt;NOM / @ADVL) the report.
</listItem>
<bodyText confidence="0.9999828">
The last type was recurrent because the
ENGCG morphology offers only a past participle
reading to -ed-forms. We prefer the verbal reading
and predicative -ed-forms are listed as exceptions
in the coding manual.
</bodyText>
<sectionHeader confidence="0.996517" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999953785714286">
A satisfactory definition of the grammatical rep-
resentation appears possible, not only at the level
of morphology, but also at the level of shallow
dependency-oriented functional syntax. In our
experiments, a practically 100% consensus was
reached at both these levels during the joint exam-
ination. Our results agree, at least at the level of
morphology, with (Leech and Eyes 1993; Marcus
et al. 1993). In our experiment, the main differ-
ences between morphology and syntax were that
(i) specifying the syntactic representation takes a
few more pages in the definition manual, and (ii)
there seem to be more cases in syntax where mul-
tiple analyses have to be accepted — but relatively
few even then.
The grammatical representation should employ
intuitively clear grammatical descriptors that (i)
represent all constructions in the language and
(ii) reflect distributional distinctions. Proposing
a too fine-grained classification of e.g. -ing forms,
as may be the case in the tagged Brown Corpus,
can make the principled selection of the appro-
priate analysis very difficult, even with detailed
manuals.
As a minor point we may add that errors due to
inattention tend to occur in the preparation of e.g.
benchmark corpora; however, almost all of them
can be eliminated using the double-blind method.
</bodyText>
<sectionHeader confidence="0.987491" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999405">
We would like to thank Jussi Piitulainen, Pasi Ta-
panainen and two EACL referees for useful com-
ments on a previous version of this paper. The
usual disclaimers hold.
</bodyText>
<sectionHeader confidence="0.998277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828523809524">
Kenneth W. Church 1992. Current Practice in
Part of Speech Tagging and Suggestions for the
Future. In Simmons (ed.), Sbornik praci: In
Honor of Henry Kueera, Michigan Slavic Studies.
Michigan. 13-48.
Elizabeth Eyes and Geoffrey Leech 1993. Syn-
tactic Annotation: Linguistic Aspects of Gram-
matical Tagging and Skeleton Parsing. In Ezra
Black, Roger Garside and Geoffrey Leech (eds.)
1993. Statistically-Driven Computer Grammars
of English: The IBM/Lancaster Approach. Am-
sterdam and Atlanta: Rodopi. 36-61.
Fred Karlsson, Atro Voutilainen, Juha Heikkila
and Arto Anttila (eds.) 1995. Constraint Gram-
mar. A Language-Independent System for Pars-
ing Unrestricted Text. Berlin and New York:
Mouton de Gruyter.
Mitchell Marcus, Beatrice Santorini and Mary
Ann Marcinkiewicz 1993. Building a Large An-
notated Corpus of English: The Penn Treebank.
Computational Linguistics 19:2. 313-330.
</reference>
<page confidence="0.998954">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.446990">
<title confidence="0.9992845">Specifying a shallow grammatical representation for parsing purposes</title>
<author confidence="0.999357">Atro Voutilainen</author>
<author confidence="0.999357">Timo Jarvinen</author>
<affiliation confidence="0.970647">Research Unit for Multilingual Language Technology</affiliation>
<address confidence="0.977283">P.O. Box 4</address>
<affiliation confidence="0.938068">FIN-00014 University of Helsinki Finland</affiliation>
<address confidence="0.51603">Atro.Voutilainen,Timo.Jarvinen0Helsinki.FI</address>
<abstract confidence="0.998669227272727">Is it possible to specify a grammatical representation (descriptors and their application guidelines) to such a degree that it can be consistently applied by different grammarians e.g. for producing a benchmark corpus for parser evaluation? Arguments for and against have been given, but very little empirical evidence. In this article we report on a double-blind experiment with a surfaceoriented morphosyntactic grammatical representation used in a large-scale English parser. We argue that a consistently applicable representation for morphology and also shallow syntax can be specified. A grammatical representation with a near-100% coverage of running text can be specified with a reasonable effort, especially if the representation is based on structural distinctions (i.e. it is structurally resolvable).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Current Practice in Part of Speech Tagging and Suggestions for the Future.</title>
<date>1992</date>
<booktitle>Sbornik praci: In Honor of Henry Kueera, Michigan Slavic Studies. Michigan.</booktitle>
<pages>13--48</pages>
<editor>In Simmons (ed.),</editor>
<contexts>
<context position="2331" citStr="Church 1992" startWordPosition="341" endWordPosition="342">t&amp;quot;) are to analyse a given data individually. The results are then automatically compared, and the differences are jointly examined by these linguists to see whether the differences are due to inattention, or whether they are intentional (i.e. there is a genuine difference in analysis). — How many percentage points of all words in running text are retain a different analysis after the differences due to inattention have been omitted? The higher this percentage, the more susceptible seems the possibility of specifying a workable grammatical representation. According to a pessimistic view (e.g. Church 1992), the part of speech of several percentage points of words in running text is impossible to agree on by different judges, even after negotiations. A more optimistic view can be found in (Leech and Eyes 1993, p. 39; Marcus et al. 1993, p. 328); they argue that a near-100% interjudge agreement is possible, provided the part-of-speech annotation is done carefully by experts. Unfortunately, they give very little empirical evidence for their position e.g. in terms of double-blind experiments. Supposing defining these lower levels of grammatical representation is so problematic, the more distinctive</context>
</contexts>
<marker>Church, 1992</marker>
<rawString>Kenneth W. Church 1992. Current Practice in Part of Speech Tagging and Suggestions for the Future. In Simmons (ed.), Sbornik praci: In Honor of Henry Kueera, Michigan Slavic Studies. Michigan. 13-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Eyes</author>
<author>Geoffrey Leech</author>
</authors>
<title>Syntactic Annotation: Linguistic Aspects of Grammatical Tagging and Skeleton Parsing.</title>
<date>1993</date>
<booktitle>Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach. Amsterdam and Atlanta: Rodopi.</booktitle>
<pages>36--61</pages>
<editor>In Ezra Black, Roger Garside and Geoffrey Leech (eds.)</editor>
<marker>Eyes, Leech, 1993</marker>
<rawString>Elizabeth Eyes and Geoffrey Leech 1993. Syntactic Annotation: Linguistic Aspects of Grammatical Tagging and Skeleton Parsing. In Ezra Black, Roger Garside and Geoffrey Leech (eds.) 1993. Statistically-Driven Computer Grammars of English: The IBM/Lancaster Approach. Amsterdam and Atlanta: Rodopi. 36-61.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen, Juha Heikkila and Arto Anttila (eds.) 1995. Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text.</title>
<location>Berlin and New York: Mouton</location>
<note>de Gruyter.</note>
<marker>Karlsson, </marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkila and Arto Anttila (eds.) 1995. Constraint Grammar. A Language-Independent System for Parsing Unrestricted Text. Berlin and New York: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
</authors>
<title>Beatrice Santorini and Mary Ann Marcinkiewicz</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>313--330</pages>
<marker>Marcus, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini and Mary Ann Marcinkiewicz 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics 19:2. 313-330.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>