<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.984389">
Searching Questions by Identifying Question Topic and Question Focus
</title>
<author confidence="0.997742">
Huizhong Duan1, Yunbo Cao1,2, Chin-Yew Lin2 and Yong Yu1
</author>
<affiliation confidence="0.759428">
1Shanghai Jiao Tong University,
Shanghai, China, 200240
</affiliation>
<email confidence="0.974634">
{summer, yyu}@apex.sjtu.edu.cn
</email>
<sectionHeader confidence="0.997242" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9983996">
This paper is concerned with the problem of
question search. In question search, given a
question as query, we are to return questions
semantically equivalent or close to the queried
question. In this paper, we propose to conduct
question search by identifying question topic
and question focus. More specifically, we first
summarize questions in a data structure con-
sisting of question topic and question focus.
Then we model question topic and question
focus in a language modeling framework for
search. We also propose to use the MDL-
based tree cut model for identifying question
topic and question focus automatically. Expe-
rimental results indicate that our approach of
identifying question topic and question focus
for search significantly outperforms the base-
line methods such as Vector Space Model
(VSM) and Language Model for Information
Retrieval (LMIR).
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999581785714286">
Over the past few years, online services have been
building up very large archives of questions and
their answers, for example, traditional FAQ servic-
es and emerging community-based Q&amp;A services
(e.g., Yahoo! Answers1, Live QnA2, and Baidu
Zhidao3).
To make use of the large archives of questions
and their answers, it is critical to have functionality
facilitating users to search previous answers. Typi-
cally, such functionality is achieved by first re-
trieving questions expected to have the same
answers as a queried question and then returning
the related answers to users. For example, given
question Q1 in Table 1, question Q2 can be re-
</bodyText>
<footnote confidence="0.9458765">
1 http://answers.yahoo.com
2 http://qna.live.com
3 http://zhidao.baidu.com
2Microsoft Research Asia,
</footnote>
<address confidence="0.349756">
Beijing, China, 100080
</address>
<email confidence="0.972623">
{yunbo.cao, cyl}@microsoft.com
</email>
<bodyText confidence="0.9990225">
turned and its answer will then be used to answer
Q1 because the answer of Q2 is expected to par-
tially satisfy the queried question Q1. This is what
we called question search. In question search, re-
turned questions are semantically equivalent or
close to the queried question.
</bodyText>
<figure confidence="0.634153875">
Query:
Q1: Any cool clubs in Berlin or Hamburg?
Expected:
Q2: What are the best/most fun clubs in Berlin?
Not Expected:
Q3: Any nice hotels in Berlin or Hamburg?
Q4: How long does it take to Hamburg from Berlin?
Q5: Cheap hotels in Berlin?
</figure>
<tableCaption confidence="0.989834">
Table 1. An Example on Question Search
</tableCaption>
<bodyText confidence="0.99987924">
Many methods have been investigated for tack-
ling the problem of question search. For example,
Jeon et al. have compared the uses of four different
retrieval methods, i.e. vector space model, Okapi,
language model, and translation-based model,
within the setting of question search (Jeon et al.,
2005b). However, all the existing methods treat
questions just as plain texts (without considering
question structure). For example, obviously, Q2
can be considered semantically closer to Q1 than
Q3-Q5 although all questions (Q2-Q5) are related
to Q1. The existing methods are not able to tell the
difference between question Q2 and questions Q3,
Q4, and Q5 in terms of their relevance to question
Q1. We will clarify this in the following.
In this paper, we propose to conduct question
search by identifying question topic and question
focus.
The question topic usually represents the major
context/constraint of a question (e.g., Berlin, Ham-
burg) which characterizes users’ interests. In con-
trast, question focus (e.g., cool club, cheap hotel)
presents certain aspect (or descriptive features) of
the question topic. For the aim of retrieving seman-
tically equivalent (or close) questions, we need to
</bodyText>
<page confidence="0.983807">
156
</page>
<note confidence="0.713925">
Proceedings of ACL-08: HLT, pages 156–164,
</note>
<page confidence="0.538095">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999979065217391">
assure that returned questions are related to the
queried question with respect to both question top-
ic and question focus. For example, in Table 1, Q2
preserves certain useful information of Q1 in the
aspects of both question topic (Berlin) and ques-
tion focus (fun club) although it loses some useful
information in question topic (Hamburg). In con-
trast, questions Q3-Q5 are not related to Q1 in
question focus (although being related in question
topic, e.g. Hamburg, Berlin), which makes them
unsuitable as the results of question search.
We also propose to use the MDL-based (Mini-
mum Description Length) tree cut model for auto-
matically identifying question topic and question
focus. Given a question as query, a structure called
question tree is constructed over the question col-
lection including the queried question and all the
related questions, and then the MDL principle is
applied to find a cut of the question tree specifying
the question topic and the question focus of each
question.
In a summary, we summarize questions in a data
structure consisting of question topic and question
focus. On the basis of this, we then propose to
model question topic and question focus in a lan-
guage modeling framework for search. To the best
of our knowledge, none of the existing studies ad-
dressed question search by modeling both question
topic and question focus.
We empirically conduct the question search with
questions about ‘travel’ and ‘computers &amp; internet’.
Both kinds of questions are from Yahoo! Answers.
Experimental results show that our approach can
significantly improve traditional methods (e.g.
VSM, LMIR) in retrieving relevant questions.
The rest of the paper is organized as follow. In
Section 2, we present our approach to question
search which is based on identifying question topic
and question focus. In Section 3, we empirically
verify the effectiveness of our approach to question
search. In Section 4, we employ a translation-based
retrieval framework for extending our approach to
fix the issue called ‘lexical chasm’. Section 5 sur-
veys the related work. Section 6 concludes the pa-
per by summarizing our work and discussing the
future directions.
</bodyText>
<sectionHeader confidence="0.912641" genericHeader="method">
2 Our Approach to Question Search
</sectionHeader>
<bodyText confidence="0.998331545454545">
Our approach to question search consists of two
steps: (a) summarize questions in a data structure
consisting of question topic and question focus; (b)
model question topic and question focus in a lan-
guage modeling framework for search.
In the step (a), we employ the MDL-based (Min-
imum Description Length) tree cut model for au-
tomatically identifying question topic and question
focus. Thus, this section will begin with a brief
review of the MDL-based tree cut model and then
follow that by an explanation of steps (a) and (b).
</bodyText>
<subsectionHeader confidence="0.994275">
2.1 The MDL-based tree cut model
</subsectionHeader>
<bodyText confidence="0.99879">
Formally, a tree cut model ܯ (Li and Abe, 1998)
can be represented by a pair consisting of a tree cut
߁, and a probability parameter vector ߠ of the same
length, that is,
</bodyText>
<equation confidence="0.98819">
ܯ ൌ ሺ߁, ߠሻ (1)
</equation>
<bodyText confidence="0.810698">
where ߁ and ߠ are
</bodyText>
<equation confidence="0.997891333333333">
߁ ൌ ሾܥଵ, ܥଶ,.. ܥ௞ሿ,
(2)
ߠ ൌ ሾ݌ሺܥଵሻ, ݌ሺܥଶሻ, ..., ݌ሺܥ௞ሻሿ
</equation>
<bodyText confidence="0.99428">
where ܥଵ, ܥଶ, ... ܥ௞ are classes determined by a cut
in the tree and ∑ ݌ሺܥ௜ሻ ൌ 1
</bodyText>
<equation confidence="0.3947195">
௞ . A ‘cut’ in a tree is
௜ୀଵ
</equation>
<bodyText confidence="0.970808333333333">
any set of nodes in the tree that defines a partition
of all the nodes, viewing each node as representing
the set of child nodes as well as itself. For example,
the cut indicated by the dash line in Figure 1 cor-
responds to three classes:ሾ݊଴, ݊ଵଵሿ,ሾ݊ଵଷ, ݊ଶସሿ, and
ሾ݊ଵଶ, ݊ଶଵ, ݊ଶଶ, ݊ଶଷሿ.
</bodyText>
<equation confidence="0.977949">
݊଴
݊ଵଵ ݊ଵଶ ݊ଵଷ
݊ଶଵ ݊ଶଶ ݊ଶଷ ݊ଶସ
</equation>
<figureCaption confidence="0.997062">
Figure 1. An Example on the Tree Cut Model
</figureCaption>
<bodyText confidence="0.999770875">
A straightforward way for determining a cut of a
tree is to collapse the nodes of less frequency into
their parent nodes. However, the method is too
heuristic for it relies much on manually tuned fre-
quency threshold. In our practice, we turn to use a
theoretically well-motivated method based on the
MDL principle. MDL is a principle of data com-
pression and statistical estimation from informa-
tion theory (Rissanen, 1978).
Given a sample ܵ and a tree cut ߁, we employ
MLE to estimate the parameters of the correspond-
ing tree cut model ܯ෡ ൌ ሺ߁, ߠ෠ሻ, where ߠ෠ denotes
the estimated parameters.
According to the MDL principle, the description
length (Li and Abe, 1998) ܮሺܯ෡,ܵሻ of the tree cut
model ܯ෡ and the sample ܵ is the sum of the model
</bodyText>
<page confidence="0.995121">
157
</page>
<bodyText confidence="0.920497666666667">
description length L(r), the parameter description
length L(B�|r) , and the data description length
L(S|F, B�), i.e.
</bodyText>
<equation confidence="0.995947">
L(M, S) = L(r) + L(B�Jr) + L(S|r, B�) (3)
</equation>
<bodyText confidence="0.999668">
The model description length L(r) is a subjec-
tive quantity which depends on the coding scheme
employed. Here, we simply assume that each tree
cut model is equally likely a priori.
The parameter description length L(B�|r) is cal-
culated as
</bodyText>
<equation confidence="0.999942">
L(B�1r) = Z x log |S |(4)
</equation>
<bodyText confidence="0.9949934">
where |S |denotes the sample size and k denotes
the number of free parameters in the tree cut model,
i.e. k equals the number of nodes in r minus one.
The data description length L(S|F, B) is calcu-
lated as
</bodyText>
<equation confidence="0.9974">
L(Slr, B�) = — ZneS lo9p�(n) (5)
</equation>
<bodyText confidence="0.503719">
where
</bodyText>
<equation confidence="0.994805333333333">
i f(c) (6)
P(n) = |0x
|�|
</equation>
<bodyText confidence="0.9999465">
where C is the class that n belongs to and f(C)
denotes the total frequency of instances in class C
in the sample S.
With the description length defined as (3), we
wish to select a tree cut model with the minimum
description length and output it as the result. Note
that the model description length L(r) can be ig-
nored because it is the same for all tree cut models.
The MDL-based tree cut model was originally
introduced for handling the problem of generaliz-
ing case frames using a thesaurus (Li and Abe,
1998). To the best of our knowledge, no existing
work utilizes it for question search. This may be
partially because of the unavailability of the re-
sources (e.g., thesaurus) which can be used for
embodying the questions in a tree structure. In Sec-
tion 2.2, we will introduce a tree structure called
question tree for representing questions.
</bodyText>
<subsectionHeader confidence="0.8204">
2.2 Identifying question topic and question
focus
</subsectionHeader>
<bodyText confidence="0.9996594">
In principle, it is possible to identify question topic
and question focus of a question by only parsing
the question itself (for example, utilizing a syntac-
tic parser). However, such a method requires accu-
rate parsing results which cannot be obtained from
the noisy data from online services.
Instead, we propose using the MDL-based tree
cut model which identifies question topics and
question foci for a set of questions together. More
specifically, the method consists of two phases:
</bodyText>
<listItem confidence="0.983366">
1) Constructing a question tree: represent the
queried question and all the related questions
in a tree structure called question tree;
2) Determining a tree cut: apply the MDL prin-
ciple to the question tree, which yields the cut
specifying question topic and question focus.
</listItem>
<subsectionHeader confidence="0.933237">
2.2.1 Constructing a question tree
</subsectionHeader>
<bodyText confidence="0.999755333333333">
In the following, with a series of definitions, we
will describe how a question tree is constructed
from a collection of questions.
Let’s begin with explaining the representation of
a question. A straightforward method is to
represent a question as a bag-of-words (possibly
ignoring stop words). However, this method cannot
discern ‘the hotels in Paris’ from ‘the Paris hotel’.
Thus, we turn to use the linguistic units carrying on
more semantic information. Specifically, we make
use of two kinds of units: BaseNP (Base Noun
Phrase) and WH-ngram. A BaseNP is defined as a
simple and non-recursive noun phrase (Cao and Li,
2002). A WH-ngram is an ngram beginning with
WH-words. The WH-words that we consider in-
clude ‘when’, ‘what’, ‘where’, ‘which’, and ‘how’.
We refer to these two kinds of units as ‘topic
terms’. With ‘topic terms’, we represent a question
as a topic chain and a set of questions as a question
tree.
Definition 1 (Topic Profile) The topic profile
Bt of a topic term t in a categorized question col-
lection is a probability distribution of categories
{p(c|t)}cEc where C is a set of categories.
</bodyText>
<equation confidence="0.99322">
p(C|t) = count(c,t) (7)
Zcec count(c,t)
</equation>
<bodyText confidence="0.996997692307692">
where count(c, t) is the frequency of the topic
term t within category c . Clearly, we
have ZcEcp(c|t) = 1.
By ‘categorized questions’, we refer to the ques-
tions that are organized in a tree of taxonomy. For
example, at Yahoo! Answers, the question “How
do I install my wireless router” is categorized as
“Computers &amp; Internet Æ Computer Networking”.
Actually, we can find categorized questions at oth-
er online services such as FAQ sites, too.
Definition 2 (Specificity) The specificity s(t) of
a topic term t is the inverse of the entropy of the
topic profile Bt. More specifically,
</bodyText>
<equation confidence="0.999097">
S(t) = 1/(—Zcecp(c|t)logp(c|t)+ e) (8)
</equation>
<page confidence="0.962414">
158
</page>
<bodyText confidence="0.979262733333333">
where 8 is a smoothing parameter used to cope
with the topic terms whose entropy is 0. In our ex-
periments, the value of 8 was set 0.001.
We use the term specificity to denote how spe-
cific a topic term is in characterizing information
needs of users who post questions. A topic term of
high specificity (e.g., Hamburg, Berlin) usually
specifies the question topic corresponding to the
main context of a question because it tends to oc-
cur only in a few categories. A topic term of low
specificity is usually used to represent the question
focus (e.g., cool club, where to see) which is rela-
tively volatile and might occur in many categories.
Definition 3 (Topic Chain) A topic chain qc of
a question q is a sequence of ordered topic terms
</bodyText>
<equation confidence="0.875145333333333">
ݐଵ ՜ ݐZ ՜ ڮ ՜ ݐ௠ such that
ݐ௜ is included in q, 1 &lt;_ ݅ &lt;_ ݉;
ݏ(ݐ௞) ൐ ݏ(ݐ௟), 1 &lt;_ ݇ ൏ ݈ &lt;_ ݉.
</equation>
<bodyText confidence="0.9264434">
For example, the topic chain of “any cool clubs
in Berlin or Hamburg?” is “Hamburg ՜ Berlin ՜
cool club” because the specificities for ‘Hamburg’,
‘Berlin’, and ‘cool club’ are 0.99, 0.62, and 0.36.
Definition 4 (Question Tree) A question tree of
a question set Q = {q௜ሽ௜ୀଵ
ே is a prefix tree built
over the topic chains Qc = {q௜ cሽ௜ୀଵ
ே of the question
set Q. Clearly, if a question set contains only one
question, its question tree will be exactly same as
the topic chain of the question.
Note that the root node of a question tree is as-
sociated with empty string as the definition of pre-
fix tree requires (Fredkin, 1960).
</bodyText>
<figureCaption confidence="0.998">
Figure 2. An Example of a Question Tree
</figureCaption>
<bodyText confidence="0.9992115">
Given the topic chains with respect to the ques-
tions in Table 1 as follow,
</bodyText>
<listItem confidence="0.9624">
• Q1: Hamburg ՜ Berlin ՜ cool club
• Q2: Berlin ՜ fun club
• Q3: Hamburg ՜ Berlin ՜ nice hotel
• Q4: Hamburg ՜ Berlin ՜ how long does it take
• Q5: Berlin ՜ cheap hotel
we can have the question tree presented in Figure 2.
</listItem>
<subsectionHeader confidence="0.623803">
2.2.2 Determining the tree cut
</subsectionHeader>
<bodyText confidence="0.996911628571428">
According to the definition of a topic chain, the
topic terms in a topic chain of a question are or-
dered by their specificity values. Thus, a cut of a
topic chain naturally separates the topic terms of
low specificity (representing question focus) from
the topic terms of high specificity (representing
question topic). Given a topic chain of a question
consisting of ݉ topic terms, there exist (݉ െ 1)
possible cuts. The question is: which cut is the best?
We propose using the MDL-based tree cut mod-
el for the search of the best cut in a topic chain.
Instead of dealing with each topic chain individual-
ly, the proposed method handles a set of questions
together. Specifically, given a queried question, we
construct a question tree consisting of both the
queried question and the related questions, and
then apply the MDL principle to select the best cut
of the question tree. For example, in Figure 2, we
hope to get the cut indicated by the dashed line.
The topic terms on the left of the dashed line
represent the question topic and those on the right
of the dashed line represent the question focus.
Note that the tree cut yields a cut for each individ-
ual topic chain (each path) within the question tree
accordingly.
A cut of a topic chain qc of a question q sepa-
rates the topic chain in two parts: HEAD and TAIL.
HEAD (denoted as ܪ(qc)) is the subsequence of
the original topic chain qc before the cut. TAIL
(denoted as ܶ(qc)) is the subsequence of qc after
the cut. Thus, qc = ܪ(qc) ՜ ܶ(qc). For instance,
given the tree cut specified in Figure 2, for the top-
ic chain of Q1 “Hamburg ՜ Berlin ՜ cool club”,
the HEAD and TAIL are “Hamburg ՜ Berlin”
and “cool club” respectively.
</bodyText>
<subsectionHeader confidence="0.7128745">
2.3 Modeling question topic and question fo-
cus for search
</subsectionHeader>
<bodyText confidence="0.999708714285714">
We employ the framework of language modeling
(for information retrieval) to develop our approach
to question search.
In the language modeling approach to informa-
tion retrieval, the relevance of a targeted question
q෤ to a queried question q is given by the probabili-
ty ݌(q|q෤) of generating the queried question q
</bodyText>
<figure confidence="0.703648928571429">
Q1: Any cool clubs in Berlin or Hamburg?
cool club
nice hotel
how long does it take
cheap hotel
fun club
ROOT
Hamburg
Berlin
Berlin
Q2: What are the most/best fun clubs in Berlin?
Q3: Any nice hotels in Berlin or Hamburg?
Q4: How long does it take to Hamburg from Berlin?
Q5: Cheap hotels in Berlin?
</figure>
<page confidence="0.98811">
159
</page>
<bodyText confidence="0.999068714285714">
from the language model formed by the targeted
question ݍ෤. The targeted question ݍ෤ is from a col-
lection ܥ of questions.
Following the framework, we propose a mixture
model for modeling question structure (namely,
question topic and question focus) within the
process of searching questions:
</bodyText>
<equation confidence="0.999888">
݌ሺݍ|ݍ෤ሻ ൌ ߣ · ݌ሺܪሺݍሻ|ܪሺݍ෤ሻሻ (9)
൅ሺ1 െ ߣሻ · ݌ሺܶሺݍሻ|ܶሺݍ෤ሻሻ
</equation>
<bodyText confidence="0.999942272727273">
In the mixture model, it is assumed that the
process of generating question topics and the
process of generating question foci are independent
from each other.
In traditional language modeling, a single multi-
nomial model ݌ሺݐ|ݍ෤ሻ over terms is estimated for
each targeted question ݍ෤. In our case, two multi-
nomial models ݌൫ݐหܪሺݍ෤ሻ൯ and ݌൫ݐหܶሺݍ෤ሻ൯ need to
be estimated for each targeted question ݍ෤.
If unigram document language models are used,
the equation (9) can then be re-written as,
</bodyText>
<equation confidence="0.999711">
݌ሺݍ|ݍ෤ሻ ൌߣ · ∏ ݌൫ݐหܪሺݍ෤ሻ൯௖௢௨௡௧ሺ௤,௧ሻ ൅
௧אுሺ௤ሻ
ܿ݋ݑ݊ݐሺݍ,ݐሻ (10)
ሺ1 െ ߣሻ · ∏ݐאܶሺݍሻ ݌൫ݐห ܶ ሺݍ෥ሻ൯
</equation>
<bodyText confidence="0.9979342">
where ܿ݋ݑ݊ݐሺݍ, ݐሻ is the frequency of ݐ within ݍ.
To avoid zero probabilities and estimate more
accurate language models, the HEAD and TAIL of
questions are smoothed using background collec-
tion,
</bodyText>
<equation confidence="0.99994575">
݌൫ݐหܪሺݍ෤ሻ൯ ൌ ߙ · ݌̂൫ݐหܪሺݍ෤ሻ൯
൅ሺ1 െ ߙሻ · ݌̂ሺݐ|ܥሻ (11)
݌൫ݐหܶሺݍ෤ሻ൯ ൌ ߚ · ݌̂൫ݐหܶሺݍ෤ሻ൯
൅ሺ1 െ ߚሻ · ݌̂ሺݐ|ܥሻ (12)
</equation>
<bodyText confidence="0.999904666666667">
where ݌̂ሺݐ|ܪሺݍ෤ሻሻ, ݌̂ሺݐ|ܶሺݍ෤ሻሻ, and ݌̂ሺݐ|ܥሻ are the
MLE estimators with respect to the HEAD of ݍ෤,
the TAIL of ݍ෤, and the collection ܥ.
</bodyText>
<sectionHeader confidence="0.997007" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.99998925">
We have conducted experiments to verify the ef-
fectiveness of our approach to question search.
Particularly, we have investigated the use of identi-
fying question topic and question focus for search.
</bodyText>
<subsectionHeader confidence="0.99504">
3.1 Dataset and evaluation measures
</subsectionHeader>
<bodyText confidence="0.999953277777778">
We made use of the questions obtained from Ya-
hoo! Answers for the evaluation. More specifically,
we utilized the resolved questions under two of the
top-level categories at Yahoo! Answers, namely
‘travel’ and ‘computers &amp; internet’. The questions
include 314,616 items from the ‘travel’ category
and 210,785 items from the ‘computers &amp; internet’
category. Each resolved question consists of three
fields: ‘title’, ‘description’, and ‘answers’. For
search we use only the ‘title’ field. It is assumed
that the titles of the questions already provide
enough semantic information for understanding
users’ information needs.
We developed two test sets, one for the category
‘travel’ denoted as ‘TRL-TST’, and the other for
‘computers &amp; internet’ denoted as ‘CI-TST’. In
order to create the test sets, we randomly selected
200 questions for each category.
To obtain the ground-truth of question search,
we employed the Vector Space Model (VSM) (Sal-
ton et al., 1975) to retrieve the top 20 results and
obtained manual judgments. The top 20 results
don’t include the queried question itself. Given a
returned result by VSM, an assessor is asked to
label it with ‘relevant’ or ‘irrelevant’. If a returned
result is considered semantically equivalent (or
close) to the queried question, the assessor will
label it as ‘relevant’; otherwise, the assessor will
label it as ‘irrelevant’. Two assessors were in-
volved in the manual judgments. Each of them was
asked to label 100 questions from ‘TRL-TST’ and
100 from ‘CI-TST’. In the process of manually
judging questions, the assessors were presented
only the titles of the questions (for both the queried
questions and the returned questions). Table 2 pro-
vides the statistics on the final test set.
</bodyText>
<table confidence="0.99875">
# Queries # Returned # Relevant
TRL-TST 200 4,000 256
CI-TST 200 4,000 510
</table>
<tableCaption confidence="0.999907">
Table 2. Statistics on the Test Data
</tableCaption>
<bodyText confidence="0.999803285714286">
We utilized two baseline methods for demon-
strating the effectiveness of our approach, the
VSM and the LMIR (language modeling method
for information retrieval) (Ponte and Croft, 1998).
We made use of three measures for evaluating
the results of question search methods. They are
MAP, R-precision, and MRR.
</bodyText>
<subsectionHeader confidence="0.999899">
3.2 Searching questions about ‘travel’
</subsectionHeader>
<bodyText confidence="0.999803">
In the experiments, we made use of the questions
about ‘travel’ to test the performance of our ap-
proach to question search. More specifically, we
used the 200 queries in the test set ‘TRL-TST’ to
search for ‘relevant’ questions from the 314,616
</bodyText>
<page confidence="0.992361">
160
</page>
<bodyText confidence="0.999781785714286">
questions categorized as ‘travel’. Note that only the
questions occurring in the test set can be evaluated.
We made use of the taxonomy of questions pro-
vided at Yahoo! Answers for the calculation of
specificity of topic terms. The taxonomy is orga-
nized in a tree structure. In the following experi-
ments, we only utilized as the categories of
questions the leaf nodes of the taxonomy tree (re-
garding ‘travel’), which includes 355 categories.
We randomly divided the test queries into five
even subsets and conducted 5-fold cross-validation
experiments. In each trial, we tuned the parameters
A, a, and /3 in the equation (10)-(12) with four of
the five subsets and then applied it to one remain-
ing subset. The experimental results reported be-
low are those averaged over the five trials.
In Table 3, our approach denoted by LMIR-
CUT is implemented exactly as equation (10).
Neither VSM nor LMIR uses the data structure
composed of question topic and question focus.
From Table 3, we see that our approach outper-
forms the baseline approaches VSM and LMIR in
terms of all the measures. We conducted a signi-
ficance test (t-test) on the improvements of our
approach over VSM and LMIR. The result indi-
cates that the improvements are statistically signif-
icant (p-value &lt; 0.05) in terms of all the evaluation
measures.
</bodyText>
<figureCaption confidence="0.940147">
Figure 3. Balancing between Question Topic and Ques-
</figureCaption>
<bodyText confidence="0.984056774193548">
tion Focus
In equation (9), we use the parameter X to bal-
ance the contribution of question topic and the con-
tribution of question focus. Figure 3 illustrates how
influential the value of X is on the performance of
question search in terms of MRR. The result was
obtained with the 200 queries directly, instead of
5-fold cross-validation. From Figure 3, we see that
our approach performs best when X is around 0.7.
That is, our approach tends to emphasize question
topic more than question focus.
We also examined the correctness of question
topics and question foci of the 200 queried ques-
tions. The question topics and question foci were
obtained with the MDL-based tree cut model au-
tomatically. In the result, 69 questions have incor-
rect question topics or question foci. Further
analysis shows that the errors came from two cate-
gories: (a) 59 questions have only the HEAD parts
(that is, none of the topic terms fall within the
TAIL part), and (b) 10 have incorrect orders of
topic terms because the specificities of topic terms
were estimated inaccurately. For questions only
having the HEAD parts, our approach (equation (9))
reduces to traditional language modeling approach.
Thus, even when the errors of category (a) occur,
our approach can still work not worse than the tra-
ditional language modeling approach. This also
explains why our approach performs best when X is
around 0.7. The error category (a) pushes our mod-
el to emphasize more in question topic.
</bodyText>
<table confidence="0.9863251875">
Methods Results
How cold does it usually get in Charlotte,
NC during winters?
VSM How long and cold are the winters in
Rochester, NY?
How cold is it in Alaska?
How cold is it in Alaska?
How cold does it get really in Toronto in
LMIR the winter?
How cold does the Mojave Desert get in
the winter?
How cold is it in Alaska?
How cold is Alaska in March and out-
LMIR- door activities?
CUT How cold does it get in Nova Scotia in the
winter?
</table>
<tableCaption confidence="0.7449575">
Table 4. Search Results for
“How cold does it get in winters in Alaska?”
</tableCaption>
<bodyText confidence="0.988458166666667">
Table 4 provides the TOP-3 search results which
are given by VSM, LMIR, and LMIR-CUT (our
approach) respectively. The questions in bold are
labeled as ‘relevant’ in the evaluation set. The que-
ried question seeks for the ‘weather’ information
about ‘Alaska’. Both VSM and LMIR rank certain
</bodyText>
<figure confidence="0.989883142857143">
MRR 0.3
0.25
0.2
0.15
0.1
0.05
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</figure>
<table confidence="0.8768856">
λ
Methods MAP R-Precision MRR
VSM 0.198 0.138 0.228
LMIR 0.203 0.154 0.248
LMIR-CUT 0.236 0.192 0.279
</table>
<tableCaption confidence="0.996014">
Table 3. Searching Questions about ‘Travel’
</tableCaption>
<page confidence="0.99796">
161
</page>
<bodyText confidence="0.9999839">
‘irrelevant’ questions higher than ‘relevant’ ques-
tions. The ‘irrelevant’ questions are not about
‘Alaska’ although they are about ‘weather’. The
reason is that neither VSM nor PVSM is aware that
the query consists of the two aspects ‘weather’
(how cold, winter) and ‘Alaska’. In contrast, our
approach assures that both aspects are matched.
Note that the HEAD part of the topic chain of the
queried question given by our approach is “Alaska”
and the TAIL part is “winter -+ how cold”.
</bodyText>
<subsectionHeader confidence="0.8753575">
3.3 Searching questions about ‘computers &amp;
internet’
</subsectionHeader>
<bodyText confidence="0.999798142857143">
In the experiments, we made use of the questions
about ‘computers &amp; internet’ to test the perfor-
mance of our proposed approach to question search.
More specifically, we used the 200 queries in the
test set ‘CI-TST” to search for ‘relevant’ questions
from the 210,785 questions categorized as ‘com-
puters &amp; internet’. For the calculation of specificity
of topic terms, we utilized as the categories of
questions the leaf nodes of the taxonomy tree re-
garding ‘computers &amp; Internet’, which include 23
categories.
We conducted 5-fold cross-validation for the pa-
rameter tuning. The experimental results reported
in Table 5 are averaged over the five trials.
</bodyText>
<table confidence="0.9952925">
Methods MAP R-Precision MRR
VSM 0.236 0.175 0.289
LMIR 0.248 0.191 0.304
LMIR-CUT 0.279 0.230 0.341
</table>
<tableCaption confidence="0.8597665">
Table 5. Searching Questions about ‘Computers &amp; In-
ternet’
</tableCaption>
<bodyText confidence="0.9993445">
Again, we see that our approach outperforms the
baseline approaches VSM and LMIR in terms of
all the measures. We conducted a significance test
(t-test) on the improvements of our approach over
VSM and LMIR. The result indicates that the im-
provements are statistically significant (p-value &lt;
0.05) in terms of all the evaluation measures.
We also conducted the experiment similar to
that in Figure 3. Figure 4 provides the result. The
trend is consistent with that in Figure 3.
We examined the correctness of (automatically
identified) question topics and question foci of the
200 queried questions, too. In the result, 65 ques-
tions have incorrect question topics or question
foci. Among them, 47 fall in the error category (a)
and 18 in the error category (b). The distribution of
errors is also similar to that in Section 3.2, which
also justifies the trend presented in Figure 4.
</bodyText>
<figure confidence="0.9909625">
MRR 0.4
0.35
0.3
0.25
0.2
0.15
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
λ
</figure>
<figureCaption confidence="0.985124">
Figure 4. Balancing between Question Topic and Ques-
</figureCaption>
<bodyText confidence="0.53217">
tion Focus
</bodyText>
<sectionHeader confidence="0.988625" genericHeader="method">
4 Using Translation Probability
</sectionHeader>
<bodyText confidence="0.999960761904762">
In the setting of question search, besides the topic
what we address in the previous sections, another
research topic is to fix lexical chasm between ques-
tions.
Sometimes, two questions that have the same
meaning use very different wording. For example,
the questions “where to stay in Hamburg?” and
“the best hotel in Hamburg?” have almost the same
meaning but are lexically different in question fo-
cus (where to stay vs. best hotel). This is the so-
called ‘lexical chasm’.
Jeon and Bruce (2007) proposed a mixture mod-
el for fixing the lexical chasm between questions.
The model is a combination of the language mod-
eling approach (for information retrieval) and
translation-based approach (for information re-
trieval). Our idea of modeling question structure
for search can naturally extend to Jeon et al.’s
model. More specifically, by using translation
probabilities, we can rewrite equation (11) and (12)
as follow:
</bodyText>
<equation confidence="0.999978833333333">
p(tJH(q)) = al &apos; p�(tJH(q))
+a2 &apos; L&apos;Ex(q)Tr(t|t&apos;) &apos; p�(t&apos;JH(q)) (13)
+(1 — al — a2) &apos; p�(t|C)
p(tJT(q)) = Q, &apos; p�(tJT(q))
+Q2 &apos; ENeT(q) Tr(t|t&apos;) &apos; p�(t&apos;JT(q)) (14)
+(1 — Q1 — Q2) &apos; p�(t|C)
</equation>
<bodyText confidence="0.999701">
where Tr(t|t&apos;) denotes the probability that topic
term t is the translation of t&apos;. In our experiments,
to estimate the probability Tr(t|t&apos;), we used the
collections of question titles and question descrip-
tions as the parallel corpus and the IBM model 1
(Brown et al., 1993) as the alignment model.
</bodyText>
<page confidence="0.993091">
162
</page>
<bodyText confidence="0.997479416666667">
Usually, users reiterate or paraphrase their ques-
tions (already described in question titles) in ques-
tion descriptions.
We utilized the new model elaborated by equa-
tion (13) and (14) for searching questions about
‘travel’ and ‘computers &amp; internet’. The new mod-
el is denoted as ‘SMT-CUT’. Table 6 provides the
evaluation results. The evaluation was conducted
with exactly the same setting as in Section 3. From
Table 6, we see that the performance of our ap-
proach can be further boosted by using translation
probability.
</bodyText>
<table confidence="0.999639428571429">
Data Methods MAP R-Precision MRR
TRL- LMIR-CUT 0.236 0.192 0.279
TST
SMT-CUT 0.266 0.225 0.308
CI- LMIR-CUT 0.279 0.230 0.341
TST
SMT-CUT 0.282 0.236 0.337
</table>
<tableCaption confidence="0.92451">
Table 6. Using Translation Probability
</tableCaption>
<sectionHeader confidence="0.999944" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999968816326531">
The major focus of previous research efforts on
question search is to tackle the lexical chasm prob-
lem between questions.
The research of question search is first con-
ducted using FAQ data. FAQ Finder (Burke et al.,
1997) heuristically combines statistical similarities
and semantic similarities between questions to rank
FAQs. Conventional vector space models are used
to calculate the statistical similarity and WordNet
(Fellbaum, 1998) is used to estimate the semantic
similarity. Sneiders (2002) proposed template
based FAQ retrieval systems. Lai et al. (2002) pro-
posed an approach to automatically mine FAQs
from the Web. Jijkoun and Rijke (2005) used su-
pervised learning methods to extend heuristic ex-
traction of Q/A pairs from FAQ pages, and treated
Q/A pair retrieval as a fielded search task.
Harabagiu et al. (2005) used a Question Answer
Database (known as QUAB) to support interactive
question answering. They compared seven differ-
ent similarity metrics for selecting related ques-
tions from QUAB and found that the concept-
based metric performed best.
Recently, the research of question search has
been further extended to the community-based
Q&amp;A data. For example, Jeon et al. (Jeon et al.,
2005a; Jeon et al., 2005b) compared four different
retrieval methods, i.e. vector space model, Okapi,
language model (LM), and translation-based model,
for automatically fixing the lexical chasm between
questions of question search. They found that the
translation-based model performed best.
However, all the existing methods treat ques-
tions just as plain texts (without considering ques-
tion structure). In this paper, we proposed to
conduct question search by identifying question
topic and question focus. To the best of our know-
ledge, none of the existing studies addressed ques-
tion search by modeling both question topic and
question focus.
Question answering (e.g., Pasca and Harabagiu,
2001; Echihabi and Marcu, 2003; Voorhees, 2004;
Metzler and Croft, 2005) relates to question search.
Question answering automatically extracts short
answers for a relatively limited class of question
types from document collections. In contrast to that,
question search retrieves answers for an unlimited
range of questions by focusing on finding semanti-
cally similar questions in an archive.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999977333333333">
In this paper, we have proposed an approach to
question search which models question topic and
question focus in a language modeling framework.
The contribution of this paper can be summa-
rized in 4-fold: (1) A data structure consisting of
question topic and question focus was proposed for
summarizing questions; (2) The MDL-based tree
cut model was employed to identify question topic
and question focus automatically; (3) A new form
of language modeling using question topic and
question focus was developed for question search;
(4) Extensive experiments have been conducted to
evaluate the proposed approach using a large col-
lection of real questions obtained from Yahoo! An-
swers.
Though we only utilize data from community-
based question answering service in our experi-
ments, we could also use categorized questions
from forum sites and FAQ sites. Thus, as future
work, we will try to investigate the use of the pro-
posed approach for other kinds of web services.
</bodyText>
<sectionHeader confidence="0.985223" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99828575">
We would like to thank Xinying Song, Shasha Li,
and Shilin Ding for their efforts on developing the
evaluation data. We would also like to thank Ste-
phan H. Stiller for his proof-reading of the paper.
</bodyText>
<page confidence="0.998676">
163
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902377049181">
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. In Proc. of ACL’03.
C. Fellbaum. 1998. WordNet: An electronic lexical da-
tabase. MIT Press.
D. Metzler and W. B. Croft. 2005. Analysis of statistical
question classification for fact-based questions. In-
formation Retrieval, 8(3), pages 481-504.
E. Fredkin. 1960. Trie memory. Communications of the
ACM, D. 3(9):490-499.
E. M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Proc. of TREC’04.
E. Sneiders. 2002. Automated question answering using
question templates that cover the conceptual model
of the database. In Proc. of the 6th International
Conference on Applications of Natural Language to
Information Systems, pages 235-239.
G. Salton, A. Wong, and C. S. Yang 1975. A vector
space model for automatic indexing. Communica-
tions of the ACM, vol. 18, nr. 11, pages 613-620.
H. Li and N. Abe. 1998. Generalizing case frames us-
ing a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2), pages 217-244.
J. Jeon and W.B. Croft. 2007. Learning translation-
based language models using Q&amp;A archives. Tech-
nical report, University of Massachusetts.
J. Jeon, W. B. Croft, and J. Lee. 2005a. Finding seman-
tically similar questions based on their answers. In
Proc. of SIGIR’05.
J. Jeon, W. B. Croft, and J. Lee. 2005b. Finding similar
questions in large question and answer archives. In
Proc. of CIKM ‘05, pages 84-90.
J. Rissanen. 1978. Modeling by shortest data description.
Automatica, vol. 14, pages. 465-471
J.M. Ponte, W.B. Croft. 1998. A language modeling
approach to information retrieval. In Proc. of
SIGIR’98.
M. A. Pasca and S. M. Harabagiu. 2001. High perfor-
mance question/answering. In Proc. of SIGIR’01,
pages 366-374.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational
Linguistics, 19(2):263-311.
R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L.
Lytinen, N. Tomuro, and S. Schoenberg. 1997. Ques-
tion answering from frequently asked question files:
Experiences with the FAQ finder system. Technical
report, University of Chicago.
S. Harabagiu, A. Hickl, J. Lehmann and D. Moldovan.
2005. Experiments with Interactive Question-
Answering. In Proc. of ACL’05.
V. Jijkoun, M. D. Rijke. 2005. Retrieving Answers from
Frequently Asked Questions Pages on the Web. In
Proc. of CIKM’05.
Y. Cao and H. Li. 2002. Base noun phrase translation
using web data and the EM algorithm. In Proc. of
COLING’02.
Y.-S. Lai, K.-A. Fung, and C.-H. Wu. 2002. Faq mining
via list detection. In Proc. of the Workshop on Multi-
lingual Summarization and Question Answering,
2002.
</reference>
<page confidence="0.998513">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.280524">
<title confidence="0.999949">Searching Questions by Identifying Question Topic and Question Focus</title>
<author confidence="0.946459">Yunbo Chin-Yew</author>
<author confidence="0.946459">Yong</author>
<affiliation confidence="0.998234">Jiao Tong University,</affiliation>
<address confidence="0.999725">Shanghai, China, 200240</address>
<email confidence="0.945034">summer@apex.sjtu.edu.cn</email>
<email confidence="0.945034">yyu@apex.sjtu.edu.cn</email>
<abstract confidence="0.9654235">This paper is concerned with the problem of question search. In question search, given a question as query, we are to return questions semantically equivalent or close to the queried question. In this paper, we propose to conduct question search by identifying question topic and question focus. More specifically, we first summarize questions in a data structure consisting of question topic and question focus. Then we model question topic and question focus in a language modeling framework for search. We also propose to use the MDLbased tree cut model for identifying question topic and question focus automatically. Experimental results indicate that our approach of identifying question topic and question focus for search significantly outperforms the baseline methods such as Vector Space Model (VSM) and Language Model for Information</abstract>
<intro confidence="0.479906">Retrieval (LMIR).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A Noisy-Channel Approach to Question Answering.</title>
<date>2003</date>
<booktitle>In Proc. of ACL’03.</booktitle>
<contexts>
<context position="30751" citStr="Echihabi and Marcu, 2003" startWordPosition="5198" endWordPosition="5201">i, language model (LM), and translation-based model, for automatically fixing the lexical chasm between questions of question search. They found that the translation-based model performed best. However, all the existing methods treat questions just as plain texts (without considering question structure). In this paper, we proposed to conduct question search by identifying question topic and question focus. To the best of our knowledge, none of the existing studies addressed question search by modeling both question topic and question focus. Question answering (e.g., Pasca and Harabagiu, 2001; Echihabi and Marcu, 2003; Voorhees, 2004; Metzler and Croft, 2005) relates to question search. Question answering automatically extracts short answers for a relatively limited class of question types from document collections. In contrast to that, question search retrieves answers for an unlimited range of questions by focusing on finding semantically similar questions in an archive. 6 Conclusions and Future Work In this paper, we have proposed an approach to question search which models question topic and question focus in a language modeling framework. The contribution of this paper can be summarized in 4-fold: (1)</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>A. Echihabi and D. Marcu. 2003. A Noisy-Channel Approach to Question Answering. In Proc. of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="29263" citStr="Fellbaum, 1998" startWordPosition="4969" endWordPosition="4970"> TRL- LMIR-CUT 0.236 0.192 0.279 TST SMT-CUT 0.266 0.225 0.308 CI- LMIR-CUT 0.279 0.230 0.341 TST SMT-CUT 0.282 0.236 0.337 Table 6. Using Translation Probability 5 Related Work The major focus of previous research efforts on question search is to tackle the lexical chasm problem between questions. The research of question search is first conducted using FAQ data. FAQ Finder (Burke et al., 1997) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs. Conventional vector space models are used to calculate the statistical similarity and WordNet (Fellbaum, 1998) is used to estimate the semantic similarity. Sneiders (2002) proposed template based FAQ retrieval systems. Lai et al. (2002) proposed an approach to automatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbase</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>W B Croft</author>
</authors>
<title>Analysis of statistical question classification for fact-based questions.</title>
<date>2005</date>
<journal>Information Retrieval,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>481--504</pages>
<contexts>
<context position="30793" citStr="Metzler and Croft, 2005" startWordPosition="5204" endWordPosition="5207">sed model, for automatically fixing the lexical chasm between questions of question search. They found that the translation-based model performed best. However, all the existing methods treat questions just as plain texts (without considering question structure). In this paper, we proposed to conduct question search by identifying question topic and question focus. To the best of our knowledge, none of the existing studies addressed question search by modeling both question topic and question focus. Question answering (e.g., Pasca and Harabagiu, 2001; Echihabi and Marcu, 2003; Voorhees, 2004; Metzler and Croft, 2005) relates to question search. Question answering automatically extracts short answers for a relatively limited class of question types from document collections. In contrast to that, question search retrieves answers for an unlimited range of questions by focusing on finding semantically similar questions in an archive. 6 Conclusions and Future Work In this paper, we have proposed an approach to question search which models question topic and question focus in a language modeling framework. The contribution of this paper can be summarized in 4-fold: (1) A data structure consisting of question t</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>D. Metzler and W. B. Croft. 2005. Analysis of statistical question classification for fact-based questions. Information Retrieval, 8(3), pages 481-504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fredkin</author>
</authors>
<title>Trie memory.</title>
<date>1960</date>
<journal>Communications of the ACM, D.</journal>
<pages>3--9</pages>
<contexts>
<context position="13642" citStr="Fredkin, 1960" startWordPosition="2316" endWordPosition="2317">ple, the topic chain of “any cool clubs in Berlin or Hamburg?” is “Hamburg ՜ Berlin ՜ cool club” because the specificities for ‘Hamburg’, ‘Berlin’, and ‘cool club’ are 0.99, 0.62, and 0.36. Definition 4 (Question Tree) A question tree of a question set Q = {qሽୀଵ ே is a prefix tree built over the topic chains Qc = {q cሽୀଵ ே of the question set Q. Clearly, if a question set contains only one question, its question tree will be exactly same as the topic chain of the question. Note that the root node of a question tree is associated with empty string as the definition of prefix tree requires (Fredkin, 1960). Figure 2. An Example of a Question Tree Given the topic chains with respect to the questions in Table 1 as follow, • Q1: Hamburg ՜ Berlin ՜ cool club • Q2: Berlin ՜ fun club • Q3: Hamburg ՜ Berlin ՜ nice hotel • Q4: Hamburg ՜ Berlin ՜ how long does it take • Q5: Berlin ՜ cheap hotel we can have the question tree presented in Figure 2. 2.2.2 Determining the tree cut According to the definition of a topic chain, the topic terms in a topic chain of a question are ordered by their specificity values. Thus, a cut of a topic chain naturally separates the topic terms of low specificity (representin</context>
</contexts>
<marker>Fredkin, 1960</marker>
<rawString>E. Fredkin. 1960. Trie memory. Communications of the ACM, D. 3(9):490-499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2004</date>
<booktitle>In Proc. of TREC’04.</booktitle>
<contexts>
<context position="30767" citStr="Voorhees, 2004" startWordPosition="5202" endWordPosition="5203">d translation-based model, for automatically fixing the lexical chasm between questions of question search. They found that the translation-based model performed best. However, all the existing methods treat questions just as plain texts (without considering question structure). In this paper, we proposed to conduct question search by identifying question topic and question focus. To the best of our knowledge, none of the existing studies addressed question search by modeling both question topic and question focus. Question answering (e.g., Pasca and Harabagiu, 2001; Echihabi and Marcu, 2003; Voorhees, 2004; Metzler and Croft, 2005) relates to question search. Question answering automatically extracts short answers for a relatively limited class of question types from document collections. In contrast to that, question search retrieves answers for an unlimited range of questions by focusing on finding semantically similar questions in an archive. 6 Conclusions and Future Work In this paper, we have proposed an approach to question search which models question topic and question focus in a language modeling framework. The contribution of this paper can be summarized in 4-fold: (1) A data structur</context>
</contexts>
<marker>Voorhees, 2004</marker>
<rawString>E. M. Voorhees. 2004. Overview of the TREC 2004 question answering track. In Proc. of TREC’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sneiders</author>
</authors>
<title>Automated question answering using question templates that cover the conceptual model of the database.</title>
<date>2002</date>
<booktitle>In Proc. of the 6th International Conference on Applications of Natural Language to Information Systems,</booktitle>
<pages>235--239</pages>
<contexts>
<context position="29324" citStr="Sneiders (2002)" startWordPosition="4978" endWordPosition="4979">8 CI- LMIR-CUT 0.279 0.230 0.341 TST SMT-CUT 0.282 0.236 0.337 Table 6. Using Translation Probability 5 Related Work The major focus of previous research efforts on question search is to tackle the lexical chasm problem between questions. The research of question search is first conducted using FAQ data. FAQ Finder (Burke et al., 1997) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs. Conventional vector space models are used to calculate the statistical similarity and WordNet (Fellbaum, 1998) is used to estimate the semantic similarity. Sneiders (2002) proposed template based FAQ retrieval systems. Lai et al. (2002) proposed an approach to automatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best. Recently, the research of question s</context>
</contexts>
<marker>Sneiders, 2002</marker>
<rawString>E. Sneiders. 2002. Automated question answering using question templates that cover the conceptual model of the database. In Proc. of the 6th International Conference on Applications of Natural Language to Information Systems, pages 235-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<pages>613--620</pages>
<contexts>
<context position="18959" citStr="Salton et al., 1975" startWordPosition="3236" endWordPosition="3240">rnet’ category. Each resolved question consists of three fields: ‘title’, ‘description’, and ‘answers’. For search we use only the ‘title’ field. It is assumed that the titles of the questions already provide enough semantic information for understanding users’ information needs. We developed two test sets, one for the category ‘travel’ denoted as ‘TRL-TST’, and the other for ‘computers &amp; internet’ denoted as ‘CI-TST’. In order to create the test sets, we randomly selected 200 questions for each category. To obtain the ground-truth of question search, we employed the Vector Space Model (VSM) (Salton et al., 1975) to retrieve the top 20 results and obtained manual judgments. The top 20 results don’t include the queried question itself. Given a returned result by VSM, an assessor is asked to label it with ‘relevant’ or ‘irrelevant’. If a returned result is considered semantically equivalent (or close) to the queried question, the assessor will label it as ‘relevant’; otherwise, the assessor will label it as ‘irrelevant’. Two assessors were involved in the manual judgments. Each of them was asked to label 100 questions from ‘TRL-TST’ and 100 from ‘CI-TST’. In the process of manually judging questions, th</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang 1975. A vector space model for automatic indexing. Communications of the ACM, vol. 18, nr. 11, pages 613-620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDL principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<pages>217--244</pages>
<contexts>
<context position="6595" citStr="Li and Abe, 1998" startWordPosition="1042" endWordPosition="1045">Search Our approach to question search consists of two steps: (a) summarize questions in a data structure consisting of question topic and question focus; (b) model question topic and question focus in a language modeling framework for search. In the step (a), we employ the MDL-based (Minimum Description Length) tree cut model for automatically identifying question topic and question focus. Thus, this section will begin with a brief review of the MDL-based tree cut model and then follow that by an explanation of steps (a) and (b). 2.1 The MDL-based tree cut model Formally, a tree cut model ܯ (Li and Abe, 1998) can be represented by a pair consisting of a tree cut ߁, and a probability parameter vector ߠ of the same length, that is, ܯ ൌ ሺ߁, ߠሻ (1) where ߁ and ߠ are ߁ ൌ ሾܥଵ, ܥଶ,.. ܥሿ, (2) ߠ ൌ ሾሺܥଵሻ, ሺܥଶሻ, ..., ሺܥሻሿ where ܥଵ, ܥଶ, ... ܥ are classes determined by a cut in the tree and ∑ ሺܥሻ ൌ 1  . A ‘cut’ in a tree is ୀଵ any set of nodes in the tree that defines a partition of all the nodes, viewing each node as representing the set of child nodes as well as itself. For example, the cut indicated by the dash line in Figure 1 corresponds to three classes:ሾ݊, ݊ଵଵሿ,ሾ݊ଵଷ, ݊ଶସሿ, and ሾ݊ଵଶ, ݊ଶଵ, ݊ଶଶ,</context>
<context position="7941" citStr="Li and Abe, 1998" startWordPosition="1302" endWordPosition="1305"> tree is to collapse the nodes of less frequency into their parent nodes. However, the method is too heuristic for it relies much on manually tuned frequency threshold. In our practice, we turn to use a theoretically well-motivated method based on the MDL principle. MDL is a principle of data compression and statistical estimation from information theory (Rissanen, 1978). Given a sample ܵ and a tree cut ߁, we employ MLE to estimate the parameters of the corresponding tree cut model ܯ ൌ ሺ߁, ߠሻ, where ߠ denotes the estimated parameters. According to the MDL principle, the description length (Li and Abe, 1998) ܮሺܯ,ܵሻ of the tree cut model ܯ and the sample ܵ is the sum of the model 157 description length L(r), the parameter description length L(B�|r) , and the data description length L(S|F, B�), i.e. L(M, S) = L(r) + L(B�Jr) + L(S|r, B�) (3) The model description length L(r) is a subjective quantity which depends on the coding scheme employed. Here, we simply assume that each tree cut model is equally likely a priori. The parameter description length L(B�|r) is calculated as L(B�1r) = Z x log |S |(4) where |S |denotes the sample size and k denotes the number of free parameters in the tree cut mode</context>
<context position="9226" citStr="Li and Abe, 1998" startWordPosition="1544" endWordPosition="1547">ption length L(S|F, B) is calculated as L(Slr, B�) = — ZneS lo9p�(n) (5) where i f(c) (6) P(n) = |0x |�| where C is the class that n belongs to and f(C) denotes the total frequency of instances in class C in the sample S. With the description length defined as (3), we wish to select a tree cut model with the minimum description length and output it as the result. Note that the model description length L(r) can be ignored because it is the same for all tree cut models. The MDL-based tree cut model was originally introduced for handling the problem of generalizing case frames using a thesaurus (Li and Abe, 1998). To the best of our knowledge, no existing work utilizes it for question search. This may be partially because of the unavailability of the resources (e.g., thesaurus) which can be used for embodying the questions in a tree structure. In Section 2.2, we will introduce a tree structure called question tree for representing questions. 2.2 Identifying question topic and question focus In principle, it is possible to identify question topic and question focus of a question by only parsing the question itself (for example, utilizing a syntactic parser). However, such a method requires accurate par</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>H. Li and N. Abe. 1998. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics, 24(2), pages 217-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
</authors>
<title>Learning translationbased language models using Q&amp;A archives.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>University of Massachusetts.</institution>
<marker>Jeon, Croft, 2007</marker>
<rawString>J. Jeon and W.B. Croft. 2007. Learning translationbased language models using Q&amp;A archives. Technical report, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J Lee</author>
</authors>
<title>Finding semantically similar questions based on their answers.</title>
<date>2005</date>
<booktitle>In Proc. of SIGIR’05.</booktitle>
<contexts>
<context position="2751" citStr="Jeon et al., 2005" startWordPosition="424" endWordPosition="427">lent or close to the queried question. Query: Q1: Any cool clubs in Berlin or Hamburg? Expected: Q2: What are the best/most fun clubs in Berlin? Not Expected: Q3: Any nice hotels in Berlin or Hamburg? Q4: How long does it take to Hamburg from Berlin? Q5: Cheap hotels in Berlin? Table 1. An Example on Question Search Many methods have been investigated for tackling the problem of question search. For example, Jeon et al. have compared the uses of four different retrieval methods, i.e. vector space model, Okapi, language model, and translation-based model, within the setting of question search (Jeon et al., 2005b). However, all the existing methods treat questions just as plain texts (without considering question structure). For example, obviously, Q2 can be considered semantically closer to Q1 than Q3-Q5 although all questions (Q2-Q5) are related to Q1. The existing methods are not able to tell the difference between question Q2 and questions Q3, Q4, and Q5 in terms of their relevance to question Q1. We will clarify this in the following. In this paper, we propose to conduct question search by identifying question topic and question focus. The question topic usually represents the major context/cons</context>
<context position="30032" citStr="Jeon et al., 2005" startWordPosition="5091" endWordPosition="5094">utomatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best. Recently, the research of question search has been further extended to the community-based Q&amp;A data. For example, Jeon et al. (Jeon et al., 2005a; Jeon et al., 2005b) compared four different retrieval methods, i.e. vector space model, Okapi, language model (LM), and translation-based model, for automatically fixing the lexical chasm between questions of question search. They found that the translation-based model performed best. However, all the existing methods treat questions just as plain texts (without considering question structure). In this paper, we proposed to conduct question search by identifying question topic and question focus. To the best of our knowledge, none of the existing studies addressed question search by modelin</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>J. Jeon, W. B. Croft, and J. Lee. 2005a. Finding semantically similar questions based on their answers. In Proc. of SIGIR’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>W B Croft</author>
<author>J Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In Proc. of CIKM ‘05,</booktitle>
<pages>84--90</pages>
<contexts>
<context position="2751" citStr="Jeon et al., 2005" startWordPosition="424" endWordPosition="427">lent or close to the queried question. Query: Q1: Any cool clubs in Berlin or Hamburg? Expected: Q2: What are the best/most fun clubs in Berlin? Not Expected: Q3: Any nice hotels in Berlin or Hamburg? Q4: How long does it take to Hamburg from Berlin? Q5: Cheap hotels in Berlin? Table 1. An Example on Question Search Many methods have been investigated for tackling the problem of question search. For example, Jeon et al. have compared the uses of four different retrieval methods, i.e. vector space model, Okapi, language model, and translation-based model, within the setting of question search (Jeon et al., 2005b). However, all the existing methods treat questions just as plain texts (without considering question structure). For example, obviously, Q2 can be considered semantically closer to Q1 than Q3-Q5 although all questions (Q2-Q5) are related to Q1. The existing methods are not able to tell the difference between question Q2 and questions Q3, Q4, and Q5 in terms of their relevance to question Q1. We will clarify this in the following. In this paper, we propose to conduct question search by identifying question topic and question focus. The question topic usually represents the major context/cons</context>
<context position="30032" citStr="Jeon et al., 2005" startWordPosition="5091" endWordPosition="5094">utomatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best. Recently, the research of question search has been further extended to the community-based Q&amp;A data. For example, Jeon et al. (Jeon et al., 2005a; Jeon et al., 2005b) compared four different retrieval methods, i.e. vector space model, Okapi, language model (LM), and translation-based model, for automatically fixing the lexical chasm between questions of question search. They found that the translation-based model performed best. However, all the existing methods treat questions just as plain texts (without considering question structure). In this paper, we proposed to conduct question search by identifying question topic and question focus. To the best of our knowledge, none of the existing studies addressed question search by modelin</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>J. Jeon, W. B. Croft, and J. Lee. 2005b. Finding similar questions in large question and answer archives. In Proc. of CIKM ‘05, pages 84-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<volume>14</volume>
<pages>465--471</pages>
<contexts>
<context position="7697" citStr="Rissanen, 1978" startWordPosition="1259" endWordPosition="1260">cut indicated by the dash line in Figure 1 corresponds to three classes:ሾ݊, ݊ଵଵሿ,ሾ݊ଵଷ, ݊ଶସሿ, and ሾ݊ଵଶ, ݊ଶଵ, ݊ଶଶ, ݊ଶଷሿ. ݊ ݊ଵଵ ݊ଵଶ ݊ଵଷ ݊ଶଵ ݊ଶଶ ݊ଶଷ ݊ଶସ Figure 1. An Example on the Tree Cut Model A straightforward way for determining a cut of a tree is to collapse the nodes of less frequency into their parent nodes. However, the method is too heuristic for it relies much on manually tuned frequency threshold. In our practice, we turn to use a theoretically well-motivated method based on the MDL principle. MDL is a principle of data compression and statistical estimation from information theory (Rissanen, 1978). Given a sample ܵ and a tree cut ߁, we employ MLE to estimate the parameters of the corresponding tree cut model ܯ ൌ ሺ߁, ߠሻ, where ߠ denotes the estimated parameters. According to the MDL principle, the description length (Li and Abe, 1998) ܮሺܯ,ܵሻ of the tree cut model ܯ and the sample ܵ is the sum of the model 157 description length L(r), the parameter description length L(B�|r) , and the data description length L(S|F, B�), i.e. L(M, S) = L(r) + L(B�Jr) + L(S|r, B�) (3) The model description length L(r) is a subjective quantity which depends on the coding scheme employed. Here, we simpl</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>J. Rissanen. 1978. Modeling by shortest data description. Automatica, vol. 14, pages. 465-471</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proc. of SIGIR’98.</booktitle>
<contexts>
<context position="20030" citStr="Ponte and Croft, 1998" startWordPosition="3413" endWordPosition="3416"> manual judgments. Each of them was asked to label 100 questions from ‘TRL-TST’ and 100 from ‘CI-TST’. In the process of manually judging questions, the assessors were presented only the titles of the questions (for both the queried questions and the returned questions). Table 2 provides the statistics on the final test set. # Queries # Returned # Relevant TRL-TST 200 4,000 256 CI-TST 200 4,000 510 Table 2. Statistics on the Test Data We utilized two baseline methods for demonstrating the effectiveness of our approach, the VSM and the LMIR (language modeling method for information retrieval) (Ponte and Croft, 1998). We made use of three measures for evaluating the results of question search methods. They are MAP, R-precision, and MRR. 3.2 Searching questions about ‘travel’ In the experiments, we made use of the questions about ‘travel’ to test the performance of our approach to question search. More specifically, we used the 200 queries in the test set ‘TRL-TST’ to search for ‘relevant’ questions from the 314,616 160 questions categorized as ‘travel’. Note that only the questions occurring in the test set can be evaluated. We made use of the taxonomy of questions provided at Yahoo! Answers for the calcu</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>J.M. Ponte, W.B. Croft. 1998. A language modeling approach to information retrieval. In Proc. of SIGIR’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Pasca</author>
<author>S M Harabagiu</author>
</authors>
<title>High performance question/answering.</title>
<date>2001</date>
<booktitle>In Proc. of SIGIR’01,</booktitle>
<pages>366--374</pages>
<contexts>
<context position="30725" citStr="Pasca and Harabagiu, 2001" startWordPosition="5194" endWordPosition="5197">e. vector space model, Okapi, language model (LM), and translation-based model, for automatically fixing the lexical chasm between questions of question search. They found that the translation-based model performed best. However, all the existing methods treat questions just as plain texts (without considering question structure). In this paper, we proposed to conduct question search by identifying question topic and question focus. To the best of our knowledge, none of the existing studies addressed question search by modeling both question topic and question focus. Question answering (e.g., Pasca and Harabagiu, 2001; Echihabi and Marcu, 2003; Voorhees, 2004; Metzler and Croft, 2005) relates to question search. Question answering automatically extracts short answers for a relatively limited class of question types from document collections. In contrast to that, question search retrieves answers for an unlimited range of questions by focusing on finding semantically similar questions in an archive. 6 Conclusions and Future Work In this paper, we have proposed an approach to question search which models question topic and question focus in a language modeling framework. The contribution of this paper can be</context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>M. A. Pasca and S. M. Harabagiu. 2001. High performance question/answering. In Proc. of SIGIR’01, pages 366-374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="28066" citStr="Brown et al., 1993" startWordPosition="4779" endWordPosition="4782"> search can naturally extend to Jeon et al.’s model. More specifically, by using translation probabilities, we can rewrite equation (11) and (12) as follow: p(tJH(q)) = al &apos; p�(tJH(q)) +a2 &apos; L&apos;Ex(q)Tr(t|t&apos;) &apos; p�(t&apos;JH(q)) (13) +(1 — al — a2) &apos; p�(t|C) p(tJT(q)) = Q, &apos; p�(tJT(q)) +Q2 &apos; ENeT(q) Tr(t|t&apos;) &apos; p�(t&apos;JT(q)) (14) +(1 — Q1 — Q2) &apos; p�(t|C) where Tr(t|t&apos;) denotes the probability that topic term t is the translation of t&apos;. In our experiments, to estimate the probability Tr(t|t&apos;), we used the collections of question titles and question descriptions as the parallel corpus and the IBM model 1 (Brown et al., 1993) as the alignment model. 162 Usually, users reiterate or paraphrase their questions (already described in question titles) in question descriptions. We utilized the new model elaborated by equation (13) and (14) for searching questions about ‘travel’ and ‘computers &amp; internet’. The new model is denoted as ‘SMT-CUT’. Table 6 provides the evaluation results. The evaluation was conducted with exactly the same setting as in Section 3. From Table 6, we see that the performance of our approach can be further boosted by using translation probability. Data Methods MAP R-Precision MRR TRL- LMIR-CUT 0.2</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Burke</author>
<author>K J Hammond</author>
<author>V A Kulyukin</author>
<author>S L Lytinen</author>
<author>N Tomuro</author>
<author>S Schoenberg</author>
</authors>
<title>Question answering from frequently asked question files: Experiences with the FAQ finder system.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>University of Chicago.</institution>
<contexts>
<context position="29046" citStr="Burke et al., 1997" startWordPosition="4940" endWordPosition="4943">e evaluation was conducted with exactly the same setting as in Section 3. From Table 6, we see that the performance of our approach can be further boosted by using translation probability. Data Methods MAP R-Precision MRR TRL- LMIR-CUT 0.236 0.192 0.279 TST SMT-CUT 0.266 0.225 0.308 CI- LMIR-CUT 0.279 0.230 0.341 TST SMT-CUT 0.282 0.236 0.337 Table 6. Using Translation Probability 5 Related Work The major focus of previous research efforts on question search is to tackle the lexical chasm problem between questions. The research of question search is first conducted using FAQ data. FAQ Finder (Burke et al., 1997) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs. Conventional vector space models are used to calculate the statistical similarity and WordNet (Fellbaum, 1998) is used to estimate the semantic similarity. Sneiders (2002) proposed template based FAQ retrieval systems. Lai et al. (2002) proposed an approach to automatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2</context>
</contexts>
<marker>Burke, Hammond, Kulyukin, Lytinen, Tomuro, Schoenberg, 1997</marker>
<rawString>R. D. Burke, K. J. Hammond, V. A. Kulyukin, S. L. Lytinen, N. Tomuro, and S. Schoenberg. 1997. Question answering from frequently asked question files: Experiences with the FAQ finder system. Technical report, University of Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
<author>J Lehmann</author>
<author>D Moldovan</author>
</authors>
<title>Experiments with Interactive QuestionAnswering.</title>
<date>2005</date>
<booktitle>In Proc. of ACL’05.</booktitle>
<contexts>
<context position="29650" citStr="Harabagiu et al. (2005)" startWordPosition="5031" endWordPosition="5034">Burke et al., 1997) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs. Conventional vector space models are used to calculate the statistical similarity and WordNet (Fellbaum, 1998) is used to estimate the semantic similarity. Sneiders (2002) proposed template based FAQ retrieval systems. Lai et al. (2002) proposed an approach to automatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best. Recently, the research of question search has been further extended to the community-based Q&amp;A data. For example, Jeon et al. (Jeon et al., 2005a; Jeon et al., 2005b) compared four different retrieval methods, i.e. vector space model, Okapi, language model (LM), and translation-based model, for automatically fixing the lexical chasm between questions of questi</context>
</contexts>
<marker>Harabagiu, Hickl, Lehmann, Moldovan, 2005</marker>
<rawString>S. Harabagiu, A. Hickl, J. Lehmann and D. Moldovan. 2005. Experiments with Interactive QuestionAnswering. In Proc. of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M D Rijke</author>
</authors>
<title>Retrieving Answers from Frequently Asked Questions Pages on the Web. In</title>
<date>2005</date>
<booktitle>Proc. of CIKM’05.</booktitle>
<contexts>
<context position="29476" citStr="Jijkoun and Rijke (2005)" startWordPosition="5001" endWordPosition="5004">s research efforts on question search is to tackle the lexical chasm problem between questions. The research of question search is first conducted using FAQ data. FAQ Finder (Burke et al., 1997) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs. Conventional vector space models are used to calculate the statistical similarity and WordNet (Fellbaum, 1998) is used to estimate the semantic similarity. Sneiders (2002) proposed template based FAQ retrieval systems. Lai et al. (2002) proposed an approach to automatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best. Recently, the research of question search has been further extended to the community-based Q&amp;A data. For example, Jeon et al. (Jeon et al., 2005a; Jeon et al., 2005b) compared four differe</context>
</contexts>
<marker>Jijkoun, Rijke, 2005</marker>
<rawString>V. Jijkoun, M. D. Rijke. 2005. Retrieving Answers from Frequently Asked Questions Pages on the Web. In Proc. of CIKM’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Cao</author>
<author>H Li</author>
</authors>
<title>Base noun phrase translation using web data and the EM algorithm.</title>
<date>2002</date>
<booktitle>In Proc. of COLING’02.</booktitle>
<contexts>
<context position="11038" citStr="Cao and Li, 2002" startWordPosition="1837" endWordPosition="1840">owing, with a series of definitions, we will describe how a question tree is constructed from a collection of questions. Let’s begin with explaining the representation of a question. A straightforward method is to represent a question as a bag-of-words (possibly ignoring stop words). However, this method cannot discern ‘the hotels in Paris’ from ‘the Paris hotel’. Thus, we turn to use the linguistic units carrying on more semantic information. Specifically, we make use of two kinds of units: BaseNP (Base Noun Phrase) and WH-ngram. A BaseNP is defined as a simple and non-recursive noun phrase (Cao and Li, 2002). A WH-ngram is an ngram beginning with WH-words. The WH-words that we consider include ‘when’, ‘what’, ‘where’, ‘which’, and ‘how’. We refer to these two kinds of units as ‘topic terms’. With ‘topic terms’, we represent a question as a topic chain and a set of questions as a question tree. Definition 1 (Topic Profile) The topic profile Bt of a topic term t in a categorized question collection is a probability distribution of categories {p(c|t)}cEc where C is a set of categories. p(C|t) = count(c,t) (7) Zcec count(c,t) where count(c, t) is the frequency of the topic term t within category c . </context>
</contexts>
<marker>Cao, Li, 2002</marker>
<rawString>Y. Cao and H. Li. 2002. Base noun phrase translation using web data and the EM algorithm. In Proc. of COLING’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-S Lai</author>
<author>K-A Fung</author>
<author>C-H Wu</author>
</authors>
<title>Faq mining via list detection.</title>
<date>2002</date>
<booktitle>In Proc. of the Workshop on Multilingual Summarization and Question Answering,</booktitle>
<contexts>
<context position="29389" citStr="Lai et al. (2002)" startWordPosition="4986" endWordPosition="4989">Table 6. Using Translation Probability 5 Related Work The major focus of previous research efforts on question search is to tackle the lexical chasm problem between questions. The research of question search is first conducted using FAQ data. FAQ Finder (Burke et al., 1997) heuristically combines statistical similarities and semantic similarities between questions to rank FAQs. Conventional vector space models are used to calculate the statistical similarity and WordNet (Fellbaum, 1998) is used to estimate the semantic similarity. Sneiders (2002) proposed template based FAQ retrieval systems. Lai et al. (2002) proposed an approach to automatically mine FAQs from the Web. Jijkoun and Rijke (2005) used supervised learning methods to extend heuristic extraction of Q/A pairs from FAQ pages, and treated Q/A pair retrieval as a fielded search task. Harabagiu et al. (2005) used a Question Answer Database (known as QUAB) to support interactive question answering. They compared seven different similarity metrics for selecting related questions from QUAB and found that the conceptbased metric performed best. Recently, the research of question search has been further extended to the community-based Q&amp;A data. </context>
</contexts>
<marker>Lai, Fung, Wu, 2002</marker>
<rawString>Y.-S. Lai, K.-A. Fung, and C.-H. Wu. 2002. Faq mining via list detection. In Proc. of the Workshop on Multilingual Summarization and Question Answering, 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>