<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.975571">
Annealing Structural Bias in Multilingual Weighted Grammar Induction*
</title>
<author confidence="0.972721">
Noah A. Smith and Jason Eisner
</author>
<affiliation confidence="0.9192965">
Department of Computer Science / Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21218 USA
</affiliation>
<email confidence="0.995537">
{nasmith,jason}@cs.jhu.edu
</email>
<sectionHeader confidence="0.993797" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906625">
We first show how a structural locality bias can improve the
accuracy of state-of-the-art dependency grammar induction
models trained by EM from unannotated examples (Klein
and Manning, 2004). Next, by annealing the free parame-
ter that controls this bias, we achieve further improvements.
We then describe an alternative kind of structural bias, to-
ward “broken” hypotheses consisting of partial structures
over segmented sentences, and show a similar pattern of im-
provement. We relate this approach to contrastive estimation
(Smith and Eisner, 2005a), apply the latter to grammar in-
duction in six languages, and show that our new approach
improves accuracy by 1–17% (absolute) over CE (and 8–30%
over EM), achieving to our knowledge the best results on this
task to date. Our method, structural annealing, is a gen-
eral technique with broad applicability to hidden-structure
discovery problems.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999325">
Inducing a weighted context-free grammar from
flat text is a hard problem. A common start-
ing point for weighted grammar induction is
the Expectation-Maximization (EM) algorithm
(Dempster et al., 1977; Baker, 1979). EM’s
mediocre performance (Table 1) reflects two prob-
lems. First, it seeks to maximize likelihood, but a
grammar that makes the training data likely does
not necessarily assign a linguistically defensible
syntactic structure. Second, the likelihood surface
is not globally concave, and learners such as the
EM algorithm can get trapped on local maxima
(Charniak, 1993).
We seek here to capitalize on the intuition that,
at least early in learning, the learner should search
primarily for string-local structure, because most
structure is local.1 By penalizing dependencies be-
tween two words that are farther apart in the string,
we obtain consistent improvements in accuracy of
the learned model (§3).
We then explore how gradually changing S over
time affects learning (§4): we start out with a
</bodyText>
<footnote confidence="0.992586555555556">
*This work was supported by a Fannie and John Hertz
Foundation fellowship to the first author and NSF ITR grant
IIS-0313193 to the second author. The views expressed are
not necessarily endorsed by the sponsors. We thank three
anonymous COLING-ACL reviewers for comments.
1To be concrete, in the corpora tested here, 95% of de-
pendency links cover &lt; 4 words (English, Bulgarian, Por-
tuguese), &lt; 5 words (German, Turkish), &lt; 6 words (Man-
darin).
</footnote>
<table confidence="0.925427625">
model selection among values of A and O(°)
worst unsup. sup. oracle
German 19.8 19.8 54.4 54.4
English 21.8 41.6 41.6 42.0
Bulgarian 24.7 44.6 45.6 45.6
Mandarin 31.8 37.2 50.0 50.0
Turkish 32.1 41.2 48.0 51.4
Portuguese 35.4 37.4 42.3 43.0
</table>
<tableCaption confidence="0.890876666666667">
Table 1: Baseline performance of EM-trained dependency
parsing models: F, on non-$ attachments in test data, with
various model selection conditions (3 initializers x 6 smooth-
ing values). The languages are listed in decreasing order by
the training set size. Experimental details can be found in the
appendix.
</tableCaption>
<bodyText confidence="0.9999476">
strong preference for short dependencies, then re-
lax the preference. The new approach, structural
annealing, often gives superior performance.
An alternative structural bias is explored in §5.
This approach views a sentence as a sequence
of one or more yields of separate, independent
trees. The points of segmentation are a hidden
variable, and during learning all possible segmen-
tations are entertained probabilistically. This al-
lows the learner to accept hypotheses that explain
the sentences as independent pieces.
In §6 we briefly review contrastive estimation
(Smith and Eisner, 2005a), relating it to the new
method, and show its performance alone and when
augmented with structural bias.
</bodyText>
<sectionHeader confidence="0.964562" genericHeader="introduction">
2 Task and Model
</sectionHeader>
<bodyText confidence="0.998986615384615">
In this paper we use a simple unlexicalized depen-
dency model due to Klein and Manning (2004).
The model is a probabilistic head automaton gram-
mar (Alshawi, 1996) with a “split” form that ren-
ders it parseable in cubic time (Eisner, 1997).
Let x = (x1, x2, ..., xn) be the sentence. x0 is a
special “wall” symbol, $, on the left of every sen-
tence. A tree y is defined by a pair of functions
yleft and yright (both {0, 1, 2,..., n} --, 211,2,...,n1)
that map each word to its sets of left and right de-
pendents, respectively. The graph is constrained
to be a projective tree rooted at $: each word ex-
cept $ has a single parent, and there are no cycles
</bodyText>
<page confidence="0.977248">
569
</page>
<note confidence="0.531952">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 569–576,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999631333333333">
or crossing dependencies.2 yleft(0) is taken to be
empty, and yright(0) contains the sentence’s single
head. Let yi denote the subtree rooted at position
i. The probability P(yi  |xi) of generating this
subtree, given its head word xi, is defined recur-
sively:
</bodyText>
<equation confidence="0.99164">
ri pstop(stop  |xi, D, [yD(i) = 01) (1)
DE{left,right}
rlX pstop(-,stop  |xi, D, firsty(j))
jEyD(i)
Xpchild(xj  |xi, D) X P(yj  |xj)
</equation>
<bodyText confidence="0.999943434782609">
where firsty(j) is a predicate defined to be true iff
xj is the closest child (on either side) to its parent
xi. The probability of the entire tree is given by
pe(x, y) = P(y0  |$). The parameters O are the
conditional distributions pstop and pchild.
Experimental baseline: EM. Following com-
mon practice, we always replace words by part-of-
speech (POS) tags before training or testing. We
used the EM algorithm to train this model on POS
sequences in six languages. Complete experimen-
tal details are given in the appendix. Performance
with unsupervised and supervised model selec-
tion across different λ values in add-λ smoothing
and three initializers O(0) is reported in Table 1.
The supervised-selected model is in the 40–55%
F1-accuracy range on directed dependency attach-
ments. (Here F1 Pz� precision Pz� recall; see ap-
pendix.) Supervised model selection, which uses
a small annotated development set, performs al-
most as well as the oracle, but unsupervised model
selection, which selects the model that maximizes
likelihood on an unannotated development set, is
often much worse.
</bodyText>
<sectionHeader confidence="0.984517" genericHeader="method">
3 Locality Bias among Trees
</sectionHeader>
<bodyText confidence="0.9977605">
Hidden-variable estimation algorithms—
including EM—typically work by iteratively
manipulating the model parameters O to improve
an objective function F(O). EM explicitly
alternates between the computation of a posterior
distribution over hypotheses, pp(y  |x) (where
y is any tree with yield x), and computing a new
parameter estimate O.3
</bodyText>
<footnote confidence="0.9415495">
2A projective parser could achieve perfect accuracy on our
English and Mandarin datasets, &gt; 99% on Bulgarian, Turk-
ish, and Portuguese, and &gt; 98% on German.
3For weighted grammar-based models, the posterior does
not need to be explicitly represented; instead expectations un-
der p® are used to compute updates to O.
</footnote>
<figureCaption confidence="0.989301">
Figure 1: Test-set F1 performance of models trained by EM
</figureCaption>
<bodyText confidence="0.980355708333333">
with a locality bias at varying S. Each curve corresponds
to a different language and shows performance of supervised
model selection within a given S, across A and O1°) values.
(See Table 3 for performance of models selected across Ss.)
We decode with S = 0, though we found that keeping the
training-time value of S would have had almost no effect. The
EM baseline corresponds to S = 0.
One way to bias a learner toward local expla-
nations is to penalize longer attachments. This
was done for supervised parsing in different ways
by Collins (1997), Klein and Manning (2003),
and McDonald et al. (2005), all of whom con-
sidered intervening material or coarse distance
classes when predicting children in a tree. Eis-
ner and Smith (2005) achieved speed and accuracy
improvements by modeling distance directly in a
ML-estimated (deficient) generative model.
Here we use string distance to measure the
length of a dependency link and consider the inclu-
sion of a sum-of-lengths feature in the probabilis-
tic model, for learning only. Keeping our original
model, we will simply multiply into the probabil-
ity of each tree another factor that penalizes long
dependencies, giving:
</bodyText>
<equation confidence="0.81257">
po(x, y) oc po(x, y)·e
</equation>
<bodyText confidence="0.9959676">
where y(i) = yleft(i) U yright(i). Note that if
δ = 0, we have the original model. As δ —* −oc,
the new model p&apos; will favor parses with shorter
dependencies. The dynamic programming algo-
rithms remain the same as before, with the appro-
priate ea|i−j |factor multiplied in at each attach-
ment between xi and xj. Note that when δ = 0,
pe - pe.
Experiment. We applied a locality bias to the
same dependency model by setting δ to different
</bodyText>
<figure confidence="0.934415473684211">
-1 -0.8 -0.6 -0.4 -0.2 0 0.2
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
German
English
Bulgarian
Mandarin
F Turkish
Portuguese
S
(EM baseline)
�
�
</figure>
<equation confidence="0.9931164">
|i − j |� �
(2)
� n �
� � � a� jEy(i)
i=1
</equation>
<page confidence="0.992589">
570
</page>
<figureCaption confidence="0.7973305">
Figure 2: Test-set Fl performance of models trained by EM
with structural annealing on the distance weight 6. Here
</figureCaption>
<bodyText confidence="0.96254725">
we show performance with add-10 smoothing, the all-zero
initializer, for three languages with three different initial val-
ues 6o. Time progresses from left to right. Note that it is
generally best to start at 6o « 0; note also the importance of
picking the right point on the curve to stop. See Table 3 for
performance of models selected across smoothing, initializa-
tion, starting, and stopping choices, in all six languages.
values in [−1, 0.2] (see Eq. 2). The same initial-
izers Off0) and smoothing conditions were tested.
Performance of supervised model selection among
models trained at different δ values is plotted in
Fig. 1. When a model is selected across all condi-
tions (3 initializers x 6 smoothing values x 7 δs)
using annotated development data, performance is
notably better than the EM baseline using the same
selection procedure (see Table 3, second column).
</bodyText>
<sectionHeader confidence="0.976201" genericHeader="method">
4 Structural Annealing
</sectionHeader>
<bodyText confidence="0.99991975">
The central idea of this paper is to gradually
change (anneal) the bias δ. Early in learning, local
dependencies are emphasized by setting δ « 0.
Then δ is iteratively increased and training re-
peated, using the last learned model to initialize.
This idea bears a strong similarity to determin-
istic annealing (DA), a technique used in clus-
tering and classification to smooth out objective
functions that are piecewise constant (hence dis-
continuous) or bumpy (non-concave) (Rose, 1998;
Ueda and Nakano, 1998). In unsupervised learn-
ing, DA iteratively re-estimates parameters like
EM, but begins by requiring that the entropy of
the posterior pp(y  |x) be maximal, then gradu-
ally relaxes this entropy constraint. Since entropy
is concave in O, the initial task is easy (maximize
a concave, continuous function). At each step the
optimization task becomes more difficult, but the
initializer is given by the previous step and, in
practice, tends to be close to a good local max-
imum of the more difficult objective. By the last
iteration the objective is the same as in EM, but the
annealed search process has acted like a good ini-
tializer. This method was applied with some suc-
cess to grammar induction models by Smith and
Eisner (2004).
In this work, instead of imposing constraints on
the entropy of the model, we manipulate bias to-
ward local hypotheses. As δ increases, we penal-
ize long dependencies less. We call this structural
annealing, since we are varying the strength of a
soft constraint (bias) on structural hypotheses. In
structural annealing, the final objective would be
the same as EM if our final δ, δf = 0, but we
found that annealing farther (δf &gt; 0) works much
better.4
Experiment: Annealing δ. We experimented
with annealing schedules for δ. We initialized at
δ0 E {−1, −0.4, −0.21, and increased δ by 0.1 (in
the first case) or 0.05 (in the others) up to δf = 3.
Models were trained to convergence at each δ-
epoch. Model selection was applied over the same
initialization and regularization conditions as be-
fore, δ0, and also over the choice of δf, with stop-
ping allowed at any stage along the δ trajectory.
Trajectories for three languages with three dif-
ferent δ0 values are plotted in Fig. 2. Generally
speaking, δ0 « 0 performs better. There is con-
sistently an early increase in performance as δ in-
creases, but the stopping δf matters tremendously.
Selected annealed-δ models surpass EM in all six
languages; see the third column of Table 3. Note
that structural annealing does not always outper-
form fixed-δ training (English and Portuguese).
This is because we only tested a few values of δ0,
since annealing requires longer runtime.
</bodyText>
<sectionHeader confidence="0.980618" genericHeader="method">
5 Structural Bias via Segmentation
</sectionHeader>
<bodyText confidence="0.999099888888889">
A related way to focus on local structure early
in learning is to broaden the set of hypothe-
ses to include partial parse structures. If x =
(x1, x2, ..., xn), the standard approach assumes
that x corresponds to the vertices of a single de-
pendency tree. Instead, we entertain every hypoth-
esis in which x is a sequence of yields from sepa-
rate, independently-generated trees. For example,
(x1, x2, x3) is the yield of one tree, (x4, x5) is the
</bodyText>
<footnote confidence="0.6158434">
4The reader may note that 6f &gt; 0 actually corresponds to
a bias toward longer attachments. A more apt description in
the context of annealing is to say that during early stages the
learner starts liking local attachments too much, and we need
to exaggerate 6 to “coax” it to new hypotheses. See Fig. 2.
</footnote>
<figure confidence="0.9984545">
-1 -0.5 0 0.5 1 1.5
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
F
S
German
Bulgarian
Turkish
</figure>
<page confidence="0.733312">
571
</page>
<figureCaption confidence="0.986113">
Figure 3: Test-set Fl performance of models trained by EM
</figureCaption>
<bodyText confidence="0.96694425">
with structural annealing on the breakage weight 3. Here
we show performance with add-10 smoothing, the all-zero
initializer, for three languages with three different initial val-
ues ,Qo. Time progresses from left (large ,Q) to right. See Ta-
ble 3 for performance of models selected across smoothing,
initialization, and stopping choices, in all six languages.
yield of a second, and (x6, ..., xn) is the yield of a
third. One extreme hypothesis is that x is n single-
node trees. At the other end of the spectrum is the
original set of hypotheses—full trees on x. Each
has a nonzero probability.
Segmented analyses are intermediate represen-
tations that may be helpful for a learner to use
to formulate notions of probable local structure,
without committing to full trees.5 We only allow
unobserved breaks, never positing a hard segmen-
tation of the training sentences. Over time, we in-
crease the bias against broken structures, forcing
the learner to commit most of its probability mass
to full trees.
</bodyText>
<subsectionHeader confidence="0.963134">
5.1 Vine Parsing
</subsectionHeader>
<bodyText confidence="0.999142285714286">
At first glance broadening the hypothesis space
to entertain all 2n−1 possible segmentations may
seem expensive. In fact the dynamic program-
ming computation is almost the same as sum-
ming or maximizing over connected dependency
trees. For the latter, we use an inside-outside al-
gorithm that computes a score for every parse tree
by computing the scores of items, or partial struc-
tures, through a bottom-up process. Smaller items
are built first, then assembled using a set of rules
defining how larger items can be built.6
Now note that any sequence of partial trees
over x can be constructed by combining the same
items into trees. The only difference is that we
</bodyText>
<footnote confidence="0.99515475">
5See also work on partial parsing as a task in its own right:
Hindle (1990) inter alia.
6See Eisner and Satta (1999) for the relevant algorithm
used in the experiments.
</footnote>
<bodyText confidence="0.999685307692308">
are willing to consider unassembled sequences of
these partial trees as hypotheses, in addition to
the fully connected trees. One way to accom-
plish this in terms of yright(0) is to say that the
root, $, is allowed to have multiple children, in-
stead of just one. Here, these children are inde-
pendent of each other (e.g., generated by a uni-
gram Markov model). In supervised dependency
parsing, Eisner and Smith (2005) showed that im-
posing a hard constraint on the whole structure—
specifically that each non-$ dependency arc cross
fewer than k words—can give guaranteed O(nk2)
runtime with little to no loss in accuracy (for sim-
ple models). This constraint could lead to highly
contrived parse trees, or none at all, for some
sentences—both are avoided by the allowance of
segmentation into a sequence of trees (each at-
tached to $). The construction of the “vine” (se-
quence of $’s children) takes only O(n) time once
the chart has been assembled.
Our broadened hypothesis model is a proba-
bilistic vine grammar with a unigram model over
$’s children. We allow (but do not require) seg-
mentation of sentences, where each independent
child of $ is the root of one of the segments. We do
not impose any constraints on dependency length.
</bodyText>
<subsectionHeader confidence="0.999527">
5.2 Modeling Segmentation
</subsectionHeader>
<bodyText confidence="0.999983625">
Now the total probability of an n-length sentence
x, marginalizing over its hidden structures, sums
up not only over trees, but over segmentations of
x. For completeness, we must include a proba-
bility model over the number of trees generated,
which could be anywhere from 1 to n. The model
over the number T of trees given a sentence of
length n will take the following log-linear form:
</bodyText>
<equation confidence="0.962863666666667">
� n
P(T = t  |n) = etβ eiβ
i=1
</equation>
<bodyText confidence="0.999963583333333">
where Q E R is the sole parameter. When Q = 0,
every value of T is equally likely. For Q « 0, the
model prefers larger structures with few breaks.
At the limit (Q —* −oc), we achieve the standard
learning setting, where the model must explain x
using a single tree. We start however at Q » 0,
where the model prefers smaller trees with more
breaks, in the limit preferring each word in x to be
its own tree. We could describe “brokenness” as a
feature in the model whose weight, Q, is chosen
extrinsically (and time-dependently), rather than
empirically—just as was done with S.
</bodyText>
<figure confidence="0.997298714285714">
0.5 0 -0.5 -1 -1.5
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
F
0
German
Bulgarian
Turkish
</figure>
<page confidence="0.96057">
572
</page>
<table confidence="0.979360714285714">
model selection among values of a2 and Off°)
worst unsup. sup. oracle
32.5 59.3 63.4 63.4
30.5 56.4 57.3 57.8
20.9 56.6 57.4 57.4
29.1 37.2 46.2 46.2
19.4 26.0 40.5 43.1
25.1 35.3 38.3 38.3
9.4 24.2 41.1 41.1
13.7 17.9 26.2 26.2
7.3 38.6 58.2 58.2
21.5 34.1 55.5 55.5
35.0 59.8 71.8 71.8
30.8 33.6 33.6 33.6
</table>
<tableCaption confidence="0.9744738">
Table 2: Performance of CE on test data, for different neigh-
borhoods and with different levels of regularization. Bold-
face marks scores better than EM-trained models selected the
same way (Table 1). The score is the F1 measure on non-$
attachments.
</tableCaption>
<bodyText confidence="0.999712107142857">
Annealing β resembles the popular bootstrap-
ping technique (Yarowsky, 1995), which starts out
aiming for high precision, and gradually improves
coverage over time. With strong bias (β » 0), we
seek a model that maintains high dependency pre-
cision on (non-$) attachments by attaching most
tags to $. Over time, as this is iteratively weak-
ened (β -* −oo), we hope to improve coverage
(dependency recall). Bootstrapping was applied
to syntax learning by Steedman et al. (2003). Our
approach differs in being able to remain partly ag-
nostic about each tag’s true parent (e.g., by giving
50% probability to attaching to $), whereas Steed-
man et al. make a hard decision to retrain on a
whole sentence fully or leave it out fully. In ear-
lier work, Brill and Marcus (1992) adopted a “lo-
cal first” iterative merge strategy for discovering
phrase structure.
Experiment: Annealing β. We experimented
with different annealing schedules for β. The ini-
tial value of β, β0, was one of {−1 , 0, 2 }. After
EM training, β was diminished by 10;this was re-
peated down to a value of βf = −3. Performance
after training at each β value is shown in Fig. 3.7
We see that, typically, there is a sharp increase
in performance somewhere during training, which
typically lessens as β -* −oo. Starting β too high
can also damage performance. This method, then,
</bodyText>
<footnote confidence="0.753061">
7Performance measures are given using a full parser that
finds the single best parse of the sentence with the learned
parsing parameters. Had we decoded with a vine parser, we
would see a precision\, recall/ curve as ,3 decreased.
</footnote>
<bodyText confidence="0.999881444444444">
is not robust to the choice of λ, β0, or βf, nor does
it always do as well as annealing δ, although con-
siderable gains are possible; see the fifth column
of Table 3.
By testing models trained with afixed value of β
(for values in [−1,1]), we ascertained that the per-
formance improvement is due largely to annealing,
not just the injection of segmentation bias (fourth
vs. fifth column of Table 3).8
</bodyText>
<sectionHeader confidence="0.9006905" genericHeader="method">
6 Comparison and Combination with
Contrastive Estimation
</sectionHeader>
<bodyText confidence="0.999745923076923">
Contrastive estimation (CE) was recently intro-
duced (Smith and Eisner, 2005a) as a class of alter-
natives to the likelihood objective function locally
maximized by EM. CE was found to outperform
EM on the task of focus in this paper, when ap-
plied to English data (Smith and Eisner, 2005b).
Here we review the method briefly, show how it
performs across languages, and demonstrate that
it can be combined effectively with structural bias.
Contrastive training defines for each example xi
a class of presumably poor, but similar, instances
called the “neighborhood,” N(xi), and seeks to
maximize
</bodyText>
<equation confidence="0.99891775">
CN(O) = 1: log pp(xi  |N(xi))
i
log EY pe (xi, y)
EX&apos;EN(Xi) EY po (x&amp;quot; y)
</equation>
<bodyText confidence="0.999973333333333">
At this point we switch to a log-linear (rather
than stochastic) parameterization of the same
weighted grammar, for ease of numerical opti-
mization. All this means is that O (specifically,
pstop and pchild in Eq. 1) is now a set of nonnega-
tive weights rather than probabilities.
Neighborhoods that can be expressed as finite-
state lattices built from xi were shown to give sig-
nificant improvements in dependency parser qual-
ity over EM. Performance of CE using two of
those neighborhoods on the current model and
datasets is shown in Table 2.9 0-mean diagonal
Gaussian smoothing was applied, with different
variances, and model selection was applied over
smoothing conditions and the same initializers as
</bodyText>
<footnote confidence="0.925779166666667">
8In principle, segmentation can be combined with the lo-
cality bias in §3 (6). In practice, we found that this usually
under-performed the EM baseline.
9We experimented with DELETE1, TRANSPOSE1, DELE-
TEORTRANSPOSE1, and LENGTH. To conserve space we
show only the latter two, which tend to perform best.
</footnote>
<figure confidence="0.933325695652174">
DORT1
LENGTH
DORT1
LENGTH
DORT1
LENGTH
DORT1
LENGTH
DORT1
LENGTH
DORT1
LENGTH
1: =
i
573
German
English
Bulgarian
Mandarin
Turkish
Portuguese
EM fixed S annealed S fixed Q annealed Q CE fixed S + CE
S S0 --+ Sf ,3 ,30 --+,3f N N, S
</figure>
<table confidence="0.998684666666667">
54.4 61.3 0.2 70.0 -0.4 → 0.4 66.2 0.4 68.9 0.5 → -2.4 63.4 DORT1 63.8 DORT1, -0.2
41.6 61.8 -0.6 53.8 -0.4 → 0.3 55.6 0.2 58.4 0.5 → 0.0 57.4 DORT1 63.5 DORT1, -0.4
45.6 49.2 -0.2 58.3 -0.4 → 0.2 47.3 -0.2 56.5 0 → -1.7 40.5 DORT1 –
50.0 51.1 -0.4 58.0 -1.0 → 0.2 38.0 0.2 57.2 0.5 → -1.4 43.4 DEL1 –
48.0 62.3 -0.2 62.4 -0.2 → -0.15 53.6 -0.2 59.4 0.5 → -0.7 58.2 DORT1 61.8 DORT1, -0.6
42.3 50.4 -0.4 50.2 -0.4 → -0.1 51.5 0.2 62.7 0.5 → -0.5 71.8 DORT1 72.6 DORT1, -0.2
</table>
<tableCaption confidence="0.995846">
Table 3: Summary comparing models trained in a variety of ways with some relevant hyperparameters. Supervised model
selection was applied in all cases, including EM (see the appendix). Boldface marks the best performance overall and trials
that this performance did not significantly surpass under a sign test (i.e., p -A 0.05). The score is the F1 measure on non-$
attachments. The fixed S + CE condition was tested only for languages where CE improved over EM.
</tableCaption>
<bodyText confidence="0.997241906666667">
before. Four of the languages have at least one ef-
fective CE condition, supporting our previous En-
glish results (Smith and Eisner, 2005b), but CE
was harmful for Bulgarian and Mandarin. Perhaps
better neighborhoods exist for these languages, or
there is some ideal neighborhood that would per-
form well for all languages.
Our approach of allowing broken trees (§5) is
a natural extension of the CE framework. Con-
trastive estimation views learning as a process of
moving posterior probability mass from (implicit)
negative examples to (explicit) positive examples.
The positive evidence, as in MLE, is taken to be
the observed data. As originally proposed, CE al-
lowed a redefinition of the implicit negative ev-
idence from “all other sentences” (as in MLE)
to “sentences like xi, but perturbed.” Allowing
segmentation of the training sentences redefines
the positive and negative evidence. Rather than
moving probability mass only to full analyses of
the training example xi, we also allow probability
mass to go to partial analyses of xi.
By injecting a bias (S 7� 0 or Q &gt; −oc) among
tree hypotheses, however, we have gone beyond
the CE framework. We have added features to
the tree model (dependency length-sum, number
of breaks), whose weights we extrinsically manip-
ulate over time to impose locality bias CN and im-
prove search on CN. Another idea, not explored
here, is to change the contents of the neighborhood
N over time.
Experiment: Locality Bias within CE. We
combined CE with a fixed-S locality bias for
neighborhoods that were successful in the earlier
CE experiment, namely DELETEORTRANSPOSE1
for German, English, Turkish, and Portuguese.
Our results, shown in the seventh column of Ta-
ble 3, show that, in all cases except Turkish, the
combination improves over either technique on its
own. We leave exploration of structural annealing
with CE to future work.
Experiment: Segmentation Bias within CE.
For (language, N) pairs where CE was effec-
tive, we trained models using CE with a fixed-
Q segmentation model. Across conditions (Q E
[−1,1]), these models performed very badly, hy-
pothesizing extremely local parse trees: typically
over 90% of dependencies were length 1 and
pointed in the same direction, compared with the
60–70% length-1 rate seen in gold standards. To
understand why, consider that the CE goal is to
maximize the score of a sentence and all its seg-
mentations while minimizing the scores of neigh-
borhood sentences and their segmentations. An n-
gram model can accomplish this, since the same
n-grams are present in all segmentations of x,
and (some) different n-grams appear in N(x)
(for LENGTH and DELETEORTRANSPOSE1). A
bigram-like model that favors monotone branch-
ing, then, is not a bad choice for a CE learner that
must account for segmentations of x and N(x).
Why doesn’t CE without segmentation resort to
n-gram-like models? Inspection of models trained
using the standard CE method (no segmentation)
with transposition-based neighborhoods TRANS-
POSE1 and DELETEORTRANSPOSE1 did have
high rates of length-1 dependencies, while the
poorly-performing DELETE1 models found low
length-1 rates. This suggests that a bias toward
locality (“n-gram-ness”) is built into the former
neighborhoods, and may partly explain why CE
works when it does. We achieved a similar locality
bias in the likelihood framework when we broad-
ened the hypothesis space, but doing so under CE
over-focuses the model on local structures.
</bodyText>
<page confidence="0.997905">
574
</page>
<sectionHeader confidence="0.995955" genericHeader="method">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.999971263157895">
We compared errors made by the selected EM con-
dition with the best overall condition, for each lan-
guage. We found that the number of corrected at-
tachments always outnumbered the number of new
errors by a factor of two or more.
Further, the new models are not getting better
by merely reversing the direction of links made
by EM; undirected accuracy also improved signif-
icantly under a sign test (p &lt; 10−6), across all six
languages. While the most common corrections
were to nouns, these account for only 25–41% of
corrections, indicating that corrections are not “all
of the same kind.”
Finally, since more than half of corrections in
every language involved reattachment to a noun
or a verb (content word), we believe the improved
models to be getting closer than EM to the deeper
semantic relations between words that, ideally,
syntactic models should uncover.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="method">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999986103448276">
One weakness of all recent weighted grammar
induction work—including Klein and Manning
(2004), Smith and Eisner (2005b), and the present
paper—is a sensitivity to hyperparameters, includ-
ing smoothing values, choice of N (for CE), and
annealing schedules—not to mention initializa-
tion. This is quite observable in the results we have
presented. An obstacle for unsupervised learn-
ing in general is the need for automatic, efficient
methods for model selection. For annealing, in-
spiration may be drawn from continuation meth-
ods; see, e.g., Elidan and Friedman (2005). Ideally
one would like to select values simultaneously for
many hyperparameters, perhaps using a small an-
notated corpus (as done here), extrinsic figures of
merit on successful learning trajectories, or plau-
sibility criteria (Eisner and Karakos, 2005).
Grammar induction serves as a tidy example
for structural annealing. In future work, we envi-
sion that other kinds of structural bias and anneal-
ing will be useful in other difficult learning prob-
lems where hidden structure is required, including
machine translation, where the structure can con-
sist of word correspondences or phrasal or recur-
sive syntax with correspondences. The technique
bears some similarity to the estimation methods
described by Brown et al. (1993), which started
by estimating simple models, using each model to
seed the next.
</bodyText>
<sectionHeader confidence="0.99259" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999939555555556">
We have presented a new unsupervised parameter
estimation method, structural annealing, for learn-
ing hidden structure that biases toward simplic-
ity and gradually weakens (anneals) the bias over
time. We applied the technique to weighted de-
pendency grammar induction and achieved a sig-
nificant gain in accuracy over EM and CE, raising
the state-of-the-art across six languages from 42–
54% to 58–73% accuracy.
</bodyText>
<sectionHeader confidence="0.998495" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99941756">
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Floresta
sint´a(c)tica: a treebank for Portuguese. In Proc. ofLREC.
H. Alshawi. 1996. Head automata and bilingual tiling:
Translation with minimal representations. In Proc. of
ACL.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation
process in the Turkish treebank. In Proc. ofLINC.
J. K. Baker. 1979. Trainable grammars for speech recogni-
tion. In Proc. of the Acoustical Society ofAmerica.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER Treebank. In Proc. of Workshop on
Treebanks and Linguistic Theories.
E. Brill and M. Marcus. 1992. Automatically acquiring
phrase structure using distributional analysis. In Proc. of
DARPA Workshop on Speech and Natural Language.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263–311.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In Proc. of CoNLL.
E. Charniak. 1993. Statistical Language Learning. MIT
Press.
M. Collins. 1997. Three generative, lexicalised models for
statistical parsing. In Proc. ofACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum like-
lihood estimation from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society B, 39:1–38.
J. Eisner and D. Karakos. 2005. Bootstrapping without the
boot. In Proc. ofHLT-EMNLP.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexical
context-free grammars and head automaton grammars. In
Proc. ofACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and hard
constraints on dependency length. In Proc. ofIWPT.
J. Eisner. 1997. Bilexical grammars and a cubic-time proba-
bilistic parser. In Proc. ofIWPT.
G. Elidan and N. Friedman. 2005. Learning hidden variable
networks: the information bottleneck approach. Journal
ofMachine Learning Research, 6:81–127.
D. Hindle. 1990. Noun classification from predicate-
argument structure. In Proc. ofACL.
D. Klein and C. D. Manning. 2002. A generative constituent-
context model for improved grammar induction. In Proc.
ofACL.
D. Klein and C. D. Manning. 2003. Fast exact inference with
a factored model for natural language parsing. In NIPS 15.
D. Klein and C. D. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In Proc. ofACL.
</reference>
<page confidence="0.992011">
575
</page>
<reference confidence="0.998336888888889">
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313–330.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc. of
ACL.
K. Oflazer, B. Say, D. Z. Hakkani-T¨ur, and G. T¨ur. 2003.
Building a Turkish treebank. In A. Abeille, editor,
Building and Exploiting Syntactically-Annotated Cor-
pora. Kluwer.
K. Rose. 1998. Deterministic annealing for clustering, com-
pression, classification, regression, and related optimiza-
tion problems. Proc. of the IEEE, 86(11):2210–2239.
K. Simov and P. Osenova. 2003. Practical annotation scheme
for an HPSG treebank of Bulgarian. In Proc. ofLINC.
K. Simov, G. Popova, and P. Osenova. 2002. HPSG-
based syntactic treebank of Bulgarian (BulTreeBank). In
A. Wilson, P. Rayson, and T. McEnery, editors, A Rain-
bow of Corpora: Corpus Linguistics and the Languages
of the World, pages 135–42. Lincom-Europa.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2004.
Design and implementation of the Bulgarian HPSG-based
Treebank. Journal of Research on Language and Compu-
tation, 2(4):495–522.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In Proc.
ofACL.
N. A. Smith and J. Eisner. 2005a. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc. of
ACL.
N. A. Smith and J. Eisner. 2005b. Guiding unsupervised
grammar induction using contrastive estimation. In Proc.
of IJCAI Workshop on Grammatical Inference Applica-
tions.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003.
Bootstrapping statistical parsers from small datasets. In
Proc. ofEACL.
N. Ueda and R. Nakano. 1998. Deterministic annealing EM
algorithm. Neural Networks, 11(2):271–282.
N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn
Chinese Treebank: Phrase structure annotation of a large
corpus. Natural Language Engineering, 10(4):1–30.
D. Yarowsky. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proc. ofACL.
</reference>
<sectionHeader confidence="0.964972" genericHeader="method">
A Experimental Setup
</sectionHeader>
<bodyText confidence="0.998835571428571">
Following the usual conventions (Klein and Man-
ning, 2002), our experiments use treebank POS
sequences of length G 10, stripped of words and
punctuation. For smoothing, we apply add-A, with
six values of A (in CE trials, we use a 0-mean di-
agonal Gaussian prior with five different values of
U2). Our training datasets are:
</bodyText>
<listItem confidence="0.994205538461538">
• 8,227 German sentences from the TIGER Tree-
bank (Brants et al., 2002),
• 5,301 English sentences from the WSJ Penn
Treebank (Marcus et al., 1993),
• 4,929 Bulgarian sentences from the BulTree-
Bank (Simov et al., 2002; Simov and Osenova,
2003; Simov et al., 2004),
• 2,775 Mandarin sentences from the Penn Chi-
nese Treebank (Xue et al., 2004),
• 2,576 Turkish sentences from the METU-
Sabanci Treebank (Atalay et al., 2003; Oflazer et
al., 2003), and
• 1,676 Portuguese sentences from the Bosque
</listItem>
<bodyText confidence="0.986499705882353">
portion of the Floresta Sint´a(c)tica Treebank
(Afonso et al., 2002).
The Bulgarian, Turkish, and Portuguese datasets
come from the CoNLL-X shared task (Buchholz
and Marsi, 2006); we thank the organizers.
When comparing a hypothesized tree y to a
gold standard y∗, precision and recall measures
are available. If every tree in the gold standard and
every hypothesis tree is such that |yright(0) |= 1,
then precision = recall = F1, since |y |= |y∗|.
|yright(0) |= 1 for all hypothesized trees in this
paper, but not all treebank trees; hence we report
the F1 measure. The test set consists of around
500 sentences (in each language).
Iterative training proceeds until either 100 it-
erations have passed, or the objective converges
within a relative tolerance of c = 10−5, whichever
occurs first.
Models trained at different hyperparameter set-
tings and with different initializers are selected
using a 500-sentence development set. Unsuper-
vised model selection means the model with the
highest training objective value on the develop-
ment set was chosen. Supervised model selection
chooses the model that performs best on the anno-
tated development set. (Oracle and worst model
selection are chosen based on performance on the
test data.)
We use three initialization methods. We run a
single special E step (to get expected counts of
model events) then a single M step that renormal-
izes to get a probabilistic model O(0). In initializer
1, the E step scores each tree as follows (only con-
nected trees are scored):
</bodyText>
<equation confidence="0.797779">
1
j∈Y(i) |i − j|)n.(1
</equation>
<bodyText confidence="0.999307">
(Proper) expectations under these scores are com-
puted using an inside-outside algorithm. Initial-
izer 2 computes expected counts directly, without
dynamic programming. For an n-length sentence,
</bodyText>
<equation confidence="0.805977">
p(yright(0) = {i}) = n1 andp(j E y(i)) a 1
|i−j|.
</equation>
<bodyText confidence="0.999916833333333">
These are scaled by an appropriate constant for
each sentence, then summed across sentences to
compute expected event counts. Initializer 3 as-
sumes a uniform distribution over hidden struc-
tures in the special E step by setting all log proba-
bilities to zero.
</bodyText>
<equation confidence="0.958627333333333">
n
2t(x,yleft,yright) =
i=1
</equation>
<page confidence="0.994147">
576
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981117">
<title confidence="0.996903">Structural Bias in Multilingual Weighted Grammar</title>
<author confidence="0.999567">A Smith Eisner</author>
<affiliation confidence="0.999592">Department of Computer Science / Center for Language and Speech Processing</affiliation>
<address confidence="0.99363">Johns Hopkins University, Baltimore, MD 21218 USA</address>
<abstract confidence="0.999464117647059">first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this to date. Our method, is a general technique with broad applicability to hidden-structure discovery problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Afonso</author>
<author>E Bick</author>
<author>R Haber</author>
<author>D Santos</author>
</authors>
<title>Floresta sint´a(c)tica: a treebank for Portuguese. In</title>
<date>2002</date>
<booktitle>Proc. ofLREC.</booktitle>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Floresta sint´a(c)tica: a treebank for Portuguese. In Proc. ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata and bilingual tiling: Translation with minimal representations.</title>
<date>1996</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4049" citStr="Alshawi, 1996" startWordPosition="629" endWordPosition="630">of separate, independent trees. The points of segmentation are a hidden variable, and during learning all possible segmentations are entertained probabilistically. This allows the learner to accept hypotheses that explain the sentences as independent pieces. In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias. 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = (x1, x2, ..., xn) be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2,..., n} --, 211,2,...,n1) that map each word to its sets of left and right dependents, respectively. The graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles 569 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the </context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata and bilingual tiling: Translation with minimal representations. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N B Atalay</author>
<author>K Oflazer</author>
<author>B Say</author>
</authors>
<title>The annotation process in the Turkish treebank.</title>
<date>2003</date>
<booktitle>In Proc. ofLINC.</booktitle>
<marker>Atalay, Oflazer, Say, 2003</marker>
<rawString>N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation process in the Turkish treebank. In Proc. ofLINC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Proc. of the Acoustical Society ofAmerica.</booktitle>
<contexts>
<context position="1384" citStr="Baker, 1979" startWordPosition="203" endWordPosition="204">ive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems. 1 Introduction Inducing a weighted context-free grammar from flat text is a hard problem. A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979). EM’s mediocre performance (Table 1) reflects two problems. First, it seeks to maximize likelihood, but a grammar that makes the training data likely does not necessarily assign a linguistically defensible syntactic structure. Second, the likelihood surface is not globally concave, and learners such as the EM algorithm can get trapped on local maxima (Charniak, 1993). We seek here to capitalize on the intuition that, at least early in learning, the learner should search primarily for string-local structure, because most structure is local.1 By penalizing dependencies between two words that ar</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. In Proc. of the Acoustical Society ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER Treebank.</title>
<date>2002</date>
<booktitle>In Proc. of Workshop on Treebanks and Linguistic Theories.</booktitle>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002. The TIGER Treebank. In Proc. of Workshop on Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>M Marcus</author>
</authors>
<title>Automatically acquiring phrase structure using distributional analysis.</title>
<date>1992</date>
<booktitle>In Proc. of DARPA Workshop on Speech and Natural Language.</booktitle>
<contexts>
<context position="18841" citStr="Brill and Marcus (1992)" startWordPosition="3181" endWordPosition="3184">ves coverage over time. With strong bias (β » 0), we seek a model that maintains high dependency precision on (non-$) attachments by attaching most tags to $. Over time, as this is iteratively weakened (β -* −oo), we hope to improve coverage (dependency recall). Bootstrapping was applied to syntax learning by Steedman et al. (2003). Our approach differs in being able to remain partly agnostic about each tag’s true parent (e.g., by giving 50% probability to attaching to $), whereas Steedman et al. make a hard decision to retrain on a whole sentence fully or leave it out fully. In earlier work, Brill and Marcus (1992) adopted a “local first” iterative merge strategy for discovering phrase structure. Experiment: Annealing β. We experimented with different annealing schedules for β. The initial value of β, β0, was one of {−1 , 0, 2 }. After EM training, β was diminished by 10;this was repeated down to a value of βf = −3. Performance after training at each β value is shown in Fig. 3.7 We see that, typically, there is a sharp increase in performance somewhere during training, which typically lessens as β -* −oo. Starting β too high can also damage performance. This method, then, 7Performance measures are given</context>
</contexts>
<marker>Brill, Marcus, 1992</marker>
<rawString>E. Brill and M. Marcus. 1992. Automatically acquiring phrase structure using distributional analysis. In Proc. of DARPA Workshop on Speech and Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL. E. Charniak.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL. E. Charniak. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="7495" citStr="Collins (1997)" startWordPosition="1203" endWordPosition="1204">s to O. Figure 1: Test-set F1 performance of models trained by EM with a locality bias at varying S. Each curve corresponds to a different language and shows performance of supervised model selection within a given S, across A and O1°) values. (See Table 3 for performance of models selected across Ss.) We decode with S = 0, though we found that keeping the training-time value of S would have had almost no effect. The EM baseline corresponds to S = 0. One way to bias a learner toward local explanations is to penalize longer attachments. This was done for supervised parsing in different ways by Collins (1997), Klein and Manning (2003), and McDonald et al. (2005), all of whom considered intervening material or coarse distance classes when predicting children in a tree. Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model. Here we use string distance to measure the length of a dependency link and consider the inclusion of a sum-of-lengths feature in the probabilistic model, for learning only. Keeping our original model, we will simply multiply into the probability of each tree another factor that penalizes long </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood estimation from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society B,</journal>
<pages>39--1</pages>
<contexts>
<context position="1370" citStr="Dempster et al., 1977" startWordPosition="199" endWordPosition="202">is approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems. 1 Introduction Inducing a weighted context-free grammar from flat text is a hard problem. A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979). EM’s mediocre performance (Table 1) reflects two problems. First, it seeks to maximize likelihood, but a grammar that makes the training data likely does not necessarily assign a linguistically defensible syntactic structure. Second, the likelihood surface is not globally concave, and learners such as the EM algorithm can get trapped on local maxima (Charniak, 1993). We seek here to capitalize on the intuition that, at least early in learning, the learner should search primarily for string-local structure, because most structure is local.1 By penalizing dependencies between two</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>D Karakos</author>
</authors>
<title>Bootstrapping without the boot.</title>
<date>2005</date>
<booktitle>In Proc. ofHLT-EMNLP.</booktitle>
<contexts>
<context position="28089" citStr="Eisner and Karakos, 2005" startWordPosition="4731" endWordPosition="4734">moothing values, choice of N (for CE), and annealing schedules—not to mention initialization. This is quite observable in the results we have presented. An obstacle for unsupervised learning in general is the need for automatic, efficient methods for model selection. For annealing, inspiration may be drawn from continuation methods; see, e.g., Elidan and Friedman (2005). Ideally one would like to select values simultaneously for many hyperparameters, perhaps using a small annotated corpus (as done here), extrinsic figures of merit on successful learning trajectories, or plausibility criteria (Eisner and Karakos, 2005). Grammar induction serves as a tidy example for structural annealing. In future work, we envision that other kinds of structural bias and annealing will be useful in other difficult learning problems where hidden structure is required, including machine translation, where the structure can consist of word correspondences or phrasal or recursive syntax with correspondences. The technique bears some similarity to the estimation methods described by Brown et al. (1993), which started by estimating simple models, using each model to seed the next. 9 Conclusion We have presented a new unsupervised</context>
</contexts>
<marker>Eisner, Karakos, 2005</marker>
<rawString>J. Eisner and D. Karakos. 2005. Bootstrapping without the boot. In Proc. ofHLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="15135" citStr="Eisner and Satta (1999)" startWordPosition="2521" endWordPosition="2524"> is almost the same as summing or maximizing over connected dependency trees. For the latter, we use an inside-outside algorithm that computes a score for every parse tree by computing the scores of items, or partial structures, through a bottom-up process. Smaller items are built first, then assembled using a set of rules defining how larger items can be built.6 Now note that any sequence of partial trees over x can be constructed by combining the same items into trees. The only difference is that we 5See also work on partial parsing as a task in its own right: Hindle (1990) inter alia. 6See Eisner and Satta (1999) for the relevant algorithm used in the experiments. are willing to consider unassembled sequences of these partial trees as hypotheses, in addition to the fully connected trees. One way to accomplish this in terms of yright(0) is to say that the root, $, is allowed to have multiple children, instead of just one. Here, these children are independent of each other (e.g., generated by a unigram Markov model). In supervised dependency parsing, Eisner and Smith (2005) showed that imposing a hard constraint on the whole structure— specifically that each non-$ dependency arc cross fewer than k words</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>N A Smith</author>
</authors>
<title>Parsing with soft and hard constraints on dependency length.</title>
<date>2005</date>
<booktitle>In Proc. ofIWPT.</booktitle>
<contexts>
<context position="7681" citStr="Eisner and Smith (2005)" startWordPosition="1231" endWordPosition="1235">rvised model selection within a given S, across A and O1°) values. (See Table 3 for performance of models selected across Ss.) We decode with S = 0, though we found that keeping the training-time value of S would have had almost no effect. The EM baseline corresponds to S = 0. One way to bias a learner toward local explanations is to penalize longer attachments. This was done for supervised parsing in different ways by Collins (1997), Klein and Manning (2003), and McDonald et al. (2005), all of whom considered intervening material or coarse distance classes when predicting children in a tree. Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model. Here we use string distance to measure the length of a dependency link and consider the inclusion of a sum-of-lengths feature in the probabilistic model, for learning only. Keeping our original model, we will simply multiply into the probability of each tree another factor that penalizes long dependencies, giving: po(x, y) oc po(x, y)·e where y(i) = yleft(i) U yright(i). Note that if δ = 0, we have the original model. As δ —* −oc, the new model p&apos; will favor parses with short</context>
<context position="15603" citStr="Eisner and Smith (2005)" startWordPosition="2601" endWordPosition="2604"> trees. The only difference is that we 5See also work on partial parsing as a task in its own right: Hindle (1990) inter alia. 6See Eisner and Satta (1999) for the relevant algorithm used in the experiments. are willing to consider unassembled sequences of these partial trees as hypotheses, in addition to the fully connected trees. One way to accomplish this in terms of yright(0) is to say that the root, $, is allowed to have multiple children, instead of just one. Here, these children are independent of each other (e.g., generated by a unigram Markov model). In supervised dependency parsing, Eisner and Smith (2005) showed that imposing a hard constraint on the whole structure— specifically that each non-$ dependency arc cross fewer than k words—can give guaranteed O(nk2) runtime with little to no loss in accuracy (for simple models). This constraint could lead to highly contrived parse trees, or none at all, for some sentences—both are avoided by the allowance of segmentation into a sequence of trees (each attached to $). The construction of the “vine” (sequence of $’s children) takes only O(n) time once the chart has been assembled. Our broadened hypothesis model is a probabilistic vine grammar with a </context>
</contexts>
<marker>Eisner, Smith, 2005</marker>
<rawString>J. Eisner and N. A. Smith. 2005. Parsing with soft and hard constraints on dependency length. In Proc. ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and a cubic-time probabilistic parser.</title>
<date>1997</date>
<booktitle>In Proc. ofIWPT.</booktitle>
<contexts>
<context position="4124" citStr="Eisner, 1997" startWordPosition="643" endWordPosition="644">able, and during learning all possible segmentations are entertained probabilistically. This allows the learner to accept hypotheses that explain the sentences as independent pieces. In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias. 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = (x1, x2, ..., xn) be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2,..., n} --, 211,2,...,n1) that map each word to its sets of left and right dependents, respectively. The graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles 569 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 569–576, Sydney, July 2006. c�2006 Association for Computational</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>J. Eisner. 1997. Bilexical grammars and a cubic-time probabilistic parser. In Proc. ofIWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Elidan</author>
<author>N Friedman</author>
</authors>
<title>Learning hidden variable networks: the information bottleneck approach.</title>
<date>2005</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>6--81</pages>
<contexts>
<context position="27836" citStr="Elidan and Friedman (2005)" startWordPosition="4694" endWordPosition="4697"> that, ideally, syntactic models should uncover. 8 Future Work One weakness of all recent weighted grammar induction work—including Klein and Manning (2004), Smith and Eisner (2005b), and the present paper—is a sensitivity to hyperparameters, including smoothing values, choice of N (for CE), and annealing schedules—not to mention initialization. This is quite observable in the results we have presented. An obstacle for unsupervised learning in general is the need for automatic, efficient methods for model selection. For annealing, inspiration may be drawn from continuation methods; see, e.g., Elidan and Friedman (2005). Ideally one would like to select values simultaneously for many hyperparameters, perhaps using a small annotated corpus (as done here), extrinsic figures of merit on successful learning trajectories, or plausibility criteria (Eisner and Karakos, 2005). Grammar induction serves as a tidy example for structural annealing. In future work, we envision that other kinds of structural bias and annealing will be useful in other difficult learning problems where hidden structure is required, including machine translation, where the structure can consist of word correspondences or phrasal or recursive</context>
</contexts>
<marker>Elidan, Friedman, 2005</marker>
<rawString>G. Elidan and N. Friedman. 2005. Learning hidden variable networks: the information bottleneck approach. Journal ofMachine Learning Research, 6:81–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structure.</title>
<date>1990</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="15094" citStr="Hindle (1990)" startWordPosition="2516" endWordPosition="2517">dynamic programming computation is almost the same as summing or maximizing over connected dependency trees. For the latter, we use an inside-outside algorithm that computes a score for every parse tree by computing the scores of items, or partial structures, through a bottom-up process. Smaller items are built first, then assembled using a set of rules defining how larger items can be built.6 Now note that any sequence of partial trees over x can be constructed by combining the same items into trees. The only difference is that we 5See also work on partial parsing as a task in its own right: Hindle (1990) inter alia. 6See Eisner and Satta (1999) for the relevant algorithm used in the experiments. are willing to consider unassembled sequences of these partial trees as hypotheses, in addition to the fully connected trees. One way to accomplish this in terms of yright(0) is to say that the root, $, is allowed to have multiple children, instead of just one. Here, these children are independent of each other (e.g., generated by a unigram Markov model). In supervised dependency parsing, Eisner and Smith (2005) showed that imposing a hard constraint on the whole structure— specifically that each non-</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicateargument structure. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>A generative constituentcontext model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. A generative constituentcontext model for improved grammar induction. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In NIPS 15.</booktitle>
<contexts>
<context position="7521" citStr="Klein and Manning (2003)" startWordPosition="1205" endWordPosition="1208">: Test-set F1 performance of models trained by EM with a locality bias at varying S. Each curve corresponds to a different language and shows performance of supervised model selection within a given S, across A and O1°) values. (See Table 3 for performance of models selected across Ss.) We decode with S = 0, though we found that keeping the training-time value of S would have had almost no effect. The EM baseline corresponds to S = 0. One way to bias a learner toward local explanations is to penalize longer attachments. This was done for supervised parsing in different ways by Collins (1997), Klein and Manning (2003), and McDonald et al. (2005), all of whom considered intervening material or coarse distance classes when predicting children in a tree. Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model. Here we use string distance to measure the length of a dependency link and consider the inclusion of a sum-of-lengths feature in the probabilistic model, for learning only. Keeping our original model, we will simply multiply into the probability of each tree another factor that penalizes long dependencies, giving: po(x</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In NIPS 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="3980" citStr="Klein and Manning (2004)" startWordPosition="616" endWordPosition="619">ored in §5. This approach views a sentence as a sequence of one or more yields of separate, independent trees. The points of segmentation are a hidden variable, and during learning all possible segmentations are entertained probabilistically. This allows the learner to accept hypotheses that explain the sentences as independent pieces. In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias. 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = (x1, x2, ..., xn) be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2,..., n} --, 211,2,...,n1) that map each word to its sets of left and right dependents, respectively. The graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles 569 Proceedings of the 21st International Co</context>
<context position="27366" citStr="Klein and Manning (2004)" startWordPosition="4621" endWordPosition="4624">y also improved significantly under a sign test (p &lt; 10−6), across all six languages. While the most common corrections were to nouns, these account for only 25–41% of corrections, indicating that corrections are not “all of the same kind.” Finally, since more than half of corrections in every language involved reattachment to a noun or a verb (content word), we believe the improved models to be getting closer than EM to the deeper semantic relations between words that, ideally, syntactic models should uncover. 8 Future Work One weakness of all recent weighted grammar induction work—including Klein and Manning (2004), Smith and Eisner (2005b), and the present paper—is a sensitivity to hyperparameters, including smoothing values, choice of N (for CE), and annealing schedules—not to mention initialization. This is quite observable in the results we have presented. An obstacle for unsupervised learning in general is the need for automatic, efficient methods for model selection. For annealing, inspiration may be drawn from continuation methods; see, e.g., Elidan and Friedman (2005). Ideally one would like to select values simultaneously for many hyperparameters, perhaps using a small annotated corpus (as done</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="7549" citStr="McDonald et al. (2005)" startWordPosition="1210" endWordPosition="1213">odels trained by EM with a locality bias at varying S. Each curve corresponds to a different language and shows performance of supervised model selection within a given S, across A and O1°) values. (See Table 3 for performance of models selected across Ss.) We decode with S = 0, though we found that keeping the training-time value of S would have had almost no effect. The EM baseline corresponds to S = 0. One way to bias a learner toward local explanations is to penalize longer attachments. This was done for supervised parsing in different ways by Collins (1997), Klein and Manning (2003), and McDonald et al. (2005), all of whom considered intervening material or coarse distance classes when predicting children in a tree. Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model. Here we use string distance to measure the length of a dependency link and consider the inclusion of a sum-of-lengths feature in the probabilistic model, for learning only. Keeping our original model, we will simply multiply into the probability of each tree another factor that penalizes long dependencies, giving: po(x, y) oc po(x, y)·e where y(i</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>B Say</author>
<author>D Z Hakkani-T¨ur</author>
<author>G T¨ur</author>
</authors>
<title>Building a Turkish treebank.</title>
<date>2003</date>
<booktitle>Building and Exploiting Syntactically-Annotated Corpora.</booktitle>
<editor>In A. Abeille, editor,</editor>
<publisher>Kluwer.</publisher>
<marker>Oflazer, Say, Hakkani-T¨ur, T¨ur, 2003</marker>
<rawString>K. Oflazer, B. Say, D. Z. Hakkani-T¨ur, and G. T¨ur. 2003. Building a Turkish treebank. In A. Abeille, editor, Building and Exploiting Syntactically-Annotated Corpora. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rose</author>
</authors>
<title>Deterministic annealing for clustering, compression, classification, regression, and related optimization problems.</title>
<date>1998</date>
<booktitle>Proc. of the IEEE,</booktitle>
<volume>86</volume>
<issue>11</issue>
<contexts>
<context position="10246" citStr="Rose, 1998" startWordPosition="1676" endWordPosition="1677"> is notably better than the EM baseline using the same selection procedure (see Table 3, second column). 4 Structural Annealing The central idea of this paper is to gradually change (anneal) the bias δ. Early in learning, local dependencies are emphasized by setting δ « 0. Then δ is iteratively increased and training repeated, using the last learned model to initialize. This idea bears a strong similarity to deterministic annealing (DA), a technique used in clustering and classification to smooth out objective functions that are piecewise constant (hence discontinuous) or bumpy (non-concave) (Rose, 1998; Ueda and Nakano, 1998). In unsupervised learning, DA iteratively re-estimates parameters like EM, but begins by requiring that the entropy of the posterior pp(y |x) be maximal, then gradually relaxes this entropy constraint. Since entropy is concave in O, the initial task is easy (maximize a concave, continuous function). At each step the optimization task becomes more difficult, but the initializer is given by the previous step and, in practice, tends to be close to a good local maximum of the more difficult objective. By the last iteration the objective is the same as in EM, but the anneal</context>
</contexts>
<marker>Rose, 1998</marker>
<rawString>K. Rose. 1998. Deterministic annealing for clustering, compression, classification, regression, and related optimization problems. Proc. of the IEEE, 86(11):2210–2239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simov</author>
<author>P Osenova</author>
</authors>
<title>Practical annotation scheme for an HPSG treebank of Bulgarian. In</title>
<date>2003</date>
<booktitle>Proc. ofLINC.</booktitle>
<marker>Simov, Osenova, 2003</marker>
<rawString>K. Simov and P. Osenova. 2003. Practical annotation scheme for an HPSG treebank of Bulgarian. In Proc. ofLINC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simov</author>
<author>G Popova</author>
<author>P Osenova</author>
</authors>
<title>HPSGbased syntactic treebank of Bulgarian (BulTreeBank).</title>
<date>2002</date>
<booktitle>A Rainbow of Corpora: Corpus Linguistics and the Languages of the World,</booktitle>
<pages>135--42</pages>
<editor>In A. Wilson, P. Rayson, and T. McEnery, editors,</editor>
<publisher>Lincom-Europa.</publisher>
<marker>Simov, Popova, Osenova, 2002</marker>
<rawString>K. Simov, G. Popova, and P. Osenova. 2002. HPSGbased syntactic treebank of Bulgarian (BulTreeBank). In A. Wilson, P. Rayson, and T. McEnery, editors, A Rainbow of Corpora: Corpus Linguistics and the Languages of the World, pages 135–42. Lincom-Europa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simov</author>
<author>P Osenova</author>
<author>A Simov</author>
<author>M Kouylekov</author>
</authors>
<title>Design and implementation of the Bulgarian HPSG-based Treebank.</title>
<date>2004</date>
<journal>Journal of Research on Language and Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<marker>Simov, Osenova, Simov, Kouylekov, 2004</marker>
<rawString>K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2004. Design and implementation of the Bulgarian HPSG-based Treebank. Journal of Research on Language and Computation, 2(4):495–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Annealing techniques for unsupervised statistical language learning.</title>
<date>2004</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="10995" citStr="Smith and Eisner (2004)" startWordPosition="1802" endWordPosition="1805">at the entropy of the posterior pp(y |x) be maximal, then gradually relaxes this entropy constraint. Since entropy is concave in O, the initial task is easy (maximize a concave, continuous function). At each step the optimization task becomes more difficult, but the initializer is given by the previous step and, in practice, tends to be close to a good local maximum of the more difficult objective. By the last iteration the objective is the same as in EM, but the annealed search process has acted like a good initializer. This method was applied with some success to grammar induction models by Smith and Eisner (2004). In this work, instead of imposing constraints on the entropy of the model, we manipulate bias toward local hypotheses. As δ increases, we penalize long dependencies less. We call this structural annealing, since we are varying the strength of a soft constraint (bias) on structural hypotheses. In structural annealing, the final objective would be the same as EM if our final δ, δf = 0, but we found that annealing farther (δf &gt; 0) works much better.4 Experiment: Annealing δ. We experimented with annealing schedules for δ. We initialized at δ0 E {−1, −0.4, −0.21, and increased δ by 0.1 (in the f</context>
</contexts>
<marker>Smith, Eisner, 2004</marker>
<rawString>N. A. Smith and J. Eisner. 2004. Annealing techniques for unsupervised statistical language learning. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="810" citStr="Smith and Eisner, 2005" startWordPosition="112" endWordPosition="115">s University, Baltimore, MD 21218 USA {nasmith,jason}@cs.jhu.edu Abstract We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems. 1 Introduction Inducing a weighted context-free grammar from flat text is a hard problem. A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979). EM’s mediocre performanc</context>
<context position="3764" citStr="Smith and Eisner, 2005" startWordPosition="578" endWordPosition="581">ls can be found in the appendix. strong preference for short dependencies, then relax the preference. The new approach, structural annealing, often gives superior performance. An alternative structural bias is explored in §5. This approach views a sentence as a sequence of one or more yields of separate, independent trees. The points of segmentation are a hidden variable, and during learning all possible segmentations are entertained probabilistically. This allows the learner to accept hypotheses that explain the sentences as independent pieces. In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias. 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = (x1, x2, ..., xn) be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2,..., n} --, 211,2,...,n1) that map each word to its set</context>
<context position="20172" citStr="Smith and Eisner, 2005" startWordPosition="3412" endWordPosition="3415">d we decoded with a vine parser, we would see a precision\, recall/ curve as ,3 decreased. is not robust to the choice of λ, β0, or βf, nor does it always do as well as annealing δ, although considerable gains are possible; see the fifth column of Table 3. By testing models trained with afixed value of β (for values in [−1,1]), we ascertained that the performance improvement is due largely to annealing, not just the injection of segmentation bias (fourth vs. fifth column of Table 3).8 6 Comparison and Combination with Contrastive Estimation Contrastive estimation (CE) was recently introduced (Smith and Eisner, 2005a) as a class of alternatives to the likelihood objective function locally maximized by EM. CE was found to outperform EM on the task of focus in this paper, when applied to English data (Smith and Eisner, 2005b). Here we review the method briefly, show how it performs across languages, and demonstrate that it can be combined effectively with structural bias. Contrastive training defines for each example xi a class of presumably poor, but similar, instances called the “neighborhood,” N(xi), and seeks to maximize CN(O) = 1: log pp(xi |N(xi)) i log EY pe (xi, y) EX&apos;EN(Xi) EY po (x&amp;quot; y) At this po</context>
<context position="23069" citStr="Smith and Eisner, 2005" startWordPosition="3925" endWordPosition="3928"> DORT1 72.6 DORT1, -0.2 Table 3: Summary comparing models trained in a variety of ways with some relevant hyperparameters. Supervised model selection was applied in all cases, including EM (see the appendix). Boldface marks the best performance overall and trials that this performance did not significantly surpass under a sign test (i.e., p -A 0.05). The score is the F1 measure on non-$ attachments. The fixed S + CE condition was tested only for languages where CE improved over EM. before. Four of the languages have at least one effective CE condition, supporting our previous English results (Smith and Eisner, 2005b), but CE was harmful for Bulgarian and Mandarin. Perhaps better neighborhoods exist for these languages, or there is some ideal neighborhood that would perform well for all languages. Our approach of allowing broken trees (§5) is a natural extension of the CE framework. Contrastive estimation views learning as a process of moving posterior probability mass from (implicit) negative examples to (explicit) positive examples. The positive evidence, as in MLE, is taken to be the observed data. As originally proposed, CE allowed a redefinition of the implicit negative evidence from “all other sent</context>
<context position="27390" citStr="Smith and Eisner (2005" startWordPosition="4625" endWordPosition="4628">tly under a sign test (p &lt; 10−6), across all six languages. While the most common corrections were to nouns, these account for only 25–41% of corrections, indicating that corrections are not “all of the same kind.” Finally, since more than half of corrections in every language involved reattachment to a noun or a verb (content word), we believe the improved models to be getting closer than EM to the deeper semantic relations between words that, ideally, syntactic models should uncover. 8 Future Work One weakness of all recent weighted grammar induction work—including Klein and Manning (2004), Smith and Eisner (2005b), and the present paper—is a sensitivity to hyperparameters, including smoothing values, choice of N (for CE), and annealing schedules—not to mention initialization. This is quite observable in the results we have presented. An obstacle for unsupervised learning in general is the need for automatic, efficient methods for model selection. For annealing, inspiration may be drawn from continuation methods; see, e.g., Elidan and Friedman (2005). Ideally one would like to select values simultaneously for many hyperparameters, perhaps using a small annotated corpus (as done here), extrinsic figure</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005a. Contrastive estimation: Training log-linear models on unlabeled data. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>J Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation.</title>
<date>2005</date>
<booktitle>In Proc. of IJCAI Workshop on Grammatical Inference Applications.</booktitle>
<contexts>
<context position="810" citStr="Smith and Eisner, 2005" startWordPosition="112" endWordPosition="115">s University, Baltimore, MD 21218 USA {nasmith,jason}@cs.jhu.edu Abstract We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004). Next, by annealing the free parameter that controls this bias, we achieve further improvements. We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement. We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this task to date. Our method, structural annealing, is a general technique with broad applicability to hidden-structure discovery problems. 1 Introduction Inducing a weighted context-free grammar from flat text is a hard problem. A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979). EM’s mediocre performanc</context>
<context position="3764" citStr="Smith and Eisner, 2005" startWordPosition="578" endWordPosition="581">ls can be found in the appendix. strong preference for short dependencies, then relax the preference. The new approach, structural annealing, often gives superior performance. An alternative structural bias is explored in §5. This approach views a sentence as a sequence of one or more yields of separate, independent trees. The points of segmentation are a hidden variable, and during learning all possible segmentations are entertained probabilistically. This allows the learner to accept hypotheses that explain the sentences as independent pieces. In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias. 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = (x1, x2, ..., xn) be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2,..., n} --, 211,2,...,n1) that map each word to its set</context>
<context position="20172" citStr="Smith and Eisner, 2005" startWordPosition="3412" endWordPosition="3415">d we decoded with a vine parser, we would see a precision\, recall/ curve as ,3 decreased. is not robust to the choice of λ, β0, or βf, nor does it always do as well as annealing δ, although considerable gains are possible; see the fifth column of Table 3. By testing models trained with afixed value of β (for values in [−1,1]), we ascertained that the performance improvement is due largely to annealing, not just the injection of segmentation bias (fourth vs. fifth column of Table 3).8 6 Comparison and Combination with Contrastive Estimation Contrastive estimation (CE) was recently introduced (Smith and Eisner, 2005a) as a class of alternatives to the likelihood objective function locally maximized by EM. CE was found to outperform EM on the task of focus in this paper, when applied to English data (Smith and Eisner, 2005b). Here we review the method briefly, show how it performs across languages, and demonstrate that it can be combined effectively with structural bias. Contrastive training defines for each example xi a class of presumably poor, but similar, instances called the “neighborhood,” N(xi), and seeks to maximize CN(O) = 1: log pp(xi |N(xi)) i log EY pe (xi, y) EX&apos;EN(Xi) EY po (x&amp;quot; y) At this po</context>
<context position="23069" citStr="Smith and Eisner, 2005" startWordPosition="3925" endWordPosition="3928"> DORT1 72.6 DORT1, -0.2 Table 3: Summary comparing models trained in a variety of ways with some relevant hyperparameters. Supervised model selection was applied in all cases, including EM (see the appendix). Boldface marks the best performance overall and trials that this performance did not significantly surpass under a sign test (i.e., p -A 0.05). The score is the F1 measure on non-$ attachments. The fixed S + CE condition was tested only for languages where CE improved over EM. before. Four of the languages have at least one effective CE condition, supporting our previous English results (Smith and Eisner, 2005b), but CE was harmful for Bulgarian and Mandarin. Perhaps better neighborhoods exist for these languages, or there is some ideal neighborhood that would perform well for all languages. Our approach of allowing broken trees (§5) is a natural extension of the CE framework. Contrastive estimation views learning as a process of moving posterior probability mass from (implicit) negative examples to (explicit) positive examples. The positive evidence, as in MLE, is taken to be the observed data. As originally proposed, CE allowed a redefinition of the implicit negative evidence from “all other sent</context>
<context position="27390" citStr="Smith and Eisner (2005" startWordPosition="4625" endWordPosition="4628">tly under a sign test (p &lt; 10−6), across all six languages. While the most common corrections were to nouns, these account for only 25–41% of corrections, indicating that corrections are not “all of the same kind.” Finally, since more than half of corrections in every language involved reattachment to a noun or a verb (content word), we believe the improved models to be getting closer than EM to the deeper semantic relations between words that, ideally, syntactic models should uncover. 8 Future Work One weakness of all recent weighted grammar induction work—including Klein and Manning (2004), Smith and Eisner (2005b), and the present paper—is a sensitivity to hyperparameters, including smoothing values, choice of N (for CE), and annealing schedules—not to mention initialization. This is quite observable in the results we have presented. An obstacle for unsupervised learning in general is the need for automatic, efficient methods for model selection. For annealing, inspiration may be drawn from continuation methods; see, e.g., Elidan and Friedman (2005). Ideally one would like to select values simultaneously for many hyperparameters, perhaps using a small annotated corpus (as done here), extrinsic figure</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>N. A. Smith and J. Eisner. 2005b. Guiding unsupervised grammar induction using contrastive estimation. In Proc. of IJCAI Workshop on Grammatical Inference Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
<author>M Osborne</author>
<author>A Sarkar</author>
<author>S Clark</author>
<author>R Hwa</author>
<author>J Hockenmaier</author>
<author>P Ruhlen</author>
<author>S Baker</author>
<author>J Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proc. ofEACL.</booktitle>
<contexts>
<context position="18551" citStr="Steedman et al. (2003)" startWordPosition="3127" endWordPosition="3130">egularization. Boldface marks scores better than EM-trained models selected the same way (Table 1). The score is the F1 measure on non-$ attachments. Annealing β resembles the popular bootstrapping technique (Yarowsky, 1995), which starts out aiming for high precision, and gradually improves coverage over time. With strong bias (β » 0), we seek a model that maintains high dependency precision on (non-$) attachments by attaching most tags to $. Over time, as this is iteratively weakened (β -* −oo), we hope to improve coverage (dependency recall). Bootstrapping was applied to syntax learning by Steedman et al. (2003). Our approach differs in being able to remain partly agnostic about each tag’s true parent (e.g., by giving 50% probability to attaching to $), whereas Steedman et al. make a hard decision to retrain on a whole sentence fully or leave it out fully. In earlier work, Brill and Marcus (1992) adopted a “local first” iterative merge strategy for discovering phrase structure. Experiment: Annealing β. We experimented with different annealing schedules for β. The initial value of β, β0, was one of {−1 , 0, 2 }. After EM training, β was diminished by 10;this was repeated down to a value of βf = −3. Pe</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proc. ofEACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueda</author>
<author>R Nakano</author>
</authors>
<title>Deterministic annealing EM algorithm.</title>
<date>1998</date>
<journal>Neural Networks,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="10270" citStr="Ueda and Nakano, 1998" startWordPosition="1678" endWordPosition="1681">better than the EM baseline using the same selection procedure (see Table 3, second column). 4 Structural Annealing The central idea of this paper is to gradually change (anneal) the bias δ. Early in learning, local dependencies are emphasized by setting δ « 0. Then δ is iteratively increased and training repeated, using the last learned model to initialize. This idea bears a strong similarity to deterministic annealing (DA), a technique used in clustering and classification to smooth out objective functions that are piecewise constant (hence discontinuous) or bumpy (non-concave) (Rose, 1998; Ueda and Nakano, 1998). In unsupervised learning, DA iteratively re-estimates parameters like EM, but begins by requiring that the entropy of the posterior pp(y |x) be maximal, then gradually relaxes this entropy constraint. Since entropy is concave in O, the initial task is easy (maximize a concave, continuous function). At each step the optimization task becomes more difficult, but the initializer is given by the previous step and, in practice, tends to be close to a good local maximum of the more difficult objective. By the last iteration the objective is the same as in EM, but the annealed search process has ac</context>
</contexts>
<marker>Ueda, Nakano, 1998</marker>
<rawString>N. Ueda and R. Nakano. 1998. Deterministic annealing EM algorithm. Neural Networks, 11(2):271–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase structure annotation of a large corpus.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>10</volume>
<issue>4</issue>
<marker>Xue, Xia, Chiou, Palmer, 2004</marker>
<rawString>N. Xue, F. Xia, F.-D. Chiou, and M. Palmer. 2004. The Penn Chinese Treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 10(4):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="18153" citStr="Yarowsky, 1995" startWordPosition="3062" endWordPosition="3063">among values of a2 and Off°) worst unsup. sup. oracle 32.5 59.3 63.4 63.4 30.5 56.4 57.3 57.8 20.9 56.6 57.4 57.4 29.1 37.2 46.2 46.2 19.4 26.0 40.5 43.1 25.1 35.3 38.3 38.3 9.4 24.2 41.1 41.1 13.7 17.9 26.2 26.2 7.3 38.6 58.2 58.2 21.5 34.1 55.5 55.5 35.0 59.8 71.8 71.8 30.8 33.6 33.6 33.6 Table 2: Performance of CE on test data, for different neighborhoods and with different levels of regularization. Boldface marks scores better than EM-trained models selected the same way (Table 1). The score is the F1 measure on non-$ attachments. Annealing β resembles the popular bootstrapping technique (Yarowsky, 1995), which starts out aiming for high precision, and gradually improves coverage over time. With strong bias (β » 0), we seek a model that maintains high dependency precision on (non-$) attachments by attaching most tags to $. Over time, as this is iteratively weakened (β -* −oo), we hope to improve coverage (dependency recall). Bootstrapping was applied to syntax learning by Steedman et al. (2003). Our approach differs in being able to remain partly agnostic about each tag’s true parent (e.g., by giving 50% probability to attaching to $), whereas Steedman et al. make a hard decision to retrain o</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proc. ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>