<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001367">
<title confidence="0.995855">
Discriminative Substring Decoding for Transliteration
</title>
<author confidence="0.963638">
Colin Cherry and Hisami Suzuki
</author>
<affiliation confidence="0.940684">
Microsoft Research
</affiliation>
<address confidence="0.944474">
One Microsoft Way
Redmond, WA, 98052
</address>
<email confidence="0.999355">
{colinc,hisamis}@microsoft.com
</email>
<sectionHeader confidence="0.99481" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807272727273">
We present a discriminative substring de-
coder for transliteration. This decoder
extends recent approaches for discrimi-
native character transduction by allow-
ing for a list of known target-language
words, an important resource for translit-
eration. Our approach improves upon
Sherif and Kondrak’s (2007b) state-of-the-
art decoder, creating a 28.5% relative im-
provement in transliteration accuracy on
a Japanese katakana-to-English task. We
also conduct a controlled comparison of
two feature paradigms for discriminative
training: indicators and hybrid generative
features. Surprisingly, the generative hy-
brid outperforms its purely discriminative
counterpart, despite losing access to rich
source-context features. Finally, we show
that machine transliterations have a posi-
tive impact on machine translation quality,
improving human judgments by 0.5 on a
4-point scale.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999779320754717">
Transliteration occurs when a word is borrowed
into a language with a different character set.
The word is transcribed into the new character
set in such a way as to maintain rough phonetic
correspondence; for example, the English word
hip-hop becomes 2#7;#7 [hippuhoppu],
when transliterated into Japanese. A task fre-
quently of interest to the NLP community is back-
transliteration, where one seeks the original word,
given the borrowed form.
We investigate machine transliteration as a
method to handle out-of-vocabulary items in a
Japanese-to-English translation system. More
often than not, this will correspond to back-
transliteration. Our goal is to prevent the copy-
ing or deletion of Japanese words when they are
missing from our statistical machine translation
(SMT) system’s translation tables. This can have
a substantial impact on the quality of SMT output,
transforming translations of questionable useful-
ness, such as:
Avoid using a 5J—A—K account.1
into the far more informative:
Avoid using a Freemail account.
Though the techniques we present here are
language-independent, we focus this study on
the task of Japanese katakana-to-English back-
transliteration. Katakana is one of the four char-
acter types used in the Japanese writing system
(along with hiragana, kanji and Roman alpha-
bet), consisting of about 50 syllabic characters.
It is used primarily to spell foreign loanwords
(e.g., !G:1L—( [chokoreeto] — chocolate),
and names (e.g., �JS(S [kurinton] — Clin-
ton). Therefore, katakana is a strong indicator
that a Japanese word can be back-transliterated.
However, katakana can also be used to spell sci-
entific names of animals and plants (e.g., t✓B
[kamo] — duck), onomatopoeic expressions (e.g.,
0~C0~C [bashabasha] — splash) and for-
eign origin words that are not transliterations (e.g.,
;!--�-7, [hochikisu] — stapler). These un-
transliterable cases constitute about 10% of the
katakana words in our data.
We employ a discriminative substring decoder
for machine transliteration. Following Sherif and
Kondrak (2007b), the decoder operates on short
source substrings, with each operation producing
one or more target characters, as shown in Fig-
ure 1. However, where previous approaches em-
ploy generative modeling, we use structured per-
ceptron training to discriminatively tune parame-
ters according to 0-1 transliteration accuracy. This
</bodyText>
<footnote confidence="0.897262">
15J—A—K is romanized as [furiimeeru]
</footnote>
<page confidence="0.785084">
1066
</page>
<note confidence="0.997852">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1066–1075,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.490672">
F A ソン
tho m son
</figure>
<figureCaption confidence="0.999724">
Figure 1: Example substring derivation
</figureCaption>
<bodyText confidence="0.979820166666666">
allows us to test novel methods for the use of tar-
get lexicons in discriminative character transduc-
tion, allowing our decoder to benefit from a list of
known target words. Perhaps more significantly,
our framework allows us to test two competing
styles of features:
</bodyText>
<listItem confidence="0.715341333333333">
• sparse indicators, designed to capture the
same channel and language modeling data
collected by previous generative models, and
• components of existing generative models,
used as real-valued features in a discrimina-
tively weighted, generative hybrid.
</listItem>
<bodyText confidence="0.999598433333333">
Note that generative hybrids are the norm in
SMT, where translation scores are provided by
a discriminative combination of generative mod-
els (Och, 2003). Substring-based transliteration
with a generative hybrid model is very similar to
existing solutions for phrasal SMT (Koehn et al.,
2003), operating on characters rather than words.
Unlike out-of-the-box phrasal SMT solutions, our
generative hybrid benefits from a target a lexicon.
As we will show, this is the difference between a
weak baseline and a strong competitor.
We demonstrate that despite recent successes in
discriminative character transduction using indi-
cator features (Jiampojamarn et al., 2008; Dreyer
et al., 2008), our generative hybrid performs sur-
prisingly well, producing our highest translitera-
tion accuracies. Researchers frequently compare
against a phrasal SMT baseline when evaluating a
new transduction technique (Freitag and Khadivi,
2007; Dreyer et al., 2008); however, we are careful
to vary only the features in our comparison. Con-
founding variables, such as alignment, decoder
and training method, are held constant.
We also include a human evaluation of
transliteration-augmented SMT output. Though
human evaluations are too expensive to allow a
comparison between transliteration systems, we
are able to show that adding our transliterations
to a production-level SMT engine results in a sub-
stantial improvement in translation quality.
</bodyText>
<sectionHeader confidence="0.95776" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999762">
This work draws inspiration from previous work
in transliteration, which we divide into similarity
and transduction-based approaches. We also dis-
cuss recent successes in discriminative character
transduction that have influenced this work.
</bodyText>
<subsectionHeader confidence="0.977401">
2.1 Similarity-based transliteration
</subsectionHeader>
<bodyText confidence="0.999927697674419">
In similarity-based transliteration, a character-
based, cross-lingual similarity metric is calculated
(or bootstrapped) from known transliteration pairs.
Given a source word s, its transliteration is the tar-
get word t most similar to s, where t is drawn from
some pool of candidates. This approach may also
be referred to as transliteration discovery.
Brill et al. (2001) describe a katakana-to-
English approach with an EM-learned edit dis-
tance, which bootstraps from a small number of
examples to learn transliteration pairs from query
logs. Bilac and Tanaka (2005) harvest translitera-
tion candidates from comparable bilingual corpora
(conference abstracts in English and Japanese),
and use distributional as well as phonetic simi-
larity to choose among them. Sherif and Kon-
drak (2007a) also bootstrap a learned edit dis-
tance for Arabic named entities, with candidate
pairs drawn from sentence or document-aligned
parallel text. Klementiev and Roth (2006) boot-
strap an SVM classifier trained to detect true
transliteration-pairs. They draw candidates from
comparable news text, using date information to
provide further clues as to aligned named entities.
Bergsma and Kondrak (2007) extend the classifi-
cation approach with features derived from a char-
acter alignment. They train from bilingual dic-
tionaries and word-aligned parallel text, selecting
negative examples to target false-friends.
The work of Hermjakob et al. (2008) is par-
ticularly relevant to this paper, as they incorpo-
rate a similarity-based transliteration system into
an Arabic-to-English SMT engine. They employ
a hand-crafted cross-lingual similarity metric, and
use capitalized n-grams from the Google n-gram
corpus as candidates. With such a huge candidate
list, a cross-lingual indexing scheme is designed
for fast candidate look-up. Their work also ad-
dresses the question of when to transliterate (as
opposed to translate), a realistic concern when de-
ploying a transliteration component in SMT. This,
however, is not of so much concern for katakana,
as it is used primarily for loanwords.
</bodyText>
<page confidence="0.983535">
1067
</page>
<subsectionHeader confidence="0.972384">
2.2 Transduction-based transliteration
</subsectionHeader>
<bodyText confidence="0.99849675">
The approach presented in this paper is an instance
of transduction-based transliteration, where the
source word is transformed into a target word us-
ing a sequence of character-level operations. The
parameters of the transduction process are learned
from a collection of transliteration pairs. These
systems do not require a list of candidates, but
many incorporate a target lexicon, favoring target
words that occur in the lexicon. This approach is
also known as transliteration generation.
The majority of transliteration generation ap-
proaches are based on the noisy channel model,
where a target t is generated according to
P(t|s) a P(s|t)P(t). This approach is typi-
fied by finite-state transliteration, where the var-
ious stages of the channel model are represented
by finite state transducers and automata. Early
systems employed a complex channel, passing
through multiple phonetic representations (Knight
and Graehl, 1998; Bilac and Tanaka, 2004), but
later versions replaced characters directly (Al-
Onaizan and Knight, 2002). Sherif and Kondrak
(2007b) extend this approach with substring oper-
ations in the style of phrasal SMT, and show that
doing so improves both accuracy as well as space
and time efficiency. Note that it is possible to in-
corporate a target lexicon by making P(t) a word
unigram model with a character-based back-off.
Li et al. (2004) present an alternative to the
noisy channel with their joint n-gram model,
which calculates P(s, t). This formulation allows
operations to be conditioned on both source and
target context. However, the inclusion of a candi-
date list is more difficult in this setting, as P(t) is
not given its own model.
Zelenko and Aone (2006) investigate a purely
discriminative, alignment-free approach to
transliteration generation. The target word is
constructed one character at a time, with each
new character triggering a suite of features,
including indicators for near-by source and target
characters, as well a generative target language
model. Freitag and Khadivi (2007) propose a dis-
criminative, latent edit distance for transliteration.
In this case, training data need not be aligned in
advance, but a latent alignment is produced during
decoding. Again, the target word is constructed
one character at a time, using edit operations
that are scored according to source and target
context features. Both approaches train using a
structured perceptron, as we do here. However,
these models represent a dramatic departure from
the existing literature, while ours has clear analogs
to the well-known noisy-channel paradigm, which
allows for useful comparisons and insights into
the advantages of discriminative training.
</bodyText>
<subsectionHeader confidence="0.996285">
2.3 Discriminative character transduction
</subsectionHeader>
<bodyText confidence="0.999988230769231">
While our chosen application is transliteration,
our decoder is influenced by recent successes in
general-purpose discriminative transduction. Ji-
ampojamarn et al. (2008) describe a discrimina-
tive letter-to-phoneme substring transducer, while
Dreyer et al. (2008) describe a discriminative char-
acter transducer with a latent derivation structure
for morphological transformations. Both models
are extremely effective, but both rely exclusively
on indicator features; they do not explore the use
of knowledge-rich generative models. Our indica-
tor system uses an extended version of the Jiampo-
jamarn et al. (2008) feature set.
</bodyText>
<sectionHeader confidence="0.997101" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.9999208">
We adopt a discriminative substring decoder for
our transliteration task. A structured percep-
tron (Collins, 2002) learns weights for our translit-
eration features, which are drawn from two broad
classes: indicator and hybrid generative features.
</bodyText>
<subsectionHeader confidence="0.999115">
3.1 Structured perceptron
</subsectionHeader>
<bodyText confidence="0.999994888888889">
The decoder’s discriminative parameters are
learned with structured perceptron training. Let
a derivation d describe a substring operation se-
quence that transliterates a source word into a tar-
get word. Given an input training corpus of such
derivations D = d1 ... dn, a vector feature func-
tion on derivations F�(d), and an initial weight vec-
tor w, the perceptron performs two steps for each
training example di E D:
</bodyText>
<equation confidence="0.985686">
� �
d� = argmaxd�D(src(di)) �� · F� (d)
</equation>
<listItem confidence="0.899714">
• Update: w = w + F�(di) − F�(�d)
</listItem>
<bodyText confidence="0.9266529">
where D(src(d)) enumerates all possible deriva-
tions with the same source side as d. To improve
generalization, the final feature vector is the aver-
age of all vectors found during learning (Collins,
2002). Accuracy on the development set is used
to select the number of times we pass through all
di E D.
Given the above framework, we require training
�
derivations D, feature vectors F, and a decoder to
</bodyText>
<listItem confidence="0.983265">
• Decode:
</listItem>
<page confidence="0.939273">
1068
</page>
<bodyText confidence="0.992503666666667">
carry out the argmax over all d reachable from a
particular source word. We describe each of these
components in turn below.
</bodyText>
<subsectionHeader confidence="0.999705">
3.2 Training derivations
</subsectionHeader>
<bodyText confidence="0.999975454545455">
Note that the above framework describes a max-
derivation decoder trained on a corpus of gold-
standard derivations, as opposed to a max-
transliteration decoder trained directly on source-
target pairs. By building the entire system on the
derivation level, we side-step issues that can oc-
cur when perceptron training with hidden deriva-
tions (Liang et al., 2006), but we also introduce the
need to transform our training source-target pairs
into training derivations.
Training derivations can be learned unsu-
pervised from source-target pairs using char-
acter alignment techniques. Previously, this
has been done using an EM-learned edit dis-
tance (Ristad and Yianilos, 1998), or generaliza-
tions thereof (Brill and Moore, 2000; Jiampoja-
marn et al., 2007). We opt for an alternative align-
ment technique, similar to the word-aligner de-
scribed by Zhang et al. (2008). This approach
employs variational EM with sparse priors, along
with hard length limits, to reduce the length of
substrings operated upon. By doing so, we hope to
learn only non-compositional transliteration units.
Our aligner produces only monotonic align-
ments, and does not allow either the source or tar-
get side of an operation to be empty. The same
restrictions are imposed during decoding. In this
way, each alignment found by variational EM is
also an unambiguous derivation. We align our
training corpus with a maximum substring length
of three characters. The same derivations are used
to train all of the transliteration systems tested in
this paper.
</bodyText>
<subsectionHeader confidence="0.97823">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999153272727273">
We employ two main types of features: indicators
and hybrid generative models. Indicators detect
binary events in a derivation, such as the presence
of a particular operation. Hybrid generative fea-
tures assign a real-valued probability to a deriva-
tion, based on statistics collected from training
derivations. There are few generative features and
each carries a substantial amount of information,
while indicators are sparse and knowledge-poor.
We treat these two classes of features as distinct.
We do so because researchers often use either one
approach or the other.2 Furthermore, it is not
clear how to optimally employ training derivations
when combining generative models and sparse in-
dicators: generative models need large amounts of
data to collect statistics and relatively little for per-
ceptron training,3 while sparse indicators require
only a large perceptron training set.
We can further divide feature space according
to the information required to calculate each fea-
ture. Both feature sets can be partitioned into the
following subtypes:
</bodyText>
<listItem confidence="0.993175285714286">
• Emission: How accurate are the operations
used by this derivation?
• Transition: Does the target string produced
by this derivation look like a well-formed tar-
get character sequence?
• Lexicon: Does the target string contain
known words from a target lexicon?
</listItem>
<sectionHeader confidence="0.586731" genericHeader="method">
Indicator Features
</sectionHeader>
<bodyText confidence="0.99996975">
Previous approaches to discriminative character
transduction tend to employ only sparse indica-
tors (Jiampojamarn et al., 2008; Dreyer et al.,
2008). This is because sparsity is not a major con-
cern in character-based domains, and sparse indi-
cators are extremely flexible.
Our emission and transition indicator features
follow Jiampojamarn et al. (2008). Emission indi-
cators are centered around an operation, such as
[1 —* tho]. Minimally, an indicator exists for
each operation. Many more source context fea-
tures can be generated by conjoining an operation
with source n-grams found within a fixed win-
dow of C characters to either side of the operation.
These source context features have minimal com-
putational cost, and they allow each operator to ac-
count for large, overlapping portions of the source,
even when the substrings being operated upon are
small. Meanwhile, transition indicators stand in
for a character-based target language model. Indi-
cators are built for each possible target n-gram, for
n = 1... K, allowing the perceptron to construct
a discriminative back-off model. Development ex-
periments lead us to select C = 3 and K = 5.
</bodyText>
<footnote confidence="0.978544142857143">
2Generative hybrids are often accompanied by a small
number of unsparse indicators, such as operation count.
3Perceptron training on the same data used for model
construction can lead to overconfidence in model quality.
One can address this problem by using a large number of
modeling-training folds (Collins et al., 2005), but we do not
do so here.
</footnote>
<page confidence="0.991944">
1069
</page>
<bodyText confidence="0.999929">
Indicator lexicon features are novel to this work.
Given access to a target lexicon with type fre-
quencies, we opt to create features that indicate
the frequencies of generated target words accord-
ing to coarse bins. Experiments on our develop-
ment set lead to the selection of 5 frequency bins:
[&lt; 2,000], [&lt; 200], [&lt; 20], [&lt; 2], [&lt; 1]. To keep
the model linear, these features are cumulative;
thus, generating a word with frequency 126 will
result in both the [&lt; 2, 000] and [&lt; 200] features
firing. Note that a single transliteration can po-
tentially generate multiple target words, and doing
so can have a major impact on how often the lex-
icon features fire. Thus, we employ another fea-
ture that indicates the introduction of a new word.
We expect these frequency indicators to be supe-
rior to a word-level unigram model, as they allow
the designer to select notable frequencies. In par-
ticular, the bins we have selected do not give any
advantage to extremely common words, as these
are generally less likely to be transliterated.
</bodyText>
<sectionHeader confidence="0.734431" genericHeader="method">
Hybrid Generative Features
</sectionHeader>
<bodyText confidence="0.998461666666667">
We begin with the three components of the gener-
ative noisy channel employed by Sherif and Kon-
drak (2007b). Their transliteration probability is:
</bodyText>
<equation confidence="0.992364">
P(t|s) a PE(s|t) · max [PT (t), PL(t)] (1)
</equation>
<bodyText confidence="0.999523333333333">
Inspired by the linear models used in SMT (Och,
2003), we can discriminatively weight the compo-
nents of this generative model, producing:
</bodyText>
<equation confidence="0.90709">
wE log PE(s|t) + wT log PT (t) + wL log PL(t)
</equation>
<bodyText confidence="0.992931978723405">
with weights w learned by perceptron training.
These three models conveniently align with our
three feature subtypes. Emission information is
provided by PE(s|t), which is estimated by maxi-
mum likelihood on the operations observed in our
training derivations. Including source context is
difficult in such a model. To compensate for this,
all systems using PE(s|t) also use composed op-
erations, which are constructed from operation se-
quences observed in the training set. This removes
the length limit on substring operations.4 PT(t)
provides transition information through a charac-
ter language model, estimated on the target side
4Derivations built by our character aligner use opera-
tions on substrings of maximum length 3. To enable per-
ceptron training with composed operations, once PE(s|t)
has been estimated by counting composed operations in the
initial alignments, we re-align our training examples with
those composed operations to maximize PE(slt), creating
new training derivations.
of the training derivations. In our implementation,
we employ a KN-smoothed 7-gram model (Kneser
and Ney, 1995). Finally, PL(t) is a unigram tar-
get word model, estimated from the same type fre-
quencies used to build our lexicon indicators.
Since we have adopted a linear model, we are
no longer constrained by the original generative
story. Therefore, we are free to incorporate other
SMT-inspired features: PES(t|s), target character
count, and operation count.5
Feature summary
The indicator and hybrid-generative feature sets
each provide a discriminative version of the noisy
channel model. In the case of transition and lexi-
con features, both systems have access to the ex-
act same information, but encode that information
differently. The lexicon encoding is the most dra-
matic difference, with the indicators using a small
number of frequency bins, and the generative uni-
gram model providing a single, real-valued feature
that is proportional to frequency.
In the case of their emission features, the
two systems actually encode different information.
Both have access to the same training derivations,
but the indicator system provides source context
through n-gram indicators, while the generative
system does so using composed operations.
</bodyText>
<subsectionHeader confidence="0.610263">
3.4 Decoder
</subsectionHeader>
<bodyText confidence="0.9997054">
Our decoder builds upon machine translation’s
monotone phrasal decoding (Zens and Ney, 2004),
or equivalently, the sequence tagging algorithm
used in semi-Markov CRFs (Sarawagi and Co-
hen, 2004). This dynamic programming (DP) de-
coder extends the Viterbi algorithm for HMMs
by operating on one or more source characters (a
substring) at each step. A DP block stores the
best scoring solution for a particular prefix. Each
block is subdivided into cells, which maintain the
context necessary to calculate target-side features.
We employ a beam, keeping only the 40 highest-
scoring cells for each block, which speeds up in-
ference at the expense of optimality. We found
that the beam had no major effect on perceptron
training, nor on the system’s final accuracy.
Previously, target lexicons have been used
primarily in finite-state transliteration, as they
are easily encoded as finite-state-acceptors (Al-
Onaizan and Knight, 2002; Sherif and Kondrak,
</bodyText>
<footnote confidence="0.8485695">
5Character and operation counts also fit in the indicator
system, but did not improve performance in development.
</footnote>
<page confidence="0.995619">
1070
</page>
<bodyText confidence="0.99985525">
2007b). It is possible to extend the DP decoder to
also use a target lexicon. By encoding the lexicon
as a trie, and adding the trie index to the context
tracked by the DP cells, we can provide access to
frequency estimates for words and word prefixes.
This has the side-effect of creating a new cell for
each target prefix; however, in the character do-
main, this remains computationally tractable.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="method">
4 Data
</sectionHeader>
<subsectionHeader confidence="0.999888">
4.1 Wikipedia training and test data
</subsectionHeader>
<bodyText confidence="0.9998643">
Our katakana-to-English training data is de-
rived from bilingually-linked Wikipedia titles.
Any Japanese Wikipedia article with an entirely
katakana title and a linked English article results
in training pair. This results in 60K transliteration
pairs; we removed 2K pairs for development, and
2K for held-out testing.
The remaining 56K training pairs are quite
noisy. As mentioned earlier, roughly 10% of our
examples are simply not transliterable, but ap-
proximate Wikipedia title translations are an even
more substantial source of noise. For example,
:1:/1 -Y7-1-\ [konpyuutageemu] — com-
puter game is aligned with the English article
Computer and video games. We found it ben-
eficial, in terms of both speed and accuracy, to
do some coarse alignment-based pruning. After
alignment, the operations used by all derivations
are counted. Any operation that is used fewer than
three times is eliminated, along with any deriva-
tion using that operation. The goal is to eliminate
loose transliteration pairs from our data, where a
word or initial is included in one language but
not the other. This results in 40K training pairs.
Despite the noise in the Wikipedia data, there are
clear advantages in using it for training transliter-
ation models: it is available for any language pair,
it reflects recent trends and events, and the amount
of data increases daily. As we will see below, the
model trained on this data performs well on a test
set from a very different domain.
All systems use development set accuracy to
select their meta-parameters, such as the number
of perceptron iterations, the size of the source-
context window, and the n-gram length used in
character language modeling. The hybrid gener-
ative system further splits the training set, using
38K derivations for the calculation of its emission
and transition models, and 2K derivations for per-
ceptron training its model weights.
</bodyText>
<subsectionHeader confidence="0.966095">
4.2 Machine translation test data
</subsectionHeader>
<bodyText confidence="0.999994529411765">
In order to see how effective our transliterator
is on out-of-domain test data, we also created
test data from a log of translation requests to
a web-based, Japanese-to-English translation ser-
vice.6 Out of 5,000 randomly selected transla-
tion requests, there are 312 cases where katakana
source words are out-of-vocabulary for the MT
system, and therefore remain untranslated. We
created a reference translation (not necessarily a
transliteration) for these katakana words by man-
ually selecting the corresponding English word(s)
in the sentence-level reference translation, which
was produced independently from this experiment.
This test set is quite divergent from the Wikipedia
titles: only 17 (5.5%) of its katakana words are
found in the Wikipedia training data, and six of
these did not agree on the English translation.
</bodyText>
<subsectionHeader confidence="0.993436">
4.3 English lexicon
</subsectionHeader>
<bodyText confidence="0.9999813">
Our English lexicon is derived from two over-
lapping data sources: the English gigaword cor-
pus (LDC2003T05; GW) and the language model
training data for our SMT system, which contains
selections from Europarl, gigaword, and web-
harvested text. Both are lowercased. We com-
bine the unigram frequency counts from the two
sources by taking the max when they overlap. The
resulting lexicon has 5M types, 2.5M of which
have frequency 1.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999985125">
In this section, we summarize development exper-
iments, and then conduct a comparison on our two
transliteration test sets. We report 0-1 accuracy: a
transliteration is only correct if it exactly matches
the reference. For the comparison experiments,
we also report 10-best accuracy, where a system
is correct if it includes the correct transliteration
somewhere in its 10-best list.
</bodyText>
<subsectionHeader confidence="0.988257">
5.1 Baselines
</subsectionHeader>
<bodyText confidence="0.99994925">
We compare our systems against a re-
implementation of Sherif and Kondrak’s (2007b)
noisy-channel substring decoder. This uses the
same PE, PT and PL models as our hybrid gen-
erative system, but employs a two-pass decoding
scheme to find the max transliteration according
to Equation 1. It represents a purely generative
solution using otherwise identical architecture.
</bodyText>
<footnote confidence="0.993311">
6http://www.microsofttranslator.com
</footnote>
<page confidence="0.997591">
1071
</page>
<bodyText confidence="0.9999706">
Since our hybrid generative system implements
a model that is very similar to those used in phrasal
SMT, we also compare against a state-of-the-art
phrasal SMT system (Moore and Quirk, 2007).
This system is trained by applying the standard
SMT pipeline to our Wikipedia title pairs, treat-
ing characters as words, using a 7-gram character-
level language model, and disabling re-ordering.
Unfortunately, the decoder’s architecture does not
allow the use of a word-level unigram model, re-
ducing the usefulness of this baseline. Instead, we
include the target lexicon as a second character-
level language model. This baseline indicates the
level of performance one can expect by applying
phrasal SMT straight out of the box.
Comparing the two baselines qualitatively, both
use a combination of generative models inspired
by the noisy channel. Sherif and Kondrak em-
ploy a word-level unigram model without discrim-
inatively weighting the models, while the Phrasal
SMT approach uses weights derived from max-
BLEU training without word-level unigrams. The
obvious question of what happens when one does
both will be answered by our hybrid generative
system.
</bodyText>
<subsectionHeader confidence="0.99975">
5.2 Development experiments
</subsectionHeader>
<bodyText confidence="0.999960375">
Table 1 shows development set accuracy for a
number of systems and feature types, along with
the model size of the corresponding systems,
where size is measured in terms of the number of
non-zero discriminatively-trained parameters. The
accuracy of the Sherif and Kondrak baseline is
shown as SK07. Despite its lack of discrimina-
tive training, word-level unigrams allow the SK07
baseline to outperform Phrasal SMT. In future ex-
periments, we compare only against SK07.
The indicator system was tested using only op-
eration indicators, with source context, transition
and lexicon indicators added incrementally. All
feature types have a substantial impact, with the
lexicon providing the boost needed to surpass the
baseline. Note that the inclusion of the five fre-
quency bins is sufficient to decrease the overall
feature count of the system by 600K, as much
fewer mistakes are made during training.
Development of the hybrid generative system
used the SK07 baseline as a starting point. The re-
sult of combining its three components into a flat
linear model, with all weights set to 1, is shown
in Table 1 as Linear SK07. This violation of
</bodyText>
<tableCaption confidence="0.997872">
Table 1: Development accuracy and model size
</tableCaption>
<table confidence="0.999670166666667">
System Acc. Size
Baseline Phrasal SMT 30.7 8
SK07 33.5 –
Indicator Operations only 3.6 6.8K
+ source context 23.9 2.8M
+ transition 28.6 3.1M
+ lexicon 44.2 2.5M
+ gen. lexicon 44.1 3.0M
Generative Linear SK07 31.7 –
+ perceptron 42.4 3
+ SMT features 44.1 6
+ ind. lexicon 44.3 12
</table>
<bodyText confidence="0.999785285714286">
conditional independence assumptions results in a
drop in accuracy. However, the + perceptron line
shows that setting the three weights with percep-
tron training results in a huge boost in accuracy,
nearly matching our indicator system. Adding fea-
tures inspired by SMT, such as PE,(tls), elimi-
nates the gap between the two.
</bodyText>
<subsectionHeader confidence="0.998439">
5.3 Development discussion
</subsectionHeader>
<bodyText confidence="0.999732111111111">
Considering their differences, the two systems’
proximity in score is quite surprising. Given the
character domain’s lack of sparsity, and the large
amount of available training data, we had expected
the hybrid generative system to behave only as
a strong baseline; instead, it matched the perfor-
mance of the indicator system. However, this
is not unprecedented: discriminatively weighted
generative models have been shown to outperform
purely discriminative competitors in various NLP
classification tasks (Raina et al., 2004; Toutanova,
2006), and remain the standard approach in statis-
tical translation modeling (Och, 2003).
Examining the development results on an
example-by-example basis, we see that the two
systems make mostly the same mistakes: for 87%
of examples, either both systems are right, or both
are wrong. The remainder represents a (relatively
small) opportunity to improve through system or
feature combination: an oracle that perfectly se-
lects between the two scores 50.6.
One opportunity for straight-forward combina-
tion is the target lexicon. Because lexicon frequen-
cies are drawn from an independent word list, and
not the transliteration training derivations, there is
no reason why both systems cannot use both lex-
icon representations. Unfortunately, doing so has
</bodyText>
<page confidence="0.998882">
1072
</page>
<tableCaption confidence="0.994725">
Table 2: Test set comparisons
</tableCaption>
<table confidence="0.9973362">
Wikipedia MT
System Acc. Top 10 Acc. Top 10
SK07 33.5 57.9 38.8 57.0
Generative 43.0 65.6 42.9 58.3
Indicator 42.5 63.5 43.6 57.7
</table>
<bodyText confidence="0.999185888888889">
little impact, as is shown in each system’s final row
in Table 1. Adding the word unigram model to
the indicator system results in slightly lower per-
formance, and a much larger model. Adding the
frequency bins to the generative system does im-
prove performance slightly, but attempts to com-
pletely replace the generative system’s word uni-
gram model with frequency bins resulted in a sub-
stantial drop in accuracy.7
</bodyText>
<subsectionHeader confidence="0.999657">
5.4 Test set comparisons
</subsectionHeader>
<bodyText confidence="0.988953285714286">
Table 2 shows the accuracies of the systems se-
lected during development on our testing data. On
the held-out Wikipedia examples, the trends ob-
served during development remain the same, with
the generative system expanding its lead. Mov-
ing to 10-best accuracies changes little, except for
slightly narrowing the gap between SK07 and the
discriminative systems.
The second column of Table 2 compares the
systems on our MT test set. As discussed ear-
lier, this data is quite different from the Wikipedia
training set, and as a result, the systems’ differ-
ences are less pronounced. 1-best accuracy still
shows the discriminative systems having a definite
advantage, but at the 10-best level, those distinc-
tions are muted.
Compared with the previous work on katakana-
to-English transliteration, these accuracies do not
look particularly high: both Knight and Graehl
(1998) and Bilac and Tanaka (2004) report accu-
racies above 60% for 1-best transliteration. We
should emphasize that this is due to the difficulty
of our test data, and that we have tested against a
baseline that has been shown to outperform Knight
and Graehl (1998). The test data was not filtered
for noise, leaving untransliterable cases and loose
translations intact. The accuracies reported above
are under-estimates of real performance: many
transliterations not matching the reference may
still be useful to a human reader, such as differ-
7Lexicon replacement experiment is not shown in Table 1.
ences in inflection (e.g., L-f-)-fF [rechinoido]
— retinoids, transliterated as retinoid), and spac-
ing (e.g. : /s7Lt-�, [shierareone]— Sierra
Leone, transliterated as sierraleone).
</bodyText>
<sectionHeader confidence="0.981227" genericHeader="method">
6 Integration with machine translation
</sectionHeader>
<bodyText confidence="0.999775625">
We used the transliterations from our indicator
system to augment a Japanese-to-English MT sys-
tem.8 This treelet-based SMT system (Quirk et
al., 2005) is trained on about 4.6M parallel sen-
tence pairs from diverse sources including bilin-
gual books, dictionaries and web publications.
Our goal is to measure the impact of machine
transliterations on end-to-end translation quality.
</bodyText>
<subsectionHeader confidence="0.980279">
6.1 Evaluation method
</subsectionHeader>
<bodyText confidence="0.999997384615385">
We use the MT-log translation pairs described
in Section 4.2 as a sentence-level translation test
set. For each katakana word left untranslated by
the baseline SMT engine, we generated 10-best
transliteration candidates and added the katakana-
English pairs to the SMT system’s translation ta-
ble. Perceptron scores were exponentiated, then
normalized, to create probabilities, which were
given to the SMT system as P (source|target);9
all other translation features were set to log 1.
We translated the test set with and without the
augmented translation table. 120 sentences were
randomly selected from the cases where the trans-
lations output by the two SMT systems differed,
and were submitted for two types of human evalu-
ation. In the absolute evaluation, each SMT out-
put was assigned a score between 1 and 4 (1 =
completely useless; 4 = perfect translation); in the
relative evaluation, the evaluators were presented
with a pair of SMT outputs, with and without the
transliteration table, and were asked to judge if
they preferred one translation over the other. In
both evaluation settings, the machine-translated
sentences were evaluated by two native speakers
of English who have no knowledge of Japanese,
with access to a reference translation.
</bodyText>
<subsectionHeader confidence="0.778302">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.9998735">
The evaluation results show that our translitera-
tor does improve the quality of SMT. The BLEU
</bodyText>
<footnote confidence="0.9988455">
8The human evaluation was carried out before we discov-
ered the effectiveness of the hybrid generative system, but
recall that the performance of the two is similar.
9The perceptron scores are more naturally interpreted as
P(target|source), but the opposite direction is generally the
highest-weighted feature in the SMT system’s linear model.
</footnote>
<page confidence="0.993525">
1073
</page>
<tableCaption confidence="0.9977">
Table 3: Relative translation evaluation
</tableCaption>
<table confidence="0.9524055">
eval2pref evaluator 1 preference sum
+translit equal baseline
+translit 95 0 2 97
equal 19 1 2 22
baseline 1 0 0 1
sum 115 1 4 120
</table>
<bodyText confidence="0.999890666666667">
score on the entire test set improved only slightly,
from 21.8 to 22.0. However, in the absolute hu-
man evaluation, the transliteration table increased
the average human judgement from 1.5 to 2 out of
a maximum score of 4. Table 3 shows the results
of the relative evaluation along with the judges’
sentence-level agreement. In 95 out of 120 cases,
both annotators agreed that the augmented table
produced a better translation than the baseline.
One might expect that any replacement of
katakana would improve the perception of MT
quality. This is not necessarily the case: it
can be more confusing to have a drastically
incorrect transliteration, such as transliterating
T�)717— [appurooda] — uploader incor-
rectly as applaud. Fortunately, Table 3 shows that
we make very few of these sorts of mistakes: the
baseline is preferred only rarely. Also note that,
according the MT 10-best accuracies in Table 2,
we would have expected to improve at most 60%
of cases, however, the human judgements indicate
that our actual rate of improvement is closer to
80%, which demonstrates that even an imperfect
transliteration is often useful.
</bodyText>
<sectionHeader confidence="0.998452" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999966736842105">
We have presented a discriminative substring de-
coder for transliteration. Our decoder is based
on recent approaches for discriminative charac-
ter transduction, extended to provide access to a
target lexicon. We have presented a comparison
of indicator and hybrid generative features in a
controlled setting, demonstrating that generative
models perform surprisingly well when discrim-
inatively weighted. We have also shown our dis-
criminative models to be superior to a state-of-the-
art generative system. Finally, we have demon-
strated that machine transliteration is immediately
useful to end-to-end SMT.
As mentioned earlier, by focusing on katakana,
we bypass the problem of deciding when to
transliterate rather than translate; next, we plan to
combine our models with a classifier that makes
such a decision, allowing us to integrate transliter-
ation into SMT for other language pairs.
</bodyText>
<sectionHeader confidence="0.994296" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999815911111111">
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in Arabic text. In ACL
Workshop on Comp. Approaches to Semitic Lan-
guages.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
ACL, pages 656–663, Prague, Czech Republic, June.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In COL-
ING, pages 597–603, Geneva, Switzerland.
Slaven Bilac and Hozumi Tanaka. 2005. Extracting
transliteration pairs from comparable corpora. In
Proceedings of the Annual Meeting of the Natural
Language Processing Society, Japan.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL, pages 286–293, Morristown, NJ.
Eric Brill, Gary Kacmarcik, and Chris Brockett.
2001. Automatically harvesting katakana-english
term pairs from search engine query logs. In Asia
Federation of Natural Language Processing.
Michael Collins, Brian Roark, and Murat Sarac¸lar.
2005. Discriminative syntactic language modeling
for speech recognition. In ACL, pages 507–514,
Ann Arbor, USA, June.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In EMNLP, pages 1080–
1089, Honolulu, Hawaii, October.
Dayne Freitag and Shahram Khadivi. 2007. A se-
quence alignment model based on the averaged per-
ceptron. In EMNLP, pages 238–247, Prague, Czech
Republic, June.
Ulf Hermjakob, Kevin Knight, and Hal Daum´e III.
2008. Name translation in statistical machine trans-
lation - learning when to transliterate. In ACL, pages
389–397, Columbus, Ohio, June.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many align-
ments and hidden markov models to letter-to-
phoneme conversion. In HLT-NAACL, pages 372–
379, Rochester, New York, April.
</reference>
<page confidence="0.904978">
1074
</page>
<reference confidence="0.999821636363637">
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL,
pages 905–913, Columbus, Ohio, June.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL, pages
82–88, New York City, USA, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing (ICASSP-95), pages 181–184.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599–612.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In ACL, pages 159–166, Barcelona, Spain, July.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL, pages 761–768, Sydney, Australia, July.
Robert Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In MT Summit XI.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In ACL, pages 160–
167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In ACL, pages 271–279, Ann
Arbor, USA, June.
Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew
McCallum. 2004. Classification with hybrid gener-
ative/discriminative models. In Advances in Neural
Information Processing Systems 16.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(5):522–
532.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In ICML.
Tarek Sherif and Grzegorz Kondrak. 2007a. Boot-
strapping a stochastic transducer for Arabic-English
transliteration extraction. In ACL, pages 864–871,
Prague, Czech Republic, June.
Tarek Sherif and Grzegorz Kondrak. 2007b.
Substring-based transliteration. In ACL, pages 944–
951, Prague, Czech Republic, June.
Kristina Toutanova. 2006. Competitive generative
models with structure learning for nlp classification
tasks. In EMNLP, pages 576–584, Sydney, Aus-
tralia, July.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In EMNLP, pages
612–617, Sydney, Australia, July.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT-NAACL, pages 257–264, Boston, USA, May.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL, pages 97–105, Columbus, Ohio, June.
</reference>
<page confidence="0.994535">
1075
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671011">
<title confidence="0.999806">Discriminative Substring Decoding for Transliteration</title>
<author confidence="0.961628">Cherry</author>
<affiliation confidence="0.906707">Microsoft</affiliation>
<address confidence="0.859945">One Microsoft Redmond, WA,</address>
<abstract confidence="0.998776260869565">We present a discriminative substring decoder for transliteration. This decoder extends recent approaches for discriminative character transduction by allowing for a list of known target-language words, an important resource for transliteration. Our approach improves upon Sherif and Kondrak’s (2007b) state-of-theart decoder, creating a 28.5% relative improvement in transliteration accuracy on a Japanese katakana-to-English task. We also conduct a controlled comparison of two feature paradigms for discriminative training: indicators and hybrid generative features. Surprisingly, the generative hybrid outperforms its purely discriminative counterpart, despite losing access to rich source-context features. Finally, we show that machine transliterations have a positive impact on machine translation quality, improving human judgments by 0.5 on a 4-point scale.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine transliteration of names in Arabic text.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Comp. Approaches to Semitic Languages.</booktitle>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine transliteration of names in Arabic text. In ACL Workshop on Comp. Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Alignment-based discriminative string similarity.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>656--663</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7083" citStr="Bergsma and Kondrak (2007)" startWordPosition="1034" endWordPosition="1037">aka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma and Kondrak (2007) extend the classification approach with features derived from a character alignment. They train from bilingual dictionaries and word-aligned parallel text, selecting negative examples to target false-friends. The work of Hermjakob et al. (2008) is particularly relevant to this paper, as they incorporate a similarity-based transliteration system into an Arabic-to-English SMT engine. They employ a hand-crafted cross-lingual similarity metric, and use capitalized n-grams from the Google n-gram corpus as candidates. With such a huge candidate list, a cross-lingual indexing scheme is designed for </context>
</contexts>
<marker>Bergsma, Kondrak, 2007</marker>
<rawString>Shane Bergsma and Grzegorz Kondrak. 2007. Alignment-based discriminative string similarity. In ACL, pages 656–663, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>A hybrid back-transliteration system for Japanese. In</title>
<date>2004</date>
<booktitle>COLING,</booktitle>
<pages>597--603</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="8959" citStr="Bilac and Tanaka, 2004" startWordPosition="1314" endWordPosition="1317"> of candidates, but many incorporate a target lexicon, favoring target words that occur in the lexicon. This approach is also known as transliteration generation. The majority of transliteration generation approaches are based on the noisy channel model, where a target t is generated according to P(t|s) a P(s|t)P(t). This approach is typified by finite-state transliteration, where the various stages of the channel model are represented by finite state transducers and automata. Early systems employed a complex channel, passing through multiple phonetic representations (Knight and Graehl, 1998; Bilac and Tanaka, 2004), but later versions replaced characters directly (AlOnaizan and Knight, 2002). Sherif and Kondrak (2007b) extend this approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P(t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P(s, t). This formulation allows operations to be conditioned on both source and target context. H</context>
<context position="32030" citStr="Bilac and Tanaka (2004)" startWordPosition="4967" endWordPosition="4970">ittle, except for slightly narrowing the gap between SK07 and the discriminative systems. The second column of Table 2 compares the systems on our MT test set. As discussed earlier, this data is quite different from the Wikipedia training set, and as a result, the systems’ differences are less pronounced. 1-best accuracy still shows the discriminative systems having a definite advantage, but at the 10-best level, those distinctions are muted. Compared with the previous work on katakanato-English transliteration, these accuracies do not look particularly high: both Knight and Graehl (1998) and Bilac and Tanaka (2004) report accuracies above 60% for 1-best transliteration. We should emphasize that this is due to the difficulty of our test data, and that we have tested against a baseline that has been shown to outperform Knight and Graehl (1998). The test data was not filtered for noise, leaving untransliterable cases and loose translations intact. The accuracies reported above are under-estimates of real performance: many transliterations not matching the reference may still be useful to a human reader, such as differ7Lexicon replacement experiment is not shown in Table 1. ences in inflection (e.g., L-f-)-</context>
</contexts>
<marker>Bilac, Tanaka, 2004</marker>
<rawString>Slaven Bilac and Hozumi Tanaka. 2004. A hybrid back-transliteration system for Japanese. In COLING, pages 597–603, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Extracting transliteration pairs from comparable corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Natural Language Processing Society,</booktitle>
<contexts>
<context position="6467" citStr="Bilac and Tanaka (2005)" startWordPosition="944" endWordPosition="947">influenced this work. 2.1 Similarity-based transliteration In similarity-based transliteration, a characterbased, cross-lingual similarity metric is calculated (or bootstrapped) from known transliteration pairs. Given a source word s, its transliteration is the target word t most similar to s, where t is drawn from some pool of candidates. This approach may also be referred to as transliteration discovery. Brill et al. (2001) describe a katakana-toEnglish approach with an EM-learned edit distance, which bootstraps from a small number of examples to learn transliteration pairs from query logs. Bilac and Tanaka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma an</context>
</contexts>
<marker>Bilac, Tanaka, 2005</marker>
<rawString>Slaven Bilac and Hozumi Tanaka. 2005. Extracting transliteration pairs from comparable corpora. In Proceedings of the Annual Meeting of the Natural Language Processing Society, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In ACL,</booktitle>
<pages>286--293</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="13404" citStr="Brill and Moore, 2000" startWordPosition="2010" endWordPosition="2013">dard derivations, as opposed to a maxtransliteration decoder trained directly on sourcetarget pairs. By building the entire system on the derivation level, we side-step issues that can occur when perceptron training with hidden derivations (Liang et al., 2006), but we also introduce the need to transform our training source-target pairs into training derivations. Training derivations can be learned unsupervised from source-target pairs using character alignment techniques. Previously, this has been done using an EM-learned edit distance (Ristad and Yianilos, 1998), or generalizations thereof (Brill and Moore, 2000; Jiampojamarn et al., 2007). We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al. (2008). This approach employs variational EM with sparse priors, along with hard length limits, to reduce the length of substrings operated upon. By doing so, we hope to learn only non-compositional transliteration units. Our aligner produces only monotonic alignments, and does not allow either the source or target side of an operation to be empty. The same restrictions are imposed during decoding. In this way, each alignment found by variational EM is also an unam</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In ACL, pages 286–293, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Gary Kacmarcik</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically harvesting katakana-english term pairs from search engine query logs.</title>
<date>2001</date>
<booktitle>In Asia Federation of Natural Language Processing.</booktitle>
<contexts>
<context position="6273" citStr="Brill et al. (2001)" startWordPosition="914" endWordPosition="917">from previous work in transliteration, which we divide into similarity and transduction-based approaches. We also discuss recent successes in discriminative character transduction that have influenced this work. 2.1 Similarity-based transliteration In similarity-based transliteration, a characterbased, cross-lingual similarity metric is calculated (or bootstrapped) from known transliteration pairs. Given a source word s, its transliteration is the target word t most similar to s, where t is drawn from some pool of candidates. This approach may also be referred to as transliteration discovery. Brill et al. (2001) describe a katakana-toEnglish approach with an EM-learned edit distance, which bootstraps from a small number of examples to learn transliteration pairs from query logs. Bilac and Tanaka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM</context>
</contexts>
<marker>Brill, Kacmarcik, Brockett, 2001</marker>
<rawString>Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001. Automatically harvesting katakana-english term pairs from search engine query logs. In Asia Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Discriminative syntactic language modeling for speech recognition.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>507--514</pages>
<location>Ann Arbor, USA,</location>
<marker>Collins, Roark, Sarac¸lar, 2005</marker>
<rawString>Michael Collins, Brian Roark, and Murat Sarac¸lar. 2005. Discriminative syntactic language modeling for speech recognition. In ACL, pages 507–514, Ann Arbor, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="11472" citStr="Collins, 2002" startWordPosition="1693" endWordPosition="1694">sduction. Jiampojamarn et al. (2008) describe a discriminative letter-to-phoneme substring transducer, while Dreyer et al. (2008) describe a discriminative character transducer with a latent derivation structure for morphological transformations. Both models are extremely effective, but both rely exclusively on indicator features; they do not explore the use of knowledge-rich generative models. Our indicator system uses an extended version of the Jiampojamarn et al. (2008) feature set. 3 Methods We adopt a discriminative substring decoder for our transliteration task. A structured perceptron (Collins, 2002) learns weights for our transliteration features, which are drawn from two broad classes: indicator and hybrid generative features. 3.1 Structured perceptron The decoder’s discriminative parameters are learned with structured perceptron training. Let a derivation d describe a substring operation sequence that transliterates a source word into a target word. Given an input training corpus of such derivations D = d1 ... dn, a vector feature function on derivations F�(d), and an initial weight vector w, the perceptron performs two steps for each training example di E D: � � d� = argmaxd�D(src(di)</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Latent-variable modeling of string transductions with finite-state methods.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>1080--1089</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="4874" citStr="Dreyer et al., 2008" startWordPosition="714" endWordPosition="717">on scores are provided by a discriminative combination of generative models (Och, 2003). Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT (Koehn et al., 2003), operating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our highest transliteration accuracies. Researchers frequently compare against a phrasal SMT baseline when evaluating a new transduction technique (Freitag and Khadivi, 2007; Dreyer et al., 2008); however, we are careful to vary only the features in our comparison. Confounding variables, such as alignment, decoder and training method, are held constant. We also include a human evaluation of transliteration-augmented SMT output. Though human evaluations are too expensive to allow a comparison between transliteration systems, we are a</context>
<context position="10987" citStr="Dreyer et al. (2008)" startWordPosition="1620" endWordPosition="1623">h approaches train using a structured perceptron, as we do here. However, these models represent a dramatic departure from the existing literature, while ours has clear analogs to the well-known noisy-channel paradigm, which allows for useful comparisons and insights into the advantages of discriminative training. 2.3 Discriminative character transduction While our chosen application is transliteration, our decoder is influenced by recent successes in general-purpose discriminative transduction. Jiampojamarn et al. (2008) describe a discriminative letter-to-phoneme substring transducer, while Dreyer et al. (2008) describe a discriminative character transducer with a latent derivation structure for morphological transformations. Both models are extremely effective, but both rely exclusively on indicator features; they do not explore the use of knowledge-rich generative models. Our indicator system uses an extended version of the Jiampojamarn et al. (2008) feature set. 3 Methods We adopt a discriminative substring decoder for our transliteration task. A structured perceptron (Collins, 2002) learns weights for our transliteration features, which are drawn from two broad classes: indicator and hybrid gene</context>
<context position="15697" citStr="Dreyer et al., 2008" startWordPosition="2366" endWordPosition="2369">perceptron training set. We can further divide feature space according to the information required to calculate each feature. Both feature sets can be partitioned into the following subtypes: • Emission: How accurate are the operations used by this derivation? • Transition: Does the target string produced by this derivation look like a well-formed target character sequence? • Lexicon: Does the target string contain known words from a target lexicon? Indicator Features Previous approaches to discriminative character transduction tend to employ only sparse indicators (Jiampojamarn et al., 2008; Dreyer et al., 2008). This is because sparsity is not a major concern in character-based domains, and sparse indicators are extremely flexible. Our emission and transition indicator features follow Jiampojamarn et al. (2008). Emission indicators are centered around an operation, such as [1 —* tho]. Minimally, an indicator exists for each operation. Many more source context features can be generated by conjoining an operation with source n-grams found within a fixed window of C characters to either side of the operation. These source context features have minimal computational cost, and they allow each operator to</context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-variable modeling of string transductions with finite-state methods. In EMNLP, pages 1080– 1089, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
<author>Shahram Khadivi</author>
</authors>
<title>A sequence alignment model based on the averaged perceptron.</title>
<date>2007</date>
<booktitle>In EMNLP,</booktitle>
<pages>238--247</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5109" citStr="Freitag and Khadivi, 2007" startWordPosition="745" endWordPosition="748">erating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our highest transliteration accuracies. Researchers frequently compare against a phrasal SMT baseline when evaluating a new transduction technique (Freitag and Khadivi, 2007; Dreyer et al., 2008); however, we are careful to vary only the features in our comparison. Confounding variables, such as alignment, decoder and training method, are held constant. We also include a human evaluation of transliteration-augmented SMT output. Though human evaluations are too expensive to allow a comparison between transliteration systems, we are able to show that adding our transliterations to a production-level SMT engine results in a substantial improvement in translation quality. 2 Background This work draws inspiration from previous work in transliteration, which we divide </context>
<context position="10033" citStr="Freitag and Khadivi (2007)" startWordPosition="1485" endWordPosition="1488">annel with their joint n-gram model, which calculates P(s, t). This formulation allows operations to be conditioned on both source and target context. However, the inclusion of a candidate list is more difficult in this setting, as P(t) is not given its own model. Zelenko and Aone (2006) investigate a purely discriminative, alignment-free approach to transliteration generation. The target word is constructed one character at a time, with each new character triggering a suite of features, including indicators for near-by source and target characters, as well a generative target language model. Freitag and Khadivi (2007) propose a discriminative, latent edit distance for transliteration. In this case, training data need not be aligned in advance, but a latent alignment is produced during decoding. Again, the target word is constructed one character at a time, using edit operations that are scored according to source and target context features. Both approaches train using a structured perceptron, as we do here. However, these models represent a dramatic departure from the existing literature, while ours has clear analogs to the well-known noisy-channel paradigm, which allows for useful comparisons and insight</context>
</contexts>
<marker>Freitag, Khadivi, 2007</marker>
<rawString>Dayne Freitag and Shahram Khadivi. 2007. A sequence alignment model based on the averaged perceptron. In EMNLP, pages 238–247, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Hal Daum´e</author>
</authors>
<title>Name translation in statistical machine translation - learning when to transliterate.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>389--397</pages>
<location>Columbus, Ohio,</location>
<marker>Hermjakob, Knight, Daum´e, 2008</marker>
<rawString>Ulf Hermjakob, Kevin Knight, and Hal Daum´e III. 2008. Name translation in statistical machine translation - learning when to transliterate. In ACL, pages 389–397, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applying many-to-many alignments and hidden markov models to letter-tophoneme conversion.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>372--379</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="13432" citStr="Jiampojamarn et al., 2007" startWordPosition="2014" endWordPosition="2018">posed to a maxtransliteration decoder trained directly on sourcetarget pairs. By building the entire system on the derivation level, we side-step issues that can occur when perceptron training with hidden derivations (Liang et al., 2006), but we also introduce the need to transform our training source-target pairs into training derivations. Training derivations can be learned unsupervised from source-target pairs using character alignment techniques. Previously, this has been done using an EM-learned edit distance (Ristad and Yianilos, 1998), or generalizations thereof (Brill and Moore, 2000; Jiampojamarn et al., 2007). We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al. (2008). This approach employs variational EM with sparse priors, along with hard length limits, to reduce the length of substrings operated upon. By doing so, we hope to learn only non-compositional transliteration units. Our aligner produces only monotonic alignments, and does not allow either the source or target side of an operation to be empty. The same restrictions are imposed during decoding. In this way, each alignment found by variational EM is also an unambiguous derivation. We align</context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applying many-to-many alignments and hidden markov models to letter-tophoneme conversion. In HLT-NAACL, pages 372– 379, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Joint processing and discriminative training for letter-to-phoneme conversion. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>905--913</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4852" citStr="Jiampojamarn et al., 2008" startWordPosition="710" endWordPosition="713">orm in SMT, where translation scores are provided by a discriminative combination of generative models (Och, 2003). Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT (Koehn et al., 2003), operating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our highest transliteration accuracies. Researchers frequently compare against a phrasal SMT baseline when evaluating a new transduction technique (Freitag and Khadivi, 2007; Dreyer et al., 2008); however, we are careful to vary only the features in our comparison. Confounding variables, such as alignment, decoder and training method, are held constant. We also include a human evaluation of transliteration-augmented SMT output. Though human evaluations are too expensive to allow a comparison between translitera</context>
<context position="10894" citStr="Jiampojamarn et al. (2008)" startWordPosition="1607" endWordPosition="1611"> a time, using edit operations that are scored according to source and target context features. Both approaches train using a structured perceptron, as we do here. However, these models represent a dramatic departure from the existing literature, while ours has clear analogs to the well-known noisy-channel paradigm, which allows for useful comparisons and insights into the advantages of discriminative training. 2.3 Discriminative character transduction While our chosen application is transliteration, our decoder is influenced by recent successes in general-purpose discriminative transduction. Jiampojamarn et al. (2008) describe a discriminative letter-to-phoneme substring transducer, while Dreyer et al. (2008) describe a discriminative character transducer with a latent derivation structure for morphological transformations. Both models are extremely effective, but both rely exclusively on indicator features; they do not explore the use of knowledge-rich generative models. Our indicator system uses an extended version of the Jiampojamarn et al. (2008) feature set. 3 Methods We adopt a discriminative substring decoder for our transliteration task. A structured perceptron (Collins, 2002) learns weights for ou</context>
<context position="15675" citStr="Jiampojamarn et al., 2008" startWordPosition="2362" endWordPosition="2365">ators require only a large perceptron training set. We can further divide feature space according to the information required to calculate each feature. Both feature sets can be partitioned into the following subtypes: • Emission: How accurate are the operations used by this derivation? • Transition: Does the target string produced by this derivation look like a well-formed target character sequence? • Lexicon: Does the target string contain known words from a target lexicon? Indicator Features Previous approaches to discriminative character transduction tend to employ only sparse indicators (Jiampojamarn et al., 2008; Dreyer et al., 2008). This is because sparsity is not a major concern in character-based domains, and sparse indicators are extremely flexible. Our emission and transition indicator features follow Jiampojamarn et al. (2008). Emission indicators are centered around an operation, such as [1 —* tho]. Minimally, an indicator exists for each operation. Many more source context features can be generated by conjoining an operation with source n-grams found within a fixed window of C characters to either side of the operation. These source context features have minimal computational cost, and they </context>
</contexts>
<marker>Jiampojamarn, Cherry, Kondrak, 2008</marker>
<rawString>Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2008. Joint processing and discriminative training for letter-to-phoneme conversion. In ACL, pages 905–913, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>82--88</pages>
<location>New York City, USA,</location>
<contexts>
<context position="6856" citStr="Klementiev and Roth (2006)" startWordPosition="1001" endWordPosition="1004">nsliteration discovery. Brill et al. (2001) describe a katakana-toEnglish approach with an EM-learned edit distance, which bootstraps from a small number of examples to learn transliteration pairs from query logs. Bilac and Tanaka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma and Kondrak (2007) extend the classification approach with features derived from a character alignment. They train from bilingual dictionaries and word-aligned parallel text, selecting negative examples to target false-friends. The work of Hermjakob et al. (2008) is particularly relevant to this paper, as they incorporate a similarity-based transliteration system into an Arabic-to-English</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Named entity transliteration and discovery from multilingual comparable corpora. In HLT-NAACL, pages 82–88, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In International Conference on Acoustics, Speech, and Signal Processing (ICASSP-95),</booktitle>
<pages>181--184</pages>
<contexts>
<context position="19583" citStr="Kneser and Ney, 1995" startWordPosition="3000" endWordPosition="3003">s the length limit on substring operations.4 PT(t) provides transition information through a character language model, estimated on the target side 4Derivations built by our character aligner use operations on substrings of maximum length 3. To enable perceptron training with composed operations, once PE(s|t) has been estimated by counting composed operations in the initial alignments, we re-align our training examples with those composed operations to maximize PE(slt), creating new training derivations. of the training derivations. In our implementation, we employ a KN-smoothed 7-gram model (Kneser and Ney, 1995). Finally, PL(t) is a unigram target word model, estimated from the same type frequencies used to build our lexicon indicators. Since we have adopted a linear model, we are no longer constrained by the original generative story. Therefore, we are free to incorporate other SMT-inspired features: PES(t|s), target character count, and operation count.5 Feature summary The indicator and hybrid-generative feature sets each provide a discriminative version of the noisy channel model. In the case of transition and lexicon features, both systems have access to the exact same information, but encode th</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP-95), pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="8934" citStr="Knight and Graehl, 1998" startWordPosition="1310" endWordPosition="1313">ems do not require a list of candidates, but many incorporate a target lexicon, favoring target words that occur in the lexicon. This approach is also known as transliteration generation. The majority of transliteration generation approaches are based on the noisy channel model, where a target t is generated according to P(t|s) a P(s|t)P(t). This approach is typified by finite-state transliteration, where the various stages of the channel model are represented by finite state transducers and automata. Early systems employed a complex channel, passing through multiple phonetic representations (Knight and Graehl, 1998; Bilac and Tanaka, 2004), but later versions replaced characters directly (AlOnaizan and Knight, 2002). Sherif and Kondrak (2007b) extend this approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P(t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P(s, t). This formulation allows operations to be conditioned on both sou</context>
<context position="32002" citStr="Knight and Graehl (1998)" startWordPosition="4962" endWordPosition="4965"> 10-best accuracies changes little, except for slightly narrowing the gap between SK07 and the discriminative systems. The second column of Table 2 compares the systems on our MT test set. As discussed earlier, this data is quite different from the Wikipedia training set, and as a result, the systems’ differences are less pronounced. 1-best accuracy still shows the discriminative systems having a definite advantage, but at the 10-best level, those distinctions are muted. Compared with the previous work on katakanato-English transliteration, these accuracies do not look particularly high: both Knight and Graehl (1998) and Bilac and Tanaka (2004) report accuracies above 60% for 1-best transliteration. We should emphasize that this is due to the difficulty of our test data, and that we have tested against a baseline that has been shown to outperform Knight and Graehl (1998). The test data was not filtered for noise, leaving untransliterable cases and loose translations intact. The accuracies reported above are under-estimates of real performance: many transliterations not matching the reference may still be useful to a human reader, such as differ7Lexicon replacement experiment is not shown in Table 1. ences</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4480" citStr="Koehn et al., 2003" startWordPosition="656" endWordPosition="659">cantly, our framework allows us to test two competing styles of features: • sparse indicators, designed to capture the same channel and language modeling data collected by previous generative models, and • components of existing generative models, used as real-valued features in a discriminatively weighted, generative hybrid. Note that generative hybrids are the norm in SMT, where translation scores are provided by a discriminative combination of generative models (Och, 2003). Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT (Koehn et al., 2003), operating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our highest transliteration accuracies. Researchers frequently compare against a phrasal SMT baseline when evaluating a new transduction techniq</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration. In</title>
<date>2004</date>
<booktitle>ACL,</booktitle>
<pages>159--166</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="9368" citStr="Li et al. (2004)" startWordPosition="1383" endWordPosition="1386">l model are represented by finite state transducers and automata. Early systems employed a complex channel, passing through multiple phonetic representations (Knight and Graehl, 1998; Bilac and Tanaka, 2004), but later versions replaced characters directly (AlOnaizan and Knight, 2002). Sherif and Kondrak (2007b) extend this approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P(t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P(s, t). This formulation allows operations to be conditioned on both source and target context. However, the inclusion of a candidate list is more difficult in this setting, as P(t) is not given its own model. Zelenko and Aone (2006) investigate a purely discriminative, alignment-free approach to transliteration generation. The target word is constructed one character at a time, with each new character triggering a suite of features, including indicators for near-by source and target characters, as we</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In ACL, pages 159–166, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In COLINGACL,</booktitle>
<pages>761--768</pages>
<location>Sydney, Australia,</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In COLINGACL, pages 761–768, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Faster beam-search decoding for phrasal statistical machine translation.</title>
<date>2007</date>
<booktitle>In MT Summit XI.</booktitle>
<contexts>
<context position="26445" citStr="Moore and Quirk, 2007" startWordPosition="4077" endWordPosition="4080">est list. 5.1 Baselines We compare our systems against a reimplementation of Sherif and Kondrak’s (2007b) noisy-channel substring decoder. This uses the same PE, PT and PL models as our hybrid generative system, but employs a two-pass decoding scheme to find the max transliteration according to Equation 1. It represents a purely generative solution using otherwise identical architecture. 6http://www.microsofttranslator.com 1071 Since our hybrid generative system implements a model that is very similar to those used in phrasal SMT, we also compare against a state-of-the-art phrasal SMT system (Moore and Quirk, 2007). This system is trained by applying the standard SMT pipeline to our Wikipedia title pairs, treating characters as words, using a 7-gram characterlevel language model, and disabling re-ordering. Unfortunately, the decoder’s architecture does not allow the use of a word-level unigram model, reducing the usefulness of this baseline. Instead, we include the target lexicon as a second characterlevel language model. This baseline indicates the level of performance one can expect by applying phrasal SMT straight out of the box. Comparing the two baselines qualitatively, both use a combination of ge</context>
</contexts>
<marker>Moore, Quirk, 2007</marker>
<rawString>Robert Moore and Chris Quirk. 2007. Faster beam-search decoding for phrasal statistical machine translation. In MT Summit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="4341" citStr="Och, 2003" startWordPosition="638" endWordPosition="639"> in discriminative character transduction, allowing our decoder to benefit from a list of known target words. Perhaps more significantly, our framework allows us to test two competing styles of features: • sparse indicators, designed to capture the same channel and language modeling data collected by previous generative models, and • components of existing generative models, used as real-valued features in a discriminatively weighted, generative hybrid. Note that generative hybrids are the norm in SMT, where translation scores are provided by a discriminative combination of generative models (Och, 2003). Substring-based transliteration with a generative hybrid model is very similar to existing solutions for phrasal SMT (Koehn et al., 2003), operating on characters rather than words. Unlike out-of-the-box phrasal SMT solutions, our generative hybrid benefits from a target a lexicon. As we will show, this is the difference between a weak baseline and a strong competitor. We demonstrate that despite recent successes in discriminative character transduction using indicator features (Jiampojamarn et al., 2008; Dreyer et al., 2008), our generative hybrid performs surprisingly well, producing our h</context>
<context position="18348" citStr="Och, 2003" startWordPosition="2814" endWordPosition="2815">re that indicates the introduction of a new word. We expect these frequency indicators to be superior to a word-level unigram model, as they allow the designer to select notable frequencies. In particular, the bins we have selected do not give any advantage to extremely common words, as these are generally less likely to be transliterated. Hybrid Generative Features We begin with the three components of the generative noisy channel employed by Sherif and Kondrak (2007b). Their transliteration probability is: P(t|s) a PE(s|t) · max [PT (t), PL(t)] (1) Inspired by the linear models used in SMT (Och, 2003), we can discriminatively weight the components of this generative model, producing: wE log PE(s|t) + wT log PT (t) + wL log PL(t) with weights w learned by perceptron training. These three models conveniently align with our three feature subtypes. Emission information is provided by PE(s|t), which is estimated by maximum likelihood on the operations observed in our training derivations. Including source context is difficult in such a model. To compensate for this, all systems using PE(s|t) also use composed operations, which are constructed from operation sequences observed in the training se</context>
<context position="29870" citStr="Och, 2003" startWordPosition="4622" endWordPosition="4623">r differences, the two systems’ proximity in score is quite surprising. Given the character domain’s lack of sparsity, and the large amount of available training data, we had expected the hybrid generative system to behave only as a strong baseline; instead, it matched the performance of the indicator system. However, this is not unprecedented: discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks (Raina et al., 2004; Toutanova, 2006), and remain the standard approach in statistical translation modeling (Och, 2003). Examining the development results on an example-by-example basis, we see that the two systems make mostly the same mistakes: for 87% of examples, either both systems are right, or both are wrong. The remainder represents a (relatively small) opportunity to improve through system or feature combination: an oracle that perfectly selects between the two scores 50.6. One opportunity for straight-forward combination is the target lexicon. Because lexicon frequencies are drawn from an independent word list, and not the transliteration training derivations, there is no reason why both systems canno</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, USA,</location>
<contexts>
<context position="32967" citStr="Quirk et al., 2005" startWordPosition="5108" endWordPosition="5111">anslations intact. The accuracies reported above are under-estimates of real performance: many transliterations not matching the reference may still be useful to a human reader, such as differ7Lexicon replacement experiment is not shown in Table 1. ences in inflection (e.g., L-f-)-fF [rechinoido] — retinoids, transliterated as retinoid), and spacing (e.g. : /s7Lt-�, [shierareone]— Sierra Leone, transliterated as sierraleone). 6 Integration with machine translation We used the transliterations from our indicator system to augment a Japanese-to-English MT system.8 This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. Our goal is to measure the impact of machine transliterations on end-to-end translation quality. 6.1 Evaluation method We use the MT-log translation pairs described in Section 4.2 as a sentence-level translation test set. For each katakana word left untranslated by the baseline SMT engine, we generated 10-best transliteration candidates and added the katakanaEnglish pairs to the SMT system’s translation table. Perceptron scores were exponentiated, then normalized</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In ACL, pages 271–279, Ann Arbor, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Yirong Shen</author>
<author>Andrew Y Ng</author>
<author>Andrew McCallum</author>
</authors>
<title>Classification with hybrid generative/discriminative models.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 16.</booktitle>
<contexts>
<context position="29770" citStr="Raina et al., 2004" startWordPosition="4606" endWordPosition="4609">ed by SMT, such as PE,(tls), eliminates the gap between the two. 5.3 Development discussion Considering their differences, the two systems’ proximity in score is quite surprising. Given the character domain’s lack of sparsity, and the large amount of available training data, we had expected the hybrid generative system to behave only as a strong baseline; instead, it matched the performance of the indicator system. However, this is not unprecedented: discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks (Raina et al., 2004; Toutanova, 2006), and remain the standard approach in statistical translation modeling (Och, 2003). Examining the development results on an example-by-example basis, we see that the two systems make mostly the same mistakes: for 87% of examples, either both systems are right, or both are wrong. The remainder represents a (relatively small) opportunity to improve through system or feature combination: an oracle that perfectly selects between the two scores 50.6. One opportunity for straight-forward combination is the target lexicon. Because lexicon frequencies are drawn from an independent wo</context>
</contexts>
<marker>Raina, Shen, Ng, McCallum, 2004</marker>
<rawString>Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew McCallum. 2004. Classification with hybrid generative/discriminative models. In Advances in Neural Information Processing Systems 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string-edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>5</issue>
<pages>532</pages>
<contexts>
<context position="13353" citStr="Ristad and Yianilos, 1998" startWordPosition="2002" endWordPosition="2005"> a maxderivation decoder trained on a corpus of goldstandard derivations, as opposed to a maxtransliteration decoder trained directly on sourcetarget pairs. By building the entire system on the derivation level, we side-step issues that can occur when perceptron training with hidden derivations (Liang et al., 2006), but we also introduce the need to transform our training source-target pairs into training derivations. Training derivations can be learned unsupervised from source-target pairs using character alignment techniques. Previously, this has been done using an EM-learned edit distance (Ristad and Yianilos, 1998), or generalizations thereof (Brill and Moore, 2000; Jiampojamarn et al., 2007). We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al. (2008). This approach employs variational EM with sparse priors, along with hard length limits, to reduce the length of substrings operated upon. By doing so, we hope to learn only non-compositional transliteration units. Our aligner produces only monotonic alignments, and does not allow either the source or target side of an operation to be empty. The same restrictions are imposed during decoding. In this way, eac</context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string-edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(5):522– 532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="20923" citStr="Sarawagi and Cohen, 2004" startWordPosition="3202" endWordPosition="3206"> number of frequency bins, and the generative unigram model providing a single, real-valued feature that is proportional to frequency. In the case of their emission features, the two systems actually encode different information. Both have access to the same training derivations, but the indicator system provides source context through n-gram indicators, while the generative system does so using composed operations. 3.4 Decoder Our decoder builds upon machine translation’s monotone phrasal decoding (Zens and Ney, 2004), or equivalently, the sequence tagging algorithm used in semi-Markov CRFs (Sarawagi and Cohen, 2004). This dynamic programming (DP) decoder extends the Viterbi algorithm for HMMs by operating on one or more source characters (a substring) at each step. A DP block stores the best scoring solution for a particular prefix. Each block is subdivided into cells, which maintain the context necessary to calculate target-side features. We employ a beam, keeping only the 40 highestscoring cells for each block, which speeds up inference at the expense of optimality. We found that the beam had no major effect on perceptron training, nor on the system’s final accuracy. Previously, target lexicons have be</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William Cohen. 2004. Semimarkov conditional random fields for information extraction. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Bootstrapping a stochastic transducer for Arabic-English transliteration extraction.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>864--871</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3096" citStr="Sherif and Kondrak (2007" startWordPosition="446" endWordPosition="449">[chokoreeto] — chocolate), and names (e.g., �JS(S [kurinton] — Clinton). Therefore, katakana is a strong indicator that a Japanese word can be back-transliterated. However, katakana can also be used to spell scientific names of animals and plants (e.g., t✓B [kamo] — duck), onomatopoeic expressions (e.g., 0~C0~C [bashabasha] — splash) and foreign origin words that are not transliterations (e.g., ;!--�-7, [hochikisu] — stapler). These untransliterable cases constitute about 10% of the katakana words in our data. We employ a discriminative substring decoder for machine transliteration. Following Sherif and Kondrak (2007b), the decoder operates on short source substrings, with each operation producing one or more target characters, as shown in Figure 1. However, where previous approaches employ generative modeling, we use structured perceptron training to discriminatively tune parameters according to 0-1 transliteration accuracy. This 15J—A—K is romanized as [furiimeeru] 1066 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1066–1075, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP F A ソン tho m son Figure 1: Example substring derivation allows us to test novel met</context>
<context position="6685" citStr="Sherif and Kondrak (2007" startWordPosition="975" endWordPosition="979"> a source word s, its transliteration is the target word t most similar to s, where t is drawn from some pool of candidates. This approach may also be referred to as transliteration discovery. Brill et al. (2001) describe a katakana-toEnglish approach with an EM-learned edit distance, which bootstraps from a small number of examples to learn transliteration pairs from query logs. Bilac and Tanaka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma and Kondrak (2007) extend the classification approach with features derived from a character alignment. They train from bilingual dictionaries and word-aligned parallel text, selecting negative examples to target false-f</context>
<context position="9063" citStr="Sherif and Kondrak (2007" startWordPosition="1329" endWordPosition="1332"> This approach is also known as transliteration generation. The majority of transliteration generation approaches are based on the noisy channel model, where a target t is generated according to P(t|s) a P(s|t)P(t). This approach is typified by finite-state transliteration, where the various stages of the channel model are represented by finite state transducers and automata. Early systems employed a complex channel, passing through multiple phonetic representations (Knight and Graehl, 1998; Bilac and Tanaka, 2004), but later versions replaced characters directly (AlOnaizan and Knight, 2002). Sherif and Kondrak (2007b) extend this approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P(t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P(s, t). This formulation allows operations to be conditioned on both source and target context. However, the inclusion of a candidate list is more difficult in this setting, as P(t) is not given its ow</context>
<context position="18210" citStr="Sherif and Kondrak (2007" startWordPosition="2788" endWordPosition="2792">n potentially generate multiple target words, and doing so can have a major impact on how often the lexicon features fire. Thus, we employ another feature that indicates the introduction of a new word. We expect these frequency indicators to be superior to a word-level unigram model, as they allow the designer to select notable frequencies. In particular, the bins we have selected do not give any advantage to extremely common words, as these are generally less likely to be transliterated. Hybrid Generative Features We begin with the three components of the generative noisy channel employed by Sherif and Kondrak (2007b). Their transliteration probability is: P(t|s) a PE(s|t) · max [PT (t), PL(t)] (1) Inspired by the linear models used in SMT (Och, 2003), we can discriminatively weight the components of this generative model, producing: wE log PE(s|t) + wT log PT (t) + wL log PL(t) with weights w learned by perceptron training. These three models conveniently align with our three feature subtypes. Emission information is provided by PE(s|t), which is estimated by maximum likelihood on the operations observed in our training derivations. Including source context is difficult in such a model. To compensate fo</context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007a. Bootstrapping a stochastic transducer for Arabic-English transliteration extraction. In ACL, pages 864–871, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Substring-based transliteration.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>944--951</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3096" citStr="Sherif and Kondrak (2007" startWordPosition="446" endWordPosition="449">[chokoreeto] — chocolate), and names (e.g., �JS(S [kurinton] — Clinton). Therefore, katakana is a strong indicator that a Japanese word can be back-transliterated. However, katakana can also be used to spell scientific names of animals and plants (e.g., t✓B [kamo] — duck), onomatopoeic expressions (e.g., 0~C0~C [bashabasha] — splash) and foreign origin words that are not transliterations (e.g., ;!--�-7, [hochikisu] — stapler). These untransliterable cases constitute about 10% of the katakana words in our data. We employ a discriminative substring decoder for machine transliteration. Following Sherif and Kondrak (2007b), the decoder operates on short source substrings, with each operation producing one or more target characters, as shown in Figure 1. However, where previous approaches employ generative modeling, we use structured perceptron training to discriminatively tune parameters according to 0-1 transliteration accuracy. This 15J—A—K is romanized as [furiimeeru] 1066 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1066–1075, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP F A ソン tho m son Figure 1: Example substring derivation allows us to test novel met</context>
<context position="6685" citStr="Sherif and Kondrak (2007" startWordPosition="975" endWordPosition="979"> a source word s, its transliteration is the target word t most similar to s, where t is drawn from some pool of candidates. This approach may also be referred to as transliteration discovery. Brill et al. (2001) describe a katakana-toEnglish approach with an EM-learned edit distance, which bootstraps from a small number of examples to learn transliteration pairs from query logs. Bilac and Tanaka (2005) harvest transliteration candidates from comparable bilingual corpora (conference abstracts in English and Japanese), and use distributional as well as phonetic similarity to choose among them. Sherif and Kondrak (2007a) also bootstrap a learned edit distance for Arabic named entities, with candidate pairs drawn from sentence or document-aligned parallel text. Klementiev and Roth (2006) bootstrap an SVM classifier trained to detect true transliteration-pairs. They draw candidates from comparable news text, using date information to provide further clues as to aligned named entities. Bergsma and Kondrak (2007) extend the classification approach with features derived from a character alignment. They train from bilingual dictionaries and word-aligned parallel text, selecting negative examples to target false-f</context>
<context position="9063" citStr="Sherif and Kondrak (2007" startWordPosition="1329" endWordPosition="1332"> This approach is also known as transliteration generation. The majority of transliteration generation approaches are based on the noisy channel model, where a target t is generated according to P(t|s) a P(s|t)P(t). This approach is typified by finite-state transliteration, where the various stages of the channel model are represented by finite state transducers and automata. Early systems employed a complex channel, passing through multiple phonetic representations (Knight and Graehl, 1998; Bilac and Tanaka, 2004), but later versions replaced characters directly (AlOnaizan and Knight, 2002). Sherif and Kondrak (2007b) extend this approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P(t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P(s, t). This formulation allows operations to be conditioned on both source and target context. However, the inclusion of a candidate list is more difficult in this setting, as P(t) is not given its ow</context>
<context position="18210" citStr="Sherif and Kondrak (2007" startWordPosition="2788" endWordPosition="2792">n potentially generate multiple target words, and doing so can have a major impact on how often the lexicon features fire. Thus, we employ another feature that indicates the introduction of a new word. We expect these frequency indicators to be superior to a word-level unigram model, as they allow the designer to select notable frequencies. In particular, the bins we have selected do not give any advantage to extremely common words, as these are generally less likely to be transliterated. Hybrid Generative Features We begin with the three components of the generative noisy channel employed by Sherif and Kondrak (2007b). Their transliteration probability is: P(t|s) a PE(s|t) · max [PT (t), PL(t)] (1) Inspired by the linear models used in SMT (Och, 2003), we can discriminatively weight the components of this generative model, producing: wE log PE(s|t) + wT log PT (t) + wL log PL(t) with weights w learned by perceptron training. These three models conveniently align with our three feature subtypes. Emission information is provided by PE(s|t), which is estimated by maximum likelihood on the operations observed in our training derivations. Including source context is difficult in such a model. To compensate fo</context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007b. Substring-based transliteration. In ACL, pages 944– 951, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
</authors>
<title>Competitive generative models with structure learning for nlp classification tasks.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>576--584</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="29788" citStr="Toutanova, 2006" startWordPosition="4610" endWordPosition="4611">E,(tls), eliminates the gap between the two. 5.3 Development discussion Considering their differences, the two systems’ proximity in score is quite surprising. Given the character domain’s lack of sparsity, and the large amount of available training data, we had expected the hybrid generative system to behave only as a strong baseline; instead, it matched the performance of the indicator system. However, this is not unprecedented: discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various NLP classification tasks (Raina et al., 2004; Toutanova, 2006), and remain the standard approach in statistical translation modeling (Och, 2003). Examining the development results on an example-by-example basis, we see that the two systems make mostly the same mistakes: for 87% of examples, either both systems are right, or both are wrong. The remainder represents a (relatively small) opportunity to improve through system or feature combination: an oracle that perfectly selects between the two scores 50.6. One opportunity for straight-forward combination is the target lexicon. Because lexicon frequencies are drawn from an independent word list, and not t</context>
</contexts>
<marker>Toutanova, 2006</marker>
<rawString>Kristina Toutanova. 2006. Competitive generative models with structure learning for nlp classification tasks. In EMNLP, pages 576–584, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
</authors>
<title>Discriminative methods for transliteration.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>612--617</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="9695" citStr="Zelenko and Aone (2006)" startWordPosition="1438" endWordPosition="1441"> this approach with substring operations in the style of phrasal SMT, and show that doing so improves both accuracy as well as space and time efficiency. Note that it is possible to incorporate a target lexicon by making P(t) a word unigram model with a character-based back-off. Li et al. (2004) present an alternative to the noisy channel with their joint n-gram model, which calculates P(s, t). This formulation allows operations to be conditioned on both source and target context. However, the inclusion of a candidate list is more difficult in this setting, as P(t) is not given its own model. Zelenko and Aone (2006) investigate a purely discriminative, alignment-free approach to transliteration generation. The target word is constructed one character at a time, with each new character triggering a suite of features, including indicators for near-by source and target characters, as well a generative target language model. Freitag and Khadivi (2007) propose a discriminative, latent edit distance for transliteration. In this case, training data need not be aligned in advance, but a latent alignment is produced during decoding. Again, the target word is constructed one character at a time, using edit operati</context>
</contexts>
<marker>Zelenko, Aone, 2006</marker>
<rawString>Dmitry Zelenko and Chinatsu Aone. 2006. Discriminative methods for transliteration. In EMNLP, pages 612–617, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>257--264</pages>
<location>Boston, USA,</location>
<contexts>
<context position="20822" citStr="Zens and Ney, 2004" startWordPosition="3188" endWordPosition="3191">rently. The lexicon encoding is the most dramatic difference, with the indicators using a small number of frequency bins, and the generative unigram model providing a single, real-valued feature that is proportional to frequency. In the case of their emission features, the two systems actually encode different information. Both have access to the same training derivations, but the indicator system provides source context through n-gram indicators, while the generative system does so using composed operations. 3.4 Decoder Our decoder builds upon machine translation’s monotone phrasal decoding (Zens and Ney, 2004), or equivalently, the sequence tagging algorithm used in semi-Markov CRFs (Sarawagi and Cohen, 2004). This dynamic programming (DP) decoder extends the Viterbi algorithm for HMMs by operating on one or more source characters (a substring) at each step. A DP block stores the best scoring solution for a particular prefix. Each block is subdivided into cells, which maintain the context necessary to calculate target-side features. We employ a beam, keeping only the 40 highestscoring cells for each block, which speeds up inference at the expense of optimality. We found that the beam had no major e</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In HLT-NAACL, pages 257–264, Boston, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="13541" citStr="Zhang et al. (2008)" startWordPosition="2034" endWordPosition="2037">erivation level, we side-step issues that can occur when perceptron training with hidden derivations (Liang et al., 2006), but we also introduce the need to transform our training source-target pairs into training derivations. Training derivations can be learned unsupervised from source-target pairs using character alignment techniques. Previously, this has been done using an EM-learned edit distance (Ristad and Yianilos, 1998), or generalizations thereof (Brill and Moore, 2000; Jiampojamarn et al., 2007). We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al. (2008). This approach employs variational EM with sparse priors, along with hard length limits, to reduce the length of substrings operated upon. By doing so, we hope to learn only non-compositional transliteration units. Our aligner produces only monotonic alignments, and does not allow either the source or target side of an operation to be empty. The same restrictions are imposed during decoding. In this way, each alignment found by variational EM is also an unambiguous derivation. We align our training corpus with a maximum substring length of three characters. The same derivations are used to tr</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In ACL, pages 97–105, Columbus, Ohio, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>