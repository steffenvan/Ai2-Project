<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000091">
<title confidence="0.9976945">
Convolution Kernels on Constituent, Dependency and Sequential
Structures for Relation Extraction
</title>
<author confidence="0.79288">
Truc-Vien T. Nguyen and Alessandro Moschitti and Giuseppe Riccardi
</author>
<affiliation confidence="0.82286">
nguyenthi,moschitti,riccardi@disi.unitn.it
Department of Information Engineering and Computer Science
University of Trento
38050 Povo (TN), Italy
</affiliation>
<sectionHeader confidence="0.963002" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957352941176">
This paper explores the use of innovative
kernels based on syntactic and semantic
structures for a target relation extraction
task. Syntax is derived from constituent
and dependency parse trees whereas se-
mantics concerns to entity types and lex-
ical sequences. We investigate the effec-
tiveness of such representations in the au-
tomated relation extraction from texts. We
process the above data by means of Sup-
port Vector Machines along with the syn-
tactic tree, the partial tree and the word
sequence kernels. Our study on the ACE
2004 corpus illustrates that the combina-
tion of the above kernels achieves high ef-
fectiveness and significantly improves the
current state-of-the-art.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99127596">
Relation Extraction (RE) is defined in ACE as the
task of finding relevant semantic relations between
pairs of entities in texts. Figure 1 shows part
of a document from ACE 2004 corpus, a collec-
tion of news articles. In the text, the relation be-
tween president and NBC’s entertainment division
describes the relationship between the first entity
(person) and the second (organization) where the
person holds a managerial position.
Several approaches have been proposed for au-
tomatically learning semantic relations from texts.
Among others, there has been increased interest in
the application of kernel methods (Zelenko et al.,
2002; Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al., 2005; Wang, 2008). Their main prop-
erty is the ability of exploiting a huge amount of
This work has been partially funded by the LiveMemo-
ries project (http://www.livememories.org/) and Expert Sys-
tem (http://www.expertsystem.net/) research grant.
Jeff Zucker, the longtime executive producer of
NBC’s ”Today” program, will be named Friday
as the new president of NBC’s entertainment
division, replacing Garth Ancier, NBC execu-
tives said.
</bodyText>
<figureCaption confidence="0.994346">
Figure 1: A document from ACE 2004 with all
entity mentions in bold.
</figureCaption>
<bodyText confidence="0.999830516129032">
features without an explicit feature representation.
This can be done by computing a kernel function
between a pair of linguistic objects, where such
function is a kind of similarity measure satisfy-
ing certain properties. An example is the sequence
kernel (Lodhi et al., 2002), where the objects are
strings of characters and the kernel function com-
putes the number of common subsequences of
characters in the two strings. Such substrings are
then weighted according to a decaying factor pe-
nalizing longer ones. In the same line, Tree Ker-
nels count the number of subtree shared by two in-
put trees. An example is that of syntactic (or sub-
set) tree kernel (SST) (Collins and Duffy, 2001),
where trees encode grammatical derivations.
Previous work on the use of kernels for RE
has exploited some similarity measures over di-
verse features (Zelenko et al., 2002; Culotta and
Sorensen, 2004; Zhang et al., 2005) or subse-
quence kernels over dependency graphs (Bunescu
and Mooney, 2005a; Wang, 2008). More specif-
ically, (Bunescu and Mooney, 2005a; Culotta
and Sorensen, 2004) use kernels over depen-
dency trees, which showed much lower accuracy
than feature-based methods (Zhao and Grishman,
2005). One problem of the dependency kernels
above is that they do not exploit the overall struc-
tural aspects of dependency trees. A more effec-
tive solution is the application of convolution ker-
nels to constituent parse trees (Zhang et al., 2006)
but this is not satisfactory from a general per-
</bodyText>
<page confidence="0.944836">
1378
</page>
<note confidence="0.996578">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999963673469388">
spective since dependency structures offer some
unique advantages, which should be exploited by
an appropriate kernel.
Therefore, studying convolution tree kernels for
dependency trees is worthwhile also considering
that, to the best of our knowledge, these models
have not been previously used for relation extrac-
tion1 task. Additionally, sequence kernels should
be included in such global study since some of
their forms have not been applied to RE.
In this paper, we study and evaluate diverse con-
volution and sequence kernels for the RE problem
by providing several kernel combinations on con-
stituent and dependency trees and sequential struc-
tures. To fully exploit the potential of dependency
trees, in addition to the SST kernel, we applied
the partial tree (PT) kernel proposed in (Moschitti,
2006), which is a general convolution tree kernel
adaptable for dependency structures. We also in-
vestigate various sequence kernels (e.g. the word
sequence kernel (WSK) (Cancedda et al., 2003))
by incorporating dependency structures into word
sequences. These are also enriched by including
information from constituent parse trees.
We conduct experiments on the standard ACE
2004 newswire and broadcast news domain. The
results show that although some kernels are less
effective than others, they exhibit properties that
are complementary to each other. In particu-
lar, we found that relation extraction can benefit
from increasing the feature space by combining
kernels (with a simple summation) exploiting the
two different parsing paradigms. Our experiments
on RE show that the current composite kernel,
which is constituent-based is more effective than
those based on dependency trees and individual
sequence kernel but at the same time their com-
binations, i.e. dependency plus constituent trees,
improve the state-of-the-art in RE. More interest-
ingly, also the combinations of various sequence
kernels gain significant better performance than
the current state-of-the-art (Zhang et al., 2005).
Overall, these results are interesting for the
computational linguistics research since they show
that the above two parsing paradigms provide dif-
ferent and important information for a semantic
task such as RE. Regarding sequence-based ker-
nels, the WSK gains better performance than pre-
vious sequence and dependency models for RE.
</bodyText>
<footnote confidence="0.686509">
1The function defined on (Culotta and Sorensen, 2004),
although on dependency trees, is not a convolution tree ker-
nel.
</footnote>
<bodyText confidence="0.999829333333333">
A review of previous work on RE is described
in Section 2. Section 3 introduces support vec-
tor machines and kernel methods whereas our spe-
cific kernels for RE are described is Section 4. The
experiments and conclusions are presented in sec-
tions 5 and 6, respectively.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.977290465116279">
To identify semantic relations using machine
learning, three learning settings have mainly been
applied, namely supervised methods (Miller et
al., 2000; Zelenko et al., 2002; Culotta and
Sorensen, 2004; Kambhatla, 2004; Zhou et al.,
2005), semi supervised methods (Brin, 1998;
Agichtein and Gravano, 2000), and unsupervised
method (Hasegawa et al., 2004). In a supervised
learning setting, representative related work can
be classified into generative models (Miller et al.,
2000), feature-based (Roth and tau Yih, 2002;
Kambhatla, 2004; Zhao and Grishman, 2005;
Zhou et al., 2005) or kernel-based methods (Ze-
lenko et al., 2002; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005a; Zhang et al., 2005;
Wang, 2008; Zhang et al., 2006).
The learning model employed in (Miller et al.,
2000) used statistical parsing techniques to learn
syntactic parse trees. It demonstrated that a lexi-
calized, probabilistic context-free parser with head
rules can be used effectively for information ex-
traction. Meanwhile, feature-based approaches
often employ various kinds of linguistic, syntac-
tic or contextual information and integrate into
the feature space. (Roth and tau Yih, 2002) ap-
plied a probabilistic approach to solve the prob-
lems of named entity and relation extraction with
the incorporation of various features such as word,
part-of-speech, and semantic information from
WordNet. (Kambhatla, 2004) employed maximum
entropy models with diverse features including
words, entity and mention types and the number
of words (if any) separating the two entities.
Recent work on Relation Extraction has mostly
employed kernel-based approaches over syntac-
tic parse trees. Kernels on parse trees were pi-
oneered by (Collins and Duffy, 2001). This
kernel function counts the number of common
subtrees, weighted appropriately, as the measure
of similarity between two parse trees. (Culotta
and Sorensen, 2004) extended this work to cal-
culate kernels between augmented dependency
trees. (Zelenko et al., 2002) proposed extracting
</bodyText>
<page confidence="0.993076">
1379
</page>
<bodyText confidence="0.998275391304348">
relations by computing kernel functions between
parse trees. (Bunescu and Mooney, 2005a) pro-
posed a shortest path dependency kernel by stipu-
lating that the information to model a relationship
between two entities can be captured by the short-
est path between them in the dependency graph.
Although approaches in RE have been domi-
nated by kernel-based methods, until now, most
of research in this line has used the kernel as some
similarity measures over diverse features (Zelenko
et al., 2002; Culotta and Sorensen, 2004; Bunescu
and Mooney, 2005a; Zhang et al., 2005; Wang,
2008). These are not convolution kernels and pro-
duce a much lower number of substructures than
the PT kernel. A recent approach successfully em-
ploys a convolution tree kernel (of type SST) over
constituent syntactic parse tree (Zhang et al., 2006;
Zhou et al., 2007), but it does not capture gram-
matical relations in dependency structure. We be-
lieve that an efficient and appropriate kernel can
be used to solve the RE problem, exploiting the
advantages of dependency structures, convolution
tree kernels and sequence kernels.
</bodyText>
<sectionHeader confidence="0.9991415" genericHeader="method">
3 Support Vector Machines and Kernel
Methods
</sectionHeader>
<bodyText confidence="0.99985025">
In this section we give a brief introduction to sup-
port vector machines, kernel methods, diverse tree
and sequence kernel spaces, which can be applied
to the RE task.
</bodyText>
<subsectionHeader confidence="0.999736">
3.1 Support Vector Machines (SVMs)
</subsectionHeader>
<bodyText confidence="0.999978176470588">
Support Vector Machines refer to a supervised ma-
chine learning technique based on the latest results
of the statistical learning theory (Vapnik, 1998).
Given a vector space and a set of training points,
i.e. positive and negative examples, SVMs find a
separating hyperplane H(~x) = ω~ x x~ + b = 0
where ω E Rn and b E R are learned by applying
the Structural Risk Minimization principle (Vap-
nik, 1995). SVMs is a binary classifier, but it can
be easily extended to multi-class classifier, e.g. by
means of the one-vs-all method (Rifkin and Pog-
gio, 2002).
One strong point of SVMs is the possibility to
apply kernel methods (robert Mller et al., 2001)
to implicitly map data in a new space where the
examples are more easily separable as described
in the next section.
</bodyText>
<subsectionHeader confidence="0.968137">
3.2 Kernel Methods
</subsectionHeader>
<bodyText confidence="0.999979416666667">
Kernel methods (Schlkopf and Smola, 2001) are
an attractive alternative to feature-based methods
since the applied learning algorithm only needs
to compute a product between a pair of objects
(by means of kernel functions), avoiding the ex-
plicit feature representation. A kernel function
is a scalar product in a possibly unknown feature
space. More precisely, The object o is mapped in
x~ with a feature function φ : O —* Rn, where O is
the set of the objects.
The kernel trick allows us to rewrite the deci-
sion hyperplane as:
</bodyText>
<equation confidence="0.993998">
( E yiαi )~xi · x~+ b =
H(~x) =
i=1..l
E yiαi E~xi · x~ + b = yiαiφ(oi) · φ(o) + b,
i=1..l i=1..l
</equation>
<bodyText confidence="0.9983101875">
where yi is equal to 1 for positive and -1 for neg-
ative examples, αi E R with αi &gt; 0, oi Vi E
11, .., l} are the training instances and the product
K(oi, o) = (φ(oi) · φ(o)) is the kernel function
associated with the mapping φ.
Kernel engineering can be carried out by com-
bining basic kernels with additive or multiplica-
tive operators or by designing specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
Regarding NLP applications, kernel methods
have attracted much interest due to their ability
of implicitly exploring huge amounts of structural
features automatically extracted from the origi-
nal object representation. The kernels for struc-
tured natural language data, such as parse tree
kernel (Collins and Duffy, 2001) and string ker-
nel (Lodhi et al., 2002) are examples of the well-
known convolution kernels used in many NLP ap-
plications.
Tree kernels represent trees in terms of their
substructures (called tree fragments). Such frag-
ments form a feature space which, in turn, is
mapped into a vector space. Tree kernels mea-
sure the similarity between pair of trees by count-
ing the number of fragments in common. There
are three important characterizations of fragment
type (Moschitti, 2006): the SubTrees (ST), the
SubSet Trees (SST) and the Partial Trees (PT). For
sake of space, we do not report the mathematical
description of them, which is available in (Vish-
wanathan and Smola, 2002), (Collins and Duffy,
</bodyText>
<page confidence="0.948767">
1380
</page>
<bodyText confidence="0.999788307692308">
2001) and (Moschitti, 2006), respectively. In con-
trast, we report some descriptions in terms of fea-
ture space that may be useful to understand the
new engineered kernels.
In principle, a SubTree (ST) is defined by tak-
ing any node along with its descendants. A Sub-
Set Tree (SST) is a more general structure which
does not necessarily include all the descendants. It
must be generated by applying the same grammat-
ical rule set, which generated the original tree. A
Partial Tree (PT) is a more general form of sub-
structures obtained by relaxing constraints over
the SST.
</bodyText>
<sectionHeader confidence="0.999351" genericHeader="method">
4 Kernels for Relation Extraction
</sectionHeader>
<bodyText confidence="0.999984888888889">
In this section we describe the previous kernels
based on constituent trees as well as new kernels
based on diverse types of trees and sequences for
relation extraction. As mentioned in the previ-
ous section, we can engineer kernels by combin-
ing tree and sequence kernels. Thus we focus on
the problem to define structure embedding the de-
sired syntactic relational information between two
named entities (NEs).
</bodyText>
<subsectionHeader confidence="0.997835">
4.1 Constituent and Dependency Structures
</subsectionHeader>
<bodyText confidence="0.999543618421053">
Syntactic parsing (or syntactic analysis) aims at
identifying grammatical structures in a text. A
parser thus captures the hidden hierarchy of the
input text and processes it into a form suitable for
further processing. There are two main paradigms
for representing syntactic information: constituent
and dependency parsing, which produces two dif-
ferent tree structures.
Constituent tree encodes structural properties
of a sentence. The parse tree contains constituents,
such as noun phrases (NP) and verb phrases (VP),
as well as terminals/part-of-speech tags, such as
determiners (DT) or nouns (NN). Figure 2.a shows
the constituent tree of the sentence: In Washing-
ton, U.S. officials are working overtime.
Dependency tree encodes grammatical rela-
tions between words in a sentence with the words
as nodes and dependency types as edges. An edge
from a word to another represents a grammatical
relation between these two. Every word in a de-
pendency tree has exactly one parent except the
root. Figure 2.b shows and example of the depen-
dency tree of the previous sentence.
Given two NEs, such as Washington and offi-
cials, both the above trees can encode the syntactic
dependencies between them. However, since each
parse tree corresponds to a sentence, there may be
more than two NEs and many relations expressed
in a sentence. Thus, the use of the entire parse
tree of the whole sentence holds two major draw-
backs: first, it may be too computationally expen-
sive for kernel calculation since the size of a com-
plete parse tree may be very large (up to 300 nodes
in the Penn Treebank (Marcus et al., 1993)); sec-
ond, there is ambiguity on the target pairs of NEs,
i.e. different NEs associated with different rela-
tions are described by the same parse tree. There-
fore, it is necessary to identify the portion of the
parse tree that best represent the useful syntactic
information.
Let e1 and e2 be two entity mentions in the same
sentence such that they are in a relationship R.
For the constituent parse tree, we used the path-
enclosed tree (PET), which was firstly proposed
in (Moschitti, 2004) for Semantic Role Labeling
and then adapted by (Zhang et al., 2005) for re-
lation extraction. It is the smallest common sub-
tree including the two entities of a relation. The
dashed frame in Figure 2.a surrounds PET associ-
ated with the two mentions, officials and Washing-
ton. Moreover, to improve the representation, two
extra nodes T1-PER, denoting the type PERSON,
and T2-LOC, denoting the type LOCATION, are
added to the parse tree, above the two target NEs,
respectively. In this example, the above PET is de-
signed to capture the relation Located-in between
the entities ”officials” and ”Washington” from the
ACE corpus. Note that, a third NE, U.S., is char-
acterized by the node GPE (GeoPolitical Entity),
where the absence of the prefix T1 or T2 before
the NE type (i.e. GPE), denotes that the NE does
not take part in the target relation.
In previous work, some dependency trees have
been used (Bunescu and Mooney, 2005a; Wang,
2008) but the employed kernel just exploited the
syntactic information concentrated in the path be-
tween e1 and e2. In contrast, we defined and stud-
ied three different dependency structures whose
potential can be fully exploited by our convolution
partial tree kernel:
- Dependency Words (DW) tree is similar to
PET adapted for dependency tree constituted
by simple words. We select the minimal sub-
tree which includes e1 and e2, and we insert
an extra node as father of the NEs, labeled
with the NE category. For example, given
</bodyText>
<page confidence="0.990376">
1381
</page>
<figureCaption confidence="0.776469">
Figure 2: The constituent and dependency parse trees integrated with entity information
the tree in Figure 2.b, we design the tree in
Figure 2.c surrounded by the dashed frames,
</figureCaption>
<bodyText confidence="0.975108066666667">
where T1-PER, T2-LOC and GPE are the ex-
tra nodes inserted as fathers of Washington,
soldier and U.S..
- Grammatical Relation (GR) tree, i.e. the DW
tree in which words are replaced by their
grammatical functions, e.g. prep, pobj and
nsubj. For example, Figure 2.d, shows the
GR tree for the previous relation: In is re-
placed by prep, U.S. by nsubj and so on.
- Grammatical Relation and Words (GRW)
tree, words and grammatical functions are
both used in the tree, where the latter are in-
serted as a father node of the former. For
example, Figure 2.e, shows such tree for the
previous relation.
</bodyText>
<subsectionHeader confidence="0.99266">
4.2 Sequential Structures
</subsectionHeader>
<bodyText confidence="0.997548">
Some sequence kernels have been used on depen-
dency structures (Bunescu and Mooney, 2005b;
Wang, 2008). These kernels just used lexical
words with some syntactic information. To fully
exploit syntactic and semantic information, we de-
fined and studied six different sequences (in a style
similar to what proposed in (Moschitti, 2008)),
which include features from constituent and de-
pendency parse trees and NEs:
</bodyText>
<listItem confidence="0.99806275">
1. Sequence of terminals (lexical words) in the
PET (SK1), e.g.:
T2-LOC Washington, U.S. T1-PER officials.
2. Sequence of part-of-speech (POS) tags in the
PET (SK2), i.e. the SK1 in which words are
replaced by their POS tags, e.g.:
T2-LOC NN , NNP T1-PER NNS.
3. Sequence of grammatical relations in the
</listItem>
<page confidence="0.779296">
1382
</page>
<listItem confidence="0.988019857142857">
PET (SK3), i.e. the SK1 in which words are
replaced by their grammatical functions, e.g.:
T2-LOC pobj , nn T1-PER nsubj.
4. Sequence of words in the DW (SK4), e.g.:
Washington T2-LOC In working T1-PER of-
ficials GPE U.S..
5. Sequence of grammatical relations in the GR
(SK5), i.e. the SK4 in which words are re-
placed by their grammatical functions, e.g.:
pobj T2-LOC prep ROOT T1-PER nsubj GPE
nn.
6. Sequence of POS tags in the DW (SK6), i.e.
the SK4 in which words are replaced by their
POS tags, e.g.:
</listItem>
<equation confidence="0.7675835">
NN T2-LOC IN VBP T1-PER NNS GPE
NNP.
</equation>
<bodyText confidence="0.998927">
It is worth noting that the potential information
contained in such sequences can be fully exploited
by the word sequence kernel.
</bodyText>
<subsectionHeader confidence="0.999705">
4.3 Combining Kernels
</subsectionHeader>
<bodyText confidence="0.999985133333333">
Given that syntactic information from different
parse trees may have different impact on relation
extraction (RE), the viable approach to study the
role of dependency and constituent parsing is to
experiment with different syntactic models and
measuring the impact in terms of RE accuracy.
For this purpose we compared the composite ker-
nel described in (Zhang et al., 2006) with the par-
tial tree kernels applied to DW, GR, and GRW
and sequence kernels based on six sequences de-
scribed above. The composite kernels include
polynomial kernel applied to entity-related feature
vector. The word sequence kernel (WSK) is al-
ways applied to sequential structures. The used
kernels are described in more detail below.
</bodyText>
<subsectionHeader confidence="0.816568">
4.3.1 Polynomial Kernel
</subsectionHeader>
<bodyText confidence="0.999919">
The basic kernel between two named entities of
the ACE documents is defined as:
</bodyText>
<equation confidence="0.992634">
KP(R1, R2) = � KE(R1.Ei, R2.Ei),
i=1,2
</equation>
<bodyText confidence="0.999969">
where R1 and R2 are two relation instances, Ei is
the ith entity of a relation instance. KE(·, ·) is a
kernel over entity features, i.e.:
</bodyText>
<equation confidence="0.682583">
KE(E1, E2) = (1 + x1 · x2)2,
</equation>
<bodyText confidence="0.999963625">
where x1 and 92 are two feature vectors extracted
from the two NEs.
For the ACE 2004, the features used include:
entity headword, entity type, entity subtype, men-
tion type, and LDC2 mention type. The last four
attributes are taken from the ACE corpus 2004. In
ACE, each mention has a head annotation and an
extent annotation.
</bodyText>
<subsectionHeader confidence="0.480194">
4.3.2 Kernel Combinations
</subsectionHeader>
<listItem confidence="0.789533">
1. Polynomial kernel plus a tree kernel:
</listItem>
<equation confidence="0.57498">
CK1 = α · KP + (1 − α) · Kx,
</equation>
<bodyText confidence="0.995068166666667">
where α is a coefficient to give more impact
to KP and Kx is either the partial tree ker-
nel applied to one the possible dependency
structures, DW, GR or GRW or the SST ker-
nel applied to PET, described in the previous
section.
</bodyText>
<listItem confidence="0.770122">
2. Polynomial kernel plus constituent plus de-
pendency tree kernels:
</listItem>
<equation confidence="0.943895">
CK2 = α · KP + (1 − α) · (KSST + KPT)
</equation>
<bodyText confidence="0.995819333333333">
where KSST is the SST kernel and KPT is
the partial tree kernel (applied to the related
structures as in point 1).
</bodyText>
<listItem confidence="0.7524825">
3. Constituent tree plus square of polynomial
kernel and dependency tree kernel:
</listItem>
<equation confidence="0.76507025">
CK3 = α · KSST + (1 − α) · (KP + KPT)2
4. Dependency word tree plus grammatical re-
lation tree kernels:
CK4 = KPT−DW + KPT−GR
</equation>
<bodyText confidence="0.988461">
where KPT−DW and KPT−GR are the par-
tial tree kernels applied to dependency struc-
tures DW and GR.
Some preliminary experiments on a validation set
showed that the second, the fourth and the fifth
combinations yield the best performance with α =
0.4 while the first and the third combinations yield
the best performance with α = 0.23.
Regarding WSK, the following combinations
are applied:
</bodyText>
<footnote confidence="0.84133">
2Linguistic Data Consortium (LDC):
http://www.ldc.upenn.edu/Projects/ACE/
</footnote>
<listItem confidence="0.433438">
5. Polynomial kernel plus dependency word
plus grammatical relation tree kernels:
</listItem>
<equation confidence="0.553609">
CK5 = α·KP+(1−α)·(KPT−DW+KPT−GR)
</equation>
<page confidence="0.525433">
1383
</page>
<figure confidence="0.814738875">
1. 5K3 + 5K4
2. 5K3 + 5K6
3. 55K = Ez=1,..,6 5Kz
4. KSST + 55K
5. C5K = α · KP + (1 − α) · (KSST + 55K)
Preliminary experiments showed that the last com-
bination yields the best performance with α =
0.23.
</figure>
<bodyText confidence="0.999433333333333">
We used a polynomial expansion to explore the
bi-gram features of i) the first and the second en-
tity participating in the relation, ii) grammatical
relations which replace words in the dependency
tree. Since the kernel function set is closed un-
der normalization, polynomial expansion and lin-
ear combination (Schlkopf and Smola, 2001), all
the illustrated composite kernels are also proper
kernels.
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999994384615385">
Our experiments aim at investigating the effec-
tiveness of convolution kernels adapted to syntac-
tic parse trees and various sequence kernels for
the RE task. For this purpose, we use the sub-
set and partial tree kernel over different kinds of
trees, namely constituent and dependency syntac-
tic parse trees. Diverse sequences are applied indi-
vidually and in combination together. We consider
our task of relation extraction as a classification
problem where categories are relation types. All
pairs of entity mentions in the same sentence are
taken to generate potential relations, which will be
processed as positive and negative examples.
</bodyText>
<subsectionHeader confidence="0.995016">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999962142857143">
We use the newswire and broadcast news domain
in the English portion of the ACE 2004 corpus
provided by LDC. This data portion includes 348
documents and 4400 relation instances. It defines
seven entity types and seven relation types. Every
relation is assigned one of the seven types: Phys-
ical, Person/Social, Employment/Membership/-
Subsidiary, Agent-Artifact, PER/ORG Affiliation,
GPE Affiliation, and Discourse. For sake of space,
we do not explain these relationships here, never-
theless, they are explicitly described in the ACE
document guidelines. There are 4400 positive and
38,696 negative examples when generating pairs
of entity mentions as potential relations.
Documents are parsed using Stanford
Parser (Klein and Manning, 2003) to pro-
duce parse trees. Potential relations are generated
by iterating all pairs of entity mentions in the same
sentence. Entity information, namely entity type,
is integrated into parse trees. To train and test our
binary relation classifier, we used SVMs. Here,
relation detection is formulated as a multiclass
classification problem. The one vs. rest strategy
is employed by selecting the instance with largest
margin as the final answer. For experimentation,
we use 5-fold cross-validation with the Tree
Kernel Tools (Moschitti, 2004) (available at
http://disi.unitn.it/˜moschitt/Tree-Kernel.htm).
</bodyText>
<subsectionHeader confidence="0.878562">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999961828571429">
In this section, we report the results of different
kernels setup over constituent (CT) and depen-
dency (DP) parse trees and sequences taken from
these parse trees. The tree kernel (TK), compos-
ite kernel (CK1, CK2, CK3, CK4, and CK5
corresponding to five combination types in Sec-
tion 4.3.2) were employed over these two syntactic
trees. For the tree kernel, we apply the SST kernel
for the path-enclosed tree (PET) of the constituent
tree and the PT kernel for three kinds of depen-
dency tree DW, GR, and GRW, described in the
previous section. The two composite kernels CK2
and CK3 are applied over both two parse trees.
The word sequence kernels are applied over six
sequences 5K1, 5K2, 5K3, 5K4, 5K5, and 5K6
(described in Section 4.3).
The results are shown in Table 1 and Table 2.
In the first table, the first column indicates the
structure used in the combination shown in the
second column, e.g. PET associated with CK1
means that the SST kernel is applied on PET (a
portion of the constituent tree) and combined with
the CK1 schema whereas PET and GR associated
with CK5 means that SST kernel is applied to
PET and PT kernel is applied to GR in CK5. The
remaining three columns report Precision, Recall
and F1 measure. The interpretation of the second
table is more immediate since the only tree ker-
nel involved is the SST kernel applied to PET and
combined by means of CK1.
We note that: first, the dependency kernels,
i.e. the results on the rows from 3 to 6 are be-
low the composite kernel CK1, i.e. 68.9. This
is the state-of-the-art in RE, designed by (Zhang
et al., 2006), where our implementation provides
</bodyText>
<page confidence="0.974797">
1384
</page>
<table confidence="0.99955675">
Parse Tree Kernel P R F
PET CK1 69.5 68.3 68.9
DW CK1 53.2 59.7 56.3
GR CK1 58.8 61.7 60.2
GRW CK1 56.1 61.2 58.5
DW and GR CK5 59.7 64.1 61.8
PET and GR CK2 70.7 69.0 69.8
CK3 70.8 70.2 70.5
</table>
<tableCaption confidence="0.995767666666667">
Table 1: Results on the ACE 2004 evaluation test
set. Six structures were experimented over the
constituent and dependency trees.
</tableCaption>
<table confidence="0.9998322">
Kernel P R F
CK1 69.5 68.3 68.9
SK1 72.0 52.8 61.0
SK2 61.7 60.0 60.8
SK3 62.6 60.7 61.6
SK4 73.1 50.3 59.7
SK5 59.0 60.7 59.8
SK6 57.7 61.8 59.7
SK3 + SK4 75.0 63.4 68.8
SK3 + SK6 66.8 65.1 65.9
SSK = Ei SKi 73.8 66.2 69.8
CSK 75.6 66.6 70.8
CK1 + SSK 76.6 67.0 71.5
(Zhou et al., 2007) 82.2 70.2 75.8
CK1 with Heuristics
</table>
<tableCaption confidence="0.9672845">
Table 2: Performance comparison on the ACE
2004 data with different kernel setups.
</tableCaption>
<bodyText confidence="0.999895444444444">
a slightly smaller result than the original version
(i.e. an F1 of about 72 using a different syntactic
parser).
Second, CK1 improves to 70.5, when the con-
tribution of PT kernel applied to GR (dependency
tree built using grammatical relations) is added.
This suggests that dependency structures are effec-
tively exploited by PT kernel and that such infor-
mation is somewhat complementary to constituent
trees.
Third, in the second table, the model CK1 +
SSK, which adds to CK1 the contribution of di-
verse sequence kernels, outperforms the state-of-
the-art by 2.6%. This suggests that the sequential
information encoded by several sequence kernels
can better represents the dependency information.
Finally, we also report in the last row (in italic)
the superior RE result by (Zhou et al., 2007).
However, to achieve this outcome the authors used
the composite kernel CK1 with several heuristics
to define an effective portion of constituent trees.
Such heuristics expand the tree and remove unnec-
essary information allowing a higher improvement
on RE. They are tuned on the target RE task so al-
though the result is impressive, we cannot use it to
compare with pure automatic learning approaches,
such us our models.
</bodyText>
<sectionHeader confidence="0.996631" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999765">
In this paper, we study the use of several types
of syntactic information: constituent and depen-
dency syntactic parse trees. A relation is repre-
sented by taking the path-enclosed tree (PET) of
the constituent tree or of the path linking two enti-
ties of the dependency tree. For the design of auto-
matic relation classifiers, we have investigated the
impact of dependency structures to the RE task.
Our novel composite kernels, which account for
the two syntactic structures, are experimented with
the appropriate convolution kernels and show sig-
nificant improvement with respect to the state-of-
the-art in RE.
Regarding future work, there are many research
line that may be followed:
i) Capturing more features by employing ex-
ternal knowledge such as ontological, lexical re-
source or WordNet-based features (Basili et al.,
2005a; Basili et al., 2005b; Bloehdorn et al., 2006;
Bloehdorn and Moschitti, 2007) or shallow se-
mantic trees, (Giuglea and Moschitti, 2004; Giu-
glea and Moschitti, 2006; Moschitti and Bejan,
2004; Moschitti et al., 2007; Moschitti, 2008;
Moschitti et al., 2008).
ii) Design a new tree-based structures, which
combines the information of both constituent and
dependency parses. From dependency trees we
can extract more precise but also more sparse
relationships (which may cause overfit). From
constituent trees, we can extract subtrees consti-
tuted by non-terminal symbols (grammar sym-
bols), which provide a better generalization (with
a risk of underfitting).
iii) Design a new kernel which can integrate the
advantages of the constituent and dependency tree.
The new tree kernel should inherit the benefits of
the three available tree kernels: ST, SST or PT.
</bodyText>
<sectionHeader confidence="0.990361" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.5169815">
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
</bodyText>
<page confidence="0.96709">
1385
</page>
<reference confidence="0.999082703703704">
lections. In Proceedings of the 5th ACM Interna-
tional Conference on Digital Libraries.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005a. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 1–8, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005b. A semantic kernel to classify texts
with very few training examples. In In Proceedings
of the Workshop on Learning in Web Search, at the.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Structure and semantics for expressive text ker-
nels. In CIKM ’07: Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 861–864, New York,
NY, USA. ACM.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining (ICDM
06), Hong Kong, 18-22 December 2006, DEC.
Sergey Brin. 1998. Extracting patterns and relations
from world wide web. In Proceeding of WebDB
Workshop at 6th International Conference on Ex-
tending Database Technology, pages 172–183.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of EMNLP, pages 724–731.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of EMNLP.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence ker-
nels. Journal of Machine Learning Research, pages
1059–1082.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neu-
ral Information Processing Systems (NIPS’2001).
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting on ACL, Barcelona,
Spain.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge discovery using framenet, verbnet and
propbank. In A. Meyers, editor, Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic Role Labeling via Framenet, Verbnet and
Propbank. In Proceedings of ACL 2006, Sydney,
Australia.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named en-
tities from large corpora. In Proceedings of the 42nd
Annual Meeting on ACL, Barcelona, Spain.
Nanda Kambhatla. 2004. Combining lexical, syntactic
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions, Barcelona, Spain.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the ACL, pages 423–430.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, , and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, pages 419–444.
Mitchell P. Marcus, Beatrice Santorini, , and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: the penn treebank. Computa-
tional Linguistics, 19(2):313–330.
Scott Miller, Heidi Fox, Lance Ramshaw, , and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st conference on North American chapter of the
ACL, pages 226–233, Seattle, USA.
Alessandro Moschitti and Cosmin Bejan. 2004. A se-
mantic kernel for predicate argument classification.
In CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question/answer classification. In Proceedings of
ACL’07, Prague, Czech Republic.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34(2):193–224.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Meeting of the ACL, Barcelona,
Spain.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning, Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM ’08: Proceeding of the 17th ACM conference
on Information and knowledge management, pages
253–262, New York, NY, USA. ACM.
Ryan Michael Rifkin and Tomaso Poggio. 2002. Ev-
erything old is new again: afresh look at historical
approaches in machine learning. PhD thesis, Mas-
sachusetts Institute of Technology.
</reference>
<page confidence="0.823801">
1386
</page>
<reference confidence="0.999882326923077">
Klaus robert Mller, Sebastian Mika, Gunnar Rtsch,
Koji Tsuda, , and Bernhard Schlkopf. 2001. An
introduction to kernel-based learning algorithms.
IEEE Transactions on Neural Networks, 12(2):181–
201.
Dan Roth and Wen tau Yih. 2002. Probabilistic rea-
soning for entity and relation recognition. In Pro-
ceedings of the COLING-2002, Taipei, Taiwan.
Bernhard Schlkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer–Verlag, New York.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. John Wiley and Sons, New York.
S.V.N. Vishwanathan and Alexander J. Smola. 2002.
Fast kernels on strings and trees. In Proceedings of
Neural Information Processing Systems.
Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In Pro-
ceedings of the 3rd International Joint Conference
on Natural Language Processing-IJCNLP.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181–201.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations be-
tween named entities from a large raw corpus using
tree similarity-based clustering. In Proceedings of
IJCNLP’2005, Lecture Notes in Computer Science
(LNCS 3651), pages 378–389, Jeju Island, South
Korea.
Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of COLING-ACL 2006, pages 825–
832.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Meeting of the
ACL, pages 419–426, Ann Arbor, Michigan, USA.
GuoDong Zhou, Jian Su, Jie Zhang, , and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Meeting of the
ACL, pages 427–434, Ann Arbor, USA, June.
GuoDong Zhou, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse
tree information. In Proceedings of EMNLP-CoNLL
2007, pages 728–736.
</reference>
<page confidence="0.993773">
1387
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978590">
<title confidence="0.9994165">Convolution Kernels on Constituent, Dependency and Structures for Relation Extraction</title>
<author confidence="0.999881">T Nguyen Moschitti</author>
<affiliation confidence="0.999355">Department of Information Engineering and Computer University of</affiliation>
<address confidence="0.996969">38050 Povo (TN), Italy</address>
<abstract confidence="0.999062777777778">This paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task. Syntax is derived from constituent and dependency parse trees whereas semantics concerns to entity types and lexical sequences. We investigate the effectiveness of such representations in the automated relation extraction from texts. We process the above data by means of Support Vector Machines along with the syntactic tree, the partial tree and the word sequence kernels. Our study on the ACE 2004 corpus illustrates that the combination of the above kernels achieves high effectiveness and significantly improves the current state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>lections</author>
</authors>
<booktitle>In Proceedings of the 5th ACM International Conference on Digital Libraries.</booktitle>
<marker>lections, </marker>
<rawString>lections. In Proceedings of the 5th ACM International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Effective use of WordNet semantics via kernel-based learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="29400" citStr="Basili et al., 2005" startWordPosition="4832" endWordPosition="4835">nt tree or of the path linking two entities of the dependency tree. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provid</context>
</contexts>
<marker>Basili, Cammisa, Moschitti, 2005</marker>
<rawString>Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2005a. Effective use of WordNet semantics via kernel-based learning. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 1–8, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A semantic kernel to classify texts with very few training examples. In</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Learning in Web Search, at the.</booktitle>
<contexts>
<context position="29400" citStr="Basili et al., 2005" startWordPosition="4832" endWordPosition="4835">nt tree or of the path linking two entities of the dependency tree. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provid</context>
</contexts>
<marker>Basili, Cammisa, Moschitti, 2005</marker>
<rawString>Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2005b. A semantic kernel to classify texts with very few training examples. In In Proceedings of the Workshop on Learning in Web Search, at the.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structure and semantics for expressive text kernels.</title>
<date>2007</date>
<booktitle>In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>861--864</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="29479" citStr="Bloehdorn and Moschitti, 2007" startWordPosition="4844" endWordPosition="4847">For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kern</context>
</contexts>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007. Structure and semantics for expressive text kernels. In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 861–864, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic kernels for text classification based on topological measures of feature similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 06), Hong Kong,</booktitle>
<pages>18--22</pages>
<contexts>
<context position="29447" citStr="Bloehdorn et al., 2006" startWordPosition="4840" endWordPosition="4843">of the dependency tree. For the design of automatic relation classifiers, we have investigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of under</context>
</contexts>
<marker>Bloehdorn, Basili, Cammisa, Moschitti, 2006</marker>
<rawString>Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2006. Semantic kernels for text classification based on topological measures of feature similarity. In Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 06), Hong Kong, 18-22 December 2006, DEC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from world wide web.</title>
<date>1998</date>
<booktitle>In Proceeding of WebDB Workshop at 6th International Conference on Extending Database Technology,</booktitle>
<pages>172--183</pages>
<contexts>
<context position="6892" citStr="Brin, 1998" startWordPosition="1059" endWordPosition="1060">dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated t</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting patterns and relations from world wide web. In Proceeding of WebDB Workshop at 6th International Conference on Extending Database Technology, pages 172–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>724--731</pages>
<contexts>
<context position="1700" citStr="Bunescu and Mooney, 2005" startWordPosition="249" endWordPosition="252">ng relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. Jeff Zucker, the longtime executive producer of NBC’s ”Today” program, will be named Friday as the new president of NBC’s entertainment division, replacing Garth Ancier, NBC executives said. Figure 1: A document from ACE 2004 with all entity mentions in bold. features without an explicit feature representation</context>
<context position="3226" citStr="Bunescu and Mooney, 2005" startWordPosition="495" endWordPosition="498">s the number of common subsequences of characters in the two strings. Such substrings are then weighted according to a decaying factor penalizing longer ones. In the same line, Tree Kernels count the number of subtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1</context>
<context position="7298" citStr="Bunescu and Mooney, 2005" startWordPosition="1118" endWordPosition="1121">ree learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be used effectively for information extraction. Meanwhile, feature-based approaches often employ various kinds of linguistic, syntactic or contextual information and integrate into the feature space. (Roth and tau Yih, 2002) applied a probabilistic approach to solve the problems of named entity and relation extraction with the inco</context>
<context position="8724" citStr="Bunescu and Mooney, 2005" startWordPosition="1329" endWordPosition="1332">tion types and the number of words (if any) separating the two entities. Recent work on Relation Extraction has mostly employed kernel-based approaches over syntactic parse trees. Kernels on parse trees were pioneered by (Collins and Duffy, 2001). This kernel function counts the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees. (Culotta and Sorensen, 2004) extended this work to calculate kernels between augmented dependency trees. (Zelenko et al., 2002) proposed extracting 1379 relations by computing kernel functions between parse trees. (Bunescu and Mooney, 2005a) proposed a shortest path dependency kernel by stipulating that the information to model a relationship between two entities can be captured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008). These are not convolution kernels and produce a much lower number of substructures than the PT kernel. A </context>
<context position="16921" citStr="Bunescu and Mooney, 2005" startWordPosition="2719" endWordPosition="2722">representation, two extra nodes T1-PER, denoting the type PERSON, and T2-LOC, denoting the type LOCATION, are added to the parse tree, above the two target NEs, respectively. In this example, the above PET is designed to capture the relation Located-in between the entities ”officials” and ”Washington” from the ACE corpus. Note that, a third NE, U.S., is characterized by the node GPE (GeoPolitical Entity), where the absence of the prefix T1 or T2 before the NE type (i.e. GPE), denotes that the NE does not take part in the target relation. In previous work, some dependency trees have been used (Bunescu and Mooney, 2005a; Wang, 2008) but the employed kernel just exploited the syntactic information concentrated in the path between e1 and e2. In contrast, we defined and studied three different dependency structures whose potential can be fully exploited by our convolution partial tree kernel: - Dependency Words (DW) tree is similar to PET adapted for dependency tree constituted by simple words. We select the minimal subtree which includes e1 and e2, and we insert an extra node as father of the NEs, labeled with the NE category. For example, given 1381 Figure 2: The constituent and dependency parse trees integr</context>
<context position="18346" citStr="Bunescu and Mooney, 2005" startWordPosition="2962" endWordPosition="2965">, soldier and U.S.. - Grammatical Relation (GR) tree, i.e. the DW tree in which words are replaced by their grammatical functions, e.g. prep, pobj and nsubj. For example, Figure 2.d, shows the GR tree for the previous relation: In is replaced by prep, U.S. by nsubj and so on. - Grammatical Relation and Words (GRW) tree, words and grammatical functions are both used in the tree, where the latter are inserted as a father node of the former. For example, Figure 2.e, shows such tree for the previous relation. 4.2 Sequential Structures Some sequence kernels have been used on dependency structures (Bunescu and Mooney, 2005b; Wang, 2008). These kernels just used lexical words with some syntactic information. To fully exploit syntactic and semantic information, we defined and studied six different sequences (in a style similar to what proposed in (Moschitti, 2008)), which include features from constituent and dependency parse trees and NEs: 1. Sequence of terminals (lexical words) in the PET (SK1), e.g.: T2-LOC Washington, U.S. T1-PER officials. 2. Sequence of part-of-speech (POS) tags in the PET (SK2), i.e. the SK1 in which words are replaced by their POS tags, e.g.: T2-LOC NN , NNP T1-PER NNS. 3. Sequence of gr</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005a. A shortest path dependency kernel for relation extraction. In Proceedings of EMNLP, pages 724–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1700" citStr="Bunescu and Mooney, 2005" startWordPosition="249" endWordPosition="252">ng relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. Jeff Zucker, the longtime executive producer of NBC’s ”Today” program, will be named Friday as the new president of NBC’s entertainment division, replacing Garth Ancier, NBC executives said. Figure 1: A document from ACE 2004 with all entity mentions in bold. features without an explicit feature representation</context>
<context position="3226" citStr="Bunescu and Mooney, 2005" startWordPosition="495" endWordPosition="498">s the number of common subsequences of characters in the two strings. Such substrings are then weighted according to a decaying factor penalizing longer ones. In the same line, Tree Kernels count the number of subtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1</context>
<context position="7298" citStr="Bunescu and Mooney, 2005" startWordPosition="1118" endWordPosition="1121">ree learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be used effectively for information extraction. Meanwhile, feature-based approaches often employ various kinds of linguistic, syntactic or contextual information and integrate into the feature space. (Roth and tau Yih, 2002) applied a probabilistic approach to solve the problems of named entity and relation extraction with the inco</context>
<context position="8724" citStr="Bunescu and Mooney, 2005" startWordPosition="1329" endWordPosition="1332">tion types and the number of words (if any) separating the two entities. Recent work on Relation Extraction has mostly employed kernel-based approaches over syntactic parse trees. Kernels on parse trees were pioneered by (Collins and Duffy, 2001). This kernel function counts the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees. (Culotta and Sorensen, 2004) extended this work to calculate kernels between augmented dependency trees. (Zelenko et al., 2002) proposed extracting 1379 relations by computing kernel functions between parse trees. (Bunescu and Mooney, 2005a) proposed a shortest path dependency kernel by stipulating that the information to model a relationship between two entities can be captured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008). These are not convolution kernels and produce a much lower number of substructures than the PT kernel. A </context>
<context position="16921" citStr="Bunescu and Mooney, 2005" startWordPosition="2719" endWordPosition="2722">representation, two extra nodes T1-PER, denoting the type PERSON, and T2-LOC, denoting the type LOCATION, are added to the parse tree, above the two target NEs, respectively. In this example, the above PET is designed to capture the relation Located-in between the entities ”officials” and ”Washington” from the ACE corpus. Note that, a third NE, U.S., is characterized by the node GPE (GeoPolitical Entity), where the absence of the prefix T1 or T2 before the NE type (i.e. GPE), denotes that the NE does not take part in the target relation. In previous work, some dependency trees have been used (Bunescu and Mooney, 2005a; Wang, 2008) but the employed kernel just exploited the syntactic information concentrated in the path between e1 and e2. In contrast, we defined and studied three different dependency structures whose potential can be fully exploited by our convolution partial tree kernel: - Dependency Words (DW) tree is similar to PET adapted for dependency tree constituted by simple words. We select the minimal subtree which includes e1 and e2, and we insert an extra node as father of the NEs, labeled with the NE category. For example, given 1381 Figure 2: The constituent and dependency parse trees integr</context>
<context position="18346" citStr="Bunescu and Mooney, 2005" startWordPosition="2962" endWordPosition="2965">, soldier and U.S.. - Grammatical Relation (GR) tree, i.e. the DW tree in which words are replaced by their grammatical functions, e.g. prep, pobj and nsubj. For example, Figure 2.d, shows the GR tree for the previous relation: In is replaced by prep, U.S. by nsubj and so on. - Grammatical Relation and Words (GRW) tree, words and grammatical functions are both used in the tree, where the latter are inserted as a father node of the former. For example, Figure 2.e, shows such tree for the previous relation. 4.2 Sequential Structures Some sequence kernels have been used on dependency structures (Bunescu and Mooney, 2005b; Wang, 2008). These kernels just used lexical words with some syntactic information. To fully exploit syntactic and semantic information, we defined and studied six different sequences (in a style similar to what proposed in (Moschitti, 2008)), which include features from constituent and dependency parse trees and NEs: 1. Sequence of terminals (lexical words) in the PET (SK1), e.g.: T2-LOC Washington, U.S. T1-PER officials. 2. Sequence of part-of-speech (POS) tags in the PET (SK2), i.e. the SK1 in which words are replaced by their POS tags, e.g.: T2-LOC NN , NNP T1-PER NNS. 3. Sequence of gr</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005b. Subsequence kernels for relation extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Jean Michel Renders</author>
</authors>
<title>Word sequence kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1059--1082</pages>
<contexts>
<context position="4877" citStr="Cancedda et al., 2003" startWordPosition="752" endWordPosition="755">luded in such global study since some of their forms have not been applied to RE. In this paper, we study and evaluate diverse convolution and sequence kernels for the RE problem by providing several kernel combinations on constituent and dependency trees and sequential structures. To fully exploit the potential of dependency trees, in addition to the SST kernel, we applied the partial tree (PT) kernel proposed in (Moschitti, 2006), which is a general convolution tree kernel adaptable for dependency structures. We also investigate various sequence kernels (e.g. the word sequence kernel (WSK) (Cancedda et al., 2003)) by incorporating dependency structures into word sequences. These are also enriched by including information from constituent parse trees. We conduct experiments on the standard ACE 2004 newswire and broadcast news domain. The results show that although some kernels are less effective than others, they exhibit properties that are complementary to each other. In particular, we found that relation extraction can benefit from increasing the feature space by combining kernels (with a simple summation) exploiting the two different parsing paradigms. Our experiments on RE show that the current com</context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, pages 1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS’2001).</booktitle>
<contexts>
<context position="2934" citStr="Collins and Duffy, 2001" startWordPosition="449" endWordPosition="452"> be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where the objects are strings of characters and the kernel function computes the number of common subsequences of characters in the two strings. Such substrings are then weighted according to a decaying factor penalizing longer ones. In the same line, Tree Kernels count the number of subtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural </context>
<context position="8346" citStr="Collins and Duffy, 2001" startWordPosition="1275" endWordPosition="1278">tion and integrate into the feature space. (Roth and tau Yih, 2002) applied a probabilistic approach to solve the problems of named entity and relation extraction with the incorporation of various features such as word, part-of-speech, and semantic information from WordNet. (Kambhatla, 2004) employed maximum entropy models with diverse features including words, entity and mention types and the number of words (if any) separating the two entities. Recent work on Relation Extraction has mostly employed kernel-based approaches over syntactic parse trees. Kernels on parse trees were pioneered by (Collins and Duffy, 2001). This kernel function counts the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees. (Culotta and Sorensen, 2004) extended this work to calculate kernels between augmented dependency trees. (Zelenko et al., 2002) proposed extracting 1379 relations by computing kernel functions between parse trees. (Bunescu and Mooney, 2005a) proposed a shortest path dependency kernel by stipulating that the information to model a relationship between two entities can be captured by the shortest path between them in the dependency graph. Although approaches </context>
<context position="12155" citStr="Collins and Duffy, 2001" startWordPosition="1920" endWordPosition="1923"> and the product K(oi, o) = (φ(oi) · φ(o)) is the kernel function associated with the mapping φ. Kernel engineering can be carried out by combining basic kernels with additive or multiplicative operators or by designing specific data objects (vectors, sequences and tree structures) for the target tasks. Regarding NLP applications, kernel methods have attracted much interest due to their ability of implicitly exploring huge amounts of structural features automatically extracted from the original object representation. The kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001) and string kernel (Lodhi et al., 2002) are examples of the wellknown convolution kernels used in many NLP applications. Tree kernels represent trees in terms of their substructures (called tree fragments). Such fragments form a feature space which, in turn, is mapped into a vector space. Tree kernels measure the similarity between pair of trees by counting the number of fragments in common. There are three important characterizations of fragment type (Moschitti, 2006): the SubTrees (ST), the SubSet Trees (SST) and the Partial Trees (PT). For sake of space, we do not report the mathematical de</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of Neural Information Processing Systems (NIPS’2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on ACL,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1674" citStr="Culotta and Sorensen, 2004" startWordPosition="245" endWordPosition="248"> in ACE as the task of finding relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. Jeff Zucker, the longtime executive producer of NBC’s ”Today” program, will be named Friday as the new president of NBC’s entertainment division, replacing Garth Ancier, NBC executives said. Figure 1: A document from ACE 2004 with all entity mentions in bold. features without an expli</context>
<context position="3133" citStr="Culotta and Sorensen, 2004" startWordPosition="480" endWordPosition="483">odhi et al., 2002), where the objects are strings of characters and the kernel function computes the number of common subsequences of characters in the two strings. Such substrings are then weighted according to a decaying factor penalizing longer ones. In the same line, Tree Kernels count the number of subtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Pr</context>
<context position="6268" citStr="Culotta and Sorensen, 2004" startWordPosition="956" endWordPosition="959">ons, i.e. dependency plus constituent trees, improve the state-of-the-art in RE. More interestingly, also the combinations of various sequence kernels gain significant better performance than the current state-of-the-art (Zhang et al., 2005). Overall, these results are interesting for the computational linguistics research since they show that the above two parsing paradigms provide different and important information for a semantic task such as RE. Regarding sequence-based kernels, the WSK gains better performance than previous sequence and dependency models for RE. 1The function defined on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi superv</context>
<context position="8513" citStr="Culotta and Sorensen, 2004" startWordPosition="1299" endWordPosition="1302">ith the incorporation of various features such as word, part-of-speech, and semantic information from WordNet. (Kambhatla, 2004) employed maximum entropy models with diverse features including words, entity and mention types and the number of words (if any) separating the two entities. Recent work on Relation Extraction has mostly employed kernel-based approaches over syntactic parse trees. Kernels on parse trees were pioneered by (Collins and Duffy, 2001). This kernel function counts the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees. (Culotta and Sorensen, 2004) extended this work to calculate kernels between augmented dependency trees. (Zelenko et al., 2002) proposed extracting 1379 relations by computing kernel functions between parse trees. (Bunescu and Mooney, 2005a) proposed a shortest path dependency kernel by stipulating that the information to model a relationship between two entities can be captured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zele</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting on ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Giuglea</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Knowledge discovery using framenet, verbnet and propbank.</title>
<date>2004</date>
<booktitle>Workshop on Ontology and Knowledge Discovering at ECML 2004,</booktitle>
<editor>In A. Meyers, editor,</editor>
<location>Pisa, Italy.</location>
<contexts>
<context position="29535" citStr="Giuglea and Moschitti, 2004" startWordPosition="4853" endWordPosition="4856">nvestigated the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent</context>
</contexts>
<marker>Giuglea, Moschitti, 2004</marker>
<rawString>Ana-Maria Giuglea and Alessandro Moschitti. 2004. Knowledge discovery using framenet, verbnet and propbank. In A. Meyers, editor, Workshop on Ontology and Knowledge Discovering at ECML 2004, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Giuglea</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic Role Labeling via Framenet, Verbnet and Propbank.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL 2006,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="29564" citStr="Giuglea and Moschitti, 2006" startWordPosition="4857" endWordPosition="4861">endency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new</context>
</contexts>
<marker>Giuglea, Moschitti, 2006</marker>
<rawString>Ana-Maria Giuglea and Alessandro Moschitti. 2006. Semantic Role Labeling via Framenet, Verbnet and Propbank. In Proceedings of ACL 2006, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on ACL,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="6971" citStr="Hasegawa et al., 2004" startWordPosition="1068" endWordPosition="1071">vious work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be use</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting on ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic and semantic features with maximum entropy models for extracting relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="6835" citStr="Kambhatla, 2004" startWordPosition="1050" endWordPosition="1051">function defined on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing tec</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic and semantic features with maximum entropy models for extracting relations. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="24366" citStr="Klein and Manning, 2003" startWordPosition="3974" endWordPosition="3977">a portion includes 348 documents and 4400 relation instances. It defines seven entity types and seven relation types. Every relation is assigned one of the seven types: Physical, Person/Social, Employment/Membership/- Subsidiary, Agent-Artifact, PER/ORG Affiliation, GPE Affiliation, and Discourse. For sake of space, we do not explain these relationships here, nevertheless, they are explicitly described in the ACE document guidelines. There are 4400 positive and 38,696 negative examples when generating pairs of entity mentions as potential relations. Documents are parsed using Stanford Parser (Klein and Manning, 2003) to produce parse trees. Potential relations are generated by iterating all pairs of entity mentions in the same sentence. Entity information, namely entity type, is integrated into parse trees. To train and test our binary relation classifier, we used SVMs. Here, relation detection is formulated as a multiclass classification problem. The one vs. rest strategy is employed by selecting the instance with largest margin as the final answer. For experimentation, we use 5-fold cross-validation with the Tree Kernel Tools (Moschitti, 2004) (available at http://disi.unitn.it/˜moschitt/Tree-Kernel.htm</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>419--444</pages>
<contexts>
<context position="2525" citStr="Lodhi et al., 2002" startWordPosition="377" endWordPosition="380">ivememories.org/) and Expert System (http://www.expertsystem.net/) research grant. Jeff Zucker, the longtime executive producer of NBC’s ”Today” program, will be named Friday as the new president of NBC’s entertainment division, replacing Garth Ancier, NBC executives said. Figure 1: A document from ACE 2004 with all entity mentions in bold. features without an explicit feature representation. This can be done by computing a kernel function between a pair of linguistic objects, where such function is a kind of similarity measure satisfying certain properties. An example is the sequence kernel (Lodhi et al., 2002), where the objects are strings of characters and the kernel function computes the number of common subsequences of characters in the two strings. Such substrings are then weighted according to a decaying factor penalizing longer ones. In the same line, Tree Kernels count the number of subtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorens</context>
<context position="12194" citStr="Lodhi et al., 2002" startWordPosition="1928" endWordPosition="1931">s the kernel function associated with the mapping φ. Kernel engineering can be carried out by combining basic kernels with additive or multiplicative operators or by designing specific data objects (vectors, sequences and tree structures) for the target tasks. Regarding NLP applications, kernel methods have attracted much interest due to their ability of implicitly exploring huge amounts of structural features automatically extracted from the original object representation. The kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001) and string kernel (Lodhi et al., 2002) are examples of the wellknown convolution kernels used in many NLP applications. Tree kernels represent trees in terms of their substructures (called tree fragments). Such fragments form a feature space which, in turn, is mapped into a vector space. Tree kernels measure the similarity between pair of trees by counting the number of fragments in common. There are three important characterizations of fragment type (Moschitti, 2006): the SubTrees (ST), the SubSet Trees (SST) and the Partial Trees (PT). For sake of space, we do not report the mathematical description of them, which is available i</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, 2002</marker>
<rawString>Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, , and Chris Watkins. 2002. Text classification using string kernels. Journal of Machine Learning Research, pages 419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Marcus, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, , and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st conference on North American chapter of the ACL,</booktitle>
<pages>226--233</pages>
<location>Seattle, USA.</location>
<contexts>
<context position="6768" citStr="Miller et al., 2000" startWordPosition="1038" endWordPosition="1041"> performance than previous sequence and dependency models for RE. 1The function defined on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning m</context>
</contexts>
<marker>Miller, Fox, Ramshaw, 2000</marker>
<rawString>Scott Miller, Heidi Fox, Lance Ramshaw, , and Ralph Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Proceedings of the 1st conference on North American chapter of the ACL, pages 226–233, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Cosmin Bejan</author>
</authors>
<title>A semantic kernel for predicate argument classification.</title>
<date>2004</date>
<booktitle>In CoNLL-2004,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="29591" citStr="Moschitti and Bejan, 2004" startWordPosition="4862" endWordPosition="4865">task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit</context>
</contexts>
<marker>Moschitti, Bejan, 2004</marker>
<rawString>Alessandro Moschitti and Cosmin Bejan. 2004. A semantic kernel for predicate argument classification. In CoNLL-2004, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question/answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="29615" citStr="Moschitti et al., 2007" startWordPosition="4866" endWordPosition="4869">ernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit the benefits of the thr</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question/answer classification. In Proceedings of ACL’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="29657" citStr="Moschitti et al., 2008" startWordPosition="4872" endWordPosition="4875">ic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit the benefits of the three available tree kernels: ST, SST or PT. </context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="15997" citStr="Moschitti, 2004" startWordPosition="2563" endWordPosition="2564">alculation since the size of a complete parse tree may be very large (up to 300 nodes in the Penn Treebank (Marcus et al., 1993)); second, there is ambiguity on the target pairs of NEs, i.e. different NEs associated with different relations are described by the same parse tree. Therefore, it is necessary to identify the portion of the parse tree that best represent the useful syntactic information. Let e1 and e2 be two entity mentions in the same sentence such that they are in a relationship R. For the constituent parse tree, we used the pathenclosed tree (PET), which was firstly proposed in (Moschitti, 2004) for Semantic Role Labeling and then adapted by (Zhang et al., 2005) for relation extraction. It is the smallest common subtree including the two entities of a relation. The dashed frame in Figure 2.a surrounds PET associated with the two mentions, officials and Washington. Moreover, to improve the representation, two extra nodes T1-PER, denoting the type PERSON, and T2-LOC, denoting the type LOCATION, are added to the parse tree, above the two target NEs, respectively. In this example, the above PET is designed to capture the relation Located-in between the entities ”officials” and ”Washingto</context>
<context position="24905" citStr="Moschitti, 2004" startWordPosition="4058" endWordPosition="4059">lations. Documents are parsed using Stanford Parser (Klein and Manning, 2003) to produce parse trees. Potential relations are generated by iterating all pairs of entity mentions in the same sentence. Entity information, namely entity type, is integrated into parse trees. To train and test our binary relation classifier, we used SVMs. Here, relation detection is formulated as a multiclass classification problem. The one vs. rest strategy is employed by selecting the instance with largest margin as the final answer. For experimentation, we use 5-fold cross-validation with the Tree Kernel Tools (Moschitti, 2004) (available at http://disi.unitn.it/˜moschitt/Tree-Kernel.htm). 5.2 Results In this section, we report the results of different kernels setup over constituent (CT) and dependency (DP) parse trees and sequences taken from these parse trees. The tree kernel (TK), composite kernel (CK1, CK2, CK3, CK4, and CK5 corresponding to five combination types in Section 4.3.2) were employed over these two syntactic trees. For the tree kernel, we apply the SST kernel for the path-enclosed tree (PET) of the constituent tree and the PT kernel for three kinds of dependency tree DW, GR, and GRW, described in the</context>
<context position="29535" citStr="Moschitti, 2004" startWordPosition="4855" endWordPosition="4856">the impact of dependency structures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In Proceedings of the 42nd Meeting of the ACL, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of the 17th European Conference on Machine Learning,</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="4690" citStr="Moschitti, 2006" startWordPosition="726" endWordPosition="727">orthwhile also considering that, to the best of our knowledge, these models have not been previously used for relation extraction1 task. Additionally, sequence kernels should be included in such global study since some of their forms have not been applied to RE. In this paper, we study and evaluate diverse convolution and sequence kernels for the RE problem by providing several kernel combinations on constituent and dependency trees and sequential structures. To fully exploit the potential of dependency trees, in addition to the SST kernel, we applied the partial tree (PT) kernel proposed in (Moschitti, 2006), which is a general convolution tree kernel adaptable for dependency structures. We also investigate various sequence kernels (e.g. the word sequence kernel (WSK) (Cancedda et al., 2003)) by incorporating dependency structures into word sequences. These are also enriched by including information from constituent parse trees. We conduct experiments on the standard ACE 2004 newswire and broadcast news domain. The results show that although some kernels are less effective than others, they exhibit properties that are complementary to each other. In particular, we found that relation extraction c</context>
<context position="12628" citStr="Moschitti, 2006" startWordPosition="2001" endWordPosition="2002"> from the original object representation. The kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001) and string kernel (Lodhi et al., 2002) are examples of the wellknown convolution kernels used in many NLP applications. Tree kernels represent trees in terms of their substructures (called tree fragments). Such fragments form a feature space which, in turn, is mapped into a vector space. Tree kernels measure the similarity between pair of trees by counting the number of fragments in common. There are three important characterizations of fragment type (Moschitti, 2006): the SubTrees (ST), the SubSet Trees (SST) and the Partial Trees (PT). For sake of space, we do not report the mathematical description of them, which is available in (Vishwanathan and Smola, 2002), (Collins and Duffy, 1380 2001) and (Moschitti, 2006), respectively. In contrast, we report some descriptions in terms of feature space that may be useful to understand the new engineered kernels. In principle, a SubTree (ST) is defined by taking any node along with its descendants. A SubSet Tree (SST) is a more general structure which does not necessarily include all the descendants. It must be ge</context>
<context position="29564" citStr="Moschitti, 2006" startWordPosition="4860" endWordPosition="4861">ctures to the RE task. Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of the 17th European Conference on Machine Learning, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>253--262</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18590" citStr="Moschitti, 2008" startWordPosition="3001" endWordPosition="3002">U.S. by nsubj and so on. - Grammatical Relation and Words (GRW) tree, words and grammatical functions are both used in the tree, where the latter are inserted as a father node of the former. For example, Figure 2.e, shows such tree for the previous relation. 4.2 Sequential Structures Some sequence kernels have been used on dependency structures (Bunescu and Mooney, 2005b; Wang, 2008). These kernels just used lexical words with some syntactic information. To fully exploit syntactic and semantic information, we defined and studied six different sequences (in a style similar to what proposed in (Moschitti, 2008)), which include features from constituent and dependency parse trees and NEs: 1. Sequence of terminals (lexical words) in the PET (SK1), e.g.: T2-LOC Washington, U.S. T1-PER officials. 2. Sequence of part-of-speech (POS) tags in the PET (SK2), i.e. the SK1 in which words are replaced by their POS tags, e.g.: T2-LOC NN , NNP T1-PER NNS. 3. Sequence of grammatical relations in the 1382 PET (SK3), i.e. the SK1 in which words are replaced by their grammatical functions, e.g.: T2-LOC pobj , nn T1-PER nsubj. 4. Sequence of words in the DW (SK4), e.g.: Washington T2-LOC In working T1-PER officials G</context>
<context position="29632" citStr="Moschitti, 2008" startWordPosition="4870" endWordPosition="4871">r the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE. Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti et al., 2007; Moschitti, 2008; Moschitti et al., 2008). ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. From dependency trees we can extract more precise but also more sparse relationships (which may cause overfit). From constituent trees, we can extract subtrees constituted by non-terminal symbols (grammar symbols), which provide a better generalization (with a risk of underfitting). iii) Design a new kernel which can integrate the advantages of the constituent and dependency tree. The new tree kernel should inherit the benefits of the three available tree</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In CIKM ’08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 253–262, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Michael Rifkin</author>
<author>Tomaso Poggio</author>
</authors>
<title>Everything old is new again: afresh look at historical approaches in machine learning.</title>
<date>2002</date>
<tech>PhD thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="10540" citStr="Rifkin and Poggio, 2002" startWordPosition="1633" endWordPosition="1637">ich can be applied to the RE task. 3.1 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique based on the latest results of the statistical learning theory (Vapnik, 1998). Given a vector space and a set of training points, i.e. positive and negative examples, SVMs find a separating hyperplane H(~x) = ω~ x x~ + b = 0 where ω E Rn and b E R are learned by applying the Structural Risk Minimization principle (Vapnik, 1995). SVMs is a binary classifier, but it can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002). One strong point of SVMs is the possibility to apply kernel methods (robert Mller et al., 2001) to implicitly map data in a new space where the examples are more easily separable as described in the next section. 3.2 Kernel Methods Kernel methods (Schlkopf and Smola, 2001) are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the explicit feature representation. A kernel function is a scalar product in a possibly unknown feature space. More precisely, The o</context>
</contexts>
<marker>Rifkin, Poggio, 2002</marker>
<rawString>Ryan Michael Rifkin and Tomaso Poggio. 2002. Everything old is new again: afresh look at historical approaches in machine learning. PhD thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus robert Mller</author>
<author>Sebastian Mika</author>
<author>Gunnar Rtsch</author>
<author>Koji Tsuda</author>
</authors>
<title>An introduction to kernel-based learning algorithms.</title>
<date>2001</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>201</pages>
<contexts>
<context position="10637" citStr="Mller et al., 2001" startWordPosition="1651" endWordPosition="1654"> a supervised machine learning technique based on the latest results of the statistical learning theory (Vapnik, 1998). Given a vector space and a set of training points, i.e. positive and negative examples, SVMs find a separating hyperplane H(~x) = ω~ x x~ + b = 0 where ω E Rn and b E R are learned by applying the Structural Risk Minimization principle (Vapnik, 1995). SVMs is a binary classifier, but it can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002). One strong point of SVMs is the possibility to apply kernel methods (robert Mller et al., 2001) to implicitly map data in a new space where the examples are more easily separable as described in the next section. 3.2 Kernel Methods Kernel methods (Schlkopf and Smola, 2001) are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the explicit feature representation. A kernel function is a scalar product in a possibly unknown feature space. More precisely, The object o is mapped in x~ with a feature function φ : O —* Rn, where O is the set of the objects. T</context>
</contexts>
<marker>Mller, Mika, Rtsch, Tsuda, 2001</marker>
<rawString>Klaus robert Mller, Sebastian Mika, Gunnar Rtsch, Koji Tsuda, , and Bernhard Schlkopf. 2001. An introduction to kernel-based learning algorithms. IEEE Transactions on Neural Networks, 12(2):181– 201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>Probabilistic reasoning for entity and relation recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING-2002,</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Roth, Yih, 2002</marker>
<rawString>Dan Roth and Wen tau Yih. 2002. Probabilistic reasoning for entity and relation recognition. In Proceedings of the COLING-2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Schlkopf</author>
<author>Alexander J Smola</author>
</authors>
<title>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="10815" citStr="Schlkopf and Smola, 2001" startWordPosition="1681" endWordPosition="1684">.e. positive and negative examples, SVMs find a separating hyperplane H(~x) = ω~ x x~ + b = 0 where ω E Rn and b E R are learned by applying the Structural Risk Minimization principle (Vapnik, 1995). SVMs is a binary classifier, but it can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002). One strong point of SVMs is the possibility to apply kernel methods (robert Mller et al., 2001) to implicitly map data in a new space where the examples are more easily separable as described in the next section. 3.2 Kernel Methods Kernel methods (Schlkopf and Smola, 2001) are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel functions), avoiding the explicit feature representation. A kernel function is a scalar product in a possibly unknown feature space. More precisely, The object o is mapped in x~ with a feature function φ : O —* Rn, where O is the set of the objects. The kernel trick allows us to rewrite the decision hyperplane as: ( E yiαi )~xi · x~+ b = H(~x) = i=1..l E yiαi E~xi · x~ + b = yiαiφ(oi) · φ(o) + b, i=1..l i=1..l where yi is equ</context>
<context position="22885" citStr="Schlkopf and Smola, 2001" startWordPosition="3751" endWordPosition="3754">ency word plus grammatical relation tree kernels: CK5 = α·KP+(1−α)·(KPT−DW+KPT−GR) 1383 1. 5K3 + 5K4 2. 5K3 + 5K6 3. 55K = Ez=1,..,6 5Kz 4. KSST + 55K 5. C5K = α · KP + (1 − α) · (KSST + 55K) Preliminary experiments showed that the last combination yields the best performance with α = 0.23. We used a polynomial expansion to explore the bi-gram features of i) the first and the second entity participating in the relation, ii) grammatical relations which replace words in the dependency tree. Since the kernel function set is closed under normalization, polynomial expansion and linear combination (Schlkopf and Smola, 2001), all the illustrated composite kernels are also proper kernels. 5 Experiments Our experiments aim at investigating the effectiveness of convolution kernels adapted to syntactic parse trees and various sequence kernels for the RE task. For this purpose, we use the subset and partial tree kernel over different kinds of trees, namely constituent and dependency syntactic parse trees. Diverse sequences are applied individually and in combination together. We consider our task of relation extraction as a classification problem where categories are relation types. All pairs of entity mentions in the</context>
</contexts>
<marker>Schlkopf, Smola, 2001</marker>
<rawString>Bernhard Schlkopf and Alexander J. Smola. 2001. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer–Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="10388" citStr="Vapnik, 1995" startWordPosition="1609" endWordPosition="1611"> Methods In this section we give a brief introduction to support vector machines, kernel methods, diverse tree and sequence kernel spaces, which can be applied to the RE task. 3.1 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique based on the latest results of the statistical learning theory (Vapnik, 1998). Given a vector space and a set of training points, i.e. positive and negative examples, SVMs find a separating hyperplane H(~x) = ω~ x x~ + b = 0 where ω E Rn and b E R are learned by applying the Structural Risk Minimization principle (Vapnik, 1995). SVMs is a binary classifier, but it can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002). One strong point of SVMs is the possibility to apply kernel methods (robert Mller et al., 2001) to implicitly map data in a new space where the examples are more easily separable as described in the next section. 3.2 Kernel Methods Kernel methods (Schlkopf and Smola, 2001) are an attractive alternative to feature-based methods since the applied learning algorithm only needs to compute a product between a pair of objects (by means of kernel fu</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer–Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="10136" citStr="Vapnik, 1998" startWordPosition="1561" endWordPosition="1562">ns in dependency structure. We believe that an efficient and appropriate kernel can be used to solve the RE problem, exploiting the advantages of dependency structures, convolution tree kernels and sequence kernels. 3 Support Vector Machines and Kernel Methods In this section we give a brief introduction to support vector machines, kernel methods, diverse tree and sequence kernel spaces, which can be applied to the RE task. 3.1 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique based on the latest results of the statistical learning theory (Vapnik, 1998). Given a vector space and a set of training points, i.e. positive and negative examples, SVMs find a separating hyperplane H(~x) = ω~ x x~ + b = 0 where ω E Rn and b E R are learned by applying the Structural Risk Minimization principle (Vapnik, 1995). SVMs is a binary classifier, but it can be easily extended to multi-class classifier, e.g. by means of the one-vs-all method (Rifkin and Poggio, 2002). One strong point of SVMs is the possibility to apply kernel methods (robert Mller et al., 2001) to implicitly map data in a new space where the examples are more easily separable as described in</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. John Wiley and Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Alexander J Smola</author>
</authors>
<title>Fast kernels on strings and trees.</title>
<date>2002</date>
<booktitle>In Proceedings of Neural Information Processing Systems.</booktitle>
<contexts>
<context position="12826" citStr="Vishwanathan and Smola, 2002" startWordPosition="2032" endWordPosition="2036">e examples of the wellknown convolution kernels used in many NLP applications. Tree kernels represent trees in terms of their substructures (called tree fragments). Such fragments form a feature space which, in turn, is mapped into a vector space. Tree kernels measure the similarity between pair of trees by counting the number of fragments in common. There are three important characterizations of fragment type (Moschitti, 2006): the SubTrees (ST), the SubSet Trees (SST) and the Partial Trees (PT). For sake of space, we do not report the mathematical description of them, which is available in (Vishwanathan and Smola, 2002), (Collins and Duffy, 1380 2001) and (Moschitti, 2006), respectively. In contrast, we report some descriptions in terms of feature space that may be useful to understand the new engineered kernels. In principle, a SubTree (ST) is defined by taking any node along with its descendants. A SubSet Tree (SST) is a more general structure which does not necessarily include all the descendants. It must be generated by applying the same grammatical rule set, which generated the original tree. A Partial Tree (PT) is a more general form of substructures obtained by relaxing constraints over the SST. 4 Ker</context>
</contexts>
<marker>Vishwanathan, Smola, 2002</marker>
<rawString>S.V.N. Vishwanathan and Alexander J. Smola. 2002. Fast kernels on strings and trees. In Proceedings of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
</authors>
<title>A re-examination of dependency path kernels for relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing-IJCNLP.</booktitle>
<contexts>
<context position="1761" citStr="Wang, 2008" startWordPosition="261" endWordPosition="262">1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. Jeff Zucker, the longtime executive producer of NBC’s ”Today” program, will be named Friday as the new president of NBC’s entertainment division, replacing Garth Ancier, NBC executives said. Figure 1: A document from ACE 2004 with all entity mentions in bold. features without an explicit feature representation. This can be done by computing a kernel function between a p</context>
<context position="3240" citStr="Wang, 2008" startWordPosition="499" endWordPosition="500">equences of characters in the two strings. Such substrings are then weighted according to a decaying factor penalizing longer ones. In the same line, Tree Kernels count the number of subtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, Sing</context>
<context position="7331" citStr="Wang, 2008" startWordPosition="1126" endWordPosition="1127"> namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be used effectively for information extraction. Meanwhile, feature-based approaches often employ various kinds of linguistic, syntactic or contextual information and integrate into the feature space. (Roth and tau Yih, 2002) applied a probabilistic approach to solve the problems of named entity and relation extraction with the incorporation of various features suc</context>
<context position="9217" citStr="Wang, 2008" startWordPosition="1413" endWordPosition="1414"> 2002) proposed extracting 1379 relations by computing kernel functions between parse trees. (Bunescu and Mooney, 2005a) proposed a shortest path dependency kernel by stipulating that the information to model a relationship between two entities can be captured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008). These are not convolution kernels and produce a much lower number of substructures than the PT kernel. A recent approach successfully employs a convolution tree kernel (of type SST) over constituent syntactic parse tree (Zhang et al., 2006; Zhou et al., 2007), but it does not capture grammatical relations in dependency structure. We believe that an efficient and appropriate kernel can be used to solve the RE problem, exploiting the advantages of dependency structures, convolution tree kernels and sequence kernels. 3 Support Vector Machines and Kernel Methods In this section we give a brief i</context>
<context position="16935" citStr="Wang, 2008" startWordPosition="2723" endWordPosition="2724">odes T1-PER, denoting the type PERSON, and T2-LOC, denoting the type LOCATION, are added to the parse tree, above the two target NEs, respectively. In this example, the above PET is designed to capture the relation Located-in between the entities ”officials” and ”Washington” from the ACE corpus. Note that, a third NE, U.S., is characterized by the node GPE (GeoPolitical Entity), where the absence of the prefix T1 or T2 before the NE type (i.e. GPE), denotes that the NE does not take part in the target relation. In previous work, some dependency trees have been used (Bunescu and Mooney, 2005a; Wang, 2008) but the employed kernel just exploited the syntactic information concentrated in the path between e1 and e2. In contrast, we defined and studied three different dependency structures whose potential can be fully exploited by our convolution partial tree kernel: - Dependency Words (DW) tree is similar to PET adapted for dependency tree constituted by simple words. We select the minimal subtree which includes e1 and e2, and we insert an extra node as father of the NEs, labeled with the NE category. For example, given 1381 Figure 2: The constituent and dependency parse trees integrated with enti</context>
<context position="18360" citStr="Wang, 2008" startWordPosition="2966" endWordPosition="2967">atical Relation (GR) tree, i.e. the DW tree in which words are replaced by their grammatical functions, e.g. prep, pobj and nsubj. For example, Figure 2.d, shows the GR tree for the previous relation: In is replaced by prep, U.S. by nsubj and so on. - Grammatical Relation and Words (GRW) tree, words and grammatical functions are both used in the tree, where the latter are inserted as a father node of the former. For example, Figure 2.e, shows such tree for the previous relation. 4.2 Sequential Structures Some sequence kernels have been used on dependency structures (Bunescu and Mooney, 2005b; Wang, 2008). These kernels just used lexical words with some syntactic information. To fully exploit syntactic and semantic information, we defined and studied six different sequences (in a style similar to what proposed in (Moschitti, 2008)), which include features from constituent and dependency parse trees and NEs: 1. Sequence of terminals (lexical words) in the PET (SK1), e.g.: T2-LOC Washington, U.S. T1-PER officials. 2. Sequence of part-of-speech (POS) tags in the PET (SK2), i.e. the SK1 in which words are replaced by their POS tags, e.g.: T2-LOC NN , NNP T1-PER NNS. 3. Sequence of grammatical rela</context>
</contexts>
<marker>Wang, 2008</marker>
<rawString>Mengqiu Wang. 2008. A re-examination of dependency path kernels for relation extraction. In Proceedings of the 3rd International Joint Conference on Natural Language Processing-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>181--201</pages>
<contexts>
<context position="1646" citStr="Zelenko et al., 2002" startWordPosition="241" endWordPosition="244">action (RE) is defined in ACE as the task of finding relevant semantic relations between pairs of entities in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. Jeff Zucker, the longtime executive producer of NBC’s ”Today” program, will be named Friday as the new president of NBC’s entertainment division, replacing Garth Ancier, NBC executives said. Figure 1: A document from ACE 2004 with all entity mentions in bol</context>
<context position="3105" citStr="Zelenko et al., 2002" startWordPosition="476" endWordPosition="479">the sequence kernel (Lodhi et al., 2002), where the objects are strings of characters and the kernel function computes the number of common subsequences of characters in the two strings. Such substrings are then weighted according to a decaying factor penalizing longer ones. In the same line, Tree Kernels count the number of subtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfacto</context>
<context position="6790" citStr="Zelenko et al., 2002" startWordPosition="1042" endWordPosition="1045">vious sequence and dependency models for RE. 1The function defined on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Mill</context>
<context position="8612" citStr="Zelenko et al., 2002" startWordPosition="1314" endWordPosition="1317">dNet. (Kambhatla, 2004) employed maximum entropy models with diverse features including words, entity and mention types and the number of words (if any) separating the two entities. Recent work on Relation Extraction has mostly employed kernel-based approaches over syntactic parse trees. Kernels on parse trees were pioneered by (Collins and Duffy, 2001). This kernel function counts the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees. (Culotta and Sorensen, 2004) extended this work to calculate kernels between augmented dependency trees. (Zelenko et al., 2002) proposed extracting 1379 relations by computing kernel functions between parse trees. (Bunescu and Mooney, 2005a) proposed a shortest path dependency kernel by stipulating that the information to model a relationship between two entities can be captured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, </context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2002</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2002. Kernel methods for relation extraction. In Proceedings of EMNLP-ACL, pages 181–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jian Su</author>
<author>Danmei Wang</author>
<author>Guodong Zhou</author>
<author>Chew Lim Tan</author>
</authors>
<title>Discovering relations between named entities from a large raw corpus using tree similarity-based clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP’2005, Lecture Notes in Computer Science (LNCS 3651),</booktitle>
<pages>378--389</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="1748" citStr="Zhang et al., 2005" startWordPosition="257" endWordPosition="260">es in texts. Figure 1 shows part of a document from ACE 2004 corpus, a collection of news articles. In the text, the relation between president and NBC’s entertainment division describes the relationship between the first entity (person) and the second (organization) where the person holds a managerial position. Several approaches have been proposed for automatically learning semantic relations from texts. Among others, there has been increased interest in the application of kernel methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2005; Wang, 2008). Their main property is the ability of exploiting a huge amount of This work has been partially funded by the LiveMemories project (http://www.livememories.org/) and Expert System (http://www.expertsystem.net/) research grant. Jeff Zucker, the longtime executive producer of NBC’s ”Today” program, will be named Friday as the new president of NBC’s entertainment division, replacing Garth Ancier, NBC executives said. Figure 1: A document from ACE 2004 with all entity mentions in bold. features without an explicit feature representation. This can be done by computing a kernel functio</context>
<context position="3154" citStr="Zhang et al., 2005" startWordPosition="484" endWordPosition="487">e objects are strings of characters and the kernel function computes the number of common subsequences of characters in the two strings. Such substrings are then weighted according to a decaying factor penalizing longer ones. In the same line, Tree Kernels count the number of subtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009</context>
<context position="5882" citStr="Zhang et al., 2005" startWordPosition="898" endWordPosition="901"> that relation extraction can benefit from increasing the feature space by combining kernels (with a simple summation) exploiting the two different parsing paradigms. Our experiments on RE show that the current composite kernel, which is constituent-based is more effective than those based on dependency trees and individual sequence kernel but at the same time their combinations, i.e. dependency plus constituent trees, improve the state-of-the-art in RE. More interestingly, also the combinations of various sequence kernels gain significant better performance than the current state-of-the-art (Zhang et al., 2005). Overall, these results are interesting for the computational linguistics research since they show that the above two parsing paradigms provide different and important information for a semantic task such as RE. Regarding sequence-based kernels, the WSK gains better performance than previous sequence and dependency models for RE. 1The function defined on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kern</context>
<context position="7319" citStr="Zhang et al., 2005" startWordPosition="1122" endWordPosition="1125">mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be used effectively for information extraction. Meanwhile, feature-based approaches often employ various kinds of linguistic, syntactic or contextual information and integrate into the feature space. (Roth and tau Yih, 2002) applied a probabilistic approach to solve the problems of named entity and relation extraction with the incorporation of various </context>
<context position="9204" citStr="Zhang et al., 2005" startWordPosition="1409" endWordPosition="1412">es. (Zelenko et al., 2002) proposed extracting 1379 relations by computing kernel functions between parse trees. (Bunescu and Mooney, 2005a) proposed a shortest path dependency kernel by stipulating that the information to model a relationship between two entities can be captured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008). These are not convolution kernels and produce a much lower number of substructures than the PT kernel. A recent approach successfully employs a convolution tree kernel (of type SST) over constituent syntactic parse tree (Zhang et al., 2006; Zhou et al., 2007), but it does not capture grammatical relations in dependency structure. We believe that an efficient and appropriate kernel can be used to solve the RE problem, exploiting the advantages of dependency structures, convolution tree kernels and sequence kernels. 3 Support Vector Machines and Kernel Methods In this section we g</context>
<context position="16065" citStr="Zhang et al., 2005" startWordPosition="2573" endWordPosition="2576">rge (up to 300 nodes in the Penn Treebank (Marcus et al., 1993)); second, there is ambiguity on the target pairs of NEs, i.e. different NEs associated with different relations are described by the same parse tree. Therefore, it is necessary to identify the portion of the parse tree that best represent the useful syntactic information. Let e1 and e2 be two entity mentions in the same sentence such that they are in a relationship R. For the constituent parse tree, we used the pathenclosed tree (PET), which was firstly proposed in (Moschitti, 2004) for Semantic Role Labeling and then adapted by (Zhang et al., 2005) for relation extraction. It is the smallest common subtree including the two entities of a relation. The dashed frame in Figure 2.a surrounds PET associated with the two mentions, officials and Washington. Moreover, to improve the representation, two extra nodes T1-PER, denoting the type PERSON, and T2-LOC, denoting the type LOCATION, are added to the parse tree, above the two target NEs, respectively. In this example, the above PET is designed to capture the relation Located-in between the entities ”officials” and ”Washington” from the ACE corpus. Note that, a third NE, U.S., is characterize</context>
</contexts>
<marker>Zhang, Su, Wang, Zhou, Tan, 2005</marker>
<rawString>Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and Chew Lim Tan. 2005. Discovering relations between named entities from a large raw corpus using tree similarity-based clustering. In Proceedings of IJCNLP’2005, Lecture Notes in Computer Science (LNCS 3651), pages 378–389, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>825--832</pages>
<contexts>
<context position="3678" citStr="Zhang et al., 2006" startWordPosition="567" endWordPosition="570">es over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP spective since dependency structures offer some unique advantages, which should be exploited by an appropriate kernel. Therefore, studying convolution tree kernels for dependency trees is worthwhile also considering that, to the best of our knowledge, these models have not been previously used for relation extraction1 task. Additionally, sequence kernels should be included in such global st</context>
<context position="7352" citStr="Zhang et al., 2006" startWordPosition="1128" endWordPosition="1131">rvised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be used effectively for information extraction. Meanwhile, feature-based approaches often employ various kinds of linguistic, syntactic or contextual information and integrate into the feature space. (Roth and tau Yih, 2002) applied a probabilistic approach to solve the problems of named entity and relation extraction with the incorporation of various features such as word, part-of-sp</context>
<context position="9458" citStr="Zhang et al., 2006" startWordPosition="1451" endWordPosition="1454">o entities can be captured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008). These are not convolution kernels and produce a much lower number of substructures than the PT kernel. A recent approach successfully employs a convolution tree kernel (of type SST) over constituent syntactic parse tree (Zhang et al., 2006; Zhou et al., 2007), but it does not capture grammatical relations in dependency structure. We believe that an efficient and appropriate kernel can be used to solve the RE problem, exploiting the advantages of dependency structures, convolution tree kernels and sequence kernels. 3 Support Vector Machines and Kernel Methods In this section we give a brief introduction to support vector machines, kernel methods, diverse tree and sequence kernel spaces, which can be applied to the RE task. 3.1 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique</context>
<context position="20044" citStr="Zhang et al., 2006" startWordPosition="3247" endWordPosition="3250">SK4 in which words are replaced by their POS tags, e.g.: NN T2-LOC IN VBP T1-PER NNS GPE NNP. It is worth noting that the potential information contained in such sequences can be fully exploited by the word sequence kernel. 4.3 Combining Kernels Given that syntactic information from different parse trees may have different impact on relation extraction (RE), the viable approach to study the role of dependency and constituent parsing is to experiment with different syntactic models and measuring the impact in terms of RE accuracy. For this purpose we compared the composite kernel described in (Zhang et al., 2006) with the partial tree kernels applied to DW, GR, and GRW and sequence kernels based on six sequences described above. The composite kernels include polynomial kernel applied to entity-related feature vector. The word sequence kernel (WSK) is always applied to sequential structures. The used kernels are described in more detail below. 4.3.1 Polynomial Kernel The basic kernel between two named entities of the ACE documents is defined as: KP(R1, R2) = � KE(R1.Ei, R2.Ei), i=1,2 where R1 and R2 are two relation instances, Ei is the ith entity of a relation instance. KE(·, ·) is a kernel over entit</context>
<context position="26564" citStr="Zhang et al., 2006" startWordPosition="4345" endWordPosition="4348">n PET (a portion of the constituent tree) and combined with the CK1 schema whereas PET and GR associated with CK5 means that SST kernel is applied to PET and PT kernel is applied to GR in CK5. The remaining three columns report Precision, Recall and F1 measure. The interpretation of the second table is more immediate since the only tree kernel involved is the SST kernel applied to PET and combined by means of CK1. We note that: first, the dependency kernels, i.e. the results on the rows from 3 to 6 are below the composite kernel CK1, i.e. 68.9. This is the state-of-the-art in RE, designed by (Zhang et al., 2006), where our implementation provides 1384 Parse Tree Kernel P R F PET CK1 69.5 68.3 68.9 DW CK1 53.2 59.7 56.3 GR CK1 58.8 61.7 60.2 GRW CK1 56.1 61.2 58.5 DW and GR CK5 59.7 64.1 61.8 PET and GR CK2 70.7 69.0 69.8 CK3 70.8 70.2 70.5 Table 1: Results on the ACE 2004 evaluation test set. Six structures were experimented over the constituent and dependency trees. Kernel P R F CK1 69.5 68.3 68.9 SK1 72.0 52.8 61.0 SK2 61.7 60.0 60.8 SK3 62.6 60.7 61.6 SK4 73.1 50.3 59.7 SK5 59.0 60.7 59.8 SK6 57.7 61.8 59.7 SK3 + SK4 75.0 63.4 68.8 SK3 + SK6 66.8 65.1 65.9 SSK = Ei SKi 73.8 66.2 69.8 CSK 75.6 66.6</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of COLING-ACL 2006, pages 825– 832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the ACL,</booktitle>
<pages>419--426</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="3437" citStr="Zhao and Grishman, 2005" startWordPosition="526" endWordPosition="529">ubtree shared by two input trees. An example is that of syntactic (or subset) tree kernel (SST) (Collins and Duffy, 2001), where trees encode grammatical derivations. Previous work on the use of kernels for RE has exploited some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Zhang et al., 2005) or subsequence kernels over dependency graphs (Bunescu and Mooney, 2005a; Wang, 2008). More specifically, (Bunescu and Mooney, 2005a; Culotta and Sorensen, 2004) use kernels over dependency trees, which showed much lower accuracy than feature-based methods (Zhao and Grishman, 2005). One problem of the dependency kernels above is that they do not exploit the overall structural aspects of dependency trees. A more effective solution is the application of convolution kernels to constituent parse trees (Zhang et al., 2006) but this is not satisfactory from a general per1378 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378–1387, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP spective since dependency structures offer some unique advantages, which should be exploited by an appropriate kernel. Therefore, studying convolution t</context>
<context position="7178" citStr="Zhao and Grishman, 2005" startWordPosition="1098" endWordPosition="1101">e presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syntactic parse trees. It demonstrated that a lexicalized, probabilistic context-free parser with head rules can be used effectively for information extraction. Meanwhile, feature-based approaches often employ various kinds of linguistic, syntactic or contextual information and integrate into the feature space. (Roth and tau</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Meeting of the ACL, pages 419–426, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the ACL,</booktitle>
<pages>427--434</pages>
<location>Ann Arbor, USA,</location>
<contexts>
<context position="6855" citStr="Zhou et al., 2005" startWordPosition="1052" endWordPosition="1055">on (Culotta and Sorensen, 2004), although on dependency trees, is not a convolution tree kernel. A review of previous work on RE is described in Section 2. Section 3 introduces support vector machines and kernel methods whereas our specific kernels for RE are described is Section 4. The experiments and conclusions are presented in sections 5 and 6, respectively. 2 Related Work To identify semantic relations using machine learning, three learning settings have mainly been applied, namely supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Sorensen, 2004; Kambhatla, 2004; Zhou et al., 2005), semi supervised methods (Brin, 1998; Agichtein and Gravano, 2000), and unsupervised method (Hasegawa et al., 2004). In a supervised learning setting, representative related work can be classified into generative models (Miller et al., 2000), feature-based (Roth and tau Yih, 2002; Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005) or kernel-based methods (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008; Zhang et al., 2006). The learning model employed in (Miller et al., 2000) used statistical parsing techniques to learn syn</context>
</contexts>
<marker>Zhou, Su, Zhang, 2005</marker>
<rawString>GuoDong Zhou, Jian Su, Jie Zhang, , and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proceedings of the 43rd Meeting of the ACL, pages 427–434, Ann Arbor, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Min Zhang</author>
<author>DongHong Ji</author>
<author>QiaoMing Zhu</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<pages>728--736</pages>
<contexts>
<context position="9478" citStr="Zhou et al., 2007" startWordPosition="1455" endWordPosition="1458">ptured by the shortest path between them in the dependency graph. Although approaches in RE have been dominated by kernel-based methods, until now, most of research in this line has used the kernel as some similarity measures over diverse features (Zelenko et al., 2002; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Zhang et al., 2005; Wang, 2008). These are not convolution kernels and produce a much lower number of substructures than the PT kernel. A recent approach successfully employs a convolution tree kernel (of type SST) over constituent syntactic parse tree (Zhang et al., 2006; Zhou et al., 2007), but it does not capture grammatical relations in dependency structure. We believe that an efficient and appropriate kernel can be used to solve the RE problem, exploiting the advantages of dependency structures, convolution tree kernels and sequence kernels. 3 Support Vector Machines and Kernel Methods In this section we give a brief introduction to support vector machines, kernel methods, diverse tree and sequence kernel spaces, which can be applied to the RE task. 3.1 Support Vector Machines (SVMs) Support Vector Machines refer to a supervised machine learning technique based on the latest</context>
<context position="27214" citStr="Zhou et al., 2007" startWordPosition="4479" endWordPosition="4482">des 1384 Parse Tree Kernel P R F PET CK1 69.5 68.3 68.9 DW CK1 53.2 59.7 56.3 GR CK1 58.8 61.7 60.2 GRW CK1 56.1 61.2 58.5 DW and GR CK5 59.7 64.1 61.8 PET and GR CK2 70.7 69.0 69.8 CK3 70.8 70.2 70.5 Table 1: Results on the ACE 2004 evaluation test set. Six structures were experimented over the constituent and dependency trees. Kernel P R F CK1 69.5 68.3 68.9 SK1 72.0 52.8 61.0 SK2 61.7 60.0 60.8 SK3 62.6 60.7 61.6 SK4 73.1 50.3 59.7 SK5 59.0 60.7 59.8 SK6 57.7 61.8 59.7 SK3 + SK4 75.0 63.4 68.8 SK3 + SK6 66.8 65.1 65.9 SSK = Ei SKi 73.8 66.2 69.8 CSK 75.6 66.6 70.8 CK1 + SSK 76.6 67.0 71.5 (Zhou et al., 2007) 82.2 70.2 75.8 CK1 with Heuristics Table 2: Performance comparison on the ACE 2004 data with different kernel setups. a slightly smaller result than the original version (i.e. an F1 of about 72 using a different syntactic parser). Second, CK1 improves to 70.5, when the contribution of PT kernel applied to GR (dependency tree built using grammatical relations) is added. This suggests that dependency structures are effectively exploited by PT kernel and that such information is somewhat complementary to constituent trees. Third, in the second table, the model CK1 + SSK, which adds to CK1 the co</context>
</contexts>
<marker>Zhou, Zhang, Ji, Zhu, 2007</marker>
<rawString>GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoMing Zhu. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of EMNLP-CoNLL 2007, pages 728–736.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>