<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999419333333333">
Empirical Risk Minimization for
Probabilistic Grammars: Sample
Complexity and Hardness of Learning
</title>
<author confidence="0.990001">
Shay B. Cohen*
</author>
<affiliation confidence="0.984701">
Columbia University
</affiliation>
<author confidence="0.844133">
Noah A. Smith**
</author>
<affiliation confidence="0.86253">
Carnegie Mellon University
</affiliation>
<bodyText confidence="0.831757181818182">
Probabilistic grammars are generative statistical models that are useful for compositional and
sequential structures. They are used ubiquitously in computational linguistics. We present a
framework, reminiscent of structural risk minimization, for empirical risk minimization of prob-
abilistic grammars using the log-loss. We derive sample complexity bounds in this framework
that apply both to the supervised setting and the unsupervised setting. By making assumptions
about the underlying distribution that are appropriate for natural language scenarios, we are able
to derive distribution-dependent sample complexity bounds for probabilistic grammars. We also
give simple algorithms for carrying out empirical risk minimization using this framework in both
the supervised and unsupervised settings. In the unsupervised case, we show that the problem of
minimizing empirical risk is NP-hard. We therefore suggest an approximate algorithm, similar
to expectation-maximization, to minimize the empirical risk.
</bodyText>
<sectionHeader confidence="0.990632" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999333166666667">
Learning from data is central to contemporary computational linguistics. It is in com-
mon in such learning to estimate a model in a parametric family using the maximum
likelihood principle. This principle applies in the supervised case (i.e., using anno-
tated data) as well as semisupervised and unsupervised settings (i.e., using unan-
notated data). Probabilistic grammars constitute a range of such parametric families
we can estimate (e.g., hidden Markov models, probabilistic context-free grammars).
These parametric families are used in diverse NLP problems ranging from syntactic
and morphological processing to applications like information extraction, question
answering, and machine translation.
Estimation of probabilistic grammars, in many cases, indeed starts with the prin-
ciple of maximum likelihood estimation (MLE). In the supervised case, and with
traditional parametrizations based on multinomial distributions, MLE amounts to
</bodyText>
<note confidence="0.870319">
* Department of Computer Science, Columbia University, New York, NY 10027, United States.
E-mail: scohen@cs.columbia.edu. This research was completed while the first author was at Carnegie
Mellon University.
** School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States.
</note>
<email confidence="0.854949">
E-mail: nasmith@cs.cmu.edu.
</email>
<note confidence="0.94266975">
Submission received: 1 November 2010; revised submission received: 21 June 2011; accepted for publication:
3 August 2011.
© 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.99996772">
normalization of rule frequencies as they are observed in data. In the unsupervised case,
on the other hand, algorithms such as expectation-maximization are available. MLE is
attractive because it offers statistical consistency if some conditions are met (i.e., if the
data are distributed according to a distribution in the family, then we will discover the
correct parameters if sufficient data is available). In addition, under some conditions it
is also an unbiased estimator.
An issue that has been far less explored in the computational linguistics literature
is the sample complexity of MLE. Here, we are interested in quantifying the number of
samples required to accurately learn a probabilistic grammar either in a supervised
or in an unsupervised way. If bounds on the requisite number of samples (known as
“sample complexity bounds”) are sufficiently tight, then they may offer guidance to
learner performance, given various amounts of data and a wide range of parametric
families. Being able to reason analytically about the amount of data to annotate, and
the relative gains in moving to a more restricted parametric family, could offer practical
advantages to language engineers.
We note that grammar learning has been studied in formal settings as a problem of
grammatical inference—learning the structure of a grammar or an automaton (Angluin
1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008,
among others). Our setting in this article is different. We assume that we have a fixed
grammar, and our goal is to estimate its parameters. This approach has shown great
empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005)
and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and
Manning 2004; Cohen and Smith 2010a) settings. There has also been some discus-
sion of sample complexity bounds for statistical parsing models, in a distribution-free
setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis
of natural language, as it has to account for pathological cases of distributions that
generate data.
We develop a framework for deriving sample complexity bounds using the max-
imum likelihood principle for probabilistic grammars in a distribution-dependent
setting. Distribution dependency is introduced here by making empirically justified
assumptions about the distributions that generate the data. Our framework uses and
significantly extends ideas that have been introduced for deriving sample complexity
bounds for probabilistic graphical models (Dasgupta 1997). Maximum likelihood esti-
mation is put in the empirical risk minimization framework (Vapnik 1998) with the loss
function being the log-loss. Following that, we develop a set of learning theoretic tools
to explore rates of estimation convergence for probabilistic grammars. We also develop
algorithms for performing empirical risk minimization.
Much research has been devoted to the problem of learning finite state automata
(which can be thought of as a class of grammars) in the Probably Approximately Correct
setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989;
Pitt 1989; Terwijn 2002). Typically, the setting in these cases is different from our setting:
Error is measured as the probability mass of strings that are not identified correctly by
the learned finite state automaton, instead of measuring KL divergence between the
automaton and the true distribution. In addition, in many cases, there is also a focus on
the distribution-free setting. To the best of our knowledge, it is still an open problem
whether finite state automata are learnable in the distribution-dependent setting when
measuring the error as the fraction of misidentified strings. Other work (Ron 1995; Ron,
Singer, and Tishby 1998; Clark and Thollard 2004; Palmer and Goldberg 2007) also gives
treatment to probabilistic automata with an error measure which is more suitable for
the probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance.
</bodyText>
<page confidence="0.745348">
480
</page>
<note confidence="0.916215">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<bodyText confidence="0.986953714285714">
These also focus on learning the structure of finite state machines. As mentioned earlier,
in our setting we assume that the grammar is fixed, and that our goal is to estimate its
parameters.
We note an important connection to an earlier study about the learnability of
probabilistic automata and hidden Markov models by Abe and Warmuth (1992). In
that study, the authors provided positive results for the sample complexity for learning
probabilistic automata—they showed that a polynomial sample is sufficient for MLE.
We demonstrate positive results for the more general class of probabilistic grammars
which goes beyond probabilistic automata. Abe and Warmuth also showed that the
problem of finding or even approximating the maximum likelihood solution for a two-
state probabilistic automaton with an alphabet of an arbitrary size is hard. Even though
these results extend to probabilistic grammars to some extent, we provide a novel proof
that illustrates the NP-hardness of identifying the maximum likelihood solution for
probabilistic grammars in the specific framework of “proper approximations” that we
define in this article. Whereas Abe and Warmuth show that the problem of maximum
likelihood maximization for two-state HMMs is not approximable within a certain
factor in time polynomial in the alphabet and the length of the observed sequence, we
show that there is no polynomial algorithm (in the length of the observed strings) that
identifies the maximum likelihood estimator in our framework. In our reduction, from
3-SAT to the problem of maximum likelihood estimation, the alphabet used is binary
and the grammar size is proportional to the length of the formula. In Abe and Warmuth,
the alphabet size varies, and the number of states is two.
This article proceeds as follows. In Section 2 we review the background necessary
from Vapnik’s (1988) empirical risk minimization framework. This framework is re-
duced to maximum likelihood estimation when a specific loss function is used: the log-
loss.1 There are some shortcomings in using the empirical risk minimization framework
in its simplest form. In its simplest form, the ERM framework is distribution-free, which
means that we make no assumptions about the distribution that generated the data.
Naively attempting to apply the ERM framework to probabilistic grammars in the
distribution-free setting does not lead to the desired sample complexity bounds. The
reason for this is that the log-loss diverges whenever small probabilities are allocated in
the learned hypothesis to structures or strings that have a rather large probability in the
probability distribution that generates the data. With a distribution-free assumption,
therefore, we would have to give treatment to distributions that are unlikely to be
true for natural language data (e.g., where some extremely long sentences are very
probable).
To correct for this, we move to an analysis in a distribution-dependent setting, by
presenting a set of assumptions about the distribution that generates the data. In Sec-
tion 3 we discuss probabilistic grammars in a general way and introduce assumptions
about the true distribution that are reasonable when our data come from natural lan-
guage examples. It is important to note that this distribution need not be a probabilistic
grammar.
The next step we take, in Section 4, is approximating the set of probabilistic grammars
over which we maximize likelihood. This is again required in order to overcome the
divergence of the log-loss for probabilities that are very small. Our approximations are
1 It is important to remember that minimizing the log-loss does not equate to minimizing the error of a
linguistic analyzer or natural language processing application. In this article we focus on the log-loss
case because we believe that probabilistic models of language phenomena have inherent usefulness
as explanatory tools in computational linguistics, aside from their use in systems.
</bodyText>
<page confidence="0.710636">
481
</page>
<note confidence="0.777808">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.999829117647059">
based on bounded approximations that have been used for deriving sample complexity
bounds for graphical models in a distribution-free setting (Dasgupta 1997).
Our approximations have two important properties: They are, by themselves, prob-
abilistic grammars from the family we are interested in estimating, and they become a
tighter approximation around the family of probabilistic grammars we are interested in
estimating as more samples are available.
Moving to the distribution-dependent setting and defining proper approximations
enables us to derive sample complexity bounds. In Section 5 we present the sample
complexity results for both the supervised and unsupervised cases. A question that
lingers at this point is whether it is computationally feasible to maximize likelihood
in our framework even when given enough samples.
In Section 6, we describe algorithms we use to estimate probabilistic grammars
in our framework, when given access to the required number of samples. We show
that in the supervised case, we can indeed maximize likelihood in our approximation
framework using a simple algorithm. For the unsupervised case, however, we show that
maximizing likelihood is NP-hard. This fact is related to a notion known in the learning
theory literature as inherent unpredictability (Kearns and Vazirani 1994): Accurate
learning is computationally hard even with enough samples. To overcome this difficulty,
we adapt the expectation-maximization algorithm (Dempster, Laird, and Rubin 1977)
to approximately maximize likelihood (or minimize log-loss) in the unsupervised case
with proper approximations.
In Section 7 we discuss some related ideas. These include the failure of an alter-
native kind of distributional assumption and connections to regularization by maxi-
mum a posteriori estimation with Dirichlet priors. Longer proofs are included in the
appendices. A table of notation that is used throughout is included as Table D.1 in
Appendix D.
This article builds on two earlier papers. In Cohen and Smith (2010b) we presented
the main sample complexity results described here; the present article includes signifi-
cant extensions, a deeper analysis of our distributional assumptions, and a discussion of
variants of these assumptions, as well as related work, such as that about the Tsybakov
noise condition. In Cohen and Smith (2010c) we proved NP-hardness for unsupervised
parameter estimation of probalistic context-free grammars (PCFGs) (without approxi-
mate families). The present article uses a similar type of proof to achieve results adapted
to empirical risk minimization in our approximation framework.
</bodyText>
<sectionHeader confidence="0.966592" genericHeader="keywords">
2. Empirical Risk Minimization and Maximum Likelihood Estimation
</sectionHeader>
<bodyText confidence="0.999993">
We begin by introducing some notation. We seek to construct a predictive model that
maps inputs from space X to outputs from space Z. In this work, X is a set of strings
using some alphabet Σ (X ⊆ Σ∗), and Z is a set of derivations allowed by a grammar
(e.g., a context-free grammar). We assume the existence of an unknown joint probability
distribution p(x,z) over X x Z. (For the most part, we will be discussing discrete input
and output spaces. This means that p will denote a probability mass function.) We are
interested in estimating the distribution p from examples, either in a supervised setting,
where we are provided with examples of the form (x, z) E X x Z, or in the unsupervised
setting, where we are provided only with examples of the form x E X. We first consider
the supervised setting and return to the unsupervised setting in Section 5. We will use
q to denote the estimated distribution.
</bodyText>
<page confidence="0.537501">
482
</page>
<note confidence="0.952493">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<bodyText confidence="0.725428333333333">
In order to estimate p as accurately as possible using q(x,z), we are interested in
minimizing the log-loss, that is, in finding qopt, from a fixed family of distributions Q
(also called “the concept space”), such that
</bodyText>
<equation confidence="0.584044333333333">
qopt = argmin � p(x,z)logq(x,z) (1)
qEQ Ep [− log q] = argmin −
qEQ x,z
</equation>
<bodyText confidence="0.993119166666667">
Note that if p ∈ Q, then this quantity achieves the minimum when qopt = p, in which case
the value of the log-loss is the entropy of p. Indeed, more generally, this optimization is
equivalent to finding q such that it minimizes the KL divergence from p to q.
Because p is unknown, we cannot hope to minimize the log-loss directly. Given a
set of examples (x1, z1), ... , (xn, zn), however, there is a natural candidate, the empirical
distribution ˜pn, for use in Equation (1) instead of p, defined as:
</bodyText>
<equation confidence="0.990575">
n
˜pn(x,z) = n−1 I{(x,z) = (xi, zi)}
i=1
</equation>
<bodyText confidence="0.999498333333333">
where I {(x, z) = (xi, zi)} is 1 if (x, z) = (xi, zi) and 0 otherwise.2 We then set up the
problem as the problem of empirical risk minimization (ERM), that is, trying to find q
such that
</bodyText>
<equation confidence="0.99923325">
q* = argmin E˜pn [−log q] (2)
qEQ
n
= argmin −n−1 log q(xi,zi)
qEQ i=1
n
= argmax n−1 log q(xi, zi) (3)
qEQ i=1
</equation>
<bodyText confidence="0.997442571428572">
Equation (3) immediately shows that minimizing empirical risk using the log-loss is
equivalent to the maximizing likelihood, which is a common statistical principle used
for estimating a probabilistic grammar in computational linguistics (Charniak 1993;
Manning and Sch¨utze 1999).3
As mentioned earlier, our goal is to estimate the probability distribution p while
quantifying how accurate our estimate is. One way to quantify the estimation accuracy
is by bounding the excess risk, which is defined as
</bodyText>
<equation confidence="0.9485865">
£p(q; Q) = £p(q) Ep [−log q] − min
q,EQ
</equation>
<bodyText confidence="0.2547045">
We are interested in bounding the excess risk for q*, £p(q*). The excess risk is
reduced to KL divergence between p and q if p ∈ Q, because in this case the quantity
minq,EQ E [− log q&apos;] is minimized with q&apos; = p, and equals the entropy of p. In a typical
2 We note that ˜pn itself is a random variable, because it depends on the sample drawn from p.
3 We note that being able to attain the minimum through an hypothesis q* is not necessarily possible in
the general case. In our instantiations of ERM for probabilistic grammars, however, the minimum can be
attained. In fact, in the unsupervised case the minimum can be attained by more than a single hypothesis.
In these cases, q* is arbitrarily chosen to be one of these minimizers.
</bodyText>
<equation confidence="0.204174">
Ep [−log q&apos;] (4)
</equation>
<page confidence="0.476603">
483
</page>
<note confidence="0.575752">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.99958025">
case, where we do not necessarily have p E Q, then the excess risk of q is bounded from
above by the KL divergence between p and q.
We can bound the excess risk by showing the double-sided convergence of the
empirical process Rn(Q), defined as follows:
</bodyText>
<equation confidence="0.918018416666667">
Rn(Q) ° sup IE˜pn [−log q] − Ep [−log q]I -+ 0 (5)
qEQ
as n -+ oo. For any c &gt; 0, if, for large enough n it holds that
sup IE˜pn [−log q] − Ep [−log q]I &lt; c (6)
qEQ
(with high probability), then we can “sandwich” the following quantities:
[−log q*]
Ep [−log qopt] &lt; Ep (7)
&lt; E˜pn [−log q*] + c
] + c
&lt; E˜pn [− log qopt
] + 2c
</equation>
<bodyText confidence="0.9848064">
&lt; Ep [−log qopt (8)
where the inequalities come from the fact that qopt minimizes the expected risk
[− log q] for q E Q. The
Ep [− log q] for q E Q, and q* minimizes the empirical risk E˜pn
consequence of Equations (7) and (8) is that the expected risk of q* is at most 2c away
from the expected risk of qopt, and as a result, we find the excess risk Ep(q*), for large
enough n, is smaller than 2c. Intuitively, this means that, under a large sample, q* does
not give much worse results than qopt under the criterion of the log-loss.
Unfortunately, the regularity conditions which are required for the convergence of
Rn(Q) do not hold because the log-loss can be unbounded. This means that a modifi-
cation is required for the empirical process in a way that will actually guarantee some
kind of convergence. We give a treatment to this in the next section.
We note that all discussion of convergence in this section has been about conver-
gence in probability. For example, we want Equation (6) to hold with high probability—
for most samples of size n. We will make this notion more rigorous in Section 2.2.
</bodyText>
<subsectionHeader confidence="0.958664">
2.1 Empirical Risk Minimization and Structural Risk Minimization Methods
</subsectionHeader>
<bodyText confidence="0.99975725">
It has been noted in the literature (Vapnik 1998; Koltchinskii 2006) that often the class Q
is too complex for empirical risk minimization using a fixed number of data points.
It is therefore desirable in these cases to create a family of subclasses {Qα  |oc E A}
that have increasing complexity. The more data we have, the more complex our Qα
can be for empirical risk minimization. Structural risk minimization (Vapnik 1998) and
the method of sieves (Grenander 1981) are examples of methods that adopt such an
approach. Structural risk minimization, for example, can be represented in many cases
as a penalization of the empirical risk method, using a regularization term.
In our case, the level of “complexity” is related to allocation of small probabilities to
derivations in the grammar by a distribution q E Q. The basic problem is this: Whenever
we have a derivation with a small probability, the log-loss becomes very large (in
absolute value), and this makes it hard to show the convergence of the empirical process
</bodyText>
<page confidence="0.869798">
484
</page>
<note confidence="0.626762">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<bodyText confidence="0.998343142857143">
Rn(Q). Because grammars can define probability distributions over infinitely many
discrete outcomes, probabilities can be arbitrarily small and log-loss can be arbitrarily
large.
To solve this issue with the complexity of Q, we define in Section 4 a series of approx-
imations {Qn  |n E N} for probabilistic grammars such that Un Qn = Q. Our framework
for empirical risk minimization is then set up to minimize the empirical risk with respect
to Qn, where n is the number of samples we draw for the learner:
</bodyText>
<equation confidence="0.919203">
q∗n = argmin E˜pn [−log q] (9)
q∈Qn
We are then interested in the convergence of the empirical process
Rn(Qn) = sup ��E˜pn [−log q] − Ep [−log q]I (10)
q∈Qn
</equation>
<bodyText confidence="0.999604">
In Section 4 we show that the minimizer q∗n is an asymptotic empirical risk minimizer
(in our specific framework), which means that Ep [− log qn] -+ Ep [− log q∗] . Because
we have Un Qn = Q, the implication of having asymptotic empirical risk minimization
is that we have Ep(q∗n; Qn) -+ Ep(q∗; Q).
</bodyText>
<subsectionHeader confidence="0.999718">
2.2 Sample Complexity Bounds
</subsectionHeader>
<bodyText confidence="0.998978190476191">
Knowing that we are interested in the convergence of Rn(Qn) = supq∈Qn |E˜pn [− log q] −
Ep [−log q] |, a natural question to ask is: “At what rate does this empirical process
converge?”
Because the quantity Rn(Qn) is a random variable, we need to give a probabilistic
treatment to its convergence. More specifically, we ask the question that is typically
asked when learnability is considered (Vapnik 1998): “How many samples n are re-
quired so that with probability 1 − b we have Rn(Qn) &lt; c?” Bounds on this number
of samples are also called “sample complexity bounds,” and in a distribution-free
setting they are described as a function N(c, b, Q), independent of the distribution p that
generates the data.
A complete distribution-free setting is not appropriate for analyzing natural lan-
guage. This setting poses technical difficulties with the convergence of Rn(Qn) and needs
to take into account pathological cases that can be ruled out in natural language data.
Instead, we will make assumptions about p, parametrize these assumptions in several
ways, and then calculate sample complexity bounds of the form N(c, b, Q, p), where the
dependence on the distribution is expressed as dependence on the parameters in the
assumptions about p.
The learning setting, then, can be described as follows. The user decides on a level
of accuracy (c) which the learning algorithm has to reach with confidence (1 − b). Then,
N(c, b, Q, p) samples are drawn from p and presented to the learning algorithm. The
learning algorithm then returns an hypothesis according to Equation (9).
</bodyText>
<sectionHeader confidence="0.867056" genericHeader="introduction">
3. Probabilistic Grammars
</sectionHeader>
<bodyText confidence="0.999908333333333">
We begin this section by discussing the family of probabilistic grammars. A probabilistic
grammar defines a probability distribution over a certain kind of structured object (a
derivation of the underlying symbolic grammar) explained step-by-step as a stochastic
</bodyText>
<page confidence="0.807376">
485
</page>
<note confidence="0.246614">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.997832272727273">
process. Hidden Markov models (HMMs), for example, can be understood as a random
walk through a probabilistic finite-state network, with an output symbol sampled at
each state. PCFGs generate phrase-structure trees by recursively rewriting nonterminal
symbols as sequences of “child” symbols (each itself either a nonterminal symbol or a
terminal symbol analogous to the emissions of an HMM).
Each step or emission of an HMM and each rewriting operation of a PCFG is
conditionally independent of the others given a single structural element (one HMM
or PCFG state); this Markov property permits efficient inference over derivations given
a string.
In general, a probabilistic grammar (G, O) defines the joint probability of a string x
and a grammatical derivation z:
</bodyText>
<equation confidence="0.99983">
q(x,z  |O,G) = K �Nk ,iψk,i(x,z) = K Nk *k,i(x,z)logOk,i (11)
H i=1 exp E i=1
k=1 k=1
</equation>
<bodyText confidence="0.99987825">
where *k,i is a function that “counts” the number of times the kth distribution’s
ith event occurs in the derivation. The parameters O are a collection of K multi-
nomials (O1, ... , OK), the kth of which includes Nk competing events. If we let Ok =
(Ok,1,... , Ok,Nk), each Ok,i is a probability, such that
</bodyText>
<equation confidence="0.99278175">
Vk,Vi, Ok,i &gt; 0
Nk
Vk, Ok,i = 1
i=1
</equation>
<bodyText confidence="0.999595608695652">
We denote by ΘG this parameter space for O. The grammar G dictates the support
of q in Equation (11). As is often the case in probabilistic modeling, there are differ-
ent ways to carve up the random variables. We can think of x and z as correlated
structure variables (often x is known if z is known), or the derivation event counts
*(x,z) = (*k,i(x,z))1&lt;_k&lt;_K,1&lt;_i&lt;_Nk as an integer-vector random variable. In this article,
we assume that x is always a deterministic function of z, so we use the distribution
p(z) interchangeably with p(x,z).
Note that there may be many derivations z for a given string x—perhaps even
infinitely many in some kinds of grammars. For HMMs, there are three kinds of multi-
nomials: a starting state multinomial, a transition multinomial per state and an emission
multinomial per state. In that case K = 2s + 1, where s is the number of states. The value
of Nk depends on whether the kth multinomial is the starting state multinomial (in
which case Nk = s), transition multinomial (Nk = s), or emission multinomial (Nk = t,
with t being the number of symbols in the HMM). For PCFGs, each multinomial
among the K multinomials corresponds to a set of Nk context-free rules headed by
the same nonterminal. The parameter Ok,i is then the probability of the ith rule for the
kth nonterminal.
We assume that G denotes a fixed grammar, such as a context-free or regular gram-
mar. We let N = EKk=1 Nk denote the total number of derivation event types. We use
D(G) to denote the set of all possible derivations of G. We define Dx(G) = {z E D(G) |
yield(z) = x}. We use deg(G) to denote the “degree” of G, i.e., deg(G) = maxk Nk. We
let |x |denote the length of the string x, and |z |= Ek1 E i=1 *k i(z) denote the “length”
(number of event tokens) of the derivation z.
</bodyText>
<page confidence="0.870399">
486
</page>
<note confidence="0.632346">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<bodyText confidence="0.9995687">
Going back to the notation in Section 2, Q would be a collection of probabilistic
grammars, parametrized by 0, and q would be a specific probabilistic grammar with
a specific 0. We therefore treat the problem of ERM with probabilistic grammars as the
problem of parameter estimation—identifying 0 from complete data or incomplete data
(strings x are visible but the derivations z are not). We can also view parameter esti-
mation as the identification of a hypothesis from the concept space Q = X(G) = {he(z) |
0 E ΘG} (where he is a distribution of the form of Equation [11]) or, equivalently, from
negated log-concept space T(G) = {− log he(z)  |0 E ΘG}. For simplicity of notation, we
assume that there is a fixed grammar G and use X to refer to X(G) and T to refer
to T(G).
</bodyText>
<subsectionHeader confidence="0.998757">
3.1 Distributional Assumptions about Language
</subsectionHeader>
<bodyText confidence="0.99900265">
In this section, we describe a parametrization of assumptions we make about the dis-
tribution p(x,z), the distribution that generates derivations from D(G) (note that p does
not have to be a probabilistic grammar). We first describe empirical evidence about the
decay of the frequency of long strings x.
Figure 1 shows the frequency of sentence length for treebanks in various lan-
guages.4 The trend in the plots clearly shows that in the extended tail of the curve, all
languages have an exponential decay of probabilities as a function of sentence length. To
test this, we performed a simple regression of frequencies using an exponential curve.
We estimated each curve for each language using a curve of the form f (l; c, o ) = clα.
This estimation was done by minimizing squared error between the frequency ver-
sus sentence length curve and the approximate version of this curve. The data points
used for the approximation are (li,pi), where li denotes sentence length and pi denotes
frequency, selected from the extended tail of the distribution. Extended tail here refers
to all points with length longer than l1, where l1 is the length with the highest frequency
in the treebank. The goal of focusing on the tail is to avoid approximating the head
of the curve, which is actually a monotonically increasing function. We plotted the
approximate curve together with a length versus frequency curve for new syntactic
data. It can be seen (Figure 1) that the approximation is rather accurate in these corpora.
As a consequence of this observation, we make a few assumptions about G and
p(x,z):
</bodyText>
<listItem confidence="0.855676333333333">
• Derivation length proportional to sentence length: There is an o &gt; 1 such
that, for all z, |z |&lt; o |yield(z)|. Further, |z |&gt; |x|. (This prohibits unary
cycles.)
• Exponential decay of derivations: There is a constant r &lt; 1 and a constant
L &gt; 0 such that p(z) &lt; Lr|z|. Note that the assumption here is about the
frequency of length of separate derivations, and not the aggregated
frequency of all sentences of a certain length (cf. the discussion above
referring to Figure 1).
4 Treebanks offer samples of cleanly segmented sentences. It is important to note that the distributions
estimated may not generalize well to samples from other domains in these languages. Our argument
is that the family of the estimated curve is reasonable, not that we can correctly estimate the curve’s
parameters.
</listItem>
<figure confidence="0.835655">
487
Computational Linguistics Volume 38, Number 3
</figure>
<figureCaption confidence="0.991797">
Figure 1
</figureCaption>
<bodyText confidence="0.998595375">
A plot of the tail of frequency vs. sentence length in treebanks for English, German, Bulgarian,
Turkish, Spanish, and Chinese. Red lines denote data from the treebank, blue lines denote an
approximation which uses an exponential function of the form f (l; c, α) = clα (the blue line uses
data which is different from the data used to estimate the curve parameters, c and α). The
parameters (c, α) are (0.19, 0.92) for English, (0.06, 0.94) for German, (0.26, 0.89) for Bulgarian,
(0.26, 0.83) for Turkish, (0.11, 0.93) for Spanish, and (0.03, 0.97) for Chinese. Squared errors are
0.0005, 0.0003, 0.0007, 0.0003, 0.001, and 0.002 for English, German, Bulgarian, Turkish, Spanish,
and Chinese, respectively.
</bodyText>
<listItem confidence="0.77939275">
• Exponential decay of strings: Let A(k) = |{z E D(G)  ||z |= k} |be the
number derivations of length k in G. We assume that A(k) is an increasing
function, and complete it such that it is defined over positive numbers by
taking A(t) ° A([t]). Takin� r as before, we assume there exists a constant
</listItem>
<bodyText confidence="0.957184333333333">
q &lt; 1, such that A2(k)rk &lt; q (and as a consequence, A(k)rk &lt; qk). This
implies that the number of derivations of length k may be exponentially
large (e.g., as with many PCFGs), but is bounded by (q/r)k.
</bodyText>
<page confidence="0.814073">
488
</page>
<note confidence="0.441161">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<listItem confidence="0.842884">
• Bounded expectations of rules: There is a B &lt; oo such that ]Ep [ψk,i(z)] &lt; B
for all k and i.
</listItem>
<bodyText confidence="0.999655411764706">
These assumptions must hold for any p whose support consists of a finite set. These
assumptions also hold in many cases when p itself is a probabilistic grammar. Also, we
note that the last requirement of bounded expectations is optional, and it can be inferred
from the rest of the requirements: B = L/(1 − q)2. We make this requirement explicit for
simplicity of notation later. We denote the family of distributions that satisfy all of these
requirements by P(o, L, r, q, B, G).
There are other cases in the literature of language learning where additional as-
sumptions are made on the learned family of models in order to obtain positive learn-
ability results. For example, Clark and Thollard (2004) put a bound on the expected
length of strings generated from any state of probabilistic finite state automata, which
resembles the exponential decay of strings we have for p in this article.
An immediate consequence of these assumptions is that the entropy of p is finite
and bounded by a quantity that depends on L, r and q.5 Bounding entropy of labels
(derivations) given inputs (sentences) is a common way to quantify the noise in a
distribution. Here, both the sentential entropy (Hs (p) = −Ex p(x) log p(x)) is bounded
as well as the derivational entropy (Hd(p) = − Ex,z p(x, z) log p(x,z)). This is stated in the
following result.
</bodyText>
<equation confidence="0.900938">
Proposition 1
Let p E P(o, L, r, q, B, G) be a distribution. Then, we have
Hs(p) &lt; Hd(p) &lt; −log L + (L logr log 1r + [(1 + logL )/log 1r] Λ \ [1l
lo g gL /
Proof
</equation>
<bodyText confidence="0.999621">
First note that Hs(p) &lt; Hd(p) holds by the data processing inequality (Cover and
Thomas 1991) because the sentential probability distribution p(x) is a coarser version
of the derivational probability distribution p(x,z). Now, consider p(x,z). For simplicity
of notation, we use p(z) instead of p(x, z). The yield of z, x, is a function of z, and therefore
can be omitted from the distribution. It holds that
</bodyText>
<equation confidence="0.999376333333333">
� p(z) logp(z)
Hd(p) = −
z
�= − p(z) log p(z) − � p(z)logp(z)
z∈Z1 z∈Z2
= Hd(p, Z1) + Hd(p, Z2)
</equation>
<bodyText confidence="0.7181415">
where Z1 = {z  |p(z) &gt; 1/e} and Z2 = {z  |p(z) &lt; 1/e}. Note that the function −o log o
reaches its maximum for o = 1/e. We therefore have
</bodyText>
<equation confidence="0.8829975">
Hd(p, Z1) &lt;|Z1|
e
</equation>
<bodyText confidence="0.6437685">
5 For simplicity and consistency with the log-loss, we measure entropy in nats, which means we use the
natural logarithm when computing entropy.
</bodyText>
<page confidence="0.686058">
489
</page>
<note confidence="0.211374">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.998172666666667">
We give a bound on |Z1|, the number of “high probability” derivations. Because we have
p(x,z) ≤ Lr|z|, we can find the maximum length of a derivation that has a probability of
more than 1/e (and hence, it may appear in Z1) by solving 1/e ≤ Lr|z |for |z|, which leads
</bodyText>
<equation confidence="0.991587833333333">
to |z |≤ log(1/eL)/ log r. Therefore, there are at most ��(1+logL)/log 1r 1 A(k) derivations in
k=1
|Z1 |and therefore we have
� � �� ��
|Z1 |≤ (1 + log L)/log 1r A (1 + log L)/log 1r
Hd(p,Z1) ≤ [(1 + log L )/log r] A ([(1 + log L)/log 1r]) (12)
</equation>
<bodyText confidence="0.996691">
where we use the monotonicity of A. Consider Hd(p, Z2) (the “low probability” deriva-
tions). We have:
</bodyText>
<equation confidence="0.991654727272727">
Hd(p,Z2) ≤ − E Lr|z |log �Lr|z|�
zEZ2
≤ − log L − (L log r) E |z|r|z|
zEZ2
≤ − log L − (L log r) 00 A(k)krk
E
k=1
00
≤ − log L − (L log r) E kqk (13)
k=1
= − log L + (i log log q1 (14)
</equation>
<bodyText confidence="0.999696">
where Equation (13) holds from the assumptions about p. Putting Equation (12) and
Equation (14) together, we obtain the result. ■
We note that another common way to quantify the noise in a distribution is through
the notion of Tsybakov noise (Tsybakov 2004; Koltchinskii 2006). We discuss this further
in Section 7.1, where we show that Tsybakov noise is too permissive, and probabilistic
grammars do not satisfy its conditions.
</bodyText>
<subsectionHeader confidence="0.999896">
3.2 Limiting the Degree of the Grammar
</subsectionHeader>
<bodyText confidence="0.999648727272727">
When approximating a family of probabilistic grammars, it is much more convenient
when the degree of the grammar is limited. In this article, we limit the degree of the
grammar by making the assumption that all Nk ≤ 2. This assumption may seem, at first
glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence,
other formalisms), this assumption does not limit the total generative capacity that we
can have across all context-free grammars.
We first show that any context-free grammar with arbitrary degree can be mapped
to a corresponding grammar with all Nk ≤ 2 that generates derivations equivalent to
derivations in the original grammar. Such a grammar is also called a “covering gram-
mar” (Nijholt 1980; Leermakers 1989). Let G be a CFG. Let A be the kth nonterminal.
Consider the rules A → αi for i ≤ Nk where A appears on the left side. For each rule
</bodyText>
<page confidence="0.639683">
490
</page>
<note confidence="0.429137">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<figureCaption confidence="0.930547">
Figure 2
</figureCaption>
<subsectionHeader confidence="0.594128">
Example of a context-free grammar and its equivalent binarized form.
</subsectionHeader>
<bodyText confidence="0.97171945">
A -+ αi, i &lt; Nk, we create a new nonterminal in G&apos; such that Ai has two rewrite rules:
Ai -+ αi and Ai -+ Ai+1. In addition, we create rules A -+ A1 and ANk -+ αNk. Figure 2
demonstrates an example of this transformation on a small context-free grammar.
It is easy to verify that the resulting grammar G&apos; has an equivalent capacity to
the original CFG, G. A simple transformation that converts each derivation in the
new grammar to a derivation in the old grammar would involve collapsing any path
of nonterminals added to G&apos; (i.e., all Ai for nonterminal A) so that we end up with
nonterminals from the original grammar only. Similarly, any derivation in G can be
converted to a derivation in G&apos; by adding new nonterminals through unary application
of rules of the form Ai -+ Ai+1. Given a derivation z in G, we denote by ΥG,G&apos;(z) the
corresponding derivation in G&apos; after adding the new non-terminals Ai to z. Throughout
this article, we will refer to the normalized form of G&apos; as a “binary normal form.”6
Note that K&apos;, the number of multinomials in the binary normal form, is a func-
tion of both the number of nonterminals in the original grammar and the number of
rules in that grammar. More specifically, we have that K&apos; = EKk=1 Nk + K. To make the
equivalence complete, we need to show that any probabilistic context-free grammar can
be translated to a PCFG with maxk Nk &lt; 2 such that the two PCFGs induce the same
equivalent distributions over derivations.
Utility Lemma 1
Let ai E [0, 1], i E {1, ... , N} such that Ei ai = 1. Define b1 = a1, c1 = 1 − a1, bi =
</bodyText>
<equation confidence="0.9862925">
� �
ail bi−1
`ai−1) (ci−1), and ci = 1 − bi for i &gt; 2. Then ai = cj bi.
j=1
</equation>
<bodyText confidence="0.918483727272727">
See Appendix A for the proof of Utility Lemma 1.
Theorem 1
Let (G, θ) be a probabilistic context-free grammar. Let G&apos; be the binarizing transforma-
tion of G as defined earlier. Then, there exists θ&apos; for G&apos; such that for any z E D(G) we
have p(z  |θ, G) = p(ΥG,G&apos;(z)  |θf, GI).
6 We note that this notion of binarization is different from previous types of binarization appearing in
computational linguistics for grammars. Typically in previous work about binarized grammars such as
CFGs, the grammars are constrained to have at most two nonterminals in the right side in Chomsky
normal form. Another form of binarization for linear context-free rewriting systems is restriction of the
fan-out of the rules to two (G´omez-Rodriguez and Satta 2009; Gildea 2010). We, however, limit the
number of rules for each nonterminal (or more generally, the number of elements in each multinomial).
</bodyText>
<figure confidence="0.597515">
491
Computational Linguistics Volume 38, Number 3
Proof
</figure>
<bodyText confidence="0.989152">
For the grammar G, index the set {1, ..., K} with nonterminals ranging from A1 to AK.
Define G&apos; as before. We need to define θ&apos;. Index the multinomials in G&apos; by (k, i), each hav-
ing two events. Let µ(k,i),1 = θk,i, µ(k,i),2 = 1 − θk,i for i = 1 and set µk,i,1 = θk,i/µ(k,i−1),2,
and µ(k,i−1),2 = 1 − µ(k,i−1),2.
</bodyText>
<equation confidence="0.952602529411765">
(G&apos;, µ) is a weighted context-free grammar such that the µ(k,i),1 corresponds to the
ith event in the k multinomial of the original grammar. Let z be a derivation in G and
z&apos; = ΥG,,G&apos;(z). Then, from Utility Lemma 1 and the construction of g&apos;, we have that:
p(z  |θ, G) = K Nk θψk,i(z)
H i=1 k,i
k=1
= K Nk ψk,i(z) θk,i
H i=1 �
k=1 l=1

µ(k,j),2 µ(k,i),1
ψk,i(z) ψk,i(z)
µ(k ,j),2 µ(k,i),1
= K Nk 2 ψk,j(z&apos;)
H H ri µ(k,j),i
k=1 j=1 i=1
= p(z&apos;  |µ, G&apos;)
</equation>
<bodyText confidence="0.999918461538462">
From Chi (1999), we know that the weighted grammar (G&apos;, µ) can be converted to
a probabilistic context-free grammar (G&apos;, θ&apos;), through a construction of θ&apos; based on µ,
such that p(z&apos;  |µ, G&apos;) = p(z&apos;  |θ&apos;, G&apos;). ■
The proof for Theorem 1 gives a construction the parameters θ&apos; of G&apos; such that (G, θ)
is equivalent to (G&apos;, θ&apos;). The construction of θ&apos; can also be reversed: Given θ&apos; for G&apos;, we
can construct θ for G so that again we have equivalence between (G, θ) and (G&apos;, θ&apos;).
In this section, we focused on presenting parametrized, empirically justified distri-
butional assumptions about language data that will make the analysis in later sections
more manageable. We showed that these assumptions bound the amount of entropy as a
function of the assumption parameters. We also made an assumption about the structure
of the grammar family, and showed that it entails no loss of generality for CFGs. Many
other formalisms can follow similar arguments to show that the structural assumption
is justified for them as well.
</bodyText>
<sectionHeader confidence="0.771248" genericHeader="method">
4. Proper Approximations
</sectionHeader>
<bodyText confidence="0.9983734">
In order to follow the empirical risk minimization described in Section 2.1, we have
to define a series of approximations for T, which we denote by the log-concept spaces
F1, T2,. ... We also have to replace two-sided uniform convergence (Equation [6]) with
convergence on the sequence of concept spaces we defined (Equation [10]). The concept
spaces in the sequence vary as a function of the number of samples we have. We next
</bodyText>
<equation confidence="0.9989355">
= K Nk ψk,i(z) 
H i=1 � �i− 1
k=1 l=1 j=1
= K Nk 
H i=1 ~i− 1
k=1 j=1
</equation>
<page confidence="0.459581">
492
</page>
<note confidence="0.358156">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<bodyText confidence="0.9998535">
construct the sequence of concept spaces, and in Section 5 we return to the learning
model. Our approximations are based on the concept of bounded approximations (Abe,
Takeuchi, and Warmuth 1991; Dasgupta 1997), which were originally designed for
graphical models.7 A bounded approximation is a subset of a concept space which is
controlled by a parameter that determines its tightness. Here we use this idea to define
a series of subsets of the original concept space F as approximations, while having two
asymptotic properties that control the series’ tightness.
Let Fm (for m ∈ {1,2,...}) be a sequence of concept spaces. We consider three
properties of elements of this sequence, which should hold for m &gt; M for a fixed M.
The first is containment in F:
</bodyText>
<equation confidence="0.517357666666667">
Fm ⊆ F
The second property is boundedness:
∃Km ≥ 0,∀f ∈ Fm, E [|f |× I{|f |≥ Km}] ≤ ebound(m)
</equation>
<bodyText confidence="0.983809166666667">
where ebound is a non-increasing function such that ebound(m) −→ 0. This states that
m→∞
the expected values of functions from Fm on values larger than some Km is small.
This is required to obtain uniform convergence results in the revised empirical risk
minimization model from Section 2.1. Note that Km can grow arbitrarily large.
The third property is tightness:
</bodyText>
<equation confidence="0.97599275">
� �
�U
∃Cm ∈ F → Fm, p {z  |Cm( f )(z) − f(z) ≥ etail(m)} �≤ etail(m)
f∈F
</equation>
<bodyText confidence="0.995520944444444">
where etail is a non-increasing function such that etail(m) −→ 0, and Cm denotes an
m→∞
operator that maps functions in F to Fm. This ensures that our approximation actually
converges to the original concept space F. We will show in Section 4.3 that this is
actually a well-motivated characterization of convergence for probabilistic grammars
in the supervised setting.
We say that the sequence Fm properly approximates F if there exist etail(m), ebound(m),
and Cm such that, for all m larger than some M, containment, boundedness, and tight-
ness all hold.
In a good approximation, Km would increase at a fast rate as a function of m and
etail(m) and ebound(m) decrease quickly as a function of m. As we will see in Section 5, we
cannot have an arbitrarily fast convergence rate (by, for example, taking a subsequence
of Fm), because the size of Km has a great effect on the number of samples required to
obtain accurate estimation.
7 There are other ways to manage the unboundedness of KL divergence in the language learning literature.
Clark and Thollard (2004), for example, decompose the KL divergence between probabilistic finite-state
automata into several terms according to a decomposition of Carrasco (1997) and then bound each term
separately.
</bodyText>
<page confidence="0.77982">
493
</page>
<table confidence="0.425553">
Computational Linguistics Volume 38, Number 3
</table>
<tableCaption confidence="0.991095">
Table 1
</tableCaption>
<bodyText confidence="0.817991333333333">
Example of a PCFG where there is more than a single way to approximate it by truncation with
γ = 0.1, because it has more than two rules. Any value of η E [0,γ] will lead to a different
approximation.
</bodyText>
<table confidence="0.98716325">
Rule θ General η = 0 η = 0.01 η = 0.005
S NP VP 0.09 0.01 0.1 0.1 0.1
S NP 0.11 0.11 − η 0.11 0.1 0.105
S VP 0.8 0.8 − γ + η 0.79 0.8 0.795
</table>
<subsectionHeader confidence="0.998893">
4.1 Constructing Proper Approximations for Probabilistic Grammars
</subsectionHeader>
<bodyText confidence="0.999809142857143">
We now focus on constructing proper approximations for probabilistic grammars whose
degree is limited to 2. Proper approximations could, in principle, be used with losses
other than the log-loss, though their main use is for unbounded losses. Starting from
this point in the article, we focus on using such proper approximations with the
log-loss.
We construct Tm. For each f E T we define a transformation T( f, γ) that shifts every
binomial parameter θk = (θk,1, θk,2) in the probabilistic grammar by at most γ:
</bodyText>
<equation confidence="0.991198333333333">
(θk,1, θk,2) +— { (γ, 1 − γ) if θk,1 &lt; γ
(1 − γ, γ) if θk,1 &gt; 1 − γ
(θk,1, θk,2) otherwise
</equation>
<bodyText confidence="0.99977975">
Note that T( f,γ) E T for any γ &lt; 1/2. Fix a constant s &gt; 1.8 We denote by T(θ,γ) the
same transformation on θ (which outputs the new shifted parameters) and we denote
by ©G(γ) = ©(γ) the set {T(θ,γ)  |θ E ©G}. For each m E N, define Tm = {T( f, m−s) |
f E T}.
When considering our approach to approximate a probabilistic grammar by in-
creasing its parameter probabilities to be over a certain threshold, it becomes clear
why we are required to limit the grammar to have only two rules and why we are
required to use the normal from Section 3.2 with grammars of degree 2. Consider the
PCFG rules in Table 1. There are different ways to move probability mass to the rule
with small probability. This leads to a problem with identifability of the approximation:
How does one decide how to reallocate probability to the small probability rules? By
binarizing the grammar in advance, we arrive at a single way to reallocate mass when
required (i.e., move mass from the high-probability rule to the low-probability rule).
This leads to a simpler proof for sample complexity bounds and a single bound (rather
than different bounds depending on different smoothing operators). We note, however,
that the choices made in binarizing the grammar imply a particular way of smoothing
the probability across the original rules.
We now describe how this construction of approximations satisfies the proper-
ties mentioned in Section 4, specifically, the boundedness property and the tightness
property.
</bodyText>
<figure confidence="0.387082">
8 By varying s we get a family of approximations. The larger s is, the tighter the approximation is. Also,
the larger s is, as we see later, the looser our sample complexity bound will be.
494
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Proposition 2
Let p E P(α, L, r, q, B, G) and let Fm be as defined earlier. There exists a constant β =
</figure>
<equation confidence="0.991111923076923">
β(L, q, p, N) &gt; 0 such that Fm has the boundedness property with Km = sN log3 m and
Ebound(m) = m−β logm.
See Appendix A for the proof of Proposition 2.
N log2 m
Next, Fm is tight with respect to F with �tail(m) = ms − 1 .
Proposition 3
Let p E P(α, L, r, q, B, G) and let Fm as defined earlier. There exists an M such that for any
m &gt; M we have
� �
�U
p {z I Cm( f )(z) − f(z) ? Etail(m)I �&lt; Etail(m)
f∈F
N log2 m
</equation>
<bodyText confidence="0.994812666666667">
for �tail(m) = ms − 1 and Cm( f ) = T( f,m−s).
See Appendix A for the proof of Proposition 3.
We now have proper approximations for probabilistic grammars. These approx-
imations are defined as a series of probabilistic grammars, related to the family of
probabilistic grammars we are interested in estimating. They consist of three prop-
erties: containment (they are a subset of the family of probabilistic grammars we
are interested in estimating), boundedness (their log-loss does not diverge to infinity
quickly), and they are tight (there is a small probability mass at which they are not tight
approximations).
</bodyText>
<subsectionHeader confidence="0.999327">
4.2 Coupling Bounded Approximations with Number of Samples
</subsectionHeader>
<bodyText confidence="0.995535909090909">
At this point, the number of samples n is decoupled from the bounded approximation
(Fm) that we choose for grammar estimation. To couple between these two, we need
to define m as a function of the number of samples, m(n). As mentioned earlier, there
is a clear trade-off between choosing a fast rate for m(n) (such as m(n) = nk for some
k &gt; 1) and a slower rate (such as m(n) = log n). The faster the rate is, the tighter the
family of approximations that we use for n samples. If the rate is too fast, however,
then Km grows quickly as well. In that case, because our sample complexity bounds are
increasing functions of such Km, the bounds will degrade.
To balance the trade-off, we choose m(n) = n. As we see later, this gives sample
complexity bounds which are asymptotically interesting for both the supervised and
unsupervised case.
</bodyText>
<figure confidence="0.4251955">
495
Computational Linguistics Volume 38, Number 3
</figure>
<subsectionHeader confidence="0.997746">
4.3 Asymptotic Empirical Risk Minimization
</subsectionHeader>
<bodyText confidence="0.99815325">
It would be compelling to determine whether the empirical risk minimizer over Fn is
an asymptotic empirical risk minimizer. This would mean that the risk of the empirical
risk minimizer over Fn converges to the risk of the maximum likelihood estimate. As a
conclusion to this section about proper approximations, we motivate the three re-
quirements that we posed on proper approximations by showing that this is indeed
true. We now unify n, the number of samples, and m, the index of the approxima-
tion of the concept space F. Let f,,∗ be the minimizer of the empirical risk over F,
( f,,∗ = argminf∈_, E˜pn [ f]) and let gn be the minimizer of the empirical risk over Fn
</bodyText>
<equation confidence="0.8191525">
(gn = argminf∈_,n E˜pn [ f]).
).
</equation>
<bodyText confidence="0.901857666666667">
Let D = {z1, ...,zn} be a sample from p(z). The operator (gn =) argminf∈_,n E˜pn[ f] is
an asymptotic empirical risk minimizer if E[E˜pn [gn] − E˜pn[ f∗ n ]] -+ 0 as n -+ oo (Shalev-
Shwartz et al. 2009). Then, we have the following
Lemma 1
Denote by ZE,n the set Uf∈_,{z  |Cn( f )(z) − f (z) &gt; c}. Denote by AE,n the event “one of
zi E D is in ZE,n.” If Fn properly approximates F, then:
</bodyText>
<equation confidence="0.938096">
E [E˜pn [gn] − E˜pn [ fn ] ] (15)
&lt; IE [Ep&amp; [Cn( f∗n )]  |A.,n] p(Ae,n) + E [E˜pn [ fn∗]  |Ae,n] p(Ae,n) + ctail(n)
</equation>
<bodyText confidence="0.876118875">
where the expectations are taken with respect to the data set D.
See Appendix A for the proof of Lemma 1.
Proposition 4
Let D = {z1,...,zn} be a sample of derivations from G. Then gn = argminf∈_,n E˜pn [ f] is
an asymptotic empirical risk minimizer.
Proof
Let f0 E F be the concept that puts uniform weights over 0, namely, 0k= (12, 12) for all k.
Note that
</bodyText>
<equation confidence="0.80487325">
|E [E˜pn [ fn∗]  |Ae,n] |p(Ae,n)
&lt; |E [E˜pn [ f0]  |Ae,n]|p(Ae,n) = lo n 2 El=1 Ek,i
E[ψk,i(zl)  |Ae,n]p(4n)
496
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Let Aj,e,n for j ∈ {1, ... , n} be the event “zj ∈ Ze,n”. Then Ae,n = Uj Aj,e,n. We have
that
E[*k,i(zl)  |A�,n]p(A�,n) ≤ E E p(zl, Aj,e,n)|zl|
j zl
E E E p(zl,Al,e,n)|zl |(16)
≤ zl p(zl)p(Aj,�,n)|zl |+
j#l zl
��
�E ≤p(Aj,c,n) �B + E[*k,i(z)  |z ∈ Ze,n]p(z ∈ Ze,n)
j#l
≤ (n − 1)Bp(z ∈ Ze,n) + E[*k,i(z)  |z ∈ Ze,n]p(z ∈ Ze,n)
</equation>
<bodyText confidence="0.985388">
where Equation (16) comes from zl being independent. Also, B is the constant from
Section 3.1. Therefore, we have:
</bodyText>
<equation confidence="0.88257125">
E[*k,i(zl)  |Ae,n]p(Ae,n)
E (E[*k,i(z)  |z ∈ Ze,n]p(z ∈ Ze,n) + (n − 1)Bp(z ∈ Ze,n))
≤
k,i
</equation>
<bodyText confidence="0.902426">
From the construction of our proper approximations (Proposition 3), we know that only
derivations of length log2 n or greater can be in Ze,n. Therefore
</bodyText>
<equation confidence="0.984898666666667">
∞
E[*k,i  |Z�,n]p(Z�,n) ≤ E p(z)*k,i(z) ≤ LA(l)rll ≤ Kqlog2 n = o(1)
z:|z|&gt;log2 n l&gt;log2 n
where K &gt; 0 is a constant. Similarly, we have p(z ∈ Ze,n) = o(n−1). This means that
|E[E˜pn[− log −fn∗]  |A.,n]|p(Ae,n) −→ 0. In addition, it can be shown that |E[E˜pn [Cn( fn∗) |
Ae,n]|p(Ae,n) n + 0 using the same proof technique we used here, while relying on the
</equation>
<bodyText confidence="0.846591">
fact that Cn( f∗n ) ∈ Tn, and therefore Cn( f∗n )(z) ≤ sN|z |log n. ■
</bodyText>
<sectionHeader confidence="0.530431" genericHeader="method">
5. Sample Complexity Bounds
</sectionHeader>
<bodyText confidence="0.929403833333333">
Equipped with the framework of proper approximations as described previously, we
now give our main sample complexity results for probabilistic grammars. These results
[ f] |. Indeed, proper approximations
hinge on the convergence of supf∈,n |E˜pn [ f] − Ep
replace the use of T in these convergence results. The rate of this convergence can be
fast, if the covering numbers for Tn do not grow too fast.
</bodyText>
<subsectionHeader confidence="0.999585">
5.1 Covering Numbers and Bounds on Covering Numbers
</subsectionHeader>
<bodyText confidence="0.998724">
We next give a brief overview of covering numbers. A cover provides a way to reduce
a class of functions to a much smaller (finite, in fact) representative class such that each
function in the original class is represented using a function in the smaller class. Let G
</bodyText>
<figure confidence="0.989745375">
1
n
En
l=1
E
k,i
497
Computational Linguistics Volume 38, Number 3
</figure>
<bodyText confidence="0.939891">
be a class of functions. Let d(f,g) be a distance measure between two functions f,g from
9. An e-cover is a subset of 9, denoted by 9&apos;, such that for every f E 9 there exists an
f&apos; E 9&apos; such that d(f,f&apos;) &lt; e. The covering number N(e, 9, d) is the size of the smallest
e-cover of 9 for the distance measure d.
We are interested in a specific distance measure which is dependent on the empirical
distribution ˜pn that describes the data z1, ...,zn. Let f,g E 9. We will use
</bodyText>
<equation confidence="0.98619875">
�
d˜pn( f, g) = E˜pn ~|f − g|~ = zED(G) |f(z) − g(z) |˜pn(z)
n En
=1i=1  |f(zi) − g(zi)
</equation>
<bodyText confidence="0.978308857142857">
Instead of using N(e, 9,d˜pn) directly, we bound this quantity with N(e, 9) = sup˜pn
N(e, 9, d˜pn), where we consider all possible samples (yielding ˜pn). The following is the
key result regarding the connection between covering numbers and the double-sided
� f~  |as n -+ oo. This result
convergence of the empirical process supfE-Tn |E˜pn � f~ − Ep
is a general-purpose result that has been used frequently to prove the convergence of
empirical processes of the type we discuss in this article.
</bodyText>
<figure confidence="0.47263125">
Lemma 2
Let Tn be a permissible class9 of functions such that for every f E Tn we have E[|f  |x
I {|f  |&lt; Kn}] &lt; ebound(n). Let Ttruncated,n = {f x I {f &lt; Kn}  |f E Tm}, namely, the set of
functions from Tn after being truncated by Kn. Then for e &gt; 0 we have
~ p sup |E˜pn [f] − Ep [f]  |&gt; 2e &lt; 8N(e/8,Ttruncated,n) exp (−1
128ne2/Kn) +ebound(n)/e
fE-Tn
provided n &gt; K2n/4e2 and ebound(n) &lt; e.
</figure>
<bodyText confidence="0.921368181818182">
See Pollard (1984; Chapter 2, pages 30–31) for the proof of Lemma 2. See also Ap-
pendix A.
Covering numbers are rather complex combinatorial quantities which are hard
to compute directly. Fortunately, they can be bounded using the pseudo-dimension
(Anthony and Bartlett 1999), a generalization of the Vapnik-Chervonenkis (VC)
dimension for real functions. In the case of our “binomialized” probabilistic grammars,
the pseudo-dimension of Tn is bounded by N, because we have Tn C T, and the
functions in T are linear with N parameters. Hence, Ttruncated,n also has pseudo-
dimension that is at most N. We then have the following.
9 The “permissible class” requirement is a mild regularity condition regarding measurability that holds for
proper approximations. We refer the reader to Pollard (1984) for more details.
</bodyText>
<figure confidence="0.319570142857143">
498
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Lemma 3
(From Pollard [1984] and Haussler [1992].) Let Tn be the proper approximations for
probabilistic grammars, for any 0 &lt; c &lt; Kn we have:
C 2eK 2eKn 1 N
N(E, Ttruncated,n) &lt; 2 n E log E
</figure>
<subsectionHeader confidence="0.977599">
5.2 Supervised Case
</subsectionHeader>
<bodyText confidence="0.9817252">
We turn to give an analysis for the supervised case. This analysis is mostly described as a
preparation for the unsupervised case. In general, the families of probabilistic grammars
we give a treatment to are parametric families, and the maximum likelihood estimator
for these families is a consistent estimator in the supervised case. In the unsupervised
case, however, lack of identifiability prevents us from getting these traditional consis-
tency results. Also, the traditional results about the consistency of MLE are based on the
assumption that the sample is generated from the parametric family we are trying to
estimate. This is not the case in our analysis, where the distribution that generates the
data does not have to be a probabilistic grammar.
Lemmas 2 and 3 can be combined to get the following sample complexity result.
Theorem 2
Let G be a grammar. Let p ∈ P(α, L, r, q, B, G) (Section 3.1). Let Tn be a proper approxima-
tion for the corresponding family of probabilistic grammars. Let z1, ...,zn be a sample
of derivations. Then there exists a constant β(L, q, p, N) and constant M such that for any
0 &lt; δ &lt; 1 and 0 &lt; E &lt; Kn and any n &gt; M and if
</bodyText>
<equation confidence="0.961001571428571">
n ≥ max � C2128Kn (2N log(16eKn/c) + log b2) log4 /b + log1/c 1
/ β ( q,p,N)j
then we have
� �
[ f]  |≤ 2~
P sup |E˜pn [ f] − Ep ≥ 1 − δ
f ∈-Tn
</equation>
<bodyText confidence="0.936217">
where Kn = sN log3 n.
</bodyText>
<subsectionHeader confidence="0.836802">
Proof Sketch
</subsectionHeader>
<bodyText confidence="0.967485">
β(L, q, p, N) is the constant from Proposition 2. The main idea in the proof is to solve for
n in the following two inequalities (based on Equation [17] [see the following]) while
relying on Lemma 3:
</bodyText>
<figure confidence="0.2951214">
8N(E/8,Ttruncated,n)exp(−1
128ne2/Kn) ≤ δ/2
Ebound(n)/E ≤ δ/2
499
Computational Linguistics Volume 38, Number 3
</figure>
<bodyText confidence="0.9996974">
Theorem 2 gives little intuition about the number of samples required for accurate
estimation of a grammar because it considers the “additive” setting: The empirical risk
is within e from the expected risk. More specifically, it is not clear how we should pick
e for the log-loss, because the log-loss can obtain arbitrary values.
We turn now to converting the additive bound in Theorem 2 to a multiplicative
bound. Multiplicative bounds can be more informative than additive bounds when the
range of the values that the log-loss can obtain is not known a priori. It is important
to note that the two views are equivalent (i.e., it is possible to convert a multiplicative
bound to an additive bound and vice versa). Let p G (0, 1) and choose e = pKn. Then,
substituting this e in Theorem 2, we get that if
</bodyText>
<figure confidence="0.656904571428571">
n &gt; max 128 (2N lo 16e + lo 32l &apos; log 4/b + log 1/p
p2 \ g p g b J (3(L,q,p,N)
then, with probability 1 − b,
E˜� f~
pn
� f~
Ep
~~~p x 2sN log3(n)
�&lt; (17)
� H(p)
�����
1
sup
f c-Tn
</figure>
<bodyText confidence="0.999666714285714">
where H(p) is the Shannon entropy of p. This stems from the fact that Ep If] &gt; H(p) for
any f. This means that if we are interested in computing a sample complexity bound
such that the ratio between the empirical risk and the expected risk (for log-loss) is
close to 1 with high probability, we need to pick up p such that the righthand side of
Equation (17) is smaller than the desired accuracy level (between 0 and 1). Note that
Equation (17) is an oracle inequality—it requires knowing the entropy of p or some
upper bound on it.
</bodyText>
<subsectionHeader confidence="0.98322">
5.3 Unsupervised Case
</subsectionHeader>
<bodyText confidence="0.998628">
In the unsupervised setting, we have n yields of derivations from the grammar, x1, ..., xn,
and our goal again is to identify grammar parameters 0 from these yields. Our concept
classes are now the sets of log marginalized distributions from Fn. For each fθ G Fn, we
define fθ&apos; as
</bodyText>
<equation confidence="0.956248666666667">
� K Nk
f&apos;θ(x) = − log exp( −fθ (z)) = − log 1: exp j: j&gt;i,k(z)0i,k
zcDx(G) zcDx(G) (k=1 i=1
</equation>
<bodyText confidence="0.989586777777778">
We denote the set of { f&apos;θ} by F&apos;n. Analogously, we define F&apos;. Note that we also need to
define the operator C&apos;n( f&apos;) as a first step towards defining F&apos;n as proper approximations
(for F&apos;) in the unsupervised setting. Let f&apos; G F&apos;. Let f be the concept in F such that
f&apos;(x) = Ez f (x, z). Then we define C&apos; n( f&apos;)(x) = Ez Cn( f )(x,z).
It does not immediately follow that F&apos;n is a proper approximation for F&apos;. It is not
hard to show that the boundedness property is satisfied with the same Kn and the same
form of ebound(n) as in Proposition 2 (we would have e&apos;bound(m) = m−β&amp;quot;logm for some
(3&apos;(L, q, p, N) = (3&apos; &gt; 0). This relies on the property of bounded derivation length of p (see
Appendix A, Proposition 7). The following result shows that we have tightness as well.
</bodyText>
<table confidence="0.349725">
500
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Utility Lemma 2
For ai, bi ≥ 0, if − log Ei ai + log Ei bi ≥ e then there exists an i such that − log ai +
log bi ≥ e.
Proposition 5
</table>
<bodyText confidence="0.706673">
There exists an M such that for any n &gt; M we have
</bodyText>
<equation confidence="0.850109714285714">
� �
��
p {x  |C&apos;n(f&apos;)(x) − f&apos;(x) ≥ etail(n)} �≤ etail(n)
fIETI
N log2 n
for etail(n) = ns − 1 and the operator C&apos;n( f) as defined earlier.
Proof Sketch
</equation>
<bodyText confidence="0.754322">
From Utility Lemma 2 we have
</bodyText>
<equation confidence="0.938941">
� �p U {x  |C&apos;n( f&apos;)(x) − f&apos;(x) ≥ etail(n)} ≤ p U {x  |∃zCn( f)(z) − f(z) ≥ etail(n)}
fIETI (fET
</equation>
<bodyText confidence="0.994511">
Define x(n) to be all x such that there exists a z with yield(z) = x and |z |≥ log2 n.
From the proof of Proposition 3 and the requirements on p, we know that there exists
an oc ≥ 1 such that
</bodyText>
<equation confidence="0.997027333333333">
�� � �
p fET{x  |∃z s.t.Cn( f )(z) − f(z) ≥ etail(n)} ≤
xEx(n)
00
≤ � p(x) ≤ E LA(k)rk ≤ etail(n)
x:jxj&gt;log2 n/α k=Llog2 n/αJ
</equation>
<bodyText confidence="0.997597363636364">
where the last inequality happens for some n larger than a fixed M. ■
Computing either the covering number or the pseudo-dimension of F&apos;n is a hard
task, because the function in the classes includes the “log-sum-exp.” Dasgupta (1997)
overcomes this problem for Bayesian networks with fixed structure by giving a bound
on the covering number for (his respective) J�&apos; which depends on the covering number
of T.
Unfortunately, we cannot fully adopt this approach, because the derivations of
a probabilistic grammar can be arbitrarily large. Instead, we present the following
proposition, which is based on the “Hidden Variable Rule” from Dasgupta (1997). This
proposition shows that the covering number of J�&apos; (or more accurately, its bounded
approximations) can be bounded in terms of the covering number of the bounded
</bodyText>
<table confidence="0.914076388888889">
p(x)
501
Computational Linguistics Volume 38, Number 3
approximations of T, and the constants which control the underlying distribution p
mentioned in Section 3.
Utility Lemma 3
For any two positive-valued sequences (a1, ... , an) and (b1, ... , bn) we have that
Ei  |log ai/bi |?  |log (E ai/ E bi) |.
Proposition 6 (Hidden Variable Rule for Probabilistic Grammars)
Let m = 4Kn
log
1 q) . Then, N(c,T&apos;truncated,n) &lt; N (2A�
l m),Ttruncated,n/
og q
Proof
Let Z(m) = {z  ||z |&lt; m} be the subset of derivations of length shorter than m. Consider
f,f0 E Ttruncated,n. Let f~ and f0~ be the corresponding functions in T�truncated,n. Then, for any
distribution p,
</table>
<equation confidence="0.9019404">
dp( f ,fo) = E  |f�(x) − f � 0(x) |p(x) &lt; E E |f(x,z) − f0(x,z) |p(x)
x x z
E= E |f(x,z) − f0(x,z) |p(x) + E E |f(x,z) − f0(x,z) |p(x)
x z∈Z(m) x z/∈Z(m)
E E E E 2Knp(x) (18)
</equation>
<table confidence="0.8093145">
&lt; z∈Z(m) |f(x,z) − f0(x,z) |p(x) +
x x z/∈Z(m)
E E E |Dx(G)|p(x)
&lt; z∈Z(m) |f(x,z) − f0(x,z) |p(x) + 2Kn
x x: |x|≥m
E E |f(x,z) − f0(x,z) |p(x) + 2Kn ∞ Λ2(k)rk
&lt; z∈Z(m) E
x k=m
qm
&lt; dp&apos;( f, f0)|Z(m) |+ 2Kn1 − q
</table>
<bodyText confidence="0.4055955">
where p&apos;(x,z) is a probability distribution that uniformly divides the probability mass
p(x) across all derivations for the specific x, that is:
</bodyText>
<equation confidence="0.945183666666667">
p,(x,z) =p(x)
|Dx(G)|
The inequality in Equation (18) stems from Utility Lemma 3.
</equation>
<bodyText confidence="0.997264333333333">
Set m to be the quantity that appears in the proposition to get the necessary result
( f~ and f are arbitrary functions in T�truncated,n and Ttruncated,n respectively. Then consider
fo and f0 to be functions from the respective covers.). ■
</bodyText>
<figure confidence="0.404449">
For the unsupervised case, then, we get the following sample complexity result.
502
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Theorem 3
</figure>
<bodyText confidence="0.9995908">
Let G be a grammar. Let T&apos;,n be a proper approximation for the corresponding family of
probabilistic grammars. Let p(x, z) be a distribution over derivations which satisfies the
requirements in Section 3.1. Let x1, ... , xn be a sample of strings from p(x). Then there
exists a constant R,(L, q, p, N) and constant M such that for any 0 &lt; b &lt; 1, 0 &lt; c &lt; Kn,
any n &gt; M, and if
</bodyText>
<equation confidence="0.9418455">
n &gt; max 128Kn C2N log C32eKn A(m)1 + log b2) log4/6 + log)/c 1
\ l J R,( q,p, j (19)
where m = 4Kn
log
c(1 − q) ,we have that
log q1
� �
� f~ &lt; 2c
p sup E˜pn � f~ − Ep &gt; 1 − b
fE-Tn
</equation>
<bodyText confidence="0.999513454545454">
where Kn = sN log3 n.
Theorem 3 states that the number of samples we require in order to accurately esti-
mate a probabilistic grammar from unparsed strings depends on the level of ambiguity
in the grammar, represented as A(m). We note that this dependence is polynomial, and
we consider this a positive result for unsupervised learning of grammars. More specif-
ically, if A is an exponential function (such as the case with PCFGs), when compared to
the supervised learning, there is an extra multiplicative factor in the sample complexity
in the unsupervised setting that behaves like ((log log Knc ).
We note that the following Equation (20) can again be reduced to a multiplicative
case, similarly to the way we described it for the supervised case. Setting c = pKn (p E
(0,1)), we get the following requirement on n:
</bodyText>
<equation confidence="0.989111">
n &gt; max 128 (2Nr32e x t(p)1321 log4/b + log1/c!
p
p2 log p + log b R, (L, q,
, N) (20)
</equation>
<bodyText confidence="0.676586">
where t(p) = log 4p(1 − q) .
log q1
</bodyText>
<sectionHeader confidence="0.986739" genericHeader="method">
6. Algorithms for Empirical Risk Minimization
</sectionHeader>
<bodyText confidence="0.999983">
We turn now to describing algorithms and their properties for minimizing empirical
risk using the framework described in Section 4.
</bodyText>
<subsectionHeader confidence="0.989985">
6.1 Supervised Case
</subsectionHeader>
<bodyText confidence="0.999526">
ERM with proper approximations leads to simple algorithms for estimating the proba-
bilities of a probabilistic grammar in the supervised setting. Given an c &gt; 0 and a b &gt; 0,
we draw n examples according to Theorem 2. We then set -y = n−s. To minimize the
log-loss with respect to these n examples, we use the proper approximation T&apos;n.
</bodyText>
<figure confidence="0.6434036">
503
Computational Linguistics Volume 38, Number 3
Note that the value of the empirical log-loss for a probabilistic grammar param-
etrized by θ is
E˜p [− log h(x, z  |θ)] = − E ˜pn(x,z)logh(x,z  |θ)
n
x,z
E= − ˜pn(x, z) K ENk ψk,i(x, z) log(θk,i)
x,z E i=1
k=1
</figure>
<equation confidence="0.926392083333333">
K
E
k=1
= −
ENk
i=1
]
log(θk,i)E˜pn [ψk,i
Because we make the assumption that deg(G) &lt; 2 (Section 3.2), we have
K
E˜pn [−logh(x,z  |θ)] = − E ~log(θk,1)E˜pn [ψk,1] + log(1 − θk,1)E˜pn [ψk,2]~ (21)
k=1
</equation>
<bodyText confidence="0.999378">
To minimize the log-loss with respect to T&apos;n, we need to minimize Equation (21) under
the constraint that γ &lt; θk,i &lt; 1 − γ and θk1 + θk,2 = 1. It can be shown that the solution
for this optimization problem is
</bodyText>
<equation confidence="0.9548338">
θk,i = min { 1 − γ, max � � � &amp;quot;� � # # (22)
�n n 2
γ, E j,k,i E E � � ˆψj,k,i&apos; �
�% %
j=1 j=1 i&apos;=1
</equation>
<bodyText confidence="0.999851909090909">
where ˆψj,k,i is the number of times that ψk,i fires in Example j. (We include a full
derivation of this result in Appendix B.) The interpretation of Equation (22) is simple:
We count the number of times a rule appears in the samples and then normalize this
value by the total number of times rules associated with the same multinomial appear
in the samples. This frequency count is the maximum likelihood solution with respect
to the full hypothesis class X (Corazza and Satta 2006; see Appendix B). Because we
constrain ourselves to obtain a value away from 0 or 1 by a margin of γ, we need to
truncate this solution, as done in Equation (22).
This truncation to a margin γ can be thought of as a smoothing factor that enables us
to compute sample complexity bounds. We explore this connection to smoothing with
a Dirichlet prior in a Maximum a posteriori (MAP) Bayesian setting in Section 7.2.
</bodyText>
<subsectionHeader confidence="0.996609">
6.2 Unsupervised Case
</subsectionHeader>
<bodyText confidence="0.9970335">
Similarly to the supervised case, minimizing the empirical log-loss in the unsupervised
setting requires minimizing (with respect to θ) the following:
</bodyText>
<equation confidence="0.930717">
E˜pn [− log h(x  |θ)] = − E ˜pn(x) log E h(x,z  |θ) (23)
x z
</equation>
<bodyText confidence="0.938095">
with the constraint that γ &lt; θk,i &lt; 1 − γ (i.e., θ E Θ(γ)) where γ = n−s. This is done
after drawing n examples according to Theorem 3.
</bodyText>
<page confidence="0.751519">
504
</page>
<note confidence="0.7194515">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
6.2.1 Hardness of ERM with Proper Approximations. It turns out that minimizing Equa-
</note>
<bodyText confidence="0.893406666666667">
tion (23) under the specified constraints is actually an NP-hard problem when G is a
PCFG. This result follows using a similar proof to the one in Cohen and Smith (2010c)
for the hardness of Viterbi training and maximizing log-likelihood for PCFGs. We turn
to giving the full derivation of this hardness result for PCFGs and the modification
required for adapting the results from Cohen and Smith to the case of having an
arbitrary -y margin constraint.
In order to show an NP-hardness result, we need to “convert” the problem of the
maximization of Equation (23) to a decision problem. We do so by stating the following
decision problem.
Problem 1(Unsupervised Minimization of the Log-Loss with Margin)
Input: A binarized context-free grammar G, a set of sentences x1, ... , xn, a value -y E
[0, 12), and a value α E [0, 1].
</bodyText>
<equation confidence="0.932247666666667">
Output:1 if there exists 0 E ©(-y) (and hence, h E X(G)) such that
E− ˜pn(x) log E h(x,z 1 0) &lt; −log(α) (24)
x z
</equation>
<bodyText confidence="0.950914375">
and 0 otherwise.
We will show the hardness result both when -y is not restricted at all as well as when
we allow -y &gt; 0. The proof of the hardness result is achieved by reducing the problem
3-SAT (Sipser 2006), known to be NP-complete, to Problem 1. The problem 3-SAT is
defined as follows:
Problem 2 (3-SAT)
Input: A formula φ _ Ami=1 (ai V bi V ci) in conjunctive normal form, such that each
clause has three literals.
Output:1 if there is a satisfying assignment for φ, and 0 otherwise.
Given an instance of the 3-SAT problem, the reduction will, in polynomial time,
create a grammar and a single string such that solving Problem 1 for this grammar and
string will yield a solution for the instance of the 3-SAT problem.
Let φ _ Ami=1 (ai V bi V ci) be an instance of the 3-SAT problem, where ai, bi, and
ci are literals over the set of variables {Y1, ... , YN} (a literal refers to a variable Yj or
its negation, ¯Yj). Let Cj be the jth clause in φ, such that Cj _ aj V bj V cj. We define the
following CFG Gφ and string to parse sφ:
</bodyText>
<listItem confidence="0.987120666666667">
1. The terminals of Gφ are the binary digits E _ {0, 1}.
2. We create N nonterminals VYr, r E {1, ... , N} and rules VYr -+ 0 and
VYr -+ 1.
3. We create N nonterminals V ¯Yr, r E {1, ... , N} and rules V ¯Yr -+ 0 and
V ¯Yr -+ 1.
4. We create UYr,1 -+ VYrV ¯Yr and UYr,0 -+ V ¯YrVYr.
5. We create the rule S1 -+ A1. For each j E {2,.. . , m}, we create a rule
Sj -+ Sj−1Aj where Sj is a new nonterminal indexed by φj ° Aji=1 Ci
and Aj is also a new nonterminal indexed by j E {1, ... , m}.
</listItem>
<figure confidence="0.4000665">
505
Computational Linguistics Volume 38, Number 3
</figure>
<listItem confidence="0.594128">
6. Let Cj = aj ∨ bj ∨ cj be clause j in φ. Let Y(aj) be the variable that aj
</listItem>
<bodyText confidence="0.845017">
mentions. Let (y1, y2, y3) be a satisfying assignment for Cj where yk ∈ {0,1}
and is the value of Y(aj), Y(bj), and Y(cj), respectively, for k ∈ {1, 2,3}. For
each such clause-satisfying assignment, we add the rule
</bodyText>
<equation confidence="0.643041">
Aj → UY(aj),y1UY(bj),y2UY(cj),y3
For each Aj, we would have at most seven rules of this form, because one
rule will be logically inconsistent with aj ∨ bj ∨ cj.
</equation>
<bodyText confidence="0.958025666666666">
7. The grammar’s start symbol is Sn.
8. The string to parse is sφ = (10)3m, that is, 3m consecutive occurrences of
the string 10.
A parse of the string sφ using Gφ will be used to get an assignment by setting
Yr = 0 if the rule VYr → 0 or V ¯Yr → 1 is used in the derivation of the parse tree, and 1
otherwise. Notice that at this point we do not exclude “contradictions” that come from
the parse tree, such as VY3 → 0 used in the tree together with VY3 → 1 or V ¯Y3 → 0. To
maintain the restriction on the degree of grammars, we convert Gφ to the binary normal
form described in Section 3.2. The following lemma gives a condition under which the
assignment is consistent (so that contradictions do not occur in the parse tree).
Lemma 4
Let φ be an instance of the 3-SAT problem, and let Gφ be a probabilistic CFG based on
the given grammar with weights θφ. If the (multiplicative) weight of the Viterbi parse
(i.e., the highest scoring parse according to the PCFG) of sφ is 1, then the assignment
extracted from the parse tree is consistent.
</bodyText>
<subsectionHeader confidence="0.772101">
Proof
</subsectionHeader>
<bodyText confidence="0.980888666666667">
Because the probability of the Viterbi parse is 1, all rules of the form {VYr, V ¯Yr} → {0,1}
which appear in the parse tree have probability 1 as well. There are two possible types
of inconsistencies. We show that neither exists in the Viterbi parse:
</bodyText>
<listItem confidence="0.947162777777778">
1. For any r, an appearance of both rules of the form VYr → 0 and VYr → 1
cannot occur because all rules that appear in the Viterbi parse tree have
probability 1.
2. For any r, an appearance of rules of the form VYr → 1 and V ¯Yr → 1 cannot
occur, because whenever we have an appearance of the rule VYr → 0, we
have an adjacent appearance of the rule V ¯Yr → 1 (because we parse
substrings of the form 10), and then we again use the fact that all rules in
the parse tree have probability 1. The case of VYr → 0 and V ¯Yr → 0 is
handled analogously.
</listItem>
<figure confidence="0.56859">
Thus, both possible inconsistencies are ruled out, resulting in a consistent assignment.
■
Figure 3 gives an example of an application of the reduction.
506
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</figure>
<figureCaption confidence="0.904851">
Figure 3
</figureCaption>
<bodyText confidence="0.934338416666667">
An example of a Viterbi parse tree which represents a satisfying assignment for
4) = (Y1 V Y2 V ¯Y4) n (¯Y1 V ¯Y2 V Y3). In 0φ, all rules appearing in the parse tree have
probability 1. The extracted assignment would be Y1 = 0, Y2 = 1, Y3 = 1, Y4 = 0.
Note that there is no usage of two different rules for a single nonterminal.
Lemma 5
Define 4) and Gφ as before. There exists 0φ such that the Viterbi parse of sφ is 1 if and
only if 4) is satisfiable. Moreover, the satisfying assignment is the one extracted from the
parse tree with weight 1 of sφ under 0φ.
Proof
(==&gt;) Assume that there is a satisfying assignment. Each clause Cj = aj V bj V cj is sat-
isfied using a tuple (y1, y2, y3), which assigns values for Y(aj), Y(bj), and Y(cj). This
assignment corresponds to the following rule:
</bodyText>
<equation confidence="0.763933">
Aj -+ UY(aj),y1UY(bj),y2UY(cj),y3
</equation>
<bodyText confidence="0.999319">
Set its probability to 1, and set all other rules of Aj to 0. In addition, for each r, if
Yr = y, set the probabilities of the rules VYr -+ y and V ¯Yr -+ 1 − y to 1 and V ¯Yr -+ y and
VYr -+ 1 − y to 0. The rest of the weights for Sj -+ Sj−1Aj are set to 1. This assignment of
rule probabilities results in a Viterbi parse of weight 1.
(&lt;-=) Assume that the Viterbi parse has probability 1. From Lemma 4, we know that we
can extract a consistent assignment from the Viterbi parse. In addition, for each clause
Cj we have a rule
</bodyText>
<equation confidence="0.76498">
Aj -+ UY(aj),y1UY(bj),y2UY(cj),y3
</equation>
<bodyText confidence="0.9894334">
that is assigned probability 1, for some (y1, y2, y3). One can verify that (y1, y2, y3) are
the values of the assignment for the corresponding variables in clause Cj, and that
they satisfy this clause. This means that each clause is satisfied by the assignment we
extracted. ■
We are now ready to prove the following result.
</bodyText>
<figure confidence="0.7166926">
Theorem 4
Problem 1 is NP-hard when either requiring -y &gt; 0 or when fixing -y = 0.
507
Computational Linguistics Volume 38, Number 3
Proof
</figure>
<bodyText confidence="0.9680625625">
We first describe the reduction for the case of γ = 0. In Problem 1, set γ = 0, α = 1,
G = Gφ, γ = 0, and x1 = sφ. If φ is satisfiable, then the left side of Equation (24) can get
value 0, by setting the rule probabilities according to Lemma 5, hence we would return
1 as the result of running Problem 1.
If φ is unsatisfiable, then we would still get value 0 only if L(G) = {sφ}. If Gφ gen-
erates a single derivation for (10)3m, then we actually do have a satisfying assignment
from Lemma 4. Otherwise (more than a single derivation), the optimal θ would have
to give fractional probabilities to rules of the form VYr -+ {0, 1} (or V ¯Yr -+ {0,1}). In
that case, it is no longer true that (10)3m is the only generated sentence, and this is a
contradiction to getting value 0 for Problem 1.
We next show that Problem 1 is NP-hard even if we require γ &gt; 0. Let γ &lt; 1
20m.
Set α = γ, and the rest of the inputs to Problem 1 the same as before. Assume that φ
is satisfiable. Let θ be the rule probabilities from Equation (5) after being shifted with a
margin of γ. Then, because there is a derivation that uses only rules that have probability
1 − γ, we have
</bodyText>
<equation confidence="0.9898555">
h(x1  |T(θ,γ),Gφ) = � p(x1,z  |T(θ,γ),Gφ)
z
&gt; (1 − γ)10m
&gt; α
</equation>
<bodyText confidence="0.979401142857143">
because the size of the parse tree for (10)3m is at most 10m (using the binarized Gφ)
and assuming α = γ &lt; (1 − γ)10m. This inequality indeed holds whenever γ &lt; 1
20m.
Therefore, we have − log h(x1  |θ) &gt; − log α. Problem 1 would return 0 in this case.
Now, assume that φ is not satisfiable. That means that any parse tree for the string
(10)3m would have to contain two different rules headed by the same non-terminal. This
means that
</bodyText>
<equation confidence="0.980351">
h(x1  |T(θ,γ),Gφ) = � p(x1,z  |T(θ,γ),Gφ)
z
&lt; γ
</equation>
<bodyText confidence="0.925947">
and therefore − log h(x1  |T(θ,γ)) &lt; − log α, and Problem 1 would return 1. ■
</bodyText>
<subsubsectionHeader confidence="0.555083">
6.2.2 An Expectation-Maximization Algorithm. Instead of solving the optimization prob-
</subsubsectionHeader>
<bodyText confidence="0.9995882">
lem implied by Equation (21), we propose a rather simple modification to the
expectation-maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to ap-
proximate the optimal solution—this algorithm finds a local maximum for the max-
imum likelihood problem using proper approximations. The modified algorithm is
given in Algorithm 1.
The modification from the usual expectation-maximization algorithm is done in the
M-step: Instead of using the expected value of the sufficient statistics by counting and
normalizing, we truncate the values by γ. It can be shown that if θ(0) E Θ(γ), then the
likelihood is guaranteed to increase (and hence, the log-loss is guaranteed to decrease)
after each iteration of the algorithm.
</bodyText>
<figure confidence="0.335686">
508
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</figure>
<figureCaption confidence="0.336365">
Algorithm 1: Expectation-Maximization Algorithm with Proper Approximations.
</figureCaption>
<bodyText confidence="0.34163">
Input: grammar G in binary normal form, initial parameters 0(0), C &gt; 0, b &gt; 0,
</bodyText>
<equation confidence="0.797321">
s &gt; 1
Output: learned parameters 0
draw x = (x1,..., xn) from p following Theorem 3; t +- 1 ;
y +- n−s;
</equation>
<bodyText confidence="0.693592">
repeat
] denotes the expected counts of event i in multinomial k
</bodyText>
<equation confidence="0.932342333333333">
// Eθ(t−1) [*k,i(z)  |xj
under the distribution ˜pn(x)p(z  |x, 0(t−1))
Compute for each training example j E {1, ... , n}, for each event i E {1, 2} in each
];
multinomial k E {1, ... , K}: ˆ*j,k,i +- Eθ(t−1) [*k,i(z)  |xj
Set 0k = min{1 − y,max{y, (Ej_1ˆ*j,k,i)/(Ej_1 E?_1ˆ*j,k,i�) }};
t + -t + 1;
until convergence;
return 0(t)
</equation>
<bodyText confidence="0.999891285714286">
The reason for this likelihood increase stems from the fact that the M-step solves
the optimization problem of minimizing the log-loss (with respect to 0 E Θ(y)) when
the posterior calculate at the E-step as the base distribution is used. This means that the
M-step minimizes (in iteration t): ]Er [ − log h(x,z  |0(t))] where the expectation is taken
with respect to the distribution r(x,z) = ˜pn(x)p(z  |x,0(t−1)). With this notion in mind,
the likelihood increase after each iteration follows from principles similar to those
described in Bishop (2006) for the EM algorithm.
</bodyText>
<sectionHeader confidence="0.909445" genericHeader="method">
7. Discussion
</sectionHeader>
<bodyText confidence="0.999885875">
Our framework can be specialized to improve the two main criteria which have a trade-
off: the tightness of the proper approximation and the sample complexity. For example,
we can improve the tightness of our proper approximations by taking a subsequence
of T&apos;n. This will make the sample complexity bound degrade, however, because Kn will
grow faster. Table 2 shows the trade-offs between parameters in our model and the
effectiveness of learning.
We note that the sample complexity bounds that we give in this article give
insight about the asymptotic behavior of grammar estimation, but are not necessarily
</bodyText>
<tableCaption confidence="0.911151">
Table 2
</tableCaption>
<table confidence="0.32682175">
Trade-off between quantities in our learning model and effectiveness of different criteria. Kn is
the constant that satisfies the boundedness property (Theorems 2 and 3) and s is a fixed constant
larger than 1 (Section 4.1).
criterion as Kn increases ... as s increases ...
tightness of proper approximation improves improves
sample complexity bound degrades degrades
509
Computational Linguistics Volume 38, Number 3
</table>
<bodyText confidence="0.994803833333333">
sufficiently tight to be used in practice. It still remains an open problem to obtain
sample complexity bounds which are sufficiently tight in this respect. For a discussion
about the connection of grammar learning in theory and practice, we refer the reader
to Clark and Lappin (2010).
It is also important to note that MLE is not the only option for estimating finite
state probabilistic grammars. There has been some recent advances in learning finite
state models (HMMs and finite state transducers) by using spectral analysis of matrices
which consist of quantities estimated from observations only (Hsu, Kakade, and Zhang
2009; Balle, Quattoni, and Carreras 2011), based on the observable operator models of
Jaeger (1999). These algorithms are not prone to local minima, and converge to the
correct model as the number of samples increases, but require some assumptions about
the underlying model that generates the data.
</bodyText>
<subsectionHeader confidence="0.992456">
7.1 Tsybakov Noise
</subsectionHeader>
<bodyText confidence="0.999889363636364">
In this article, we chose to introduce assumptions about distributions that generate
natural language data. The choice of these assumptions was motivated by observations
about properties shared among treebanks. The main consequence of making these
assumptions is bounding the amount of noise in the distribution (i.e., the amount of
variation in probabilities across labels given a fixed input).
There are other ways to restrict the noise in a distribution. One condition for such
noise restriction, which has received considerable recent attention in the statistical liter-
ature, is the Tsybakov noise condition (Tsybakov 2004; Koltchinskii 2006). Showing that
a distribution satisfies the Tsybakov noise condition enables the use of techniques (e.g.,
from Koltchinskii 2006) for deriving distribution-dependent sample complexity bounds
that depend on the parameters of the noise. It is therefore of interest to see whether
Tsybakov noise holds under the assumptions presented in Section 3.1. We show that
this is not the case, and that Tsybakov noise is too permissive. In fact, we show that p
can be a probabilistic grammar itself (and hence, satisfy the assumptions in Section 3.1),
and still not satisfy the Tsybakov noise conditions.
Tsybakov noise was originally introduced for classification problems (Tsybakov
2004), and was later extended to more general settings, such as the one we are facing in
this article (Koltchinskii 2006). We now explain the definition of Tsybakov noise in our
context.
Let C &gt; 0 and K &gt; 1. We say that a distribution p(x,z) satisfies the (C, K) Tsybakov
noise condition if for any c &gt; 0 and h,g E X such that h,g E {h&apos;  |£p(h&apos;,X) &lt; c}, we
have
</bodyText>
<equation confidence="0.942592666666667">
� �)2] dist(g,h) ° Ep
(loghl &lt; Cc 1/κ (25)
g h
</equation>
<bodyText confidence="0.999888">
This interpretation of Tsybakov noise implies that the diameter of the set of functions
from the concept class that has small excess risk should shrink to 0 at the rate in
Equation (25). Distribution-dependent bounds from Koltchinskii (2006) are monotone
with respect to the diameter of this set of functions, and therefore demonstrating that it
goes to 0 enables sharper derivations of sample complexity bounds.
</bodyText>
<page confidence="0.598848">
510
</page>
<note confidence="0.65841">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<bodyText confidence="0.992886333333333">
We turn now to illustrating that the Tsybakov condition does not hold for proba-
bilistic grammars in most cases. Let G be a probabilistic grammar. Define A = AG(θ) as
a matrix such that
</bodyText>
<equation confidence="0.983555">
E ,i),(k�,i�) [*k,i X ψkI,iI]
(AG(θ))(k
FI*k JFI*kIJI1
</equation>
<bodyText confidence="0.964507428571429">
Theorem 5
Let G be a grammar with K &gt; 2 and degree 2. Assume that p is (G, θ∗) for some θ∗, such
that θ∗1,1 = θ∗2,1 = µ and that c1 &lt; c2. If AG(θ∗) is positive definite, then p does not satisfy
the Tsybakov noise condition for any (C, κ), where C &gt; 0 and κ &gt; 1.
See Appendix C for the proof of Theorem 5.
In Appendix C we show that AG(θ) is positive semi-definite for any choice of θ.
The main intuition behind the proof is that given a probabilistic grammar p, we can
construct an hypothesis h such that the KL divergence between p and h is small, but
dist(p,h) is lower-bounded and is not close to 0.
We conclude that probabilistic grammars, as generative distributions of data, do
not generally satisfy the Tsybakov noise condition. This motivates an alternative choice
of assumptions that could lead to better understanding of rates of convergences and
bounds on the excess risk. Section 3.1 states such assumptions which were also justified
empirically.
</bodyText>
<subsectionHeader confidence="0.996772">
7.2 Comparison to Dirichlet Maximum A Posteriori Solutions
</subsectionHeader>
<bodyText confidence="0.999968">
The transformation T(θ,γ) from Section 4.1 can be thought of as a smoother for the
probabilities θ: It ensures that the probability of each rule is at least γ (and as a result,
the probabilities of all rules cannot exceed 1 − γ). Adding pseudo-counts to frequency
counts is also a common way to smooth probabilities in models based on multinomial
distributions, including probabilistic grammars (Manning and Sch¨utze 1999). These
pseudo-counts can be framed as a maximum a posteriori (MAP) alternative to the
maximum likelihood problem, with the choice of Bayesian prior over the parameters in
the form of a Dirichlet distribution. In comparison to our framework, with (symmetric)
Dirichlet smoothing, instead of truncating the probabilities with a margin γ we would
set the probability of each rule (in the supervised setting) to
</bodyText>
<equation confidence="0.999559666666667">
Enj=1 ˆψj,k,i + α − 1 Ej=
ˆθk,i = n 1 ˆψj,k,1 + Ej=n
1ˆψj,k,2 + 2(α − 1) (26)
</equation>
<bodyText confidence="0.996452444444444">
for i = 1, 2, where ˆψk,i are the counts in the data of event i in multinomial k for Example j.
Dirichlet smoothing can be formulated as the result of adding a symmetric Dirichlet
prior over the parameters θk,i with hyperparameter α. Then Equation (26) is the mode
of the posterior after observing ˆψk,i appearances of event i in multinomial k.
The effect of Dirichlet smoothing becomes weaker as we have more samples,
because the frequency counts ˆψj,k,i become dominant in both the numerator and the
denominator when there are more data. In this sense, the prior’s effect on learning
diminishes as we use more data. A similar effect occurs in our framework: γ = n−s
where n is the number of samples—the more samples we have, the more we trust the
</bodyText>
<page confidence="0.485909">
511
</page>
<note confidence="0.406093">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.9998202">
counts in the data to be reliable. There is a subtle difference, however. With the Dirichlet
MAP solution, the smoothing is less dominant only if the counts of the features are large,
regardless of the number of samples we have. With our framework, smoothing depends
only on the number of samples we have. These two scenarios are related, of course: The
more samples we have, the more likely it is that the counts of the events will grow large.
</bodyText>
<subsectionHeader confidence="0.987031">
7.3 Other Derivations of Sample Complexity Bounds
</subsectionHeader>
<bodyText confidence="0.998999647058824">
In this section, we discuss other possible solutions to the problem of deriving sample
complexity bounds for probabilistic grammars.
7.3.1 Using Talagrand’s Inequality. Our bounds are based on VC theory together with
classical results for empirical processes (Pollard 1984). There have been some recent
developments to the derivation of rates of convergence in statistical learning theory
(Massart 2000; Bartlett, Bousquet, and Mendelson 2005; Koltchinskii 2006), most
prominently through the use of Talagrand’s inequality (Talagrand 1994), which is a
concentration of measure inequality, in the spirit of Lemma 2.
The bounds achieved with Talagrand’s inequality are also distribution-dependent,
and are based on the diameter of the e-minimal set—the set of hypotheses which have
an excess risk smaller than e. We saw in Section 7.1 that the diameter of the e-minimal
set does not follow the Tsybakov noise condition, but it is perhaps possible to find
meaningful bounds for it, in which case we may be able to get tighter bounds using
Talagrand’s inequality. We note that it may be possible to obtain data-dependent bounds
for the diameter of the e-minimal set, following Koltchinskii (2006), by calculating the
diameter of the e-minimal set using ˜pn.
7.3.2 Simpler Bounds for the Supervised Case. As noted in Section 6.1, minimizing empirical
risk with the log-loss leads to a simple frequency count for calculating the estimated
parameters of the grammar. In Corazza and Satta (2006), it has been also noted that to
minimize the non-empirical risk, it is necessary to set the parameters of the grammar to
the normalized expected count of the features.
This means that we can get bounds on the deviation of a certain parameter from
the optimal parameter by applying modifications to rather simple inequalities such
as Hoeffding’s inequality, which determines the probability of the average of a set of
i.i.d. random variables deviating from its mean. The modification would require us
to split the event space into two cases: one in which the count of some features is
larger than some fixed value (which will happen with small probability because of the
bounded expectation of features), and one in which they are all smaller than that fixed
value. Handling these two cases separately is necessary because Hoeffding’s inequality
requires that the count of the rules is bounded.
The bound on the deviation from the mean of the parameters (the true probability)
can potentially lead to a bound on the excess risk in the supervised case. This formula-
tion of the problem would not generalize to the unsupervised case, however, where the
empirical risk minimization does not amount to simple frequency count.
</bodyText>
<subsectionHeader confidence="0.993171">
7.4 Open Problems
</subsectionHeader>
<bodyText confidence="0.9988315">
We conclude the discussion with some directions for further exploration and future
work.
</bodyText>
<note confidence="0.652027">
512
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<bodyText confidence="0.965834529411765">
7.4.1 Sample Complexity Bounds with Semi-Supervised Learning. Our bounds focus on the
supervised case and the unsupervised case. There is a trivial extension to the semi-
supervised case. Consider the objective function to be the sum of the likelihood for the
labeled data together with the marginalized likelihood of the unlabeled data (this sum
could be a weighted sum). Then, use the sample complexity bounds for each summand
to derive a sample complexity bound on this sum.
It would be more interesting to extend our results to frameworks such as the one
described by Balcan and Blum (2010). In that case, our discussion of sample complexity
would attempt to identify how unannotated data can reduce the space of candidate
probabilistic grammars to a smaller set, after which we can use the annotated data
to estimate the final grammar. This reduction of the space is accomplished through a
notion of compatibility, a type of fitness that the learner believes the estimated grammar
should have given the distribution that generates the data. The key challenge in the
case of probabilistic grammars would be to properly define this compatibility notion
such that it fits the log-loss. If this is achieved, then similar machinery to that described
in this paper (with proper approximations) can be followed to derive semi-supervised
sample complexity bounds for probabilistic grammars.
</bodyText>
<subsubsectionHeader confidence="0.950336">
7.4.2 Sharper Bounds for the Pseudo-Dimension of Probabilistic Grammars. The pseudo-
</subsubsectionHeader>
<bodyText confidence="0.999979736842106">
dimension of a probabilistic grammar with the log-loss is bounded by the number of
parameters in the grammar, because the logarithm of a distribution generated by a
probabilistic grammar is a linear function. Typically the set of counts for the feature
vectors of a probabilistic grammar resides in a subspace of a dimension which is smaller
than the full dimension specified by the number of parameters, however. The reason for
this is that there are usually relationships (which are often linear) between the elements
in the feature counts. For example, with HMMs, the total feature count for emissions
should equal the total feature count for transitions. With PCFGs, the total number of
times that nonterminal rules fire equals the total number of times that features with
that nonerminal in the right-hand side fired, again reducing the pseudo-dimension. An
open problem that remains is characterization of the exact value pseudo-dimension for
a given grammar, determined by consideration of various properties of that grammar.
We conjecture, however, that a lower bound on the pseudo-dimension would be rather
close to the full dimension of the grammar (the number of parameters).
It is interesting to note that there has been some work to identify the VC dimension
and pseudo-dimension for certain types of grammars. Bane, Riggle, and Sonderegger
(2010), for example, calculated the VC dimension for constraint-based grammars.
Ishigami and Tani (1993, 1997) computed the VC dimension for finite state automata
with various properties.
</bodyText>
<subsectionHeader confidence="0.981725">
7.5 Conclusion
</subsectionHeader>
<bodyText confidence="0.999952625">
We presented a framework for performing empirical risk minimization for probabilis-
tic grammars, in which sample complexity bounds, for the supervised case and the
unsupervised case, can be derived. Our framework is based on the idea of bounded
approximations used in the past to derive sample complexity bounds for graphical
models.
Our framework required assumptions about the probability distribution that gener-
ates sentences or derivations in the language of the given grammar. These assumptions
were tested using corpora, and found to fit the data well.
</bodyText>
<page confidence="0.680992">
513
</page>
<note confidence="0.482598">
Computational Linguistics Volume 38, Number 3
</note>
<bodyText confidence="0.99949475">
We also discussed algorithms that can be used for minimizing empirical risk in
our framework, given enough samples. We showed that directly trying to minimize
empirical risk in the unsupervised case is NP-hard, and suggested an approximation
based on an expectation-maximization algorithm.
</bodyText>
<sectionHeader confidence="0.960536" genericHeader="method">
Appendix A. Proofs
</sectionHeader>
<bodyText confidence="0.973381">
We include in this appendix proofs for several results in the article.
</bodyText>
<subsectionHeader confidence="0.367482">
Utility Lemma 1
</subsectionHeader>
<bodyText confidence="0.706311">
Let ai E [0, 1], i E {1, ... , N} such that Ei ai = 1. Define b1 = a1, c1 = 1 − a1, bi =
</bodyText>
<equation confidence="0.920975888888889">
 
`aai
a l bi−1
J ( ci−1), and ci = 1 − bi for i &gt; 2. Then ai = cj bi.
j=1
Proof
Proof by induction on i E {1, ... , N}. Clearly, the statement holds for i = 1. Assume it
holds for arbitrary i &lt; N. Then:
       
�ai � i−1� i−1�
ai+1 = ai+1 =    cibi+1
ai cj  bi  ai+1  
ai =  bi
cj
bi
j=1 j=1

cj  bi+1
</equation>
<bodyText confidence="0.932057142857143">
and this completes the proof. ■
Lemma 1
Denote by Z,,n the set Uf∈_T{z  |Cn( f )(z) − f(z) &gt; c}. Denote by A,,n the event “one of
zi E D is in ZE,n.” If Tn properly approximates T, then:
E [E˜pn [gn] − E˜pn [fn ] ] (A.1)
&lt; IE [Ep&amp; [Cn( f∗n )]  |A.,n]� �p(Ae,n) + IE [E˜pn [fn∗]  |Ae,n] p(Ae,n) + ctail(n)
where the expectations are taken with respect to the data set D.
</bodyText>
<equation confidence="0.836434333333333">
Proof
Consider the following:
� � �
E E˜pn � gn� − E˜pn � f∗ n
� � �
= E E˜pn � gn� − E˜pn �Cn( f∗ n )� + E˜pn �Cn( f∗ n )� − E˜pn � f∗ n
[E˜pn [ gn] − E˜pn [Cn ( fn∗)] ] + E [ pn [Cn ( fn∗)] − E˜pn [f∗n ] ]
i
j=1
</equation>
<page confidence="0.713805">
514
</page>
<note confidence="0.6794855">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Note first that E [E˜pn [ gn] − E˜pn [Cn( f∗n )]] ≤ 0, by the definition of gn as the mini-
</note>
<bodyText confidence="0.501907">
mizer of the empirical risk. We next bound E [E˜pn [Cn( f∗n )] − E˜pn [ f∗ ]]. We know from
n
the requirement of proper approximation that we have
</bodyText>
<equation confidence="0.576038">
E [E˜pn [Cn ( f∗n )] − Ep˜n [ fn ] ] = E [E˜pn [Cn ( fn∗ )] − pn [ fn ]  |A e,n] p(A
+ E [ pn [Cn( f∗n )] − E˜pn [ fn ]  |¬AE n] (1 − p(Ae,n))
</equation>
<bodyText confidence="0.5976585">
≤ |E [E˜pn [Cn( f∗n )]  |Ae,n] |p(Ae,n) + |E [E˜pn [ fn ]  |Ae,n] |p(Ae,n) + etail(n)
and that equals the right side of Equation (Appendix A.1). ■
</bodyText>
<equation confidence="0.89795125">
Proposition 2
Let p ∈ T(α, L, r, q, B, G) and let T&apos;m be as defined earlier. There exists a constant β =
β(L, q, p, N) &gt; 0 such that T&apos;m has the boundedness property with Km = sN log3 m and
Ebound(m) = m−β logm.
Proof
Let f ∈ T&apos;m. Let Z(m) = {z  ||z |≤ log2 m}. Then, for all z ∈ Z(m) we have |f(z)|
=
− Ei,k ψ(k, i) log θk,i ≤ Ei,k ψ(k, i)(p log m) ≤ sN log3 m = Km, where the first inequality
</equation>
<bodyText confidence="0.802912">
follows from f ∈ T&apos;m (θk,i ≥ m−s) and the second from |z |≤ log2 m. In addition, from the
requirements on p we have
</bodyText>
<equation confidence="0.990394714285714">
� �
E [|f  |× I {|f  |≥ Km} ] ≤ (sN log3 m) × 1: LA(k)rkk ≤ (κ log3 m) × (glog2 ml
k&gt;log2 m \ J
for κ = sNL
(1 − q)2 . Finally, for β(L, q, p, N) ° log κ + 1 + log 1q = β &gt; 0 and if m &gt; 1 then
( ) (qlog2 m)
κlog3 m ≤ m−βlogm. m
</equation>
<bodyText confidence="0.654133">
Utility Lemma 4
(From [Dasgupta 1997].) Let a ∈ [0, 1] and let b = a if a ∈ [γ,1 − γ], b = γ if a ≤ γ,
and b = 1 − γ if a ≥ 1 − γ. Then for any c ≤ 1/2 such that γ ≤ c/(1 + c) we have
</bodyText>
<equation confidence="0.8557846">
log a/b ≤ c.
Proposition 3
Let p ∈ T(α, L, r, q, B, G) and let T&apos;m as defined earlier. There exists an M such that for any
m &gt; M we have
� �
�U
p {z  |Cm( f )(z) − f(z) ≥ Etail(m)} �≤ Etail(m)
f∈F
N log2 m
for �tail(m) = ms − 1 and Cm( f ) = T( f,m−s).
</equation>
<page confidence="0.577637">
515
</page>
<note confidence="0.363954">
Computational Linguistics Volume 38, Number 3
Proof
Let Z(m) be the set of derivations of size bigger than log2 m. Let f ∈ T. Define f&apos; =
T(f, m−s). For any z ∈/ Z(m) we have that
</note>
<equation confidence="0.993415833333333">
K
f&apos;(z) − f(z) = − E (φk,1(z) log θk,1 + φk,2(z) log θk,2 − φk,1(z) log θ&apos; k,1 − φk,1(z) log θ&apos; k,2 )
k=1
K log2 m(max{0,log(θ&apos;k,1/θk,1)} + max{0,log(θ&apos; (A.2)
≤ E k,2/θk,2)})
k=1
</equation>
<bodyText confidence="0.9235805">
Without loss of generality, assume �tail(n)/N log2 m ≤ 1/2. Let γ = �tail(m)/N log2 m = 1 + ctail(m)/N log2 m
1/ms. From Utility Lemma 4 we have that log(θ&apos;k,i/θk,i) ≤ ctail(m)/N logm. Plug this
into Equation A.2 (N = 2K) to get that for all z ∈/ Z(m) we have f&apos;(z) − f(z) ≤ ctail(m).
It remains to show that the measure p(Z(m)) ≤ etail(m). Note that Ezcz(m) p(z) ≤
</bodyText>
<equation confidence="0.946024666666667">
Ek&gt;log2 m LA(k)rk ≤ L Ek&gt;log2 m qk = Lqlog2 m/(1 − q) &lt; etail(m) for m &gt; M where M is
fixed. ■
Proposition 7
</equation>
<bodyText confidence="0.907827">
There exists a β&apos;(L, p, q, N) &gt; 0 such that T&apos;m has the boundedness property with Km =
sNlog3 m and ebound(m) = m−β&apos; logm.
Proof
From the requirement of p, we know that for any x we have a z such that yield(z) = x
and |z |≤ α|x|. Therefore, if we let X(m) = {x  ||x |≤ log2 m/α}, then we have for any
f ∈ T&apos;m and x ∈ X(m) that f (x) ≤ sN log3 m = Km (similarly to the proof of Proposition 2).
Denote by f1(x, z) the function in Tm such that f (x) = − log Ez exp(−f1(x, z)).
In addition, from the requirements on p and the definition of Km we have
</bodyText>
<equation confidence="0.998238">
[ ] � p(x)f(x)I { f ≥ Km}
E  |f |× I { |f |≥ Km} =
x
�= p(x)f(x)
x:jxj&gt;log2 m/α
≤ � p(x)f1(x,z(x))
x:jxj&gt;log2 m/α
</equation>
<page confidence="0.469585">
516
</page>
<note confidence="0.501929">
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
</note>
<bodyText confidence="0.582019">
where z(x) is some derivation for x. We have
</bodyText>
<equation confidence="0.9966127">
E p(x)f1(x,z(x)) ≤ � � p(x, z)f1(x, z(x))
x:jxj&gt;log2 m/α x:jxj&gt;log2 m/α zEDx(G)
�≤ sN log m E p(x,z)|z(x)|
x:jxj&gt;log2 m/α z
�≤ sN log m A(k)rkk
k&gt;log2 m
�≤ sN log m qkk ≤ K log mqlog2m
k&gt;log2 m
for some constant K &gt; 0. Finally, for some β&apos;(L, p, q, N) = β&apos; &gt; 0 and some constant M,
�qlog2 m~
</equation>
<bodyText confidence="0.58641">
if m &gt; M then K log m ≤ m−β, logm.
</bodyText>
<subsectionHeader confidence="0.412207">
Utility Lemma 2
</subsectionHeader>
<bodyText confidence="0.881792">
For ai, bi ≥ 0, if − log Ei ai + log Ei bi ≥ c then there exists an i such that − log ai +
log bi ≥ c.
Proof
Assume − log ai + log bi &lt; c for all i. Then, bi/ai &lt; eE, therefore Ei bi/ Ei ai &lt; eE, there-
fore − log Ei ai + log Ei bi &lt; c which is a contradiction to − log Ei ai + log Ei bi ≥ c.
■
The next lemma is the main concentation of measure result that we use. Its proof
requires some simple modification to the proof given for Theorem 24 in Pollard (1984,
pages 30–31).
Lemma 2
Let Fn be a permissible class of functions such that for every f ∈ Fn we have E[|f  |×
I {|f  |≤ Kn}] ≤ cbound(n). Let Ftruncated,n = { f × I { f ≤ Kn}  |f ∈ Fm}, that is, the set of
functions from Fn after being truncated by Kn. Then for c &gt; 0 we have
~ p sup|E˜pn [f] − Ep [f] |&gt; 2c ≤ 8N(c/8,Ftruncated,n)exp(−1
</bodyText>
<figure confidence="0.877855866666667">
128nc2/9) + cbound(n)/c
fE-Tn
provided n ≥ K2n/4c2 and cbound(n) &lt; c.
Proof
First note that
sup |E˜pn � f~ − Ep � f~  |≤ sup |E˜pn � fI{|f |≤ Kn}� − Ep � fI{|f |≤ Kn}� |
fE-Tn fE-Tn
+ sup E˜pn [|f |I{|f  |≤ Kn}] + sup Ep [|f |I{|f  |≤ Kn}]
fE-Tn f E-Tn
517
Computational Linguistics Volume 38, Number 3
We have supf∈-Tn ]Ep L|f |&apos;{|f  |≤ Kn}] ≤ ebound(n) &lt; c, and also, from Markov in-
equality, we have
P(sup &amp;quot;&amp;quot;˜pn L |f |&apos; {|f  |≤ Kn}] &gt; 0 ≤ Ebound(n)/c
f ∈-Tn
</figure>
<bodyText confidence="0.842438333333333">
At this point, we can follow the proof of Theorem 24 in Pollard (1984), and its
extension on pages 30–31 to get Lemma 2, using the shifted set of functions Ftruncated,n.
■
</bodyText>
<sectionHeader confidence="0.756201" genericHeader="method">
Appendix B. Minimizing Log-Loss for Probabilistic Grammars
</sectionHeader>
<bodyText confidence="0.9884165">
Central to our algorithms for minimizing the log-loss (both in the supervised case and
the unsupervised case) is a convex optimization problem of the form
</bodyText>
<equation confidence="0.9956198">
ck,1 log θk,1 + ck,2 log θk,2
such that ∀k ∈ {1, ... , K} :
θk,1 + θk,2 = 1
γ ≤ θk,1 ≤ 1 − γ
γ ≤ θk,2 ≤ 1 − γ
</equation>
<bodyText confidence="0.9998995">
for constants ck,i which depend on ˜pn or some other intermediate distribution in the
case of the expectation-maximization algorithm and γ which is a margin determined
by the number of samples. This minimization problem can be decomposed into several
optimization problems, one for each k, each having the following form:
</bodyText>
<equation confidence="0.9981428">
maxc1β1 + c2β2 (B.1)
β
such that exp(β1) + exp(β2) = 1 (B.2)
γ ≤ β1 ≤ 1 − γ (B.3)
γ ≤ β2 ≤ 1 − γ (B.4)
</equation>
<bodyText confidence="0.9461118">
where ci ≥ 0 and 1/2 &gt; γ ≥ 0. Ignore for a moment the constraints γ ≤ βi ≤ 1 − γ. In
that case, this can be thought of as a regular maximum likelihood estimation problem,
so βi = ci/(c1 + c2). We give a derivation of this result in this simple case for completion.
We use Lagranian multipliers to solve this problem. Let F(β1, β2) = c1β1 + c2β2. Define
the Lagrangian:
</bodyText>
<figure confidence="0.891409866666667">
K
E
k=1
min
θ
g(λ) = inf L(λ, β)
β
= inf
c1β1 + c2β2 + λ(exp(β1) + exp(β2) − 1)
β
518
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
Taking the derivative of the term we minimize in the Lagrangian, we have
∂L = ci + λexp(βi)
∂βi
</figure>
<bodyText confidence="0.915376">
Setting the derivatives to 0 for minimization, we have
</bodyText>
<equation confidence="0.985492">
g(λ) = c1 log(−c1/λ) + c2 log(−c2/λ) + λ(−c1/λ − c2/λ − 1) (B.5)
</equation>
<bodyText confidence="0.999294">
g(λ) is the objective function of the dual problem of Equation (B.1)–Equation (B.2).
We would like to minimize Equation (B.5) with respect to λ. The derivative of g(λ) is
</bodyText>
<equation confidence="0.9534405">
∂g
∂λ = −c1/λ − c2/λ − 1
</equation>
<bodyText confidence="0.9947394">
hence when equating the derivative of g(λ) to 0, we get λ = −(c1 + c2), and therefore
the solution is β∗i = log (ci/(c1 + c2)). We need to verify that the solution to the dual
problem indeed gets the optimal value for the primal. Because the primal problem is
convex, it is sufficient to verify that the Karush-Kuhn-Tucker (KKT) conditions hold
(Boyd and Vandenberghe 2004). Indeed, we have
</bodyText>
<equation confidence="0.999731">
∂βi (β∗) + λ ∂h
∂F ∂βi (β∗) = ci − (c1 + c2) xci
c1 + c2
</equation>
<bodyText confidence="0.9689779">
= 0
where h(β) ° exp(β) + exp(β) − 1 stands for the equality constraint. The rest of the
KKT conditions trivially hold, therefore β∗ is the optimal solution for Equations (B.1)–
(B.2).
Note that if 1 − γ &lt; ci/(c1 + c2) &lt; γ, then this is the solution even when again
adding the constraints in Equation (B.3) and (B.4). When c1/(c1 + c2) &lt; γ, then the
solution is β∗1 = γ and β∗2 = 1 − γ. Similarly, when c2/(c1 + c2) &lt; γ then the solution is
β∗2 = γ and β∗1 = 1 − γ. We describe why this is true for the first case. The second case
follows very similarly. Assume c1/(c1 + c2) &lt; γ. We want to show that for any choice of
β E [0, 1] such that β &gt; γ we have
</bodyText>
<figure confidence="0.669306782608696">
c1 logγ + c2 log(1 − γ) &gt; c1 log β + c2 log(1 − β)
Divide both sides of the inequality by c1 + c2 and we get that we need to show that
c1 c1 + c2 log(γ/β) + c1 + c2 log I 1 − R &gt; 0
Because we have β &gt; γ, and we also have c1/(c1 + c2) &lt; γ, it is sufficient to show that
γ log(γ/β) + (1 − γ) logI 1 − R I &gt; 0 (B.6)
Equation (B.6) is precisely the definition of the KL divergence between the distribu-
tion of a coin with probability γ of heads and the distribution of a coin with probability β
519
Computational Linguistics Volume 38, Number 3
of heads, and therefore the right side in Equation (B.6) is positive, and we get what
we need.
Appendix C. Counterexample to Tsybakov Noise (Proofs)
Lemma 6
A = AG(0) is positive semi-definite for any probabilistic grammar (G, 0).
Proof
Let dk,i be a collection of constants. Define the random variable:
We have that � dk,i
R(z) = �*k,i�ψk,i(z)
i,k E
)E [R2] = E � A(k,i),(k,,i,)dk,idki,ii
i,il k,kl
which is always larger or equal to 0. Therefore, A is positive semi-definite. ■
Lemma 7
</figure>
<bodyText confidence="0.779028">
Let 0 &lt; µ &lt; 1/2, c1, c2 ≥ 0. Let K, C &gt; 0. Also, assume that c1 ≤ c2. For any c &gt; 0, define:
</bodyText>
<equation confidence="0.898601">
~Cc1/κ + c/2 11
c1 ) J = α1 µ
1
b = µ (exp (−Cc/κc2+ c/2)1 = α2µ
t(c) = c1 (1 − a) + c2 (1 − b) − (c1 + c2) exp(c/2)
</equation>
<bodyText confidence="0.667486666666667">
Then, for small enough c, we have t(c) ≤ 0.
Proof
We have that t(c) ≤ 0 if
</bodyText>
<equation confidence="0.955619125">
ac2 + bc1 ≥ − (c1 + c2)(1 − a)(1 − b) exp(c/2) + c1 + c2
1 − µ
= (c1 + c2) 1 − (1 − a)(1 − b) (C.1)
(1 − µ) exp(−c/2))
First, show that
(1 − a)(1 − b) &gt; 1 − µ (C.2)
(1 − µ) exp(−c/2) —
�a = µ exp
</equation>
<figure confidence="0.516290583333333">
520
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
which happens if (after substituting a = α1µ, b = α2µ)
µ &lt; (α1 + α2 − 2)/(1 − α1α2)
Note we have α1α2 &gt; 1 because c1 &lt; c2. In addition, we have α1 + α2 − 2 &gt; 0 for small
enough c (can be shown by taking the derivative, with respect to c of α1 + α2 − 2, which
is always positive for small enough c, and in addition, noticing that the value of α1 +
α2 − 2 is 0 when c = 0.) Therefore, Equation (C.2) is true.
Substituting Equation (C.2) in Equation (C.1), we have that t(c) &lt; 0 if
ac2 + bc1 &gt; (c1 + c2)µ
which is equivalent to
c2α1 + c1α2 &gt; c1 + c2 (C.3)
</figure>
<bodyText confidence="0.912832625">
Taking again the derivative of the left side of Equation (C.3), we have that it is an
increasing function of c (if c1 &lt; c2), and in addition at c = 0 it obtains the value c1 + c2.
Therefore, Equation (C.3) holds, and therefore t(c) &lt; 0 for small enough c. ■
Theorem 5
Let G be a grammar with K &gt; 2 and degree 2. Assume that p is (G, 0∗) for some 0∗, such
that 0∗1,1 = 0∗2,1 = µ and that c1 &lt; c2. If AG(0∗) is positive definite, then p does not satisfy
the Tsybakov noise condition for any (C, K), where C &gt; 0 and K &gt; 1.
Proof
</bodyText>
<construct confidence="0.689142">
Define A to be the eigenvalue of AG(0) with the smallest value (A is positive). Also,
define v(0) to be a vector indexed by k, i such that
</construct>
<equation confidence="0.968546625">
0
vk,i(0) = E [*k,i] log k,i
0k,i �
Simple algebra shows that for any h E X(G) (and the fact that p E X(G)), we have
£p(h) = DKL(pllh) = (Ep [*k,1] log eks + Ep [*k,1] log \1−0∗
1 − 0k,1
k,1/ /
k-1
</equation>
<bodyText confidence="0.77419575">
For a C &gt; 0 and K &gt; 1, define α = Cc1/&amp;quot;. Let c &lt; α. First, we construct an h such
that DKL(pllh) &lt; c + c/2 but dist(p,h) &gt; Cc1/&amp;quot; as c -+ 0. The construction follows.
Parametrize h by 0 such that 0 is identical to 0∗ except for k = 1, 2, in which case we
have
</bodyText>
<figure confidence="0.828078333333333">
α + c/2 α + c/2
01,1 = 01,1 exp c1 = µ exp c1 (C.4)
02,1 = 02,1 (exp (−α c2 c/2 = µ (exp (−α c2 c/2 (C.5)
521
Computational Linguistics Volume 38, Number 3
Note that µ ≤ θ1,1 ≤ 1/2 and θ2,1 &lt; µ. Then, we have that
</figure>
<table confidence="0.906052272727273">
DKL(pllh) = � ��
~K
k,1
Ep �ψk,1� log θ∗
θk,1 + Ep
k,1 �ψk,1� log ~1 − θ∗
1 − θk,1
k-1
1 − θ∗ 1 − θ∗k,2
= � + c1 log 1 − θ1,1 + c2 log 1 − θ2,1
k,1
</table>
<equation confidence="0.774396">
1 − µ 1 − µ
= � + c1 log 1 − θ1,1 + c2 log 1 − θ2,1
We also have
1 − µ 1 − µ (C.6)
c1 log 1 − θ1,1 + c2 log 1 − θ2,1 ≤ 0
1 − µ 1 − µ
1 − θ1,1 + c2 × 1 − θ2,1 ≤ c1 + c2 (C.7)
(This can be shown by dividing Equation [C.6] by c1 + c2 and then using the concavity of
the logarithm function.) From Lemma 7, we have that Equation (C.7) holds. Therefore,
DKL(pllh) ≤ 2c
Now, consider the following, which can be shown through algebraic manipulation:
,� ~2- � � �
� � k,i
log p log θ∗ log θ∗ k�,i�
E�ψk,i × ψk�,i�� �
dist(p,h) = E =
h θk,i θk�,i�
k,k� i,i�
Then, additional algebraic simplification shows that
,� ~2-
log p
E = v(θ)Av(θ)�
h
A fact from linear algebra states that
v(θ)Av(θ)T ≥ λ||v(θ)||22
</equation>
<bodyText confidence="0.944442">
where λ is the smallest eigenvalue in A. From the construction of θ and Equation (C.4)–
(C.5), we have that ||v(θ)||2 2 &gt; α2. Therefore,
</bodyText>
<figure confidence="0.947531071428571">
,� ~2-
log p
E ≥ λα2
h
√
which means dist(p,h) ≥ λC�1/κ. Therefore, p does not satisfy the Tsybakov noise
condition with parameters (D, κ) for any D &gt; 0. ■
if
c1 ×
522
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
ERM
Grammars
Proper Approximations
</figure>
<sectionHeader confidence="0.630568" genericHeader="conclusions">
Appendix D. Notation
</sectionHeader>
<tableCaption confidence="0.991376">
Table D.1 gives a table of notation for symbols used throughout this article.
Table 1
Table of notation symbols used in this article.
</tableCaption>
<table confidence="0.953794476190476">
Symbol Description 1st Mention
X Instance space (natural language sentences) Sec. 2
Z Output space (grammar derivations) Sec. 2
p Distribution generating the data Sec. 2
Q Concept space, a family of distributions Sec. 2
q An estimated distribution Sec. 2
qopt Risk minimizer Eq. 1
n Number of available samples Sec. 2
˜pn Empirical distribution Sec. 2
q∗ Empirical risk minimizer Eq. 2
£p(q; Q) Excess risk Eq. 4
Rn(Q) Empirical process for the log-loss Eq. 5
G Grammar (for example, CFG rules) Sec. 3
θ Probabilistic grammar parameters Sec. 3
K Number of multinomials in the probabilistic grammar Eq. 11
Nk Size of the kth multinomial of the probabilistic grammar Eq. 11
�K
N k=1 Nk Sec. 3
x Sentence in the language of the grammar Sec. 3
z Derivation in the grammar Sec. 3
ψk,i(x,z) Count of the ith event firing in the kth multinomial in x and z Eq. 11
ΘG Parameter space for a given probabilistic grammar G Eq. 11
θ Parameters for a probabilistic grammar Eq. 11
deg(G) The degree of G, maxk Nk Sec. 3
Dx(G) The set of derivations for string x Sec. 3
x, x(G) Concept space, a set of probabilistic grammars Sec. 3
T, T(G) Negated log-concept space, {−log h  |h E x(G)} Sec. 3
L Constant determining distributional assumption Sec. 3.1
q Constant determining distributional assumption Sec. 3.1
r Constant determining distributional assumption Sec. 3.1
Tn Element n in a proper approximation (contained in T) Sec. 4
ctail(n) Convergence rate for the boundedness property Sec. 4
Ebound(n) Convergence rate for the tightness property Sec. 4
Cn( f ) A map for f E T to f~ E Tn Sec. 4
T(θ, γ) Parameters θ with shifted probabilities Sec. 4.1
T( f, γ) f E T with shifted probabilities Sec. 4.1
ΘG(γ) Set of parameters {T(θ,γ)  |θ E ΘG} for a given G Sec. 4.1
s A constant larger than 1 on which boundedness property
depends
β(L, q, p, N) A constant on which sample complexity depends for the su-
pervised case
T~ Element n in a proper approximation (contained in T) Sec. 4
</table>
<figure confidence="0.862122272727273">
n
Cn( f ) A map for f E T to f~ E Tn Sec. 4
c&apos; (n) Convergence rate for the soundness property Sec. 4
C~boundail (n) Convergence rate for the tightness property Sec. 4
β&apos;(L, q, p, N) A constant on which sample complexity depends for the
unsupervised case
Sec. 4.1
Prop. 2
Sec. 5.3
523
Computational Linguistics Volume 38, Number 3
</figure>
<sectionHeader confidence="0.984493" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.986135166666667">
The authors thank the anonymous reviewers
for their comments and Avrim Blum, Steve
Hanneke, Mark Johnson, John Lafferty, Dan
Roth, and Eric Xing for useful conversations.
This research was supported by National
Science Foundation grant IIS-0915187.
</bodyText>
<sectionHeader confidence="0.991269" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992898884462151">
Abe, N., J. Takeuchi, and M. Warmuth.
1991. Polynomial learnability
of probabilistic concepts with
respect to the Kullback-Leiber
divergence. In Proceedings of the
Conference on Learning Theory,
pages 277–289.
Abe, N. and M. Warmuth. 1992. On
the computational complexity of
approximating distributions by
probabilistic automata. Machine
Learning, 2:205–260.
Angluin, D. 1987. Learning regular sets from
queries and counterexamples. Information
and Computation, 75:87–106.
Anthony, M. and P. L. Bartlett. 1999.
Neural Network Learning: Theoretical
Foundations. Cambridge
University Press.
Balcan, M. and A. Blum. 2010.
A discriminative model for semi-
supervised learning. Journal of the
Association for Computing Machinery,
57(3):1–46.
Balle, B., A. Quattoni, and X. Carreras.
2011. A spectral learning algorithm for
finite state transducers. In Proceedings
of the European Conference on Machine
Learning/the Principles and Practice of
Knowledge Discovery in Databases,
pages 156–171.
Bane, M., J. Riggle, and M. Sonderegger.
2010. The VC dimension of
constraint-based grammars.
Lingua, 120(5):1194–1208.
Bartlett, P., O. Bousquet, and S. Mendelson.
2005. Local Rademacher complexities.
Annals of Statistics, 33(4):1497–1537.
Bishop, C. M. 2006. Pattern Recognition and
Machine Learning. Springer, Berlin.
Boyd, S. and L. Vandenberghe. 2004.
Convex Optimization. Cambridge
University Press.
Carrasco, R. 1997. Accurate computation
of the relative entropy between
stochastic regular grammars.
Theoretical Informatics and Applications,
31(5):437–444.
Carroll, G. and E. Charniak. 1992. Two
experiments on learning probabilistic
dependency grammars from corpora.
Technical report, Brown University,
Providence, RI.
Charniak, E. 1993. Statistical Language
Learning. MIT Press, Cambridge, MA.
Charniak, E. and M. Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings of
the Association for Computational Linguistics,
pages 173–180.
Chi, Z. 1999. Statistical properties of
probabilistic context-free grammars.
Computational Linguistics, 25(1):131–160.
Clark, A., R. Eyraud, and A. Habrard. 2008.
A polynomial algorithm for the inference
of context free languages. In Proceedings of
the International Colloquium on Grammatical
Inference, pages 29–42.
Clark, A. and S. Lappin. 2010. Unsupervised
learning and grammar induction.
In Alexander Clark, Chris Fox, and
Shalom Lappin, editors, The Handbook
of Computational Linguistics and Natural
Language Processing. Wiley-Blackwell,
London, pages 197–220.
Clark, A. and F. Thollard. 2004.
PAC-learnability of probabilistic
deterministic finite state automata.
Journal of Machine Learning Research,
5:473–497.
Cohen, S. B. and N. A. Smith. 2010a.
Covariance in unsupervised learning of
probabilistic grammars. Journal of Machine
Learning Research, 11:3017–3051.
Cohen, S. B. and N. A. Smith. 2010b.
Empirical risk minimization with
approximations of probabilistic
grammars. In Proceedings of the
Advances in Neural Information
Processing Systems, pages 424–432.
Cohen, S. B. and N. A. Smith. 2010c. Viterbi
training for PCFGs: Hardness results and
competitiveness of uniform initialization.
In Proceedings of the Association for
Computational Linguistics, pages 1502–1511.
Collins, M. 2003. Head-driven statistical
models for natural language processing.
Computational Linguistics, 29:589–637.
Collins, M. 2004. Parameter estimation for
statistical parsing models: Theory and
practice of distribution-free methods.
In H. Bunt, J. Carroll, and G. Satta, Text,
Speech and Language Technology (New
Developments in Parsing Technology).
Kluwer, Dordrecht, pages 19–55.
Corazza, A. and G. Satta. 2006. Cross-entropy
and estimation of probabilistic context-free
524
Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars
grammars. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics, pages 335–342.
Cover, T. M. and J. A. Thomas. 1991.
Elements of Information Theory. Wiley,
London.
Dasgupta, S. 1997. The sample complexity
of learning fixed-structure bayesian
networks. Machine Learning,
29(2–3):165–180.
de la Higuera, C. 2005. A bibliographical
study of grammatical inference. Pattern
Recognition, 38:1332–1348.
Dempster, A., N. Laird, and D. Rubin. 1977.
Maximum likelihood estimation from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B,
39:1–38.
Gildea, D. 2010. Optimal parsing strategies
for linear context-free rewriting systems.
In Proceedings of the North American Chapter
of the Association for Computational
Linguistics, pages 769–776.
G´omez-Rodriguez, C. and G. Satta.
2009. An optimal-time binarization
algorithm for linear context-free
rewriting systems with fan-out two.
In Proceedings of the Association for
Computational Linguistics-International
Joint Conference on Natural Language
Processing, pages 985–993.
Grenander, U. 1981. Abstract Inference. Wiley,
New York.
Haussler, D. 1992. Decision-theoretic
generalizations of the PAC model
for neural net and other learning
applications. Information and
Computation, 100:78–150.
Hsu, D., S. M. Kakade, and T. Zhang.
2009. A spectral algorithm for
learning hidden Markov models.
In Proceedings of the Conference on
Learning Theory.
Ishigami, Y. and S. Tani. 1993. The
VC-dimensions of finite automata
with n states. In Proceedings of
Algorithmic Learning Theory,
pages 328–341.
Ishigami, Y. and S. Tani. 1997.
VC-dimensions of finite automata and
commutative finite automata with k letters
and n states. Applied Mathematics,
74(3):229–240.
Jaeger, H. 1999. Observable operator models
for discrete stochastic time series. Neural
Computation, 12:1371–1398.
Kearns, M. and L. Valiant. 1989.
Cryptographic limitations on learning
Boolean formulae and finite automata.
In Proceedings of the 21st Association
for Computing Machinery Symposium
on the Theory of Computing,
pages 433–444.
Kearns, M. J. and U. V. Vazirani. 1994.
An Introduction to Computational
Learning Theory. MIT Press,
Cambridge, MA.
Klein, D. and C. D. Manning. 2004.
Corpus-based induction of syntactic
structure: Models of dependency and
constituency. In Proceedings of the
Association for Computational Linguistics,
pages 478–487.
Koltchinskii, V. 2006. Local Rademacher
complexities and oracle inequalities
in risk minimization. The Annals of
Statistics, 34(6):2593–2656.
Leermakers, R. 1989. How to cover a
grammar. In Proceedings of the Association
for Computational Linguistics,
pages 135–142.
Manning, C. D. and H. Sch¨utze. 1999.
Foundations of Statistical Natural
Language Processing. MIT Press,
Cambridge, MA.
Massart, P. 2000. Some applications of
concentration inequalities to statistics.
Annales de la Facult´e des Sciences de
Toulouse, IX(2):245–303.
Nijholt, A. 1980. Context-Free Grammars:
Covers, Normal Forms, and Parsing
(volume 93 of Lecture Notes in
Computer Science). Springer-Verlag,
Berlin.
Palmer, N. and P. W. Goldberg. 2007.
PAC-learnability of probabilistic
deterministic finite state automata
in terms of variation distance.
In Proceedings of Algorithmic Learning
Theory, pages 157–170.
Pereira, F. C. N. and Y. Schabes. 1992.
Inside-outside reestimation from partially
bracketed corpora. In Proceedings of the
Association for Computational Linguistics,
pages 128–135.
Pitt, L. 1989. Inductive inference, DFAs, and
computational complexity. Analogical and
Inductive Inference, 397:18–44.
Pollard, D. 1984. Convergence of Stochastic
Processes. Springer-Verlag, New York.
Ron, D. 1995. Automata Learning and Its
Applications. Ph.D. thesis, Hebrew
University of Jerusalem.
Ron, D., Y. Singer, and N. Tishby.1998.
On the learnability and usage of acyclic
probabilistic finite automata. Journal
of Computer and System Sciences,
56(2):133–152.
525
Computational Linguistics Volume 38, Number 3
Shalev-Shwartz, S., O. Shamir, K. Sridharan,
and N. Srebro. 2009. Learnability and
stability in the general learning setting.
In Proceedings of the Conference on
Learning Theory.
Sipser, M. 2006. Introduction to the Theory of
Computation, Second Edition. Thomson
Course Technology, Boston, MA.
Talagrand, M. 1994. Sharper bounds for
Gaussian and empirical processes.
Annals of Probability, 22:28–76.
Terwijn, S. A. 2002. On the learnability of
hidden Markov models. In P. Adriaans,
H. Fernow, &amp; M. van Zaane. Grammatical
Inference: Algorithms and Applications
(Lecture Notes in Computer Science).
Springer, Berlin, pages 344–348.
Tsybakov, A. 2004. Optimal aggregation of
classifiers in statistical learning. The Annals
of Statistics, 32(1):135–166.
Vapnik, V. N. 1998. Statistical Learning Theory.
Wiley-Interscience, New York.
</reference>
<page confidence="0.937633">
526
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.213492">
<title confidence="0.730856333333333">Empirical Risk Minimization for Probabilistic Grammars: Sample Complexity and Hardness of Learning</title>
<affiliation confidence="0.86409225">B. Columbia University A. Carnegie Mellon University</affiliation>
<abstract confidence="0.987069181818182">Probabilistic grammars are generative statistical models that are useful for compositional and sequential structures. They are used ubiquitously in computational linguistics. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of probabilistic grammars using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting. By making assumptions about the underlying distribution that are appropriate for natural language scenarios, we are able to derive distribution-dependent sample complexity bounds for probabilistic grammars. We also give simple algorithms for carrying out empirical risk minimization using this framework in both the supervised and unsupervised settings. In the unsupervised case, we show that the problem of minimizing empirical risk is NP-hard. We therefore suggest an approximate algorithm, similar to expectation-maximization, to minimize the empirical risk.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>J Takeuchi</author>
<author>M Warmuth</author>
</authors>
<title>Polynomial learnability of probabilistic concepts with respect to the Kullback-Leiber divergence.</title>
<date>1991</date>
<booktitle>In Proceedings of the Conference on Learning Theory,</booktitle>
<pages>277--289</pages>
<marker>Abe, Takeuchi, Warmuth, 1991</marker>
<rawString>Abe, N., J. Takeuchi, and M. Warmuth. 1991. Polynomial learnability of probabilistic concepts with respect to the Kullback-Leiber divergence. In Proceedings of the Conference on Learning Theory, pages 277–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>M Warmuth</author>
</authors>
<title>On the computational complexity of approximating distributions by probabilistic automata.</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--205</pages>
<contexts>
<context position="7205" citStr="Abe and Warmuth (1992)" startWordPosition="1050" endWordPosition="1053">d Goldberg 2007) also gives treatment to probabilistic automata with an error measure which is more suitable for the probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance. 480 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars These also focus on learning the structure of finite state machines. As mentioned earlier, in our setting we assume that the grammar is fixed, and that our goal is to estimate its parameters. We note an important connection to an earlier study about the learnability of probabilistic automata and hidden Markov models by Abe and Warmuth (1992). In that study, the authors provided positive results for the sample complexity for learning probabilistic automata—they showed that a polynomial sample is sufficient for MLE. We demonstrate positive results for the more general class of probabilistic grammars which goes beyond probabilistic automata. Abe and Warmuth also showed that the problem of finding or even approximating the maximum likelihood solution for a twostate probabilistic automaton with an alphabet of an arbitrary size is hard. Even though these results extend to probabilistic grammars to some extent, we provide a novel proof </context>
</contexts>
<marker>Abe, Warmuth, 1992</marker>
<rawString>Abe, N. and M. Warmuth. 1992. On the computational complexity of approximating distributions by probabilistic automata. Machine Learning, 2:205–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Learning regular sets from queries and counterexamples.</title>
<date>1987</date>
<journal>Information and Computation,</journal>
<pages>75--87</pages>
<contexts>
<context position="4056" citStr="Angluin 1987" startWordPosition="574" endWordPosition="575">vised way. If bounds on the requisite number of samples (known as “sample complexity bounds”) are sufficiently tight, then they may offer guidance to learner performance, given various amounts of data and a wide range of parametric families. Being able to reason analytically about the amount of data to annotate, and the relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Colli</context>
</contexts>
<marker>Angluin, 1987</marker>
<rawString>Angluin, D. 1987. Learning regular sets from queries and counterexamples. Information and Computation, 75:87–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Anthony</author>
<author>P L Bartlett</author>
</authors>
<title>Neural Network Learning: Theoretical Foundations.</title>
<date>1999</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="52993" citStr="Anthony and Bartlett 1999" startWordPosition="9127" endWordPosition="9130"> of functions such that for every f E Tn we have E[|f |x I {|f |&lt; Kn}] &lt; ebound(n). Let Ttruncated,n = {f x I {f &lt; Kn} |f E Tm}, namely, the set of functions from Tn after being truncated by Kn. Then for e &gt; 0 we have ~ p sup |E˜pn [f] − Ep [f] |&gt; 2e &lt; 8N(e/8,Ttruncated,n) exp (−1 128ne2/Kn) +ebound(n)/e fE-Tn provided n &gt; K2n/4e2 and ebound(n) &lt; e. See Pollard (1984; Chapter 2, pages 30–31) for the proof of Lemma 2. See also Appendix A. Covering numbers are rather complex combinatorial quantities which are hard to compute directly. Fortunately, they can be bounded using the pseudo-dimension (Anthony and Bartlett 1999), a generalization of the Vapnik-Chervonenkis (VC) dimension for real functions. In the case of our “binomialized” probabilistic grammars, the pseudo-dimension of Tn is bounded by N, because we have Tn C T, and the functions in T are linear with N parameters. Hence, Ttruncated,n also has pseudodimension that is at most N. We then have the following. 9 The “permissible class” requirement is a mild regularity condition regarding measurability that holds for proper approximations. We refer the reader to Pollard (1984) for more details. 498 Cohen and Smith Empirical Risk Minimization for Probabili</context>
</contexts>
<marker>Anthony, Bartlett, 1999</marker>
<rawString>Anthony, M. and P. L. Bartlett. 1999. Neural Network Learning: Theoretical Foundations. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Balcan</author>
<author>A Blum</author>
</authors>
<title>A discriminative model for semisupervised learning.</title>
<date>2010</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>57</volume>
<issue>3</issue>
<contexts>
<context position="87658" citStr="Balcan and Blum (2010)" startWordPosition="15426" endWordPosition="15429">on for Probabilistic Grammars 7.4.1 Sample Complexity Bounds with Semi-Supervised Learning. Our bounds focus on the supervised case and the unsupervised case. There is a trivial extension to the semisupervised case. Consider the objective function to be the sum of the likelihood for the labeled data together with the marginalized likelihood of the unlabeled data (this sum could be a weighted sum). Then, use the sample complexity bounds for each summand to derive a sample complexity bound on this sum. It would be more interesting to extend our results to frameworks such as the one described by Balcan and Blum (2010). In that case, our discussion of sample complexity would attempt to identify how unannotated data can reduce the space of candidate probabilistic grammars to a smaller set, after which we can use the annotated data to estimate the final grammar. This reduction of the space is accomplished through a notion of compatibility, a type of fitness that the learner believes the estimated grammar should have given the distribution that generates the data. The key challenge in the case of probabilistic grammars would be to properly define this compatibility notion such that it fits the log-loss. If thi</context>
</contexts>
<marker>Balcan, Blum, 2010</marker>
<rawString>Balcan, M. and A. Blum. 2010. A discriminative model for semisupervised learning. Journal of the Association for Computing Machinery, 57(3):1–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Balle</author>
<author>A Quattoni</author>
<author>X Carreras</author>
</authors>
<title>A spectral learning algorithm for finite state transducers.</title>
<date>2011</date>
<booktitle>In Proceedings of the European Conference on Machine Learning/the Principles and Practice of Knowledge Discovery in Databases,</booktitle>
<pages>156--171</pages>
<marker>Balle, Quattoni, Carreras, 2011</marker>
<rawString>Balle, B., A. Quattoni, and X. Carreras. 2011. A spectral learning algorithm for finite state transducers. In Proceedings of the European Conference on Machine Learning/the Principles and Practice of Knowledge Discovery in Databases, pages 156–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bane</author>
<author>J Riggle</author>
<author>M Sonderegger</author>
</authors>
<title>The VC dimension of constraint-based grammars.</title>
<date>2010</date>
<journal>Lingua,</journal>
<volume>120</volume>
<issue>5</issue>
<marker>Bane, Riggle, Sonderegger, 2010</marker>
<rawString>Bane, M., J. Riggle, and M. Sonderegger. 2010. The VC dimension of constraint-based grammars. Lingua, 120(5):1194–1208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bartlett</author>
<author>O Bousquet</author>
<author>S Mendelson</author>
</authors>
<title>Local Rademacher complexities.</title>
<date>2005</date>
<journal>Annals of Statistics,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Bartlett, Bousquet, Mendelson, 2005</marker>
<rawString>Bartlett, P., O. Bousquet, and S. Mendelson. 2005. Local Rademacher complexities. Annals of Statistics, 33(4):1497–1537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="76483" citStr="Bishop (2006)" startWordPosition="13592" endWordPosition="13593">?_1ˆ*j,k,i�) }}; t + -t + 1; until convergence; return 0(t) The reason for this likelihood increase stems from the fact that the M-step solves the optimization problem of minimizing the log-loss (with respect to 0 E Θ(y)) when the posterior calculate at the E-step as the base distribution is used. This means that the M-step minimizes (in iteration t): ]Er [ − log h(x,z |0(t))] where the expectation is taken with respect to the distribution r(x,z) = ˜pn(x)p(z |x,0(t−1)). With this notion in mind, the likelihood increase after each iteration follows from principles similar to those described in Bishop (2006) for the EM algorithm. 7. Discussion Our framework can be specialized to improve the two main criteria which have a tradeoff: the tightness of the proper approximation and the sample complexity. For example, we can improve the tightness of our proper approximations by taking a subsequence of T&apos;n. This will make the sample complexity bound degrade, however, because Kn will grow faster. Table 2 shows the trade-offs between parameters in our model and the effectiveness of learning. We note that the sample complexity bounds that we give in this article give insight about the asymptotic behavior of</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Bishop, C. M. 2006. Pattern Recognition and Machine Learning. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boyd</author>
<author>L Vandenberghe</author>
</authors>
<title>Convex Optimization.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="99301" citStr="Boyd and Vandenberghe 2004" startWordPosition="17792" endWordPosition="17795">c1 log(−c1/λ) + c2 log(−c2/λ) + λ(−c1/λ − c2/λ − 1) (B.5) g(λ) is the objective function of the dual problem of Equation (B.1)–Equation (B.2). We would like to minimize Equation (B.5) with respect to λ. The derivative of g(λ) is ∂g ∂λ = −c1/λ − c2/λ − 1 hence when equating the derivative of g(λ) to 0, we get λ = −(c1 + c2), and therefore the solution is β∗i = log (ci/(c1 + c2)). We need to verify that the solution to the dual problem indeed gets the optimal value for the primal. Because the primal problem is convex, it is sufficient to verify that the Karush-Kuhn-Tucker (KKT) conditions hold (Boyd and Vandenberghe 2004). Indeed, we have ∂βi (β∗) + λ ∂h ∂F ∂βi (β∗) = ci − (c1 + c2) xci c1 + c2 = 0 where h(β) ° exp(β) + exp(β) − 1 stands for the equality constraint. The rest of the KKT conditions trivially hold, therefore β∗ is the optimal solution for Equations (B.1)– (B.2). Note that if 1 − γ &lt; ci/(c1 + c2) &lt; γ, then this is the solution even when again adding the constraints in Equation (B.3) and (B.4). When c1/(c1 + c2) &lt; γ, then the solution is β∗1 = γ and β∗2 = 1 − γ. Similarly, when c2/(c1 + c2) &lt; γ then the solution is β∗2 = γ and β∗1 = 1 − γ. We describe why this is true for the first case. The second</context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>Boyd, S. and L. Vandenberghe. 2004. Convex Optimization. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carrasco</author>
</authors>
<title>Accurate computation of the relative entropy between stochastic regular grammars.</title>
<date>1997</date>
<journal>Theoretical Informatics and Applications,</journal>
<volume>31</volume>
<issue>5</issue>
<contexts>
<context position="42629" citStr="Carrasco (1997)" startWordPosition="7201" endWordPosition="7202">t a fast rate as a function of m and etail(m) and ebound(m) decrease quickly as a function of m. As we will see in Section 5, we cannot have an arbitrarily fast convergence rate (by, for example, taking a subsequence of Fm), because the size of Km has a great effect on the number of samples required to obtain accurate estimation. 7 There are other ways to manage the unboundedness of KL divergence in the language learning literature. Clark and Thollard (2004), for example, decompose the KL divergence between probabilistic finite-state automata into several terms according to a decomposition of Carrasco (1997) and then bound each term separately. 493 Computational Linguistics Volume 38, Number 3 Table 1 Example of a PCFG where there is more than a single way to approximate it by truncation with γ = 0.1, because it has more than two rules. Any value of η E [0,γ] will lead to a different approximation. Rule θ General η = 0 η = 0.01 η = 0.005 S NP VP 0.09 0.01 0.1 0.1 0.1 S NP 0.11 0.11 − η 0.11 0.1 0.105 S VP 0.8 0.8 − γ + η 0.79 0.8 0.795 4.1 Constructing Proper Approximations for Probabilistic Grammars We now focus on constructing proper approximations for probabilistic grammars whose degree is lim</context>
</contexts>
<marker>Carrasco, 1997</marker>
<rawString>Carrasco, R. 1997. Accurate computation of the relative entropy between stochastic regular grammars. Theoretical Informatics and Applications, 31(5):437–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>E Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<tech>Technical report,</tech>
<institution>Brown University,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="4438" citStr="Carroll and Charniak 1992" startWordPosition="634" endWordPosition="637">arametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced he</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Carroll, G. and E. Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. Technical report, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="15941" citStr="Charniak 1993" startWordPosition="2444" endWordPosition="2445">d of p, defined as: n ˜pn(x,z) = n−1 I{(x,z) = (xi, zi)} i=1 where I {(x, z) = (xi, zi)} is 1 if (x, z) = (xi, zi) and 0 otherwise.2 We then set up the problem as the problem of empirical risk minimization (ERM), that is, trying to find q such that q* = argmin E˜pn [−log q] (2) qEQ n = argmin −n−1 log q(xi,zi) qEQ i=1 n = argmax n−1 log q(xi, zi) (3) qEQ i=1 Equation (3) immediately shows that minimizing empirical risk using the log-loss is equivalent to the maximizing likelihood, which is a common statistical principle used for estimating a probabilistic grammar in computational linguistics (Charniak 1993; Manning and Sch¨utze 1999).3 As mentioned earlier, our goal is to estimate the probability distribution p while quantifying how accurate our estimate is. One way to quantify the estimation accuracy is by bounding the excess risk, which is defined as £p(q; Q) = £p(q) Ep [−log q] − min q,EQ We are interested in bounding the excess risk for q*, £p(q*). The excess risk is reduced to KL divergence between p and q if p ∈ Q, because in this case the quantity minq,EQ E [− log q&apos;] is minimized with q&apos; = p, and equals the entropy of p. In a typical 2 We note that ˜pn itself is a random variable, becau</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Charniak, E. 1993. Statistical Language Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="4390" citStr="Charniak and Johnson 2005" startWordPosition="627" endWordPosition="630">e relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent s</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the Association for Computational Linguistics, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="38521" citStr="Chi (1999)" startWordPosition="6490" endWordPosition="6491"> each having two events. Let µ(k,i),1 = θk,i, µ(k,i),2 = 1 − θk,i for i = 1 and set µk,i,1 = θk,i/µ(k,i−1),2, and µ(k,i−1),2 = 1 − µ(k,i−1),2. (G&apos;, µ) is a weighted context-free grammar such that the µ(k,i),1 corresponds to the ith event in the k multinomial of the original grammar. Let z be a derivation in G and z&apos; = ΥG,,G&apos;(z). Then, from Utility Lemma 1 and the construction of g&apos;, we have that: p(z |θ, G) = K Nk θψk,i(z) H i=1 k,i k=1 = K Nk ψk,i(z) θk,i H i=1 � k=1 l=1  µ(k,j),2 µ(k,i),1 ψk,i(z) ψk,i(z) µ(k ,j),2 µ(k,i),1 = K Nk 2 ψk,j(z&apos;) H H ri µ(k,j),i k=1 j=1 i=1 = p(z&apos; |µ, G&apos;) From Chi (1999), we know that the weighted grammar (G&apos;, µ) can be converted to a probabilistic context-free grammar (G&apos;, θ&apos;), through a construction of θ&apos; based on µ, such that p(z&apos; |µ, G&apos;) = p(z&apos; |θ&apos;, G&apos;). ■ The proof for Theorem 1 gives a construction the parameters θ&apos; of G&apos; such that (G, θ) is equivalent to (G&apos;, θ&apos;). The construction of θ&apos; can also be reversed: Given θ&apos; for G&apos;, we can construct θ for G so that again we have equivalence between (G, θ) and (G&apos;, θ&apos;). In this section, we focused on presenting parametrized, empirically justified distributional assumptions about language data that will make the</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Chi, Z. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
<author>R Eyraud</author>
<author>A Habrard</author>
</authors>
<title>A polynomial algorithm for the inference of context free languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Colloquium on Grammatical Inference,</booktitle>
<pages>29--42</pages>
<marker>Clark, Eyraud, Habrard, 2008</marker>
<rawString>Clark, A., R. Eyraud, and A. Habrard. 2008. A polynomial algorithm for the inference of context free languages. In Proceedings of the International Colloquium on Grammatical Inference, pages 29–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
<author>S Lappin</author>
</authors>
<title>Unsupervised learning and grammar induction.</title>
<date>2010</date>
<booktitle>The Handbook of Computational Linguistics and Natural Language Processing.</booktitle>
<pages>197--220</pages>
<editor>In Alexander Clark, Chris Fox, and Shalom Lappin, editors,</editor>
<publisher>Wiley-Blackwell,</publisher>
<location>London,</location>
<contexts>
<context position="77839" citStr="Clark and Lappin (2010)" startWordPosition="13806" endWordPosition="13809">t criteria. Kn is the constant that satisfies the boundedness property (Theorems 2 and 3) and s is a fixed constant larger than 1 (Section 4.1). criterion as Kn increases ... as s increases ... tightness of proper approximation improves improves sample complexity bound degrades degrades 509 Computational Linguistics Volume 38, Number 3 sufficiently tight to be used in practice. It still remains an open problem to obtain sample complexity bounds which are sufficiently tight in this respect. For a discussion about the connection of grammar learning in theory and practice, we refer the reader to Clark and Lappin (2010). It is also important to note that MLE is not the only option for estimating finite state probabilistic grammars. There has been some recent advances in learning finite state models (HMMs and finite state transducers) by using spectral analysis of matrices which consist of quantities estimated from observations only (Hsu, Kakade, and Zhang 2009; Balle, Quattoni, and Carreras 2011), based on the observable operator models of Jaeger (1999). These algorithms are not prone to local minima, and converge to the correct model as the number of samples increases, but require some assumptions about the</context>
</contexts>
<marker>Clark, Lappin, 2010</marker>
<rawString>Clark, A. and S. Lappin. 2010. Unsupervised learning and grammar induction. In Alexander Clark, Chris Fox, and Shalom Lappin, editors, The Handbook of Computational Linguistics and Natural Language Processing. Wiley-Blackwell, London, pages 197–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
<author>F Thollard</author>
</authors>
<title>PAC-learnability of probabilistic deterministic finite state automata.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--473</pages>
<contexts>
<context position="4081" citStr="Clark and Thollard 2004" startWordPosition="576" endWordPosition="579">bounds on the requisite number of samples (known as “sample complexity bounds”) are sufficiently tight, then they may offer guidance to learner performance, given various amounts of data and a wide range of parametric families. Being able to reason analytically about the amount of data to annotate, and the relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distributio</context>
<context position="6572" citStr="Clark and Thollard 2004" startWordPosition="953" endWordPosition="956"> in these cases is different from our setting: Error is measured as the probability mass of strings that are not identified correctly by the learned finite state automaton, instead of measuring KL divergence between the automaton and the true distribution. In addition, in many cases, there is also a focus on the distribution-free setting. To the best of our knowledge, it is still an open problem whether finite state automata are learnable in the distribution-dependent setting when measuring the error as the fraction of misidentified strings. Other work (Ron 1995; Ron, Singer, and Tishby 1998; Clark and Thollard 2004; Palmer and Goldberg 2007) also gives treatment to probabilistic automata with an error measure which is more suitable for the probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance. 480 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars These also focus on learning the structure of finite state machines. As mentioned earlier, in our setting we assume that the grammar is fixed, and that our goal is to estimate its parameters. We note an important connection to an earlier study about the learnability of probabilistic automata and hidden Markov</context>
<context position="31202" citStr="Clark and Thollard (2004)" startWordPosition="5114" endWordPosition="5117">. These assumptions also hold in many cases when p itself is a probabilistic grammar. Also, we note that the last requirement of bounded expectations is optional, and it can be inferred from the rest of the requirements: B = L/(1 − q)2. We make this requirement explicit for simplicity of notation later. We denote the family of distributions that satisfy all of these requirements by P(o, L, r, q, B, G). There are other cases in the literature of language learning where additional assumptions are made on the learned family of models in order to obtain positive learnability results. For example, Clark and Thollard (2004) put a bound on the expected length of strings generated from any state of probabilistic finite state automata, which resembles the exponential decay of strings we have for p in this article. An immediate consequence of these assumptions is that the entropy of p is finite and bounded by a quantity that depends on L, r and q.5 Bounding entropy of labels (derivations) given inputs (sentences) is a common way to quantify the noise in a distribution. Here, both the sentential entropy (Hs (p) = −Ex p(x) log p(x)) is bounded as well as the derivational entropy (Hd(p) = − Ex,z p(x, z) log p(x,z)). Th</context>
<context position="42476" citStr="Clark and Thollard (2004)" startWordPosition="7179" endWordPosition="7182">etail(m), ebound(m), and Cm such that, for all m larger than some M, containment, boundedness, and tightness all hold. In a good approximation, Km would increase at a fast rate as a function of m and etail(m) and ebound(m) decrease quickly as a function of m. As we will see in Section 5, we cannot have an arbitrarily fast convergence rate (by, for example, taking a subsequence of Fm), because the size of Km has a great effect on the number of samples required to obtain accurate estimation. 7 There are other ways to manage the unboundedness of KL divergence in the language learning literature. Clark and Thollard (2004), for example, decompose the KL divergence between probabilistic finite-state automata into several terms according to a decomposition of Carrasco (1997) and then bound each term separately. 493 Computational Linguistics Volume 38, Number 3 Table 1 Example of a PCFG where there is more than a single way to approximate it by truncation with γ = 0.1, because it has more than two rules. Any value of η E [0,γ] will lead to a different approximation. Rule θ General η = 0 η = 0.01 η = 0.005 S NP VP 0.09 0.01 0.1 0.1 0.1 S NP 0.11 0.11 − η 0.11 0.1 0.105 S VP 0.8 0.8 − γ + η 0.79 0.8 0.795 4.1 Constr</context>
</contexts>
<marker>Clark, Thollard, 2004</marker>
<rawString>Clark, A. and F. Thollard. 2004. PAC-learnability of probabilistic deterministic finite state automata. Journal of Machine Learning Research, 5:473–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Covariance in unsupervised learning of probabilistic grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--3017</pages>
<contexts>
<context position="4510" citStr="Cohen and Smith 2010" startWordPosition="646" endWordPosition="649">note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justified assumptions about the distributions t</context>
<context position="12905" citStr="Cohen and Smith (2010" startWordPosition="1918" endWordPosition="1921">is difficulty, we adapt the expectation-maximization algorithm (Dempster, Laird, and Rubin 1977) to approximately maximize likelihood (or minimize log-loss) in the unsupervised case with proper approximations. In Section 7 we discuss some related ideas. These include the failure of an alternative kind of distributional assumption and connections to regularization by maximum a posteriori estimation with Dirichlet priors. Longer proofs are included in the appendices. A table of notation that is used throughout is included as Table D.1 in Appendix D. This article builds on two earlier papers. In Cohen and Smith (2010b) we presented the main sample complexity results described here; the present article includes significant extensions, a deeper analysis of our distributional assumptions, and a discussion of variants of these assumptions, as well as related work, such as that about the Tsybakov noise condition. In Cohen and Smith (2010c) we proved NP-hardness for unsupervised parameter estimation of probalistic context-free grammars (PCFGs) (without approximate families). The present article uses a similar type of proof to achieve results adapted to empirical risk minimization in our approximation framework.</context>
<context position="65884" citStr="Cohen and Smith (2010" startWordPosition="11562" endWordPosition="11565">pirical log-loss in the unsupervised setting requires minimizing (with respect to θ) the following: E˜pn [− log h(x |θ)] = − E ˜pn(x) log E h(x,z |θ) (23) x z with the constraint that γ &lt; θk,i &lt; 1 − γ (i.e., θ E Θ(γ)) where γ = n−s. This is done after drawing n examples according to Theorem 3. 504 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars 6.2.1 Hardness of ERM with Proper Approximations. It turns out that minimizing Equation (23) under the specified constraints is actually an NP-hard problem when G is a PCFG. This result follows using a similar proof to the one in Cohen and Smith (2010c) for the hardness of Viterbi training and maximizing log-likelihood for PCFGs. We turn to giving the full derivation of this hardness result for PCFGs and the modification required for adapting the results from Cohen and Smith to the case of having an arbitrary -y margin constraint. In order to show an NP-hardness result, we need to “convert” the problem of the maximization of Equation (23) to a decision problem. We do so by stating the following decision problem. Problem 1(Unsupervised Minimization of the Log-Loss with Margin) Input: A binarized context-free grammar G, a set of sentences x1</context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>Cohen, S. B. and N. A. Smith. 2010a. Covariance in unsupervised learning of probabilistic grammars. Journal of Machine Learning Research, 11:3017–3051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Empirical risk minimization with approximations of probabilistic grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems,</booktitle>
<pages>424--432</pages>
<contexts>
<context position="4510" citStr="Cohen and Smith 2010" startWordPosition="646" endWordPosition="649">note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justified assumptions about the distributions t</context>
<context position="12905" citStr="Cohen and Smith (2010" startWordPosition="1918" endWordPosition="1921">is difficulty, we adapt the expectation-maximization algorithm (Dempster, Laird, and Rubin 1977) to approximately maximize likelihood (or minimize log-loss) in the unsupervised case with proper approximations. In Section 7 we discuss some related ideas. These include the failure of an alternative kind of distributional assumption and connections to regularization by maximum a posteriori estimation with Dirichlet priors. Longer proofs are included in the appendices. A table of notation that is used throughout is included as Table D.1 in Appendix D. This article builds on two earlier papers. In Cohen and Smith (2010b) we presented the main sample complexity results described here; the present article includes significant extensions, a deeper analysis of our distributional assumptions, and a discussion of variants of these assumptions, as well as related work, such as that about the Tsybakov noise condition. In Cohen and Smith (2010c) we proved NP-hardness for unsupervised parameter estimation of probalistic context-free grammars (PCFGs) (without approximate families). The present article uses a similar type of proof to achieve results adapted to empirical risk minimization in our approximation framework.</context>
<context position="65884" citStr="Cohen and Smith (2010" startWordPosition="11562" endWordPosition="11565">pirical log-loss in the unsupervised setting requires minimizing (with respect to θ) the following: E˜pn [− log h(x |θ)] = − E ˜pn(x) log E h(x,z |θ) (23) x z with the constraint that γ &lt; θk,i &lt; 1 − γ (i.e., θ E Θ(γ)) where γ = n−s. This is done after drawing n examples according to Theorem 3. 504 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars 6.2.1 Hardness of ERM with Proper Approximations. It turns out that minimizing Equation (23) under the specified constraints is actually an NP-hard problem when G is a PCFG. This result follows using a similar proof to the one in Cohen and Smith (2010c) for the hardness of Viterbi training and maximizing log-likelihood for PCFGs. We turn to giving the full derivation of this hardness result for PCFGs and the modification required for adapting the results from Cohen and Smith to the case of having an arbitrary -y margin constraint. In order to show an NP-hardness result, we need to “convert” the problem of the maximization of Equation (23) to a decision problem. We do so by stating the following decision problem. Problem 1(Unsupervised Minimization of the Log-Loss with Margin) Input: A binarized context-free grammar G, a set of sentences x1</context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>Cohen, S. B. and N. A. Smith. 2010b. Empirical risk minimization with approximations of probabilistic grammars. In Proceedings of the Advances in Neural Information Processing Systems, pages 424–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>N A Smith</author>
</authors>
<title>Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>1502--1511</pages>
<contexts>
<context position="4510" citStr="Cohen and Smith 2010" startWordPosition="646" endWordPosition="649">note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justified assumptions about the distributions t</context>
<context position="12905" citStr="Cohen and Smith (2010" startWordPosition="1918" endWordPosition="1921">is difficulty, we adapt the expectation-maximization algorithm (Dempster, Laird, and Rubin 1977) to approximately maximize likelihood (or minimize log-loss) in the unsupervised case with proper approximations. In Section 7 we discuss some related ideas. These include the failure of an alternative kind of distributional assumption and connections to regularization by maximum a posteriori estimation with Dirichlet priors. Longer proofs are included in the appendices. A table of notation that is used throughout is included as Table D.1 in Appendix D. This article builds on two earlier papers. In Cohen and Smith (2010b) we presented the main sample complexity results described here; the present article includes significant extensions, a deeper analysis of our distributional assumptions, and a discussion of variants of these assumptions, as well as related work, such as that about the Tsybakov noise condition. In Cohen and Smith (2010c) we proved NP-hardness for unsupervised parameter estimation of probalistic context-free grammars (PCFGs) (without approximate families). The present article uses a similar type of proof to achieve results adapted to empirical risk minimization in our approximation framework.</context>
<context position="65884" citStr="Cohen and Smith (2010" startWordPosition="11562" endWordPosition="11565">pirical log-loss in the unsupervised setting requires minimizing (with respect to θ) the following: E˜pn [− log h(x |θ)] = − E ˜pn(x) log E h(x,z |θ) (23) x z with the constraint that γ &lt; θk,i &lt; 1 − γ (i.e., θ E Θ(γ)) where γ = n−s. This is done after drawing n examples according to Theorem 3. 504 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars 6.2.1 Hardness of ERM with Proper Approximations. It turns out that minimizing Equation (23) under the specified constraints is actually an NP-hard problem when G is a PCFG. This result follows using a similar proof to the one in Cohen and Smith (2010c) for the hardness of Viterbi training and maximizing log-likelihood for PCFGs. We turn to giving the full derivation of this hardness result for PCFGs and the modification required for adapting the results from Cohen and Smith to the case of having an arbitrary -y margin constraint. In order to show an NP-hardness result, we need to “convert” the problem of the maximization of Equation (23) to a decision problem. We do so by stating the following decision problem. Problem 1(Unsupervised Minimization of the Log-Loss with Margin) Input: A binarized context-free grammar G, a set of sentences x1</context>
</contexts>
<marker>Cohen, Smith, 2010</marker>
<rawString>Cohen, S. B. and N. A. Smith. 2010c. Viterbi training for PCFGs: Hardness results and competitiveness of uniform initialization. In Proceedings of the Association for Computational Linguistics, pages 1502–1511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--589</pages>
<contexts>
<context position="4362" citStr="Collins 2003" startWordPosition="625" endWordPosition="626">notate, and the relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars i</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Collins, M. 2003. Head-driven statistical models for natural language processing. Computational Linguistics, 29:589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In</title>
<date>2004</date>
<pages>pages</pages>
<publisher>Kluwer,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="4664" citStr="Collins 2004" startWordPosition="670" endWordPosition="671"> 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justified assumptions about the distributions that generate the data. Our framework uses and significantly extends ideas that have been introduced for deriving sample complexity bounds for probabilisti</context>
</contexts>
<marker>Collins, 2004</marker>
<rawString>Collins, M. 2004. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In H. Bunt, J. Carroll, and G. Satta, Text, Speech and Language Technology (New Developments in Parsing Technology). Kluwer, Dordrecht, pages 19–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Corazza</author>
<author>G Satta</author>
</authors>
<title>Cross-entropy and estimation of probabilistic context-free</title>
<date>2006</date>
<pages>524</pages>
<contexts>
<context position="64775" citStr="Corazza and Satta 2006" startWordPosition="11363" endWordPosition="11366">olution for this optimization problem is θk,i = min { 1 − γ, max � � � &amp;quot;� � # # (22) �n n 2 γ, E j,k,i E E � � ˆψj,k,i&apos; � �% % j=1 j=1 i&apos;=1 where ˆψj,k,i is the number of times that ψk,i fires in Example j. (We include a full derivation of this result in Appendix B.) The interpretation of Equation (22) is simple: We count the number of times a rule appears in the samples and then normalize this value by the total number of times rules associated with the same multinomial appear in the samples. This frequency count is the maximum likelihood solution with respect to the full hypothesis class X (Corazza and Satta 2006; see Appendix B). Because we constrain ourselves to obtain a value away from 0 or 1 by a margin of γ, we need to truncate this solution, as done in Equation (22). This truncation to a margin γ can be thought of as a smoothing factor that enables us to compute sample complexity bounds. We explore this connection to smoothing with a Dirichlet prior in a Maximum a posteriori (MAP) Bayesian setting in Section 7.2. 6.2 Unsupervised Case Similarly to the supervised case, minimizing the empirical log-loss in the unsupervised setting requires minimizing (with respect to θ) the following: E˜pn [− log </context>
<context position="85663" citStr="Corazza and Satta (2006)" startWordPosition="15101" endWordPosition="15104">imal set does not follow the Tsybakov noise condition, but it is perhaps possible to find meaningful bounds for it, in which case we may be able to get tighter bounds using Talagrand’s inequality. We note that it may be possible to obtain data-dependent bounds for the diameter of the e-minimal set, following Koltchinskii (2006), by calculating the diameter of the e-minimal set using ˜pn. 7.3.2 Simpler Bounds for the Supervised Case. As noted in Section 6.1, minimizing empirical risk with the log-loss leads to a simple frequency count for calculating the estimated parameters of the grammar. In Corazza and Satta (2006), it has been also noted that to minimize the non-empirical risk, it is necessary to set the parameters of the grammar to the normalized expected count of the features. This means that we can get bounds on the deviation of a certain parameter from the optimal parameter by applying modifications to rather simple inequalities such as Hoeffding’s inequality, which determines the probability of the average of a set of i.i.d. random variables deviating from its mean. The modification would require us to split the event space into two cases: one in which the count of some features is larger than som</context>
</contexts>
<marker>Corazza, Satta, 2006</marker>
<rawString>Corazza, A. and G. Satta. 2006. Cross-entropy and estimation of probabilistic context-free 524</rawString>
</citation>
<citation valid="false">
<authors>
<author>Cohen</author>
<author>Smith</author>
</authors>
<title>Empirical Risk Minimization for Probabilistic Grammars grammars.</title>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>335--342</pages>
<marker>Cohen, Smith, </marker>
<rawString>Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars grammars. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 335–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>Wiley,</publisher>
<location>London.</location>
<contexts>
<context position="32095" citStr="Cover and Thomas 1991" startWordPosition="5284" endWordPosition="5287">e and bounded by a quantity that depends on L, r and q.5 Bounding entropy of labels (derivations) given inputs (sentences) is a common way to quantify the noise in a distribution. Here, both the sentential entropy (Hs (p) = −Ex p(x) log p(x)) is bounded as well as the derivational entropy (Hd(p) = − Ex,z p(x, z) log p(x,z)). This is stated in the following result. Proposition 1 Let p E P(o, L, r, q, B, G) be a distribution. Then, we have Hs(p) &lt; Hd(p) &lt; −log L + (L logr log 1r + [(1 + logL )/log 1r] Λ \ [1l lo g gL / Proof First note that Hs(p) &lt; Hd(p) holds by the data processing inequality (Cover and Thomas 1991) because the sentential probability distribution p(x) is a coarser version of the derivational probability distribution p(x,z). Now, consider p(x,z). For simplicity of notation, we use p(z) instead of p(x, z). The yield of z, x, is a function of z, and therefore can be omitted from the distribution. It holds that � p(z) logp(z) Hd(p) = − z �= − p(z) log p(z) − � p(z)logp(z) z∈Z1 z∈Z2 = Hd(p, Z1) + Hd(p, Z2) where Z1 = {z |p(z) &gt; 1/e} and Z2 = {z |p(z) &lt; 1/e}. Note that the function −o log o reaches its maximum for o = 1/e. We therefore have Hd(p, Z1) &lt;|Z1| e 5 For simplicity and consistency wi</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, T. M. and J. A. Thomas. 1991. Elements of Information Theory. Wiley, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
</authors>
<title>The sample complexity of learning fixed-structure bayesian networks.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="5298" citStr="Dasgupta 1997" startWordPosition="756" endWordPosition="757">ree setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justified assumptions about the distributions that generate the data. Our framework uses and significantly extends ideas that have been introduced for deriving sample complexity bounds for probabilistic graphical models (Dasgupta 1997). Maximum likelihood estimation is put in the empirical risk minimization framework (Vapnik 1998) with the loss function being the log-loss. Following that, we develop a set of learning theoretic tools to explore rates of estimation convergence for probabilistic grammars. We also develop algorithms for performing empirical risk minimization. Much research has been devoted to the problem of learning finite state automata (which can be thought of as a class of grammars) in the Probably Approximately Correct setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 198</context>
<context position="11034" citStr="Dasgupta 1997" startWordPosition="1640" endWordPosition="1641">proximations are 1 It is important to remember that minimizing the log-loss does not equate to minimizing the error of a linguistic analyzer or natural language processing application. In this article we focus on the log-loss case because we believe that probabilistic models of language phenomena have inherent usefulness as explanatory tools in computational linguistics, aside from their use in systems. 481 Computational Linguistics Volume 38, Number 3 based on bounded approximations that have been used for deriving sample complexity bounds for graphical models in a distribution-free setting (Dasgupta 1997). Our approximations have two important properties: They are, by themselves, probabilistic grammars from the family we are interested in estimating, and they become a tighter approximation around the family of probabilistic grammars we are interested in estimating as more samples are available. Moving to the distribution-dependent setting and defining proper approximations enables us to derive sample complexity bounds. In Section 5 we present the sample complexity results for both the supervised and unsupervised cases. A question that lingers at this point is whether it is computationally feas</context>
<context position="40333" citStr="Dasgupta 1997" startWordPosition="6805" endWordPosition="6806">, T2,. ... We also have to replace two-sided uniform convergence (Equation [6]) with convergence on the sequence of concept spaces we defined (Equation [10]). The concept spaces in the sequence vary as a function of the number of samples we have. We next = K Nk ψk,i(z)  H i=1 � �i− 1 k=1 l=1 j=1 = K Nk  H i=1 ~i− 1 k=1 j=1 492 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars construct the sequence of concept spaces, and in Section 5 we return to the learning model. Our approximations are based on the concept of bounded approximations (Abe, Takeuchi, and Warmuth 1991; Dasgupta 1997), which were originally designed for graphical models.7 A bounded approximation is a subset of a concept space which is controlled by a parameter that determines its tightness. Here we use this idea to define a series of subsets of the original concept space F as approximations, while having two asymptotic properties that control the series’ tightness. Let Fm (for m ∈ {1,2,...}) be a sequence of concept spaces. We consider three properties of elements of this sequence, which should hold for m &gt; M for a fixed M. The first is containment in F: Fm ⊆ F The second property is boundedness: ∃Km ≥ 0,∀</context>
<context position="59240" citStr="Dasgupta (1997)" startWordPosition="10322" endWordPosition="10323"> ≥ etail(n)} ≤ p U {x |∃zCn( f)(z) − f(z) ≥ etail(n)} fIETI (fET Define x(n) to be all x such that there exists a z with yield(z) = x and |z |≥ log2 n. From the proof of Proposition 3 and the requirements on p, we know that there exists an oc ≥ 1 such that �� � � p fET{x |∃z s.t.Cn( f )(z) − f(z) ≥ etail(n)} ≤ xEx(n) 00 ≤ � p(x) ≤ E LA(k)rk ≤ etail(n) x:jxj&gt;log2 n/α k=Llog2 n/αJ where the last inequality happens for some n larger than a fixed M. ■ Computing either the covering number or the pseudo-dimension of F&apos;n is a hard task, because the function in the classes includes the “log-sum-exp.” Dasgupta (1997) overcomes this problem for Bayesian networks with fixed structure by giving a bound on the covering number for (his respective) J�&apos; which depends on the covering number of T. Unfortunately, we cannot fully adopt this approach, because the derivations of a probabilistic grammar can be arbitrarily large. Instead, we present the following proposition, which is based on the “Hidden Variable Rule” from Dasgupta (1997). This proposition shows that the covering number of J�&apos; (or more accurately, its bounded approximations) can be bounded in terms of the covering number of the bounded p(x) 501 Comput</context>
<context position="93461" citStr="Dasgupta 1997" startWordPosition="16546" endWordPosition="16547">operty with Km = sN log3 m and Ebound(m) = m−β logm. Proof Let f ∈ T&apos;m. Let Z(m) = {z ||z |≤ log2 m}. Then, for all z ∈ Z(m) we have |f(z)| = − Ei,k ψ(k, i) log θk,i ≤ Ei,k ψ(k, i)(p log m) ≤ sN log3 m = Km, where the first inequality follows from f ∈ T&apos;m (θk,i ≥ m−s) and the second from |z |≤ log2 m. In addition, from the requirements on p we have � � E [|f |× I {|f |≥ Km} ] ≤ (sN log3 m) × 1: LA(k)rkk ≤ (κ log3 m) × (glog2 ml k&gt;log2 m \ J for κ = sNL (1 − q)2 . Finally, for β(L, q, p, N) ° log κ + 1 + log 1q = β &gt; 0 and if m &gt; 1 then ( ) (qlog2 m) κlog3 m ≤ m−βlogm. m Utility Lemma 4 (From [Dasgupta 1997].) Let a ∈ [0, 1] and let b = a if a ∈ [γ,1 − γ], b = γ if a ≤ γ, and b = 1 − γ if a ≥ 1 − γ. Then for any c ≤ 1/2 such that γ ≤ c/(1 + c) we have log a/b ≤ c. Proposition 3 Let p ∈ T(α, L, r, q, B, G) and let T&apos;m as defined earlier. There exists an M such that for any m &gt; M we have � � �U p {z |Cm( f )(z) − f(z) ≥ Etail(m)} �≤ Etail(m) f∈F N log2 m for �tail(m) = ms − 1 and Cm( f ) = T( f,m−s). 515 Computational Linguistics Volume 38, Number 3 Proof Let Z(m) be the set of derivations of size bigger than log2 m. Let f ∈ T. Define f&apos; = T(f, m−s). For any z ∈/ Z(m) we have that K f&apos;(z) − f(z) =</context>
</contexts>
<marker>Dasgupta, 1997</marker>
<rawString>Dasgupta, S. 1997. The sample complexity of learning fixed-structure bayesian networks. Machine Learning, 29(2–3):165–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>de la Higuera</author>
<author>C</author>
</authors>
<title>A bibliographical study of grammatical inference. Pattern Recognition,</title>
<date>2005</date>
<pages>38--1332</pages>
<marker>Higuera, C, 2005</marker>
<rawString>de la Higuera, C. 2005. A bibliographical study of grammatical inference. Pattern Recognition, 38:1332–1348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood estimation from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society B,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A., N. Laird, and D. Rubin. 1977. Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Optimal parsing strategies for linear context-free rewriting systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>769--776</pages>
<contexts>
<context position="37560" citStr="Gildea 2010" startWordPosition="6303" endWordPosition="6304">mation of G as defined earlier. Then, there exists θ&apos; for G&apos; such that for any z E D(G) we have p(z |θ, G) = p(ΥG,G&apos;(z) |θf, GI). 6 We note that this notion of binarization is different from previous types of binarization appearing in computational linguistics for grammars. Typically in previous work about binarized grammars such as CFGs, the grammars are constrained to have at most two nonterminals in the right side in Chomsky normal form. Another form of binarization for linear context-free rewriting systems is restriction of the fan-out of the rules to two (G´omez-Rodriguez and Satta 2009; Gildea 2010). We, however, limit the number of rules for each nonterminal (or more generally, the number of elements in each multinomial). 491 Computational Linguistics Volume 38, Number 3 Proof For the grammar G, index the set {1, ..., K} with nonterminals ranging from A1 to AK. Define G&apos; as before. We need to define θ&apos;. Index the multinomials in G&apos; by (k, i), each having two events. Let µ(k,i),1 = θk,i, µ(k,i),2 = 1 − θk,i for i = 1 and set µk,i,1 = θk,i/µ(k,i−1),2, and µ(k,i−1),2 = 1 − µ(k,i−1),2. (G&apos;, µ) is a weighted context-free grammar such that the µ(k,i),1 corresponds to the ith event in the k mu</context>
</contexts>
<marker>Gildea, 2010</marker>
<rawString>Gildea, D. 2010. Optimal parsing strategies for linear context-free rewriting systems. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 769–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G´omez-Rodriguez</author>
<author>G Satta</author>
</authors>
<title>An optimal-time binarization algorithm for linear context-free rewriting systems with fan-out two.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics-International Joint Conference on Natural Language Processing,</booktitle>
<pages>985--993</pages>
<marker>G´omez-Rodriguez, Satta, 2009</marker>
<rawString>G´omez-Rodriguez, C. and G. Satta. 2009. An optimal-time binarization algorithm for linear context-free rewriting systems with fan-out two. In Proceedings of the Association for Computational Linguistics-International Joint Conference on Natural Language Processing, pages 985–993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Grenander</author>
</authors>
<title>Abstract Inference.</title>
<date>1981</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="19260" citStr="Grenander 1981" startWordPosition="3060" endWordPosition="3061"> samples of size n. We will make this notion more rigorous in Section 2.2. 2.1 Empirical Risk Minimization and Structural Risk Minimization Methods It has been noted in the literature (Vapnik 1998; Koltchinskii 2006) that often the class Q is too complex for empirical risk minimization using a fixed number of data points. It is therefore desirable in these cases to create a family of subclasses {Qα |oc E A} that have increasing complexity. The more data we have, the more complex our Qα can be for empirical risk minimization. Structural risk minimization (Vapnik 1998) and the method of sieves (Grenander 1981) are examples of methods that adopt such an approach. Structural risk minimization, for example, can be represented in many cases as a penalization of the empirical risk method, using a regularization term. In our case, the level of “complexity” is related to allocation of small probabilities to derivations in the grammar by a distribution q E Q. The basic problem is this: Whenever we have a derivation with a small probability, the log-loss becomes very large (in absolute value), and this makes it hard to show the convergence of the empirical process 484 Cohen and Smith Empirical Risk Minimiza</context>
</contexts>
<marker>Grenander, 1981</marker>
<rawString>Grenander, U. 1981. Abstract Inference. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Decision-theoretic generalizations of the PAC model for neural net and other learning applications.</title>
<date>1992</date>
<journal>Information and Computation,</journal>
<pages>100--78</pages>
<marker>Haussler, 1992</marker>
<rawString>Haussler, D. 1992. Decision-theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, 100:78–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>T Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden Markov models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Learning Theory.</booktitle>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>Hsu, D., S. M. Kakade, and T. Zhang. 2009. A spectral algorithm for learning hidden Markov models. In Proceedings of the Conference on Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ishigami</author>
<author>S Tani</author>
</authors>
<title>The VC-dimensions of finite automata with n states.</title>
<date>1993</date>
<booktitle>In Proceedings of Algorithmic Learning Theory,</booktitle>
<pages>328--341</pages>
<contexts>
<context position="89992" citStr="Ishigami and Tani (1993" startWordPosition="15788" endWordPosition="15791">ing the pseudo-dimension. An open problem that remains is characterization of the exact value pseudo-dimension for a given grammar, determined by consideration of various properties of that grammar. We conjecture, however, that a lower bound on the pseudo-dimension would be rather close to the full dimension of the grammar (the number of parameters). It is interesting to note that there has been some work to identify the VC dimension and pseudo-dimension for certain types of grammars. Bane, Riggle, and Sonderegger (2010), for example, calculated the VC dimension for constraint-based grammars. Ishigami and Tani (1993, 1997) computed the VC dimension for finite state automata with various properties. 7.5 Conclusion We presented a framework for performing empirical risk minimization for probabilistic grammars, in which sample complexity bounds, for the supervised case and the unsupervised case, can be derived. Our framework is based on the idea of bounded approximations used in the past to derive sample complexity bounds for graphical models. Our framework required assumptions about the probability distribution that generates sentences or derivations in the language of the given grammar. These assumptions w</context>
</contexts>
<marker>Ishigami, Tani, 1993</marker>
<rawString>Ishigami, Y. and S. Tani. 1993. The VC-dimensions of finite automata with n states. In Proceedings of Algorithmic Learning Theory, pages 328–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ishigami</author>
<author>S Tani</author>
</authors>
<title>VC-dimensions of finite automata and commutative finite automata with k letters and n states.</title>
<date>1997</date>
<journal>Applied Mathematics,</journal>
<volume>74</volume>
<issue>3</issue>
<marker>Ishigami, Tani, 1997</marker>
<rawString>Ishigami, Y. and S. Tani. 1997. VC-dimensions of finite automata and commutative finite automata with k letters and n states. Applied Mathematics, 74(3):229–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jaeger</author>
</authors>
<title>Observable operator models for discrete stochastic time series.</title>
<date>1999</date>
<journal>Neural Computation,</journal>
<pages>12--1371</pages>
<contexts>
<context position="78281" citStr="Jaeger (1999)" startWordPosition="13876" endWordPosition="13877">ch are sufficiently tight in this respect. For a discussion about the connection of grammar learning in theory and practice, we refer the reader to Clark and Lappin (2010). It is also important to note that MLE is not the only option for estimating finite state probabilistic grammars. There has been some recent advances in learning finite state models (HMMs and finite state transducers) by using spectral analysis of matrices which consist of quantities estimated from observations only (Hsu, Kakade, and Zhang 2009; Balle, Quattoni, and Carreras 2011), based on the observable operator models of Jaeger (1999). These algorithms are not prone to local minima, and converge to the correct model as the number of samples increases, but require some assumptions about the underlying model that generates the data. 7.1 Tsybakov Noise In this article, we chose to introduce assumptions about distributions that generate natural language data. The choice of these assumptions was motivated by observations about properties shared among treebanks. The main consequence of making these assumptions is bounding the amount of noise in the distribution (i.e., the amount of variation in probabilities across labels given </context>
</contexts>
<marker>Jaeger, 1999</marker>
<rawString>Jaeger, H. 1999. Observable operator models for discrete stochastic time series. Neural Computation, 12:1371–1398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kearns</author>
<author>L Valiant</author>
</authors>
<title>Cryptographic limitations on learning Boolean formulae and finite automata.</title>
<date>1989</date>
<contexts>
<context position="5899" citStr="Kearns and Valiant 1989" startWordPosition="847" endWordPosition="850">models (Dasgupta 1997). Maximum likelihood estimation is put in the empirical risk minimization framework (Vapnik 1998) with the loss function being the log-loss. Following that, we develop a set of learning theoretic tools to explore rates of estimation convergence for probabilistic grammars. We also develop algorithms for performing empirical risk minimization. Much research has been devoted to the problem of learning finite state automata (which can be thought of as a class of grammars) in the Probably Approximately Correct setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989; Pitt 1989; Terwijn 2002). Typically, the setting in these cases is different from our setting: Error is measured as the probability mass of strings that are not identified correctly by the learned finite state automaton, instead of measuring KL divergence between the automaton and the true distribution. In addition, in many cases, there is also a focus on the distribution-free setting. To the best of our knowledge, it is still an open problem whether finite state automata are learnable in the distribution-dependent setting when measuring the error as the fraction of misidentified strings. Ot</context>
</contexts>
<marker>Kearns, Valiant, 1989</marker>
<rawString>Kearns, M. and L. Valiant. 1989. Cryptographic limitations on learning Boolean formulae and finite automata.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 21st Association for Computing Machinery Symposium on the Theory of Computing,</booktitle>
<pages>433--444</pages>
<marker></marker>
<rawString>In Proceedings of the 21st Association for Computing Machinery Symposium on the Theory of Computing, pages 433–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Kearns</author>
<author>U V Vazirani</author>
</authors>
<title>An Introduction to Computational Learning Theory.</title>
<date>1994</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="12200" citStr="Kearns and Vazirani 1994" startWordPosition="1813" endWordPosition="1816">lingers at this point is whether it is computationally feasible to maximize likelihood in our framework even when given enough samples. In Section 6, we describe algorithms we use to estimate probabilistic grammars in our framework, when given access to the required number of samples. We show that in the supervised case, we can indeed maximize likelihood in our approximation framework using a simple algorithm. For the unsupervised case, however, we show that maximizing likelihood is NP-hard. This fact is related to a notion known in the learning theory literature as inherent unpredictability (Kearns and Vazirani 1994): Accurate learning is computationally hard even with enough samples. To overcome this difficulty, we adapt the expectation-maximization algorithm (Dempster, Laird, and Rubin 1977) to approximately maximize likelihood (or minimize log-loss) in the unsupervised case with proper approximations. In Section 7 we discuss some related ideas. These include the failure of an alternative kind of distributional assumption and connections to regularization by maximum a posteriori estimation with Dirichlet priors. Longer proofs are included in the appendices. A table of notation that is used throughout is</context>
</contexts>
<marker>Kearns, Vazirani, 1994</marker>
<rawString>Kearns, M. J. and U. V. Vazirani. 1994. An Introduction to Computational Learning Theory. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>478--487</pages>
<contexts>
<context position="4488" citStr="Klein and Manning 2004" startWordPosition="642" endWordPosition="645"> language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justified assumptions abo</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Klein, D. and C. D. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedings of the Association for Computational Linguistics, pages 478–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Koltchinskii</author>
</authors>
<title>Local Rademacher complexities and oracle inequalities in risk minimization.</title>
<date>2006</date>
<journal>The Annals of Statistics,</journal>
<volume>34</volume>
<issue>6</issue>
<contexts>
<context position="18861" citStr="Koltchinskii 2006" startWordPosition="2992" endWordPosition="2993"> hold because the log-loss can be unbounded. This means that a modification is required for the empirical process in a way that will actually guarantee some kind of convergence. We give a treatment to this in the next section. We note that all discussion of convergence in this section has been about convergence in probability. For example, we want Equation (6) to hold with high probability— for most samples of size n. We will make this notion more rigorous in Section 2.2. 2.1 Empirical Risk Minimization and Structural Risk Minimization Methods It has been noted in the literature (Vapnik 1998; Koltchinskii 2006) that often the class Q is too complex for empirical risk minimization using a fixed number of data points. It is therefore desirable in these cases to create a family of subclasses {Qα |oc E A} that have increasing complexity. The more data we have, the more complex our Qα can be for empirical risk minimization. Structural risk minimization (Vapnik 1998) and the method of sieves (Grenander 1981) are examples of methods that adopt such an approach. Structural risk minimization, for example, can be represented in many cases as a penalization of the empirical risk method, using a regularization </context>
<context position="33930" citStr="Koltchinskii 2006" startWordPosition="5647" endWordPosition="5648">1) ≤ [(1 + log L )/log r] A ([(1 + log L)/log 1r]) (12) where we use the monotonicity of A. Consider Hd(p, Z2) (the “low probability” derivations). We have: Hd(p,Z2) ≤ − E Lr|z |log �Lr|z|� zEZ2 ≤ − log L − (L log r) E |z|r|z| zEZ2 ≤ − log L − (L log r) 00 A(k)krk E k=1 00 ≤ − log L − (L log r) E kqk (13) k=1 = − log L + (i log log q1 (14) where Equation (13) holds from the assumptions about p. Putting Equation (12) and Equation (14) together, we obtain the result. ■ We note that another common way to quantify the noise in a distribution is through the notion of Tsybakov noise (Tsybakov 2004; Koltchinskii 2006). We discuss this further in Section 7.1, where we show that Tsybakov noise is too permissive, and probabilistic grammars do not satisfy its conditions. 3.2 Limiting the Degree of the Grammar When approximating a family of probabilistic grammars, it is much more convenient when the degree of the grammar is limited. In this article, we limit the degree of the grammar by making the assumption that all Nk ≤ 2. This assumption may seem, at first glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence, other formalisms), this assumption does not limit the total generativ</context>
<context position="79147" citStr="Koltchinskii 2006" startWordPosition="14006" endWordPosition="14007"> introduce assumptions about distributions that generate natural language data. The choice of these assumptions was motivated by observations about properties shared among treebanks. The main consequence of making these assumptions is bounding the amount of noise in the distribution (i.e., the amount of variation in probabilities across labels given a fixed input). There are other ways to restrict the noise in a distribution. One condition for such noise restriction, which has received considerable recent attention in the statistical literature, is the Tsybakov noise condition (Tsybakov 2004; Koltchinskii 2006). Showing that a distribution satisfies the Tsybakov noise condition enables the use of techniques (e.g., from Koltchinskii 2006) for deriving distribution-dependent sample complexity bounds that depend on the parameters of the noise. It is therefore of interest to see whether Tsybakov noise holds under the assumptions presented in Section 3.1. We show that this is not the case, and that Tsybakov noise is too permissive. In fact, we show that p can be a probabilistic grammar itself (and hence, satisfy the assumptions in Section 3.1), and still not satisfy the Tsybakov noise conditions. Tsybako</context>
<context position="80468" citStr="Koltchinskii (2006)" startWordPosition="14231" endWordPosition="14232">to more general settings, such as the one we are facing in this article (Koltchinskii 2006). We now explain the definition of Tsybakov noise in our context. Let C &gt; 0 and K &gt; 1. We say that a distribution p(x,z) satisfies the (C, K) Tsybakov noise condition if for any c &gt; 0 and h,g E X such that h,g E {h&apos; |£p(h&apos;,X) &lt; c}, we have � �)2] dist(g,h) ° Ep (loghl &lt; Cc 1/κ (25) g h This interpretation of Tsybakov noise implies that the diameter of the set of functions from the concept class that has small excess risk should shrink to 0 at the rate in Equation (25). Distribution-dependent bounds from Koltchinskii (2006) are monotone with respect to the diameter of this set of functions, and therefore demonstrating that it goes to 0 enables sharper derivations of sample complexity bounds. 510 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars We turn now to illustrating that the Tsybakov condition does not hold for probabilistic grammars in most cases. Let G be a probabilistic grammar. Define A = AG(θ) as a matrix such that E ,i),(k�,i�) [*k,i X ψkI,iI] (AG(θ))(k FI*k JFI*kIJI1 Theorem 5 Let G be a grammar with K &gt; 2 and degree 2. Assume that p is (G, θ∗) for some θ∗, such that θ∗1,1 = θ∗2</context>
<context position="84638" citStr="Koltchinskii 2006" startWordPosition="14938" endWordPosition="14939">of course: The more samples we have, the more likely it is that the counts of the events will grow large. 7.3 Other Derivations of Sample Complexity Bounds In this section, we discuss other possible solutions to the problem of deriving sample complexity bounds for probabilistic grammars. 7.3.1 Using Talagrand’s Inequality. Our bounds are based on VC theory together with classical results for empirical processes (Pollard 1984). There have been some recent developments to the derivation of rates of convergence in statistical learning theory (Massart 2000; Bartlett, Bousquet, and Mendelson 2005; Koltchinskii 2006), most prominently through the use of Talagrand’s inequality (Talagrand 1994), which is a concentration of measure inequality, in the spirit of Lemma 2. The bounds achieved with Talagrand’s inequality are also distribution-dependent, and are based on the diameter of the e-minimal set—the set of hypotheses which have an excess risk smaller than e. We saw in Section 7.1 that the diameter of the e-minimal set does not follow the Tsybakov noise condition, but it is perhaps possible to find meaningful bounds for it, in which case we may be able to get tighter bounds using Talagrand’s inequality. We</context>
</contexts>
<marker>Koltchinskii, 2006</marker>
<rawString>Koltchinskii, V. 2006. Local Rademacher complexities and oracle inequalities in risk minimization. The Annals of Statistics, 34(6):2593–2656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Leermakers</author>
</authors>
<title>How to cover a grammar.</title>
<date>1989</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>135--142</pages>
<contexts>
<context position="34875" citStr="Leermakers 1989" startWordPosition="5802" endWordPosition="5803">cle, we limit the degree of the grammar by making the assumption that all Nk ≤ 2. This assumption may seem, at first glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence, other formalisms), this assumption does not limit the total generative capacity that we can have across all context-free grammars. We first show that any context-free grammar with arbitrary degree can be mapped to a corresponding grammar with all Nk ≤ 2 that generates derivations equivalent to derivations in the original grammar. Such a grammar is also called a “covering grammar” (Nijholt 1980; Leermakers 1989). Let G be a CFG. Let A be the kth nonterminal. Consider the rules A → αi for i ≤ Nk where A appears on the left side. For each rule 490 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars Figure 2 Example of a context-free grammar and its equivalent binarized form. A -+ αi, i &lt; Nk, we create a new nonterminal in G&apos; such that Ai has two rewrite rules: Ai -+ αi and Ai -+ Ai+1. In addition, we create rules A -+ A1 and ANk -+ αNk. Figure 2 demonstrates an example of this transformation on a small context-free grammar. It is easy to verify that the resulting grammar G&apos; has an eq</context>
</contexts>
<marker>Leermakers, 1989</marker>
<rawString>Leermakers, R. 1989. How to cover a grammar. In Proceedings of the Association for Computational Linguistics, pages 135–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, C. D. and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Massart</author>
</authors>
<title>Some applications of concentration inequalities to statistics.</title>
<date>2000</date>
<booktitle>Annales de la Facult´e des Sciences de Toulouse, IX(2):245–303.</booktitle>
<contexts>
<context position="84578" citStr="Massart 2000" startWordPosition="14931" endWordPosition="14932"> of samples we have. These two scenarios are related, of course: The more samples we have, the more likely it is that the counts of the events will grow large. 7.3 Other Derivations of Sample Complexity Bounds In this section, we discuss other possible solutions to the problem of deriving sample complexity bounds for probabilistic grammars. 7.3.1 Using Talagrand’s Inequality. Our bounds are based on VC theory together with classical results for empirical processes (Pollard 1984). There have been some recent developments to the derivation of rates of convergence in statistical learning theory (Massart 2000; Bartlett, Bousquet, and Mendelson 2005; Koltchinskii 2006), most prominently through the use of Talagrand’s inequality (Talagrand 1994), which is a concentration of measure inequality, in the spirit of Lemma 2. The bounds achieved with Talagrand’s inequality are also distribution-dependent, and are based on the diameter of the e-minimal set—the set of hypotheses which have an excess risk smaller than e. We saw in Section 7.1 that the diameter of the e-minimal set does not follow the Tsybakov noise condition, but it is perhaps possible to find meaningful bounds for it, in which case we may be</context>
</contexts>
<marker>Massart, 2000</marker>
<rawString>Massart, P. 2000. Some applications of concentration inequalities to statistics. Annales de la Facult´e des Sciences de Toulouse, IX(2):245–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nijholt</author>
</authors>
<title>Context-Free Grammars: Covers, Normal Forms, and Parsing</title>
<date>1980</date>
<booktitle>of Lecture Notes in Computer Science).</booktitle>
<volume>93</volume>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="34857" citStr="Nijholt 1980" startWordPosition="5800" endWordPosition="5801">. In this article, we limit the degree of the grammar by making the assumption that all Nk ≤ 2. This assumption may seem, at first glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence, other formalisms), this assumption does not limit the total generative capacity that we can have across all context-free grammars. We first show that any context-free grammar with arbitrary degree can be mapped to a corresponding grammar with all Nk ≤ 2 that generates derivations equivalent to derivations in the original grammar. Such a grammar is also called a “covering grammar” (Nijholt 1980; Leermakers 1989). Let G be a CFG. Let A be the kth nonterminal. Consider the rules A → αi for i ≤ Nk where A appears on the left side. For each rule 490 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars Figure 2 Example of a context-free grammar and its equivalent binarized form. A -+ αi, i &lt; Nk, we create a new nonterminal in G&apos; such that Ai has two rewrite rules: Ai -+ αi and Ai -+ Ai+1. In addition, we create rules A -+ A1 and ANk -+ αNk. Figure 2 demonstrates an example of this transformation on a small context-free grammar. It is easy to verify that the resulting gr</context>
</contexts>
<marker>Nijholt, 1980</marker>
<rawString>Nijholt, A. 1980. Context-Free Grammars: Covers, Normal Forms, and Parsing (volume 93 of Lecture Notes in Computer Science). Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Palmer</author>
<author>P W Goldberg</author>
</authors>
<title>PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance.</title>
<date>2007</date>
<booktitle>In Proceedings of Algorithmic Learning Theory,</booktitle>
<pages>157--170</pages>
<contexts>
<context position="6599" citStr="Palmer and Goldberg 2007" startWordPosition="957" endWordPosition="960">ent from our setting: Error is measured as the probability mass of strings that are not identified correctly by the learned finite state automaton, instead of measuring KL divergence between the automaton and the true distribution. In addition, in many cases, there is also a focus on the distribution-free setting. To the best of our knowledge, it is still an open problem whether finite state automata are learnable in the distribution-dependent setting when measuring the error as the fraction of misidentified strings. Other work (Ron 1995; Ron, Singer, and Tishby 1998; Clark and Thollard 2004; Palmer and Goldberg 2007) also gives treatment to probabilistic automata with an error measure which is more suitable for the probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance. 480 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars These also focus on learning the structure of finite state machines. As mentioned earlier, in our setting we assume that the grammar is fixed, and that our goal is to estimate its parameters. We note an important connection to an earlier study about the learnability of probabilistic automata and hidden Markov models by Abe and Warmuth </context>
</contexts>
<marker>Palmer, Goldberg, 2007</marker>
<rawString>Palmer, N. and P. W. Goldberg. 2007. PAC-learnability of probabilistic deterministic finite state automata in terms of variation distance. In Proceedings of Algorithmic Learning Theory, pages 157–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="4464" citStr="Pereira and Schabes 1992" startWordPosition="638" endWordPosition="641">er practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a fixed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically j</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Pereira, F. C. N. and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings of the Association for Computational Linguistics, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pitt</author>
</authors>
<title>Inductive inference, DFAs, and computational complexity. Analogical and Inductive Inference,</title>
<date>1989</date>
<pages>397--18</pages>
<contexts>
<context position="5910" citStr="Pitt 1989" startWordPosition="851" endWordPosition="852">aximum likelihood estimation is put in the empirical risk minimization framework (Vapnik 1998) with the loss function being the log-loss. Following that, we develop a set of learning theoretic tools to explore rates of estimation convergence for probabilistic grammars. We also develop algorithms for performing empirical risk minimization. Much research has been devoted to the problem of learning finite state automata (which can be thought of as a class of grammars) in the Probably Approximately Correct setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989; Pitt 1989; Terwijn 2002). Typically, the setting in these cases is different from our setting: Error is measured as the probability mass of strings that are not identified correctly by the learned finite state automaton, instead of measuring KL divergence between the automaton and the true distribution. In addition, in many cases, there is also a focus on the distribution-free setting. To the best of our knowledge, it is still an open problem whether finite state automata are learnable in the distribution-dependent setting when measuring the error as the fraction of misidentified strings. Other work (R</context>
</contexts>
<marker>Pitt, 1989</marker>
<rawString>Pitt, L. 1989. Inductive inference, DFAs, and computational complexity. Analogical and Inductive Inference, 397:18–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pollard</author>
</authors>
<title>Convergence of Stochastic Processes.</title>
<date>1984</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="52736" citStr="Pollard (1984" startWordPosition="9089" endWordPosition="9090">nvergence of the empirical process supfE-Tn |E˜pn � f~ − Ep is a general-purpose result that has been used frequently to prove the convergence of empirical processes of the type we discuss in this article. Lemma 2 Let Tn be a permissible class9 of functions such that for every f E Tn we have E[|f |x I {|f |&lt; Kn}] &lt; ebound(n). Let Ttruncated,n = {f x I {f &lt; Kn} |f E Tm}, namely, the set of functions from Tn after being truncated by Kn. Then for e &gt; 0 we have ~ p sup |E˜pn [f] − Ep [f] |&gt; 2e &lt; 8N(e/8,Ttruncated,n) exp (−1 128ne2/Kn) +ebound(n)/e fE-Tn provided n &gt; K2n/4e2 and ebound(n) &lt; e. See Pollard (1984; Chapter 2, pages 30–31) for the proof of Lemma 2. See also Appendix A. Covering numbers are rather complex combinatorial quantities which are hard to compute directly. Fortunately, they can be bounded using the pseudo-dimension (Anthony and Bartlett 1999), a generalization of the Vapnik-Chervonenkis (VC) dimension for real functions. In the case of our “binomialized” probabilistic grammars, the pseudo-dimension of Tn is bounded by N, because we have Tn C T, and the functions in T are linear with N parameters. Hence, Ttruncated,n also has pseudodimension that is at most N. We then have the fo</context>
<context position="84449" citStr="Pollard 1984" startWordPosition="14912" endWordPosition="14913">s of the features are large, regardless of the number of samples we have. With our framework, smoothing depends only on the number of samples we have. These two scenarios are related, of course: The more samples we have, the more likely it is that the counts of the events will grow large. 7.3 Other Derivations of Sample Complexity Bounds In this section, we discuss other possible solutions to the problem of deriving sample complexity bounds for probabilistic grammars. 7.3.1 Using Talagrand’s Inequality. Our bounds are based on VC theory together with classical results for empirical processes (Pollard 1984). There have been some recent developments to the derivation of rates of convergence in statistical learning theory (Massart 2000; Bartlett, Bousquet, and Mendelson 2005; Koltchinskii 2006), most prominently through the use of Talagrand’s inequality (Talagrand 1994), which is a concentration of measure inequality, in the spirit of Lemma 2. The bounds achieved with Talagrand’s inequality are also distribution-dependent, and are based on the diameter of the e-minimal set—the set of hypotheses which have an excess risk smaller than e. We saw in Section 7.1 that the diameter of the e-minimal set d</context>
<context position="96279" citStr="Pollard (1984" startWordPosition="17190" endWordPosition="17191">og2 m for some constant K &gt; 0. Finally, for some β&apos;(L, p, q, N) = β&apos; &gt; 0 and some constant M, �qlog2 m~ if m &gt; M then K log m ≤ m−β, logm. Utility Lemma 2 For ai, bi ≥ 0, if − log Ei ai + log Ei bi ≥ c then there exists an i such that − log ai + log bi ≥ c. Proof Assume − log ai + log bi &lt; c for all i. Then, bi/ai &lt; eE, therefore Ei bi/ Ei ai &lt; eE, therefore − log Ei ai + log Ei bi &lt; c which is a contradiction to − log Ei ai + log Ei bi ≥ c. ■ The next lemma is the main concentation of measure result that we use. Its proof requires some simple modification to the proof given for Theorem 24 in Pollard (1984, pages 30–31). Lemma 2 Let Fn be a permissible class of functions such that for every f ∈ Fn we have E[|f |× I {|f |≤ Kn}] ≤ cbound(n). Let Ftruncated,n = { f × I { f ≤ Kn} |f ∈ Fm}, that is, the set of functions from Fn after being truncated by Kn. Then for c &gt; 0 we have ~ p sup|E˜pn [f] − Ep [f] |&gt; 2c ≤ 8N(c/8,Ftruncated,n)exp(−1 128nc2/9) + cbound(n)/c fE-Tn provided n ≥ K2n/4c2 and cbound(n) &lt; c. Proof First note that sup |E˜pn � f~ − Ep � f~ |≤ sup |E˜pn � fI{|f |≤ Kn}� − Ep � fI{|f |≤ Kn}� | fE-Tn fE-Tn + sup E˜pn [|f |I{|f |≤ Kn}] + sup Ep [|f |I{|f |≤ Kn}] fE-Tn f E-Tn 517 Computation</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>Pollard, D. 1984. Convergence of Stochastic Processes. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ron</author>
</authors>
<title>Automata Learning and Its Applications.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Hebrew University of Jerusalem.</institution>
<contexts>
<context position="6517" citStr="Ron 1995" startWordPosition="946" endWordPosition="947">9; Terwijn 2002). Typically, the setting in these cases is different from our setting: Error is measured as the probability mass of strings that are not identified correctly by the learned finite state automaton, instead of measuring KL divergence between the automaton and the true distribution. In addition, in many cases, there is also a focus on the distribution-free setting. To the best of our knowledge, it is still an open problem whether finite state automata are learnable in the distribution-dependent setting when measuring the error as the fraction of misidentified strings. Other work (Ron 1995; Ron, Singer, and Tishby 1998; Clark and Thollard 2004; Palmer and Goldberg 2007) also gives treatment to probabilistic automata with an error measure which is more suitable for the probabilistic setting, such as Kullback-Lielder (KL) divergence or variation distance. 480 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars These also focus on learning the structure of finite state machines. As mentioned earlier, in our setting we assume that the grammar is fixed, and that our goal is to estimate its parameters. We note an important connection to an earlier study about the l</context>
</contexts>
<marker>Ron, 1995</marker>
<rawString>Ron, D. 1995. Automata Learning and Its Applications. Ph.D. thesis, Hebrew University of Jerusalem.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D Ron</author>
<author>Y Singer</author>
<author>N Tishby 1998</author>
</authors>
<title>On the learnability and usage of acyclic probabilistic finite automata.</title>
<journal>Journal of Computer and System Sciences, 56(2):133–152. Computational Linguistics</journal>
<volume>38</volume>
<marker>Ron, Singer, 1998, </marker>
<rawString>Ron, D., Y. Singer, and N. Tishby.1998. On the learnability and usage of acyclic probabilistic finite automata. Journal of Computer and System Sciences, 56(2):133–152. Computational Linguistics Volume 38, Number 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>O Shamir</author>
<author>K Sridharan</author>
<author>N Srebro</author>
</authors>
<title>Learnability and stability in the general learning setting.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Learning Theory.</booktitle>
<marker>Shalev-Shwartz, Shamir, Sridharan, Srebro, 2009</marker>
<rawString>Shalev-Shwartz, S., O. Shamir, K. Sridharan, and N. Srebro. 2009. Learnability and stability in the general learning setting. In Proceedings of the Conference on Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sipser</author>
</authors>
<title>Introduction to the Theory of Computation, Second Edition.</title>
<date>2006</date>
<publisher>Thomson Course Technology,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="66864" citStr="Sipser 2006" startWordPosition="11744" endWordPosition="11745">ximization of Equation (23) to a decision problem. We do so by stating the following decision problem. Problem 1(Unsupervised Minimization of the Log-Loss with Margin) Input: A binarized context-free grammar G, a set of sentences x1, ... , xn, a value -y E [0, 12), and a value α E [0, 1]. Output:1 if there exists 0 E ©(-y) (and hence, h E X(G)) such that E− ˜pn(x) log E h(x,z 1 0) &lt; −log(α) (24) x z and 0 otherwise. We will show the hardness result both when -y is not restricted at all as well as when we allow -y &gt; 0. The proof of the hardness result is achieved by reducing the problem 3-SAT (Sipser 2006), known to be NP-complete, to Problem 1. The problem 3-SAT is defined as follows: Problem 2 (3-SAT) Input: A formula φ _ Ami=1 (ai V bi V ci) in conjunctive normal form, such that each clause has three literals. Output:1 if there is a satisfying assignment for φ, and 0 otherwise. Given an instance of the 3-SAT problem, the reduction will, in polynomial time, create a grammar and a single string such that solving Problem 1 for this grammar and string will yield a solution for the instance of the 3-SAT problem. Let φ _ Ami=1 (ai V bi V ci) be an instance of the 3-SAT problem, where ai, bi, and c</context>
</contexts>
<marker>Sipser, 2006</marker>
<rawString>Sipser, M. 2006. Introduction to the Theory of Computation, Second Edition. Thomson Course Technology, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Talagrand</author>
</authors>
<title>Sharper bounds for Gaussian and empirical processes.</title>
<date>1994</date>
<booktitle>Annals of Probability,</booktitle>
<pages>22--28</pages>
<contexts>
<context position="84715" citStr="Talagrand 1994" startWordPosition="14948" endWordPosition="14949">e events will grow large. 7.3 Other Derivations of Sample Complexity Bounds In this section, we discuss other possible solutions to the problem of deriving sample complexity bounds for probabilistic grammars. 7.3.1 Using Talagrand’s Inequality. Our bounds are based on VC theory together with classical results for empirical processes (Pollard 1984). There have been some recent developments to the derivation of rates of convergence in statistical learning theory (Massart 2000; Bartlett, Bousquet, and Mendelson 2005; Koltchinskii 2006), most prominently through the use of Talagrand’s inequality (Talagrand 1994), which is a concentration of measure inequality, in the spirit of Lemma 2. The bounds achieved with Talagrand’s inequality are also distribution-dependent, and are based on the diameter of the e-minimal set—the set of hypotheses which have an excess risk smaller than e. We saw in Section 7.1 that the diameter of the e-minimal set does not follow the Tsybakov noise condition, but it is perhaps possible to find meaningful bounds for it, in which case we may be able to get tighter bounds using Talagrand’s inequality. We note that it may be possible to obtain data-dependent bounds for the diamete</context>
</contexts>
<marker>Talagrand, 1994</marker>
<rawString>Talagrand, M. 1994. Sharper bounds for Gaussian and empirical processes. Annals of Probability, 22:28–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Terwijn</author>
</authors>
<title>On the learnability of hidden Markov models. In</title>
<date>2002</date>
<booktitle>Grammatical Inference: Algorithms and Applications (Lecture Notes in Computer Science).</booktitle>
<pages>344--348</pages>
<publisher>Springer,</publisher>
<location>Berlin,</location>
<contexts>
<context position="5925" citStr="Terwijn 2002" startWordPosition="853" endWordPosition="854">lihood estimation is put in the empirical risk minimization framework (Vapnik 1998) with the loss function being the log-loss. Following that, we develop a set of learning theoretic tools to explore rates of estimation convergence for probabilistic grammars. We also develop algorithms for performing empirical risk minimization. Much research has been devoted to the problem of learning finite state automata (which can be thought of as a class of grammars) in the Probably Approximately Correct setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989; Pitt 1989; Terwijn 2002). Typically, the setting in these cases is different from our setting: Error is measured as the probability mass of strings that are not identified correctly by the learned finite state automaton, instead of measuring KL divergence between the automaton and the true distribution. In addition, in many cases, there is also a focus on the distribution-free setting. To the best of our knowledge, it is still an open problem whether finite state automata are learnable in the distribution-dependent setting when measuring the error as the fraction of misidentified strings. Other work (Ron 1995; Ron, S</context>
</contexts>
<marker>Terwijn, 2002</marker>
<rawString>Terwijn, S. A. 2002. On the learnability of hidden Markov models. In P. Adriaans, H. Fernow, &amp; M. van Zaane. Grammatical Inference: Algorithms and Applications (Lecture Notes in Computer Science). Springer, Berlin, pages 344–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tsybakov</author>
</authors>
<title>Optimal aggregation of classifiers in statistical learning.</title>
<date>2004</date>
<journal>The Annals of Statistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="33910" citStr="Tsybakov 2004" startWordPosition="5645" endWordPosition="5646">)/log 1r Hd(p,Z1) ≤ [(1 + log L )/log r] A ([(1 + log L)/log 1r]) (12) where we use the monotonicity of A. Consider Hd(p, Z2) (the “low probability” derivations). We have: Hd(p,Z2) ≤ − E Lr|z |log �Lr|z|� zEZ2 ≤ − log L − (L log r) E |z|r|z| zEZ2 ≤ − log L − (L log r) 00 A(k)krk E k=1 00 ≤ − log L − (L log r) E kqk (13) k=1 = − log L + (i log log q1 (14) where Equation (13) holds from the assumptions about p. Putting Equation (12) and Equation (14) together, we obtain the result. ■ We note that another common way to quantify the noise in a distribution is through the notion of Tsybakov noise (Tsybakov 2004; Koltchinskii 2006). We discuss this further in Section 7.1, where we show that Tsybakov noise is too permissive, and probabilistic grammars do not satisfy its conditions. 3.2 Limiting the Degree of the Grammar When approximating a family of probabilistic grammars, it is much more convenient when the degree of the grammar is limited. In this article, we limit the degree of the grammar by making the assumption that all Nk ≤ 2. This assumption may seem, at first glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence, other formalisms), this assumption does not limit</context>
<context position="79127" citStr="Tsybakov 2004" startWordPosition="14004" endWordPosition="14005">le, we chose to introduce assumptions about distributions that generate natural language data. The choice of these assumptions was motivated by observations about properties shared among treebanks. The main consequence of making these assumptions is bounding the amount of noise in the distribution (i.e., the amount of variation in probabilities across labels given a fixed input). There are other ways to restrict the noise in a distribution. One condition for such noise restriction, which has received considerable recent attention in the statistical literature, is the Tsybakov noise condition (Tsybakov 2004; Koltchinskii 2006). Showing that a distribution satisfies the Tsybakov noise condition enables the use of techniques (e.g., from Koltchinskii 2006) for deriving distribution-dependent sample complexity bounds that depend on the parameters of the noise. It is therefore of interest to see whether Tsybakov noise holds under the assumptions presented in Section 3.1. We show that this is not the case, and that Tsybakov noise is too permissive. In fact, we show that p can be a probabilistic grammar itself (and hence, satisfy the assumptions in Section 3.1), and still not satisfy the Tsybakov noise</context>
</contexts>
<marker>Tsybakov, 2004</marker>
<rawString>Tsybakov, A. 2004. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics, 32(1):135–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley-Interscience,</publisher>
<location>New York.</location>
<contexts>
<context position="5395" citStr="Vapnik 1998" startWordPosition="770" endWordPosition="771">logical cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justified assumptions about the distributions that generate the data. Our framework uses and significantly extends ideas that have been introduced for deriving sample complexity bounds for probabilistic graphical models (Dasgupta 1997). Maximum likelihood estimation is put in the empirical risk minimization framework (Vapnik 1998) with the loss function being the log-loss. Following that, we develop a set of learning theoretic tools to explore rates of estimation convergence for probabilistic grammars. We also develop algorithms for performing empirical risk minimization. Much research has been devoted to the problem of learning finite state automata (which can be thought of as a class of grammars) in the Probably Approximately Correct setting, leading to the conclusion that it is a very hard problem (Kearns and Valiant 1989; Pitt 1989; Terwijn 2002). Typically, the setting in these cases is different from our setting:</context>
<context position="18841" citStr="Vapnik 1998" startWordPosition="2990" endWordPosition="2991"> Rn(Q) do not hold because the log-loss can be unbounded. This means that a modification is required for the empirical process in a way that will actually guarantee some kind of convergence. We give a treatment to this in the next section. We note that all discussion of convergence in this section has been about convergence in probability. For example, we want Equation (6) to hold with high probability— for most samples of size n. We will make this notion more rigorous in Section 2.2. 2.1 Empirical Risk Minimization and Structural Risk Minimization Methods It has been noted in the literature (Vapnik 1998; Koltchinskii 2006) that often the class Q is too complex for empirical risk minimization using a fixed number of data points. It is therefore desirable in these cases to create a family of subclasses {Qα |oc E A} that have increasing complexity. The more data we have, the more complex our Qα can be for empirical risk minimization. Structural risk minimization (Vapnik 1998) and the method of sieves (Grenander 1981) are examples of methods that adopt such an approach. Structural risk minimization, for example, can be represented in many cases as a penalization of the empirical risk method, usi</context>
<context position="21288" citStr="Vapnik 1998" startWordPosition="3408" endWordPosition="3409">eans that Ep [− log qn] -+ Ep [− log q∗] . Because we have Un Qn = Q, the implication of having asymptotic empirical risk minimization is that we have Ep(q∗n; Qn) -+ Ep(q∗; Q). 2.2 Sample Complexity Bounds Knowing that we are interested in the convergence of Rn(Qn) = supq∈Qn |E˜pn [− log q] − Ep [−log q] |, a natural question to ask is: “At what rate does this empirical process converge?” Because the quantity Rn(Qn) is a random variable, we need to give a probabilistic treatment to its convergence. More specifically, we ask the question that is typically asked when learnability is considered (Vapnik 1998): “How many samples n are required so that with probability 1 − b we have Rn(Qn) &lt; c?” Bounds on this number of samples are also called “sample complexity bounds,” and in a distribution-free setting they are described as a function N(c, b, Q), independent of the distribution p that generates the data. A complete distribution-free setting is not appropriate for analyzing natural language. This setting poses technical difficulties with the convergence of Rn(Qn) and needs to take into account pathological cases that can be ruled out in natural language data. Instead, we will make assumptions abou</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vapnik, V. N. 1998. Statistical Learning Theory. Wiley-Interscience, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>