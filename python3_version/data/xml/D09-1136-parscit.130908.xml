<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.985509">
Bayesian Learning of Phrasal Tree-to-String Templates
</title>
<author confidence="0.999451">
Ding Liu and Daniel Gildea
</author>
<affiliation confidence="0.9972695">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.72073">
Rochester, NY 14627
</address>
<email confidence="0.998211">
{dliu, gildea}@cs.rochester.edu
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999762">
We examine the problem of overcoming
noisy word-level alignments when learn-
ing tree-to-string translation rules. Our
approach introduces new rules, and re-
estimates rule probabilities using EM. The
major obstacles to this approach are the
very reasons that word-alignments are
used for rule extraction: the huge space
of possible rules, as well as controlling
overfitting. By carefully controlling which
portions of the original alignments are re-
analyzed, and by using Bayesian infer-
ence during re-analysis, we show signifi-
cant improvement over the baseline rules
extracted from word-level alignments.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999798016393443">
Non-parametric Bayesian methods have been suc-
cessfully applied to directly learn phrase pairs
from a bilingual corpus with little or no depen-
dence on word alignments (Blunsom et al., 2008;
DeNero et al., 2008). Because such approaches di-
rectly learn a generative model over phrase pairs,
they are theoretically preferable to the standard
heuristics for extracting the phrase pairs from the
many-to-one word-level alignments produced by
the IBM series models (Brown et al., 1993) or
the Hidden Markov Model (HMM) (Vogel et al.,
1996). We wish to apply this direct, Bayesian ap-
proach to learn better translation rules for syntax-
based statistical MT (SSMT), by which we specif-
ically refer to MT systems using Tree-to-String
(TTS) translation templates derived from syntax
trees (Liu et al., 2006; Huang et al., 2006; Gal-
ley et al., 2006; May and Knight, 2007), as op-
posed to formally syntactic systems such as Hi-
ero (Chiang, 2007). The stumbling block pre-
venting us from taking this approach is the ex-
tremely large space of possible TTS templates
when no word alignments are given. Given a sen-
tence pair and syntax tree over one side, there
are an exponential number of potential TTS tem-
plates and a polynomial number of phrase pairs.
In this paper, we explore methods for restricting
the space of possible TTS templates under con-
sideration, while still allowing good templates to
emerge directly from the data as much as possible.
We find an improvement in translation accuracy
through, first, using constraints to limit the number
of new templates, second, using Bayesian methods
to limit which of these new templates are favored
when re-analyzing the training data with EM, and,
third, experimenting with different renormaliza-
tion techniques for the EM re-analysis.
We introduce two constraints to limit the num-
ber of TTS templates that we extract directly from
tree/string pairs without using word alignments.
The first constraint is to limit direct TTS tem-
plate extraction to the part of the corpus where
word alignment tools such as GIZA++ do poorly.
There is no reason not to re-use the good align-
ments from GIZA++, which holds a very compet-
itive baseline performance. As already mentioned,
the noisy alignments from GIZA++ are likely
to cross the boundaries of the tree constituents,
which leads to comparatively big TTS templates.
We use this fact as a heuristic to roughly distin-
guish noisy from good word alignments.1 Here
we define big templates as those with more than
8 symbols in their right hand sides (RHSs). The
word alignments in big templates are considered
to be noisy and will be recomposed by extracting
smaller TTS templates. Another reason to do ex-
traction on big templates is that the applicability
of big templates to new sentences is very limited
due to their size, and the portion of the training
data from which they are extracted is effectively
wasted. The second constraint, after choosing the
</bodyText>
<footnote confidence="0.9938095">
1Precisely differentiating the noisy/good word alignments
is as hard as correctly aligning the words.
</footnote>
<page confidence="0.892162">
1308
</page>
<note confidence="0.996624">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1308–1317,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.998056142857143">
extraction site, is to extract the TTS templates all
the way down to the leaves of the hosting tem-
plates. This constraint limits the number of possi-
ble left hand sides (LHSs) to be equal to the num-
ber of tree nodes in the hosting templates. The
entire extraction process can be summarized in 3
steps:
</bodyText>
<listItem confidence="0.9740189">
1. Compute word alignments using GIZA++,
and generate the basic TTS templates.
2. Select big templates from the basic TTS tem-
plates in step 1, and extract smaller TTS tem-
plates all the way down to the bottom from
big templates, without considering the pre-
computed word alignments.
3. Combine TTS templates from step 1 and step
2 and estimate their probabilities using Vari-
ational Bayes with a Dirichlet Process prior.
</listItem>
<bodyText confidence="0.994964625">
In step 2, since there are no constraints from the
pre-computed word alignments, we have complete
freedom in generating all possible TTS templates
to overcome noisy word alignments. We use vari-
ational EM to approximate the inference of our
Bayesian model and explore different normaliza-
tion methods for the TTS templates. A two-stage
normalization is proposed by combining LHS-
based normalization with normalization based on
the root of the LHS, and is shown to be the best
model when used with variational EM.
Galley et al. (2006) recompose the TTS tem-
plates by inserting unaligned target words and
combining small templates into bigger ones. The
recomposed templates are then re-estimated using
the EM algorithm described in Graehl and Knight
(2004). This approach also generates TTS tem-
plates beyond the precomputed word alignments,
but the freedom is only granted over unaligned tar-
get words, and most of the pre-computed word
alignments remain unchanged. Other prior ap-
proaches towards improving TTS templates fo-
cus on improving the word alignment performance
over the classic models such as IBM series mod-
els and Hidden Markov Model (HMM), which do
not consider the syntactic structure of the align-
ing languages and produce syntax-violating align-
ments. DeNero and Klein (2007) use a syntax-
based distance in an HMM word alignment model
to favor syntax-friendly alignments. Fossum et al.
(2008) start from the GIZA++ alignment and in-
crementally delete bad links based on a discrim-
</bodyText>
<figureCaption confidence="0.9998325">
Figure 1: 5 small TTS templates are extracted based on the
correct word alignments (left), but only 1 big TTS template
(right) can be extracted when the cross-boundary noisy align-
ments are added in.
</figureCaption>
<bodyText confidence="0.999896041666667">
inative model with syntactic features. This ap-
proach can only find a better subset of the GIZA++
alignment and requires a parallel corpus with gold-
standard word alignment for training the discrim-
inative model. May and Knight (2007) factorize
the word alignment into a set of re-orderings rep-
resented by the TTS templates and build a hierar-
chical syntax-based word alignment model. The
problem is that the TTS templates are generated
by the word alignments from GIZA++, which lim-
its the potential of the syntactic re-alignment. As
shown by these prior approaches, directly improv-
ing the word alignment either falls into the frame-
work of many-to-one alignment, or is substantially
confined by the word alignment it builds upon.
The remainder of the paper focuses on the
Bayesian approach to learning TTS templates and
is organized as follows: Section 2 describes the
procedure for generating the candidate TTS tem-
plates; Section 3 describes the inference methods
used to learn the TTS templates; Section 4 gives
the empirical results, Section 5 discusses the char-
acteristics of the learned TTS templates, and Sec-
tion 6 presents the conclusion.
</bodyText>
<sectionHeader confidence="0.86328" genericHeader="method">
2 Extracting Phrasal TTS Templates
</sectionHeader>
<bodyText confidence="0.999924416666667">
The Tree-to-String (TTS) template, the most im-
portant component of a SSMT system, usually
contains three parts: a fragment of a syntax tree
in its left hand side (LHS), a sequence of words
and variables in its right hand side (RHS), and
a probability indicating how likely the template
is to be used in translation. The RHS of a TTS
template shows one possible translation and re-
ordering of its LHS. The variables in a TTS tem-
plate are further transformed using other TTS tem-
plates, and the recursive process continues until
there are no variables left. There are two ways
</bodyText>
<page confidence="0.990755">
1309
</page>
<figure confidence="0.539540384615385">
Union
Heuristic
33.0%
28.6%
9-12 13-20 ≥21
Ch-En 18.2% 17.4% 64.4%
En-Ch 15.9% 20.7% 63.4%
Union 9.8% 15.1% 75.1%
Heuristic 24.6% 27.9% 47.5%
Ch-En
En-Ch
45.9%
20.1%
</figure>
<tableCaption confidence="0.8840595">
Table 1: Percentage of corpus used to generate big templates,
based on different word alignments
</tableCaption>
<figure confidence="0.953830888888889">
S
NP1 PP AUX ADJ
1
NP3 的 NP1 很美
in NP3 is
of
NP VP
2 3 4
beautiful
</figure>
<figureCaption confidence="0.9846186">
Figure 2: Examples of valid and invalid templates extracted
from a big Template. Template 1, invalid, doesn’t go all the
way down to the bottom. Template 2 is valid. Template 3, in-
valid, doesn’t have the same set of variables in its LHS/RHS.
Template 4, invalid, is not a phrasal TTS template.
</figureCaption>
<bodyText confidence="0.999654052631579">
that TTS templates are commonly used in ma-
chine translation. The first is synchronous pars-
ing (Galley et al., 2006; May and Knight, 2007),
where TTS templates are used to construct syn-
chronous parse trees for an input sentence, and
the translations will be generated once the syn-
chronous trees are built up. The other way is
the TTS transducer (Liu et al., 2006; Huang et
al., 2006), where TTS templates are used just as
their name indicates: to transform a source parse
tree (or forest) into the proper target string. Since
synchronous parsing considers all possible syn-
chronous parse trees of the source sentence, it is
less constrained than TTS transducers and hence
requires more computational power. In this paper,
we use a TTS transducer to test the performance of
different TTS templates, but our techniques could
also be applied to SSMT systems based on syn-
chronous parsing.
</bodyText>
<subsectionHeader confidence="0.987693">
2.1 Baseline Approach: TTS Templates
Obeying Word Alignment
</subsectionHeader>
<bodyText confidence="0.969417176470588">
TTS templates are commonly generated by de-
composing a pair of aligned source syntax tree
and target string into smaller pairs of tree frag-
ments and target string (i.e., the TTS templates).
To keep the number of TTS templates to a manage-
able scale, only the non-decomposable TTS tem-
plates are generated. This algorithm is referred to
as GHKM (Galley et al., 2004) and is widely used
in SSMT systems (Galley et al., 2006; Liu et al.,
2006; Huang et al., 2006). The word alignment
used in GHKM is usually computed independent
of the syntactic structure, and as DeNero and Klein
(2007) and May and Knight (2007) have noted,
Table 2: In the selected big templates, the distribution of
words in the templates of different sizes, which are measured
based on the number of symbols in their RHSs
is not the best for SSMT systems. In fact, noisy
word alignments cause more damage to a SSMT
system than to a phrase based SMT system, be-
cause the TTS templates can only be derived from
tree constituents. If some noisy alignments happen
to cross over the boundaries of two constituents,
as shown in Figure 2, a much bigger tree frag-
ment will be extracted as a TTS template. Even
though the big TTS templates still carry the orig-
inal alignment information, they have much less
chance of getting matched beyond the syntax tree
where they were extracted, as we show in Sec-
tion 4. In other words, a few cross-boundary noisy
alignments could disable a big portion of a training
syntax tree, while for a phrase-based SMT system,
their effect is limited to the phrases they align. As
a rough measure of how the training corpus is af-
fected by the big templates, we calculated the dis-
tribution of target words in big and non-big TTS
templates. The word alignment is computed using
GIZA++2 for the selected 73,597 sentence pairs in
the FBIS corpus in both directions and then com-
bined using union and heuristic diagonal growing
(Koehn et al., 2003). Table 1 shows that big
templates consume 20.1% to 45.9% of the training
corpus depending on different types of word align-
ments. The statistics indicate that a significant
portion of the training corpus is simply wasted,
if the TTS templates are extracted based on word
alignments from GIZA++. On the other hand, it
shows the potential for improving an SSMT sys-
tem if we can efficiently re-use the wasted train-
ing corpus. By further examining the selected big
templates, we find that the most common form of
big templates is a big skeleton template starting
</bodyText>
<footnote confidence="0.8132475">
2GIZA++ is available at
http://www.fjoch.com/GIZA++.html
</footnote>
<page confidence="0.978145">
1310
</page>
<bodyText confidence="0.999981230769231">
from the root of the source syntax tree, and hav-
ing many terminals (words) misaligned in the bot-
tom. Table 2 shows, in the selected big templates,
the distribution of words in the templates of differ-
ent sizes (measured based on the number of sym-
bols in their RHS). We can see that based on ei-
ther type of word alignment, the most common
big templates are the TTS templates with more
than 20 symbols in their RHSs, which are gen-
erally the big skeleton templates. The advantage
of such big skeleton templates is that they usually
have good marginal accuracy3 and allow accurate
smaller TTS templates to emerge.
</bodyText>
<subsectionHeader confidence="0.9979115">
2.2 Liberating Phrasal TTS Templates From
Noisy Word Alignments
</subsectionHeader>
<bodyText confidence="0.999936">
To generate better TTS templates, we use a more
direct way than modifying the underlying word
alignment: extract smaller phrasal TTS tem-
plates from the big templates without looking at
their pre-computed word alignments. We define
phrasal TTS templates as those with more than
one symbol (word or non-terminal) in their LHS.
The reason to consider only phrasal TTS tem-
plates is that they are more robust than the word-
level TTS templates in addressing the complicated
word alignments involved in big templates, which
are usually not the simple type of one-to-many or
many-to-one. Abandoning the pre-computed word
alignments in big templates, an extracted smaller
TTS template can have many possible RHSs, as
long as the two sides have the same set of vari-
ables. Note that the freedom is only given to the
alignments of the words; for the variables in the
big templates, we respect the pre-computed word
alignments. To keep the extracted smaller TTS
templates to a manageable scale, the following two
constraints are applied:
</bodyText>
<listItem confidence="0.9942982">
1. The LHS of extracted TTS templates should
go all the way down to the bottom of the LHS
of the big templates. This constraint ensures
that at most N LHSs can be extracted from
one big Template, where N is the number of
tree nodes in the big Template’s LHS.
2. The number of leaves (including both words
and variables) in an extracted TTS template’s
LHS should not exceed 6. This constraint
limits the size of the extracted TTS templates.
</listItem>
<bodyText confidence="0.5884165">
3Here, marginal accuracy means the correctness of the
TTS template’s RHS corresponding to its LHS.
</bodyText>
<equation confidence="0.672861">
( VP ( AUX is ) ( ADJ beautiful ) ) 很美
( PP ( IN of ) NP3 ) NP3 的
( NP NP1 ( PP ( IN of ) NP3 ) ) NP3 的 NP1
( NP NP1 ( PP ( IN of ) NP3 ) ) NP3 的 NP1 很美
</equation>
<figureCaption confidence="0.9823115">
Figure 3: All valid templates that can be extracted from the
example in Figure 2.1
</figureCaption>
<bodyText confidence="0.405022666666667">
for all template t do
if size(t.rhs) &gt; 8 then
for all tree node s in t.lhs do
</bodyText>
<construct confidence="0.89573">
subt = subtree(s, t.lhs);
if leaf num(subt) ≤ 6 then
for i=1:size(t.rhs) do
for j=i:size(t.rhs) do
if valid(subt, i, j) then
create template(subt, i, j);
</construct>
<figureCaption confidence="0.9996055">
Figure 4: Algorithm that liberates smaller TTS Templates
from big templates
</figureCaption>
<bodyText confidence="0.999961482758621">
As we show in Section 4, use of bigger TTS
templates brings very limited performance
gain.
Figure 2.2 describes the template liberating algo-
rithm running in O(NM2), where N denotes the
number of tree nodes in the LHS of the input big
Template and M denotes the length of the RHS. In
the algorithm, function valid returns true if there
are the same set of variables in the left/right hand
side of an extracted TTS template; subtree(x, y)
denotes the sub-tree in y which is rooted at x and
goes all the way down to y’s bottom. Figure 2.1
shows valid and invalid TTS templates which can
be extracted from an example hosting TTS tem-
plate. Note that, in order to keep the example
simple, the hosting TTS template only has 4 sym-
bols in its RHS, which does not qualify as a big
template according to our definition. Figure 2.2
shows the complete set of valid TTS templates
which can be extracted from the example TTS
template. The subscripts of the non-terminals are
used to differentiate identical non-terminals in dif-
ferent positions. The extraction process blindly
releases smaller TTS templates from the big tem-
plates, among which only a small fraction are cor-
rect TTS templates. Therefore, we need an infer-
ence method to raise the weight of the correct tem-
plates and decrease the weight of the noisy tem-
plates.
</bodyText>
<page confidence="0.986806">
1311
</page>
<sectionHeader confidence="0.993719" genericHeader="method">
3 Estimating TTS Template Probability
</sectionHeader>
<bodyText confidence="0.999961818181818">
The Expectation-Maximization (EM) algorithm
(Dempster et al., 1977) can be used to estimate
the TTS templates’ probabilities, given a genera-
tive model addressing how a pair of source syn-
tax tree and target string is generated. There are
two commonly used generative models for syntax-
based MT systems, each of which corresponds to
a normalization method for the TTS templates.
The LHS-based normalization (LHSN) (Liu et al.,
2006; Huang et al., 2006), corresponds to the
generative process where the source syntax sub-
tree is first generated, and then the target string
is generated given the source syntax subtree. The
other one is normalization based on the root of
the LHS (ROOTN) (Galley et al., 2006), corre-
sponding to the generative process where, given
the root of the syntax subtree, the LHS syntax sub-
tree and the RHS string are generated simultane-
ously. By omitting the decomposition probability
in the LHS-based generative model, the two gen-
erative models share the same formula for comput-
ing the probability of a training instance:
</bodyText>
<equation confidence="0.609481">
�Pr(t)
</equation>
<bodyText confidence="0.999919695652174">
where T and S denote the source syntax tree and
target string respectively, R denotes the decompo-
sition of (T, S), and t denotes the TTS template.
The expected counts of the TTS templates can then
be efficiently computed using an inside-outside-
like dynamic programming algorithm (May and
Knight, 2007).
LHSN, as shown by Galley et al. (2006), cannot
accurately restore the true conditional probabili-
ties of the target sentences given the source sen-
tences in the training corpus. This indicates that
LHSN is not good at predicting unseen sentences
or at translating new sentences. But this deficiency
does not affect its ability to estimate the expected
counts of the TTS templates, because the posteri-
ors of the TTS templates only depend on the com-
parative probabilities of the different derivations
of a training instance (a pair of tree and string).
In fact, as we show in Section 4, LHSN is bet-
ter than ROOTN in liberating smaller TTS tem-
plates out of the big templates, since it is less bi-
ased to the big templates in the EM training.4 Be-
cause the two normalization methods have their
</bodyText>
<footnote confidence="0.9807235">
4Based on LHSN, the difference between the probabil-
ity of a big Template and the product of the probabilities of
</footnote>
<table confidence="0.550966">
E-step:
for all pair of syntax tree T and target string S do
for all TTS Template t do
EC(t)+ = ER:t∈R Pr(T,S,R)β ;
ER0 Pr(T S R0)β
Increase β;
M-step:
for all TTS Template t do
</table>
<equation confidence="0.898208428571428">
if it is the last iteration then
EC(t)
Pr(t) = Et0:t0.root=t.root EC(t0);
else
EC(t)
Pr(t) =
Et0:t0.lhs=t.lhs EC(t0);
</equation>
<figureCaption confidence="0.9929">
Figure 5: EM Algorithm For Estimating TTS Templates
</figureCaption>
<bodyText confidence="0.999899333333333">
own strength and weakness, both of them are used
in our EM algorithm: LHSN is used in all EM
iterations except the last one to compute the ex-
pected counts of the TTS templates, and ROOTN
is used in the last EM iteration to compute the final
probabilities of the TTS templates. This two-stage
normalization method is denoted as MIXN in this
paper.
Deterministic Annealing (Rose et al., 1992) is
is used in our system to speed up the training
process, similar to Goldwater et al. (2006). We
start from a high temperature and gradually de-
crease the temperature to 1; we find that the ini-
tial high temperature can also help small templates
to survive the initial iterations. The complete EM
framework is sketched in Figure 3, where Q is the
inverse of the specified temperature, and EC de-
notes the expected count.
</bodyText>
<subsectionHeader confidence="0.9911725">
3.1 Bayesian Inference with the Dirichlet
Process Prior
</subsectionHeader>
<bodyText confidence="0.993531470588235">
Bayesian inference plus the Dirichlet Process (DP)
have been shown to effectively prevent MT mod-
els from overfitting the training data (DeNero et
al., 2008; Blunsom et al., 2008). A similar ap-
proach can be applied here for SSMT by consider-
ing each TTS template as a cluster, and using DP
to adjust the number of TTS templates according
to the training data. Note that even though there
is a size limitation on the liberated phrasal TTS
templates, standard EM will still tend to overfit
the training data by pushing up the probabilities of
the big templates from the noisy word alignments.
The complete generative process, integrating the
DP prior and the generative models described in
its decomposing TTS templates is much less than the one
based on ROOTN, thus LHSN gives comparably more ex-
pected counts to the smaller TTS templates than ROOTN.
</bodyText>
<equation confidence="0.9261765">
Pr(T, S) = � Pr(T, S, R) = � H
R R V∈R
1312
else
Pr(t) = / (exp(ψ(EC(t)+αG0(t)))
p(�(Pt0:t0.lhs=t.lhs EC(t0))+α));
</equation>
<figureCaption confidence="0.820157">
Figure 6: M-step of the Variational EM
</figureCaption>
<equation confidence="0.97477675">
Section 3.1, is given below:
Br  |{αr, Gr0} ∼ DP(αr, Gr0)
t  |Bt.root ∼ Bt.root
(T, 5)  |{5G, {t}, B} ∼ 5G({t}, B)
</equation>
<bodyText confidence="0.999972888888889">
where G0 is a base distribution of the TTS tem-
plates, t denotes a TTS template, Bt.root denotes
the multinomial distribution over TTS templates
with the same root as t, 5G denotes the generative
model for a pair of tree and string in Section 3.1,
and α is a free parameter which adjusts the rate at
which new TTS templates are generated.
It is intractable to do exact inference under the
Bayesian framework, even with a conjugate prior
such as DP. Two methods are commonly used
for approximate inference: Markov chain Monte
Carlo (MCMC) (DeNero et al., 2008), and Vari-
ational Bayesian (VB) inference (Blunsom et al.,
2008). In this paper, the latter approach is used be-
cause it requires less running time. The E-step of
VB is exactly the same as standard EM, and in the
M-step the digamma function ψ and the base dis-
tribution G0 are used to increase the uncertainty of
the model. Similar to standard EM, both LHS- and
root-based normalizations are used in the M-step,
as shown in Figure 3.1. For the TTS templates,
which are also pairs of subtrees and strings, a natu-
ral choice of G0 is the generative models described
in Section 3.1. Because G0 estimates the probabil-
ity of the new TTS templates, the root-based gen-
erative model is superior to the LHS-based gener-
ative model and used in our approach.
</bodyText>
<subsectionHeader confidence="0.995468">
3.2 Initialization
</subsectionHeader>
<bodyText confidence="0.9999473">
Since the EM algorithm only converges to a lo-
cal minimum, proper initializations are needed to
achieve good performance for both standard EM
and variational EM. For the baseline templates
derived from word alignments, the initial counts
are set to the raw counts in the training corpus.
For the templates blindly extracted from big tem-
plates, the raw count of a LHS tree fragment is
distributed among their RHSs based on the like-
lihood of the template, computed by combining
</bodyText>
<figure confidence="0.789871555555556">
for all big template t do
for all template g extracted from t do
g.count = g.lhs.count = 0;
for all template g extracted from t do
g.count += w in(g)×w out(g, t);
g.lhs.count += w in(g)×w out(g, t);
for all template g extracted from t do
g.init += g.count
g.lhs.count ;
</figure>
<figureCaption confidence="0.996886">
Figure 7: Compute the initial counts of the liberated TTS
templates
</figureCaption>
<bodyText confidence="0.99695575">
the word-based inside/outside scores. The algo-
rithm is sketched in Figure 3.2, where the inside
score w in(g) is the product of the IBM Model 1
scores in both directions, computed based on the
words in g’s LHS and RHS. The outside score
w out(g, t) is computed similarly, except that the
IBM Model 1 scores are computed based on the
words in the hosting template t’s LHS/RHS ex-
cluding the words in g’s LHS/RHS. The initial
probabilities of the TTS templates are then com-
puted by normalizing their initial counts using
LHSN or ROOTN.
</bodyText>
<sectionHeader confidence="0.999583" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999856136363637">
We train an English-to-Chinese translation sys-
tem using the FBIS corpus, where 73,597 sentence
pairs are selected as the training data, and 500 sen-
tence pairs with no more than 25 words on the Chi-
nese side are selected for both the development
and test data.5 Charniak (2000)’s parser, trained
on the Penn Treebank, is used to generate the En-
glish syntax trees. Modified Kneser-Ney trigram
models are trained using SRILM (Stolcke, 2002)
upon the Chinese portion of the training data. The
trigram language model, as well as the TTS tem-
plates generated based on different methods, are
used in the TTS transducer. The model weights
of the transducer are tuned based on the develop-
ment set using a grid-based line search, and the
translation results are evaluated based on a single
Chinese reference6 using BLEU-4 (Papineni et al.,
2002). Huang et al. (2006) used character-based
BLEU as a way of normalizing inconsistent Chi-
nese word segmentation, but we avoid this prob-
lem as the training, development, and test data are
from the same source.
</bodyText>
<footnote confidence="0.9824334">
5The total 74,597 sentence pairs used in experiments are
those in the FBIS corpus whose English part can be parsed
using Charniak (2000)’s parser.
6BLEU-4 scores based on a single reference are much
lower than the ones based on multiple references.
</footnote>
<bodyText confidence="0.738298">
for all TTS Template t do
if it is the last iteration then
</bodyText>
<equation confidence="0.99362375">
exp(ψ(EC(t)+αG0(t)))
Pr(t) =
ex
p(ψ((P t0:t0.root=t.root EC(t0))+α));
</equation>
<page confidence="0.942196">
1313
</page>
<table confidence="0.768904">
E2C C2E Union Heuristic
w/ Big
w/o Big
</table>
<tableCaption confidence="0.994016">
Table 3: BLEU-4 scores (test set) of systems based on
GIZA++ word alignments
Table 4: BLEU-4 scores (test set) of the union alignment, us-
ing TTS templates up to a certain size, in terms of the number
of leaves in their LHSs
</tableCaption>
<figure confidence="0.99272125">
13.37 12.66 14.55 14.28
13.20 12.62 14.53 14.21
&lt; 5 &lt; 6 &lt; 7 &lt; 8 &lt; oo
BLEU-4
14.27 14.42 14.43 14.45 14.55
temperature parameter β
0.1 0.3 0.5 0.7 0.8 0.9 1.0 1.0 1.0 1.0
20.5
19.5
18.5
17.5
21
20
19
18
17
MIXN-EM
LHSN-VB
LHSN-EM
ROOTN-EM
ROOTN-VB
MIXN-VB
1 2 3 4 5 6 7 8 9 10
iteration
</figure>
<subsectionHeader confidence="0.997511">
4.1 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999696405405405">
GHKM (Galley et al., 2004) is used to generate
the baseline TTS templates based on the word
alignments computed using GIZA++ and different
combination methods, including union and the di-
agonal growing heuristic (Koehn et al., 2003). We
also tried combining alignments from GIZA++
based on intersection, but it is worse than both
single-direction alignments, due to its low cover-
age of training corpus and the incomplete transla-
tions it generates. The baseline translation results
based on ROOTN are shown in Table 4.1. The first
two columns in the table show the results of the
two single direction alignments. e2c and c2e de-
note the many English words to one Chinese word
alignment and the many Chinese words to one En-
glish word alignment, respectively. The two rows
show the results with and without the big tem-
plates, from which we can see that removing the
big templates does not affect performance much;
this verifies our postulate that the big templates
have very little chance of being used in the trans-
lation. Table 4.1, using the union alignments as
the representative and measuring a template’s size
by the number of leaves in its LHS, also demon-
strates that using big TTS templates brings very
limited performance gain.
The result that the union-based combination
outperforms either single direction alignments and
even the heuristic-based combination, combined
with the statistics of the disabled corpus in Sec-
tion 2.2, shows that more disabled training cor-
pus actually leads to better performance. This can
be explained by the fact that the union alignments
have the largest number of noisy alignments gath-
ered together in the big templates, and thus have
the least amount of noisy alignments which lead
to small and low-quality TTS templates.
</bodyText>
<figureCaption confidence="0.9931745">
Figure 8: BLEU-4 scores (development set) of annealing EM
and annealing VB in each iteration.
</figureCaption>
<subsectionHeader confidence="0.998365">
4.2 Learning Phrasal TTS Templates
</subsectionHeader>
<bodyText confidence="0.999054828571428">
To test our learning methods, we start with the
TTS templates generated based on e2c, c2e, and
union alignments using GHKM. This gives us
0.98M baseline templates. We use the big tem-
plates from the union alignments as the basis
and extract 10.92M new phrasal TTS templates,
which, for convenience, are denoted by NEW-
PHR. Because based on Table 1 and Table 2
the union alignment has the greatest number of
alignment links and therefore produces the largest
rules, this gives us the greatest flexibility in re-
aligning the input sentences. The baseline TTS
templates as well as NEW-PHR are initialized us-
ing the method in Section 3.3 for both annealing
EM and annealing VB. To simplify the experi-
ments, the same Dirichlet Process prior is used for
all multinomial distributions of the TTS templates
with different roots. Go in the Dirichlet prior is
computed based on the 1-level TTS templates se-
lected from the baseline TTS templates, so that the
big templates are efficiently penalized. The train-
ing algorithms follow the same annealing sched-
ule, where the temperature parameter β is initial-
ized to 0.1, and gradually increased to 1.
We experiment with the two training algo-
rithms, annealing EM and annealing VB, with dif-
ferent normalization methods. The experimental
results based on the development data are shown
in Figure 4.2, where the free parameter α of an-
nealing VB is set to 1, 100, and 100 respec-
tively for ROOTN, LHSN, and MIXN. The re-
sults verify that LHSN is worse than ROOTN in
predicting the translations, since MIXN outper-
forms LHSN with both annealing EM and VB.
ROOTN is on par with MIXN and much better
</bodyText>
<page confidence="0.967528">
1314
</page>
<table confidence="0.9953812">
Max Likelihood Annealing EM Annealing VB
w/o new-phr with new-phr w/o new-phr with new-phr w/o new-phr with new-phr
LHSN 14.05 13.16 14.31 15.33 14.82 16.15
ROOTN 14.50 13.49 14.90 16.06 14.76 16.12
MIXN NA NA 14.82 16.37 14.93 16.84
</table>
<tableCaption confidence="0.941183">
Table 5: BLEU-4 scores (test set) of different systems.
</tableCaption>
<table confidence="0.9999562">
Initial Template Final Template
number new-phr% number new-phr%
ROOTN 11.9M 91.8% 408.0K 21.9%
LHSN 11.9M 91.8% 557.2K 29.8%
MIXN 11.9M 91.8% 500.5K 27.6%
</table>
<tableCaption confidence="0.986525">
Table 6: The total number of templates and the percentage of
NEW-PHR, in the beginning and end of annealing VB
</tableCaption>
<bodyText confidence="0.999618405405405">
than LHSN when annealing EM is used; but with
annealing VB, it is outperformed by MIXN by
a large margin and is even slightly worse than
LHSN. This indicates that ROOTN is not giv-
ing large expected counts to NEW-PHR and leaves
very little space for VB to further improve the re-
sults. For all the normalization methods, anneal-
ing VB outperforms annealing EM and maintains
a longer ascending path, showing better control of
overfitting for the Bayesian models. Figure 4.2
shows the optimized results of the development
set based on annealing VB with different α. The
best performance is achieved as α approaches 1,
100, and 100 for ROOTN, LHSN and MIXN re-
spectively. The α parameter can be viewed as a
weight used to balance the expected counts and
the probabilities from Go. Thus it is reasonable
for LHSN and MIXN to have bigger optimal α
than ROOTN, since ROOTN gives lower expected
counts to NEW-PHR than LHSN and MIXN do.
To see the contribution of the phrasal template
extraction in the performance gain, MT experi-
ments are conducted by turning this component
on and off. Results on the test set, obtained by
using parameters optimized on the development
set, are shown in Table 4.2. The template counts
used in the Max-Likelihood training are the same
as the ones used in the initialization of anneal-
ing EM and VB. Results show that for annealing
EM and VB, use of NEW-PHR greatly improves
performance, while for the Max-Likelihood train-
ing, use of NEW-PHR hurts performance. This
is not surprising, because Max-Likelihood train-
ing cannot efficiently filter out the noisy phrasal
templates introduced in the initial NEW-PHR. An-
other observation is that annealing VB does not al-
ways outperform annealing EM. With NEW-PHR
</bodyText>
<figure confidence="0.9369245">
0.1 1 10 100 1000
α
</figure>
<figureCaption confidence="0.988398">
Figure 9: BLEU-4 scores (development set) of annealing VB
with different α.
</figureCaption>
<bodyText confidence="0.998471565217391">
turned on, annealing VB shows consistent supe-
riority over annealing EM; while without NEW-
PHR, it only outperforms annealing EM based on
LHSN and MIXN, and the improvement is not as
big as when NEW-PHR is turned on. This indi-
cates that without NEW-PHR, there is less need
to use VB to shrink down the size of the tem-
plate set. Table 4.2 shows the statistics of the ini-
tial template set including NEW-PHR and the final
TTS template set after annealing VB is conducted,
where we can see annealing VB efficiently re-
duces NEW-PHR to a relatively small size and re-
sults in much more compact systems than the sys-
tem based on the baseline templates from GIZA++
alignments. Comparing with the best GIZA++-
based system union, our best system, utilizing
NEW-PHR and the two-stage template normaliza-
tion, demonstrates the strength of annealing VB
by an absolute improvement of 2.29% in BLEU-
4 score, from 14.55 to 16.84. This improvement
is significant at p &lt; 0.005 based on 2000 itera-
tions of paired bootstrap re-sampling of the test
set (Koehn, 2004).
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9997252">
Our experimental results are obtained based on
a relatively small training corpus, the improved
performance may be questionable when a larger
training corpus is used. Someone may wonder if
the performance gain primarily comes from the
</bodyText>
<figure confidence="0.9426526">
21
20.8
20.6
20.4
20.2
20
19.8
MIXN
ROOTN
LHSN
</figure>
<page confidence="0.941587">
1315
</page>
<table confidence="0.998672285714286">
Many-to-one Alignment
( VP ( VB make ) ( NP ( DT a ) ( JJ complete ) ( NN statement ) ) ) rt%)-,it
( S ( VP VBG ( NP ( DT the ) ( NN mass ) ( NN line ) ) PP ) ) PP VBG �Aj%_j
( PP ( TO to ) ( NP ( DT the ) ( JJS greatest ) ( NN extent ) ) ) AMPA A
( PP ( IN of ) ( NP ( JJ peaceful ) ( NNP coexistence ) ) ) fnYAA
Many-to-many Alignment
( VP ( VBN based ) ( PP ( IN on ) ( NP ( JJ actual ) ( NNS needs ) ) ) ) J k �IVT f,R
( PP ( IN into ) ( NP ( NP ( DT the ) ( NNS hands ) ) PP ) ) PP �S
( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) � � ( Lam,
( SBAR ( S ( NP ( DT the ) ( VBG aging ) NN ) ( VP ( aux is ) NP ) ) ) NN iz ft ) NP
( NP NP1 PP ( , , ) ( VP ( VBN centered ) ( PP ( IN around ) NP2 ) ) ) NP2 � �� � NP1 PP
Allowance of Bad Word Segmentation
( NP ( NP ( NNP japan ) ( POS &apos;s ) ) ( NNP sdf ) ( NNP navy ) ) ��� S ���
( NP ( PDT all ) ( NP ( NNS people ) ( POS &apos;s ) ) ( NNS organizations ) )�J` �� �
</table>
<figureCaption confidence="0.9863">
Figure 10: Examples of the learned TTS templates
</figureCaption>
<bodyText confidence="0.999917772727273">
reduced out of vocabulary (OOV) ratio. We ex-
amined the OOV ratio of the test set with/without
the learned TTS templates, and found the differ-
ence was very small. In fact, our method is de-
signed to learn the phrasal TTS templates, and ex-
plictly avoids lexical pairs. To further understand
the characteristics of the learned TTS templates,
we list some representative templates in Figure 4.2
classified in 3 groups. The group Many-to-one
Alignment and Many-to-many Alignment show the
TTS templates based on complicated word align-
ments, which are difficult to compute based on the
existing word alignment models. These templates
do not have rare English words, whose translation
cannot be found outside the big templates. The
difficulty lies in the non-literal translation of the
source words, which are unlikely to learnt by soly
increasing the size of the training corpus. One
other interesting observation is that our learning
method is tolerant to noisy Chinese word segmen-
tation, as shown in group Allowance of Bad Word
Segmentation.
</bodyText>
<sectionHeader confidence="0.99968" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999539157894737">
This paper proposes a Bayesian model for extract-
ing the Tree-to-String templates directly from the
data. By limiting the extraction to the big tem-
plates from the pre-computed word alignments
and applying a set of constraints, we restrict the
space of possible TTS templates under consider-
ation, while still allowing new and more accurate
templates to emerge from the training data. The
empirical results demonstrate the strength of our
approach, which outperforms the GIZA++-based
systems by a large margin. This encourages a
move from word-alignment-based systems to sys-
tems based on consistent, end-to-end probabilistic
modeling. Because our Bayesian model employs
a very simple prior, more sophisticated generative
models provide a possible direction for further ex-
perimentation.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999558">
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Neu-
ral Information Processing Systems (NIPS).
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL-01, pages
132–139.
</reference>
<page confidence="0.819004">
1316
</page>
<reference confidence="0.99993443939394">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(1):1–21.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL-07, pages 17–24.
John DeNero, Alexandre Bouchard-Cote, and Dan
Klein. 2008. Sampling alignment structure under
a bayesian translation model. In EMNLP08.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using syntax to improveword alignment pre-
cision for syntax-based machine translation. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, Columbus, Ohio. ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of NAACL-04, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL-06, pages 961–968, July.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the Human Language Technology Conference/North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-04.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bi-
ennial Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, Edmonton, Alberta.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388–395, Barcelona, Spain, July.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL-06,
Sydney, Australia, July.
J. May and K. Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of
EMNLP.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
ofACL-02.
K. Rose, E. Gurewitz, and G. C. Fox. 1992. Vec-
tor quantization by deterministic annealing. IEEE
Transactions on Information Theory, 38(4):1249–
1257, July.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In International Conference on
Spoken Language Processing, volume 2, pages 901–
904.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In COLING-96, pages 836–841.
</reference>
<page confidence="0.993867">
1317
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.907704">
<title confidence="0.999683">Bayesian Learning of Phrasal Tree-to-String Templates</title>
<author confidence="0.968246">Liu</author>
<affiliation confidence="0.9997605">Department of Computer University of</affiliation>
<address confidence="0.96207">Rochester, NY</address>
<abstract confidence="0.998396875">We examine the problem of overcoming noisy word-level alignments when learning tree-to-string translation rules. Our approach introduces new rules, and reestimates rule probabilities using EM. The major obstacles to this approach are the very reasons that word-alignments are used for rule extraction: the huge space of possible rules, as well as controlling overfitting. By carefully controlling which portions of the original alignments are reanalyzed, and by using Bayesian inference during re-analysis, we show significant improvement over the baseline rules extracted from word-level alignments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2008</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="1000" citStr="Blunsom et al., 2008" startWordPosition="140" endWordPosition="143">using EM. The major obstacles to this approach are the very reasons that word-alignments are used for rule extraction: the huge space of possible rules, as well as controlling overfitting. By carefully controlling which portions of the original alignments are reanalyzed, and by using Bayesian inference during re-analysis, we show significant improvement over the baseline rules extracted from word-level alignments. 1 Introduction Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (Blunsom et al., 2008; DeNero et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al.,</context>
<context position="20041" citStr="Blunsom et al., 2008" startWordPosition="3399" endWordPosition="3402">d up the training process, similar to Goldwater et al. (2006). We start from a high temperature and gradually decrease the temperature to 1; we find that the initial high temperature can also help small templates to survive the initial iterations. The complete EM framework is sketched in Figure 3, where Q is the inverse of the specified temperature, and EC denotes the expected count. 3.1 Bayesian Inference with the Dirichlet Process Prior Bayesian inference plus the Dirichlet Process (DP) have been shown to effectively prevent MT models from overfitting the training data (DeNero et al., 2008; Blunsom et al., 2008). A similar approach can be applied here for SSMT by considering each TTS template as a cluster, and using DP to adjust the number of TTS templates according to the training data. Note that even though there is a size limitation on the liberated phrasal TTS templates, standard EM will still tend to overfit the training data by pushing up the probabilities of the big templates from the noisy word alignments. The complete generative process, integrating the DP prior and the generative models described in its decomposing TTS templates is much less than the one based on ROOTN, thus LHSN gives comp</context>
<context position="21598" citStr="Blunsom et al., 2008" startWordPosition="3673" endWordPosition="3676">here G0 is a base distribution of the TTS templates, t denotes a TTS template, Bt.root denotes the multinomial distribution over TTS templates with the same root as t, 5G denotes the generative model for a pair of tree and string in Section 3.1, and α is a free parameter which adjusts the rate at which new TTS templates are generated. It is intractable to do exact inference under the Bayesian framework, even with a conjugate prior such as DP. Two methods are commonly used for approximate inference: Markov chain Monte Carlo (MCMC) (DeNero et al., 2008), and Variational Bayesian (VB) inference (Blunsom et al., 2008). In this paper, the latter approach is used because it requires less running time. The E-step of VB is exactly the same as standard EM, and in the M-step the digamma function ψ and the base distribution G0 are used to increase the uncertainty of the model. Similar to standard EM, both LHS- and root-based normalizations are used in the M-step, as shown in Figure 3.1. For the TTS templates, which are also pairs of subtrees and strings, a natural choice of G0 is the generative models described in Section 3.1. Because G0 estimates the probability of the new TTS templates, the root-based generativ</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. Bayesian synchronous grammar induction. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1291" citStr="Brown et al., 1993" startWordPosition="184" endWordPosition="187">n inference during re-analysis, we show significant improvement over the baseline rules extracted from word-level alignments. 1 Introduction Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (Blunsom et al., 2008; DeNero et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given.</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-01,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="23932" citStr="Charniak (2000)" startWordPosition="4088" endWordPosition="4089">HS and RHS. The outside score w out(g, t) is computed similarly, except that the IBM Model 1 scores are computed based on the words in the hosting template t’s LHS/RHS excluding the words in g’s LHS/RHS. The initial probabilities of the TTS templates are then computed by normalizing their initial counts using LHSN or ROOTN. 4 Experiments We train an English-to-Chinese translation system using the FBIS corpus, where 73,597 sentence pairs are selected as the training data, and 500 sentence pairs with no more than 25 words on the Chinese side are selected for both the development and test data.5 Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the English syntax trees. Modified Kneser-Ney trigram models are trained using SRILM (Stolcke, 2002) upon the Chinese portion of the training data. The trigram language model, as well as the TTS templates generated based on different methods, are used in the TTS transducer. The model weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of NAACL-01, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1740" citStr="Chiang, 2007" startWordPosition="264" endWordPosition="265">ferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible. We find an improvement in translation accuracy through, first, using constraints to limit the numb</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="16465" citStr="Dempster et al., 1977" startWordPosition="2784" endWordPosition="2787">definition. Figure 2.2 shows the complete set of valid TTS templates which can be extracted from the example TTS template. The subscripts of the non-terminals are used to differentiate identical non-terminals in different positions. The extraction process blindly releases smaller TTS templates from the big templates, among which only a small fraction are correct TTS templates. Therefore, we need an inference method to raise the weight of the correct templates and decrease the weight of the noisy templates. 1311 3 Estimating TTS Template Probability The Expectation-Maximization (EM) algorithm (Dempster et al., 1977) can be used to estimate the TTS templates’ probabilities, given a generative model addressing how a pair of source syntax tree and target string is generated. There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates. The LHS-based normalization (LHSN) (Liu et al., 2006; Huang et al., 2006), corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree. The other one is normalization based on the root of t</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="5997" citStr="DeNero and Klein (2007)" startWordPosition="962" endWordPosition="965">templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). This approach also generates TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged. Other prior approaches towards improving TTS templates focus on improving the word alignment performance over the classic models such as IBM series models and Hidden Markov Model (HMM), which do not consider the syntactic structure of the aligning languages and produce syntax-violating alignments. DeNero and Klein (2007) use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments. Fossum et al. (2008) start from the GIZA++ alignment and incrementally delete bad links based on a discrimFigure 1: 5 small TTS templates are extracted based on the correct word alignments (left), but only 1 big TTS template (right) can be extracted when the cross-boundary noisy alignments are added in. inative model with syntactic features. This approach can only find a better subset of the GIZA++ alignment and requires a parallel corpus with goldstandard word alignment for training the discriminat</context>
<context position="10301" citStr="DeNero and Klein (2007)" startWordPosition="1695" endWordPosition="1698">ach: TTS Templates Obeying Word Alignment TTS templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string (i.e., the TTS templates). To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated. This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems (Galley et al., 2006; Liu et al., 2006; Huang et al., 2006). The word alignment used in GHKM is usually computed independent of the syntactic structure, and as DeNero and Klein (2007) and May and Knight (2007) have noted, Table 2: In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems. In fact, noisy word alignments cause more damage to a SSMT system than to a phrase based SMT system, because the TTS templates can only be derived from tree constituents. If some noisy alignments happen to cross over the boundaries of two constituents, as shown in Figure 2, a much bigger tree fragment will be extracted as a TTS template. Even though the big </context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In Proceedings of ACL-07, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cote</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a bayesian translation model.</title>
<date>2008</date>
<booktitle>In EMNLP08.</booktitle>
<contexts>
<context position="1022" citStr="DeNero et al., 2008" startWordPosition="144" endWordPosition="147">stacles to this approach are the very reasons that word-alignments are used for rule extraction: the huge space of possible rules, as well as controlling overfitting. By carefully controlling which portions of the original alignments are reanalyzed, and by using Bayesian inference during re-analysis, we show significant improvement over the baseline rules extracted from word-level alignments. 1 Introduction Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (Blunsom et al., 2008; DeNero et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2</context>
<context position="20018" citStr="DeNero et al., 2008" startWordPosition="3395" endWordPosition="3398">in our system to speed up the training process, similar to Goldwater et al. (2006). We start from a high temperature and gradually decrease the temperature to 1; we find that the initial high temperature can also help small templates to survive the initial iterations. The complete EM framework is sketched in Figure 3, where Q is the inverse of the specified temperature, and EC denotes the expected count. 3.1 Bayesian Inference with the Dirichlet Process Prior Bayesian inference plus the Dirichlet Process (DP) have been shown to effectively prevent MT models from overfitting the training data (DeNero et al., 2008; Blunsom et al., 2008). A similar approach can be applied here for SSMT by considering each TTS template as a cluster, and using DP to adjust the number of TTS templates according to the training data. Note that even though there is a size limitation on the liberated phrasal TTS templates, standard EM will still tend to overfit the training data by pushing up the probabilities of the big templates from the noisy word alignments. The complete generative process, integrating the DP prior and the generative models described in its decomposing TTS templates is much less than the one based on ROOT</context>
<context position="21534" citStr="DeNero et al., 2008" startWordPosition="3663" endWordPosition="3666">, Gr0) t |Bt.root ∼ Bt.root (T, 5) |{5G, {t}, B} ∼ 5G({t}, B) where G0 is a base distribution of the TTS templates, t denotes a TTS template, Bt.root denotes the multinomial distribution over TTS templates with the same root as t, 5G denotes the generative model for a pair of tree and string in Section 3.1, and α is a free parameter which adjusts the rate at which new TTS templates are generated. It is intractable to do exact inference under the Bayesian framework, even with a conjugate prior such as DP. Two methods are commonly used for approximate inference: Markov chain Monte Carlo (MCMC) (DeNero et al., 2008), and Variational Bayesian (VB) inference (Blunsom et al., 2008). In this paper, the latter approach is used because it requires less running time. The E-step of VB is exactly the same as standard EM, and in the M-step the digamma function ψ and the base distribution G0 are used to increase the uncertainty of the model. Similar to standard EM, both LHS- and root-based normalizations are used in the M-step, as shown in Figure 3.1. For the TTS templates, which are also pairs of subtrees and strings, a natural choice of G0 is the generative models described in Section 3.1. Because G0 estimates th</context>
</contexts>
<marker>DeNero, Bouchard-Cote, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cote, and Dan Klein. 2008. Sampling alignment structure under a bayesian translation model. In EMNLP08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using syntax to improveword alignment precision for syntax-based machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<publisher>ACL.</publisher>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6113" citStr="Fossum et al. (2008)" startWordPosition="981" endWordPosition="984">tes TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged. Other prior approaches towards improving TTS templates focus on improving the word alignment performance over the classic models such as IBM series models and Hidden Markov Model (HMM), which do not consider the syntactic structure of the aligning languages and produce syntax-violating alignments. DeNero and Klein (2007) use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments. Fossum et al. (2008) start from the GIZA++ alignment and incrementally delete bad links based on a discrimFigure 1: 5 small TTS templates are extracted based on the correct word alignments (left), but only 1 big TTS template (right) can be extracted when the cross-boundary noisy alignments are added in. inative model with syntactic features. This approach can only find a better subset of the GIZA++ alignment and requires a parallel corpus with goldstandard word alignment for training the discriminative model. May and Knight (2007) factorize the word alignment into a set of re-orderings represented by the TTS temp</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using syntax to improveword alignment precision for syntax-based machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, Columbus, Ohio. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-04,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="10082" citStr="Galley et al., 2004" startWordPosition="1656" endWordPosition="1659">omputational power. In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing. 2.1 Baseline Approach: TTS Templates Obeying Word Alignment TTS templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string (i.e., the TTS templates). To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated. This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems (Galley et al., 2006; Liu et al., 2006; Huang et al., 2006). The word alignment used in GHKM is usually computed independent of the syntactic structure, and as DeNero and Klein (2007) and May and Knight (2007) have noted, Table 2: In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems. In fact, noisy word alignments cause more damage to a SSMT system than to a phrase based SMT system, because the TTS templates can only be de</context>
<context position="25677" citStr="Galley et al., 2004" startWordPosition="4396" endWordPosition="4399">.root EC(t0))+α)); 1313 E2C C2E Union Heuristic w/ Big w/o Big Table 3: BLEU-4 scores (test set) of systems based on GIZA++ word alignments Table 4: BLEU-4 scores (test set) of the union alignment, using TTS templates up to a certain size, in terms of the number of leaves in their LHSs 13.37 12.66 14.55 14.28 13.20 12.62 14.53 14.21 &lt; 5 &lt; 6 &lt; 7 &lt; 8 &lt; oo BLEU-4 14.27 14.42 14.43 14.45 14.55 temperature parameter β 0.1 0.3 0.5 0.7 0.8 0.9 1.0 1.0 1.0 1.0 20.5 19.5 18.5 17.5 21 20 19 18 17 MIXN-EM LHSN-VB LHSN-EM ROOTN-EM ROOTN-VB MIXN-VB 1 2 3 4 5 6 7 8 9 10 iteration 4.1 Baseline Systems GHKM (Galley et al., 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al., 2003). We also tried combining alignments from GIZA++ based on intersection, but it is worse than both single-direction alignments, due to its low coverage of training corpus and the incomplete translations it generates. The baseline translation results based on ROOTN are shown in Table 4.1. The first two columns in the table show the results of the two single direction alignments. e2c and c2e den</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of NAACL-04, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1646" citStr="Galley et al., 2006" startWordPosition="244" endWordPosition="248">ause such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible. We f</context>
<context position="5246" citStr="Galley et al. (2006)" startWordPosition="845" endWordPosition="848"> and estimate their probabilities using Variational Bayes with a Dirichlet Process prior. In step 2, since there are no constraints from the pre-computed word alignments, we have complete freedom in generating all possible TTS templates to overcome noisy word alignments. We use variational EM to approximate the inference of our Bayesian model and explore different normalization methods for the TTS templates. A two-stage normalization is proposed by combining LHSbased normalization with normalization based on the root of the LHS, and is shown to be the best model when used with variational EM. Galley et al. (2006) recompose the TTS templates by inserting unaligned target words and combining small templates into bigger ones. The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). This approach also generates TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged. Other prior approaches towards improving TTS templates focus on improving the word alignment performance over the classic models such as IBM series models and Hidden Marko</context>
<context position="8891" citStr="Galley et al., 2006" startWordPosition="1456" endWordPosition="1459">stic 24.6% 27.9% 47.5% Ch-En En-Ch 45.9% 20.1% Table 1: Percentage of corpus used to generate big templates, based on different word alignments S NP1 PP AUX ADJ 1 NP3 的 NP1 很美 in NP3 is of NP VP 2 3 4 beautiful Figure 2: Examples of valid and invalid templates extracted from a big Template. Template 1, invalid, doesn’t go all the way down to the bottom. Template 2 is valid. Template 3, invalid, doesn’t have the same set of variables in its LHS/RHS. Template 4, invalid, is not a phrasal TTS template. that TTS templates are commonly used in machine translation. The first is synchronous parsing (Galley et al., 2006; May and Knight, 2007), where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006), where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power. In this p</context>
<context position="10138" citStr="Galley et al., 2006" startWordPosition="1667" endWordPosition="1670">r to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing. 2.1 Baseline Approach: TTS Templates Obeying Word Alignment TTS templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string (i.e., the TTS templates). To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated. This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems (Galley et al., 2006; Liu et al., 2006; Huang et al., 2006). The word alignment used in GHKM is usually computed independent of the syntactic structure, and as DeNero and Klein (2007) and May and Knight (2007) have noted, Table 2: In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems. In fact, noisy word alignments cause more damage to a SSMT system than to a phrase based SMT system, because the TTS templates can only be derived from tree constituents. If some noisy alignments h</context>
<context position="17101" citStr="Galley et al., 2006" startWordPosition="2891" endWordPosition="2894">estimate the TTS templates’ probabilities, given a generative model addressing how a pair of source syntax tree and target string is generated. There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates. The LHS-based normalization (LHSN) (Liu et al., 2006; Huang et al., 2006), corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree. The other one is normalization based on the root of the LHS (ROOTN) (Galley et al., 2006), corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance: �Pr(t) where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template. The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynam</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING/ACL-06, pages 961–968, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</booktitle>
<contexts>
<context position="19481" citStr="Goldwater et al. (2006)" startWordPosition="3305" endWordPosition="3308">n EC(t) Pr(t) = Et0:t0.root=t.root EC(t0); else EC(t) Pr(t) = Et0:t0.lhs=t.lhs EC(t0); Figure 5: EM Algorithm For Estimating TTS Templates own strength and weakness, both of them are used in our EM algorithm: LHSN is used in all EM iterations except the last one to compute the expected counts of the TTS templates, and ROOTN is used in the last EM iteration to compute the final probabilities of the TTS templates. This two-stage normalization method is denoted as MIXN in this paper. Deterministic Annealing (Rose et al., 1992) is is used in our system to speed up the training process, similar to Goldwater et al. (2006). We start from a high temperature and gradually decrease the temperature to 1; we find that the initial high temperature can also help small templates to survive the initial iterations. The complete EM framework is sketched in Figure 3, where Q is the inverse of the specified temperature, and EC denotes the expected count. 3.1 Bayesian Inference with the Dirichlet Process Prior Bayesian inference plus the Dirichlet Process (DP) have been shown to effectively prevent MT models from overfitting the training data (DeNero et al., 2008; Blunsom et al., 2008). A similar approach can be applied here</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics (HLT/NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-04.</booktitle>
<contexts>
<context position="5466" citStr="Graehl and Knight (2004)" startWordPosition="878" endWordPosition="881">ssible TTS templates to overcome noisy word alignments. We use variational EM to approximate the inference of our Bayesian model and explore different normalization methods for the TTS templates. A two-stage normalization is proposed by combining LHSbased normalization with normalization based on the root of the LHS, and is shown to be the best model when used with variational EM. Galley et al. (2006) recompose the TTS templates by inserting unaligned target words and combining small templates into bigger ones. The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). This approach also generates TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged. Other prior approaches towards improving TTS templates focus on improving the word alignment performance over the classic models such as IBM series models and Hidden Markov Model (HMM), which do not consider the syntactic structure of the aligning languages and produce syntax-violating alignments. DeNero and Klein (2007) use a syntaxbased distance in an HMM word alignment model to favor s</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proceedings of NAACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1625" citStr="Huang et al., 2006" startWordPosition="240" endWordPosition="243">o et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as m</context>
<context position="9159" citStr="Huang et al., 2006" startWordPosition="1504" endWordPosition="1507">ed from a big Template. Template 1, invalid, doesn’t go all the way down to the bottom. Template 2 is valid. Template 3, invalid, doesn’t have the same set of variables in its LHS/RHS. Template 4, invalid, is not a phrasal TTS template. that TTS templates are commonly used in machine translation. The first is synchronous parsing (Galley et al., 2006; May and Knight, 2007), where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006), where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power. In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing. 2.1 Baseline Approach: TTS Templates Obeying Word Alignment TTS templates are commonly generated by</context>
<context position="16847" citStr="Huang et al., 2006" startWordPosition="2848" endWordPosition="2851">fore, we need an inference method to raise the weight of the correct templates and decrease the weight of the noisy templates. 1311 3 Estimating TTS Template Probability The Expectation-Maximization (EM) algorithm (Dempster et al., 1977) can be used to estimate the TTS templates’ probabilities, given a generative model addressing how a pair of source syntax tree and target string is generated. There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates. The LHS-based normalization (LHSN) (Liu et al., 2006; Huang et al., 2006), corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree. The other one is normalization based on the root of the LHS (ROOTN) (Galley et al., 2006), corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance: �Pr(t) </context>
<context position="24511" citStr="Huang et al. (2006)" startWordPosition="4182" endWordPosition="4185">elopment and test data.5 Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the English syntax trees. Modified Kneser-Ney trigram models are trained using SRILM (Stolcke, 2002) upon the Chinese portion of the training data. The trigram language model, as well as the TTS templates generated based on different methods, are used in the TTS transducer. The model weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. 5The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak (2000)’s parser. 6BLEU-4 scores based on a single reference are much lower than the ones based on multiple references. for all TTS Template t do if it is the last iteration then exp(ψ(EC(t)+αG0(t))) Pr(t) = ex p(ψ((P t0:t0.root=t.root EC(t0))+α)); 1313 E2C C2E Union Heuristic w/ Big</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-03,</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="11645" citStr="Koehn et al., 2003" startWordPosition="1933" endWordPosition="1936"> tree where they were extracted, as we show in Section 4. In other words, a few cross-boundary noisy alignments could disable a big portion of a training syntax tree, while for a phrase-based SMT system, their effect is limited to the phrases they align. As a rough measure of how the training corpus is affected by the big templates, we calculated the distribution of target words in big and non-big TTS templates. The word alignment is computed using GIZA++2 for the selected 73,597 sentence pairs in the FBIS corpus in both directions and then combined using union and heuristic diagonal growing (Koehn et al., 2003). Table 1 shows that big templates consume 20.1% to 45.9% of the training corpus depending on different types of word alignments. The statistics indicate that a significant portion of the training corpus is simply wasted, if the TTS templates are extracted based on word alignments from GIZA++. On the other hand, it shows the potential for improving an SSMT system if we can efficiently re-use the wasted training corpus. By further examining the selected big templates, we find that the most common form of big templates is a big skeleton template starting 2GIZA++ is available at http://www.fjoch.</context>
<context position="25882" citStr="Koehn et al., 2003" startWordPosition="4428" endWordPosition="4431">S templates up to a certain size, in terms of the number of leaves in their LHSs 13.37 12.66 14.55 14.28 13.20 12.62 14.53 14.21 &lt; 5 &lt; 6 &lt; 7 &lt; 8 &lt; oo BLEU-4 14.27 14.42 14.43 14.45 14.55 temperature parameter β 0.1 0.3 0.5 0.7 0.8 0.9 1.0 1.0 1.0 1.0 20.5 19.5 18.5 17.5 21 20 19 18 17 MIXN-EM LHSN-VB LHSN-EM ROOTN-EM ROOTN-VB MIXN-VB 1 2 3 4 5 6 7 8 9 10 iteration 4.1 Baseline Systems GHKM (Galley et al., 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al., 2003). We also tried combining alignments from GIZA++ based on intersection, but it is worse than both single-direction alignments, due to its low coverage of training corpus and the incomplete translations it generates. The baseline translation results based on ROOTN are shown in Table 4.1. The first two columns in the table show the results of the two single direction alignments. e2c and c2e denote the many English words to one Chinese word alignment and the many Chinese words to one English word alignment, respectively. The two rows show the results with and without the big templates, from which</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL-03, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="32580" citStr="Koehn, 2004" startWordPosition="5566" endWordPosition="5567">e set after annealing VB is conducted, where we can see annealing VB efficiently reduces NEW-PHR to a relatively small size and results in much more compact systems than the system based on the baseline templates from GIZA++ alignments. Comparing with the best GIZA++- based system union, our best system, utilizing NEW-PHR and the two-stage template normalization, demonstrates the strength of annealing VB by an absolute improvement of 2.29% in BLEU4 score, from 14.55 to 16.84. This improvement is significant at p &lt; 0.005 based on 2000 iterations of paired bootstrap re-sampling of the test set (Koehn, 2004). 5 Discussion Our experimental results are obtained based on a relatively small training corpus, the improved performance may be questionable when a larger training corpus is used. Someone may wonder if the performance gain primarily comes from the 21 20.8 20.6 20.4 20.2 20 19.8 MIXN ROOTN LHSN 1315 Many-to-one Alignment ( VP ( VB make ) ( NP ( DT a ) ( JJ complete ) ( NN statement ) ) ) rt%)-,it ( S ( VP VBG ( NP ( DT the ) ( NN mass ) ( NN line ) ) PP ) ) PP VBG �Aj%_j ( PP ( TO to ) ( NP ( DT the ) ( JJS greatest ) ( NN extent ) ) ) AMPA A ( PP ( IN of ) ( NP ( JJ peaceful ) ( NNP coexiste</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL-06,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="1605" citStr="Liu et al., 2006" startWordPosition="236" endWordPosition="239">t al., 2008; DeNero et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directl</context>
<context position="9138" citStr="Liu et al., 2006" startWordPosition="1500" endWordPosition="1503"> templates extracted from a big Template. Template 1, invalid, doesn’t go all the way down to the bottom. Template 2 is valid. Template 3, invalid, doesn’t have the same set of variables in its LHS/RHS. Template 4, invalid, is not a phrasal TTS template. that TTS templates are commonly used in machine translation. The first is synchronous parsing (Galley et al., 2006; May and Knight, 2007), where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006), where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power. In this paper, we use a TTS transducer to test the performance of different TTS templates, but our techniques could also be applied to SSMT systems based on synchronous parsing. 2.1 Baseline Approach: TTS Templates Obeying Word Alignment TTS templates are </context>
<context position="16826" citStr="Liu et al., 2006" startWordPosition="2844" endWordPosition="2847">S templates. Therefore, we need an inference method to raise the weight of the correct templates and decrease the weight of the noisy templates. 1311 3 Estimating TTS Template Probability The Expectation-Maximization (EM) algorithm (Dempster et al., 1977) can be used to estimate the TTS templates’ probabilities, given a generative model addressing how a pair of source syntax tree and target string is generated. There are two commonly used generative models for syntaxbased MT systems, each of which corresponds to a normalization method for the TTS templates. The LHS-based normalization (LHSN) (Liu et al., 2006; Huang et al., 2006), corresponds to the generative process where the source syntax subtree is first generated, and then the target string is generated given the source syntax subtree. The other one is normalization based on the root of the LHS (ROOTN) (Galley et al., 2006), corresponding to the generative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a train</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of COLING/ACL-06, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J May</author>
<author>K Knight</author>
</authors>
<title>Syntactic re-alignment models for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1669" citStr="May and Knight, 2007" startWordPosition="249" endWordPosition="252">directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs. In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible. We find an improvement in t</context>
<context position="6629" citStr="May and Knight (2007)" startWordPosition="1068" endWordPosition="1071">axbased distance in an HMM word alignment model to favor syntax-friendly alignments. Fossum et al. (2008) start from the GIZA++ alignment and incrementally delete bad links based on a discrimFigure 1: 5 small TTS templates are extracted based on the correct word alignments (left), but only 1 big TTS template (right) can be extracted when the cross-boundary noisy alignments are added in. inative model with syntactic features. This approach can only find a better subset of the GIZA++ alignment and requires a parallel corpus with goldstandard word alignment for training the discriminative model. May and Knight (2007) factorize the word alignment into a set of re-orderings represented by the TTS templates and build a hierarchical syntax-based word alignment model. The problem is that the TTS templates are generated by the word alignments from GIZA++, which limits the potential of the syntactic re-alignment. As shown by these prior approaches, directly improving the word alignment either falls into the framework of many-to-one alignment, or is substantially confined by the word alignment it builds upon. The remainder of the paper focuses on the Bayesian approach to learning TTS templates and is organized as</context>
<context position="8914" citStr="May and Knight, 2007" startWordPosition="1460" endWordPosition="1463">% Ch-En En-Ch 45.9% 20.1% Table 1: Percentage of corpus used to generate big templates, based on different word alignments S NP1 PP AUX ADJ 1 NP3 的 NP1 很美 in NP3 is of NP VP 2 3 4 beautiful Figure 2: Examples of valid and invalid templates extracted from a big Template. Template 1, invalid, doesn’t go all the way down to the bottom. Template 2 is valid. Template 3, invalid, doesn’t have the same set of variables in its LHS/RHS. Template 4, invalid, is not a phrasal TTS template. that TTS templates are commonly used in machine translation. The first is synchronous parsing (Galley et al., 2006; May and Knight, 2007), where TTS templates are used to construct synchronous parse trees for an input sentence, and the translations will be generated once the synchronous trees are built up. The other way is the TTS transducer (Liu et al., 2006; Huang et al., 2006), where TTS templates are used just as their name indicates: to transform a source parse tree (or forest) into the proper target string. Since synchronous parsing considers all possible synchronous parse trees of the source sentence, it is less constrained than TTS transducers and hence requires more computational power. In this paper, we use a TTS tran</context>
<context position="10327" citStr="May and Knight (2007)" startWordPosition="1700" endWordPosition="1703">ord Alignment TTS templates are commonly generated by decomposing a pair of aligned source syntax tree and target string into smaller pairs of tree fragments and target string (i.e., the TTS templates). To keep the number of TTS templates to a manageable scale, only the non-decomposable TTS templates are generated. This algorithm is referred to as GHKM (Galley et al., 2004) and is widely used in SSMT systems (Galley et al., 2006; Liu et al., 2006; Huang et al., 2006). The word alignment used in GHKM is usually computed independent of the syntactic structure, and as DeNero and Klein (2007) and May and Knight (2007) have noted, Table 2: In the selected big templates, the distribution of words in the templates of different sizes, which are measured based on the number of symbols in their RHSs is not the best for SSMT systems. In fact, noisy word alignments cause more damage to a SSMT system than to a phrase based SMT system, because the TTS templates can only be derived from tree constituents. If some noisy alignments happen to cross over the boundaries of two constituents, as shown in Figure 2, a much bigger tree fragment will be extracted as a TTS template. Even though the big TTS templates still carry </context>
<context position="17748" citStr="May and Knight, 2007" startWordPosition="2997" endWordPosition="3000">erative process where, given the root of the syntax subtree, the LHS syntax subtree and the RHS string are generated simultaneously. By omitting the decomposition probability in the LHS-based generative model, the two generative models share the same formula for computing the probability of a training instance: �Pr(t) where T and S denote the source syntax tree and target string respectively, R denotes the decomposition of (T, S), and t denotes the TTS template. The expected counts of the TTS templates can then be efficiently computed using an inside-outsidelike dynamic programming algorithm (May and Knight, 2007). LHSN, as shown by Galley et al. (2006), cannot accurately restore the true conditional probabilities of the target sentences given the source sentences in the training corpus. This indicates that LHSN is not good at predicting unseen sentences or at translating new sentences. But this deficiency does not affect its ability to estimate the expected counts of the TTS templates, because the posteriors of the TTS templates only depend on the comparative probabilities of the different derivations of a training instance (a pair of tree and string). In fact, as we show in Section 4, LHSN is better </context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>J. May and K. Knight. 2007. Syntactic re-alignment models for machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL-02.</booktitle>
<contexts>
<context position="24490" citStr="Papineni et al., 2002" startWordPosition="4178" endWordPosition="4181">elected for both the development and test data.5 Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the English syntax trees. Modified Kneser-Ney trigram models are trained using SRILM (Stolcke, 2002) upon the Chinese portion of the training data. The trigram language model, as well as the TTS templates generated based on different methods, are used in the TTS transducer. The model weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. 5The total 74,597 sentence pairs used in experiments are those in the FBIS corpus whose English part can be parsed using Charniak (2000)’s parser. 6BLEU-4 scores based on a single reference are much lower than the ones based on multiple references. for all TTS Template t do if it is the last iteration then exp(ψ(EC(t)+αG0(t))) Pr(t) = ex p(ψ((P t0:t0.root=t.root EC(t0))+α)); 1313 E2C C2E U</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings ofACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Rose</author>
<author>E Gurewitz</author>
<author>G C Fox</author>
</authors>
<title>Vector quantization by deterministic annealing.</title>
<date>1992</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>38</volume>
<issue>4</issue>
<pages>1257</pages>
<contexts>
<context position="19387" citStr="Rose et al., 1992" startWordPosition="3287" endWordPosition="3290">Pr(T S R0)β Increase β; M-step: for all TTS Template t do if it is the last iteration then EC(t) Pr(t) = Et0:t0.root=t.root EC(t0); else EC(t) Pr(t) = Et0:t0.lhs=t.lhs EC(t0); Figure 5: EM Algorithm For Estimating TTS Templates own strength and weakness, both of them are used in our EM algorithm: LHSN is used in all EM iterations except the last one to compute the expected counts of the TTS templates, and ROOTN is used in the last EM iteration to compute the final probabilities of the TTS templates. This two-stage normalization method is denoted as MIXN in this paper. Deterministic Annealing (Rose et al., 1992) is is used in our system to speed up the training process, similar to Goldwater et al. (2006). We start from a high temperature and gradually decrease the temperature to 1; we find that the initial high temperature can also help small templates to survive the initial iterations. The complete EM framework is sketched in Figure 3, where Q is the inverse of the specified temperature, and EC denotes the expected count. 3.1 Bayesian Inference with the Dirichlet Process Prior Bayesian inference plus the Dirichlet Process (DP) have been shown to effectively prevent MT models from overfitting the tra</context>
</contexts>
<marker>Rose, Gurewitz, Fox, 1992</marker>
<rawString>K. Rose, E. Gurewitz, and G. C. Fox. 1992. Vector quantization by deterministic annealing. IEEE Transactions on Information Theory, 38(4):1249– 1257, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="24093" citStr="Stolcke, 2002" startWordPosition="4113" endWordPosition="4114">S/RHS excluding the words in g’s LHS/RHS. The initial probabilities of the TTS templates are then computed by normalizing their initial counts using LHSN or ROOTN. 4 Experiments We train an English-to-Chinese translation system using the FBIS corpus, where 73,597 sentence pairs are selected as the training data, and 500 sentence pairs with no more than 25 words on the Chinese side are selected for both the development and test data.5 Charniak (2000)’s parser, trained on the Penn Treebank, is used to generate the English syntax trees. Modified Kneser-Ney trigram models are trained using SRILM (Stolcke, 2002) upon the Chinese portion of the training data. The trigram language model, as well as the TTS templates generated based on different methods, are used in the TTS transducer. The model weights of the transducer are tuned based on the development set using a grid-based line search, and the translation results are evaluated based on a single Chinese reference6 using BLEU-4 (Papineni et al., 2002). Huang et al. (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same sou</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In International Conference on Spoken Language Processing, volume 2, pages 901– 904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING-96,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="1345" citStr="Vogel et al., 1996" startWordPosition="194" endWordPosition="197">provement over the baseline rules extracted from word-level alignments. 1 Introduction Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments (Blunsom et al., 2008; DeNero et al., 2008). Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al., 1993) or the Hidden Markov Model (HMM) (Vogel et al., 1996). We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (Liu et al., 2006; Huang et al., 2006; Galley et al., 2006; May and Knight, 2007), as opposed to formally syntactic systems such as Hiero (Chiang, 2007). The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given. Given a sentence pair and syntax tree over one side, </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In COLING-96, pages 836–841.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>