<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.9990825">
Simultaneous Word-Morpheme Alignment for
Statistical Machine Translation
</title>
<author confidence="0.997526">
Elif Eyig¨oz Daniel Gildea Kemal Oflazer
</author>
<affiliation confidence="0.811190333333333">
Computer Science Computer Science Computer Science
University of Rochester University of Rochester Carnegie Mellon University
Rochester, NY 14627 Rochester, NY 14627 PO Box 24866, Doha, Qatar
</affiliation>
<sectionHeader confidence="0.981095" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998865533333333">
Current word alignment models for statisti-
cal machine translation do not address mor-
phology beyond merely splitting words. We
present a two-level alignment model that dis-
tinguishes between words and morphemes, in
which we embed an IBM Model 1 inside an
HMM based word alignment model. The
model jointly induces word and morpheme
alignments using an EM algorithm. We eval-
uated our model on Turkish-English parallel
data. We obtained significant improvement of
BLEU scores over IBM Model 4. Our results
indicate that utilizing information from mor-
phology improves the quality of word align-
ments.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908941176471">
All current state-of-the-art approaches to SMT rely
on an automatically word-aligned corpus. However,
current alignment models do not take into account
the morpheme, the smallest unit of syntax, beyond
merely splitting words. Since morphology has not
been addressed explicitly in word alignment models,
researchers have resorted to tweaking SMT systems
by manipulating the content and the form of what
should be the so-called “word”.
Since the word is the smallest unit of translation
from the standpoint of word alignment models, the
central focus of research on translating morphologi-
cally rich languages has been decomposition of mor-
phologically complex words into tokens of the right
granularity and representation for machine transla-
tion. Chung and Gildea (2009) and Naradowsky and
Toutanova (2011) use unsupervised methods to find
</bodyText>
<page confidence="0.988747">
32
</page>
<bodyText confidence="0.999828277777778">
word segmentations that create a one-to-one map-
ping of words in both languages. Al-Onaizan et al.
(1999), ˇCmejrek et al. (2003), and Goldwater and
McClosky (2005) manipulate morphologically rich
languages by selective lemmatization. Lee (2004)
attempts to learn the probability of deleting or merg-
ing Arabic morphemes for Arabic to English trans-
lation. Niessen and Ney (2000) split German com-
pound nouns, and merge German phrases that cor-
respond to a single English word. Alternatively,
Yeniterzi and Oflazer (2010) manipulate words of
the morphologically poor side of a language pair
to mimic having a morphological structure similar
to the richer side via exploiting syntactic structure,
in order to improve the similarity of words on both
sides of the translation.
We present an alignment model that assumes in-
ternal structure for words, and we can legitimately
talk about words and their morphemes in line with
the linguistic conception of these terms. Our model
avoids the problem of collapsing words and mor-
phemes into one single category. We adopt a two-
level representation of alignment: the first level in-
volves word alignment, the second level involves
morpheme alignment in the scope of a given word
alignment. The model jointly induces word and
morpheme alignments using an EM algorithm.
We develop our model in two stages. Our initial
model is analogous to IBM Model 1: the first level
is a bag of words in a pair of sentences, and the sec-
ond level is a bag of morphemes. In this manner,
we embed one IBM Model 1 in the scope of another
IBM Model 1. At the second stage, by introducing
distortion probabilities at the word level, we develop
an HMM extension of the initial model.
We evaluated the performance of our model on the
</bodyText>
<note confidence="0.450491">
Proceedings of NAACL-HLT 2013, pages 32–40,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9991802">
Turkish-English pair both on hand-aligned data and
by running end-to-end machine translation experi-
ments. To evaluate our results, we created gold word
alignments for 75 Turkish-English sentences. We
obtain significant improvement of AER and BLEU
scores over IBM Model 4. Section 2.1 introduces
the concept of morpheme alignment in terms of its
relation to word alignment. Section 2.2 presents
the derivation of the EM algorithm and Section 3
presents the results of our experiments.
</bodyText>
<sectionHeader confidence="0.998317" genericHeader="method">
2 Two-level Alignment Model (TAM)
</sectionHeader>
<subsectionHeader confidence="0.861577">
2.1 Morpheme Alignment
</subsectionHeader>
<bodyText confidence="0.999973153846154">
Following the standard alignment models of Brown
et al. (1993), we assume one-to-many alignment for
both words and morphemes. A word alignment aw
(or only a) is a function mapping a set of word po-
sitions in a source language sentence to a set of
word positions in a target language sentence. A mor-
pheme alignment am is a function mapping a set of
morpheme positions in a source language sentence
to a set of morpheme positions in a target language
sentence. A morpheme position is a pair of integers
(j, k), which defines a word position j and a relative
morpheme position k in the word at position j. The
alignments below are depicted in Figures 1 and 2.
</bodyText>
<equation confidence="0.948623">
aw(1) = 1 am(2,1) = (1, 1) aw(2) = 1
</equation>
<bodyText confidence="0.970338941176471">
Figure 1 shows a word alignment between two sen-
tences. Figure 2 shows the morpheme alignment be-
tween same sentences. We assume that all unaligned
morphemes in a sentence map to a special null mor-
pheme.
A morpheme alignment am and a word alignment
aw are compatible if and only if they satisfy the fol-
lowing conditions: If the morpheme alignment am
maps a morpheme of e to a morpheme of f, then the
word alignment aw maps e to f. If the word align-
ment aw maps e to f, then the morpheme alignment
am maps at least one morpheme of e to a morpheme
of f. If the word alignment aw maps e to null, then
all of its morphemes are mapped to null. In sum, a
morpheme alignment am and a word alignment aw
are compatible if and only if:
d j, k, m, n E N+, I s, t E N+
</bodyText>
<equation confidence="0.993482">
[am(j, k) = (m, n) aw(j) = m] n
[aw(j) = m am(j, s) = (m, t)] n
[aw(j) = null am(j, k) = null] (1)
</equation>
<bodyText confidence="0.999388526315789">
Please note that, according to this definition of com-
patibility, ‘am(j, k) = null’ does not necessarily im-
ply ‘aw(j) = null’.
A word alignment induces a set of compati-
ble morpheme alignments. However, a morpheme
alignment induces a unique word alignment. There-
fore, if a morpheme alignment am and a word align-
ment aw are compatible, then the word alignment is
aw is recoverable from the morpheme alignment am.
The two-level alignment model (TAM), like
IBM Model 1, defines an alignment between words
of a sentence pair. In addition, it defines a mor-
pheme alignment between the morphemes of a sen-
tence pair.
The problem domain of IBM Model 1 is defined
over alignments between words, which is depicted
as the gray box in Figure 1. In Figure 2, the smaller
boxes embedded inside the main box depict the new
problem domain of TAM. Given the word align-
ments in Figure 1, we are presented with a new
alignment problem defined over their morphemes.
The new alignment problem is constrained by the
given word alignment. We, like IBM Model 1, adopt
a bag-of-morphemes approach to this new problem.
We thus embed one IBM Model 1 into the scope of
another IBM Model 1, and formulate a second-order
interpretation of IBM Model 1.
TAM, like IBM Model 1, assumes that words and
morphemes are translated independently of their
context. The units of translation are both words and
morphemes. Both the word alignment aw and the
morpheme alignment am are hidden variables that
need to be learned from the data using the EM algo-
rithm.
In IBM Model 1, p(e|f), the probability of trans-
lating the sentence f into e with any alignment is
computed by summing over all possible word align-
ments:
</bodyText>
<equation confidence="0.929427">
p(e|f) = � p(a, e|f)
a
</equation>
<page confidence="0.996538">
33
</page>
<figureCaption confidence="0.999627">
Figure 1: Word alignment Figure 2: Morpheme alignment
</figureCaption>
<bodyText confidence="0.999751">
In TAM, the probability of translating the sentence
f into e with any alignment is computed by sum-
ming over all possible word alignments and all pos-
sible morpheme alignments that are compatible with
a given word alignment aw:
</bodyText>
<equation confidence="0.978226">
p(e|f) = X p(aw, e|f) X p(am, e|aw, f) (2)
aw am
</equation>
<bodyText confidence="0.999450857142857">
where am stands for a morpheme alignment. Since
the morpheme alignment am is in the scope of a
given word alignment aw, am is constrained by aw.
In IBM Model 1, we compute the probability of
translating the sentence f into e by summing over
all possible word alignments between the words of f
and e:
</bodyText>
<equation confidence="0.922444">
t(ej|fi) (3)
</equation>
<bodyText confidence="0.923717333333333">
where t(ej  |fi) is the word translation probability
of ej given fi. R(e, f) substitutes P(le|lf)
(lf +1)le for easy
readability.1
In TAM, the probability of translating the sen-
tence f into e is computed as follows:
</bodyText>
<equation confidence="0.925763">
Word
!t(ekj |fni )
Morpheme
</equation>
<bodyText confidence="0.999898666666667">
where fni is the nth morpheme of the word at po-
sition i. The right part of this equation, the con-
tribution of morpheme translation probabilities, is
</bodyText>
<equation confidence="0.534873">
1le = |e |is the number of words in sentence e and lf = |f|.
</equation>
<bodyText confidence="0.999970523809524">
in the scope of the left part. In the right part, we
compute the probability of translating the word fi
into the word ej by summing over all possible mor-
pheme alignments between the morphemes of ej and
fi. R(ej, fi) is equivalent to R(e, f) except for the
fact that its domain is not the set of sentences but
the set of words. The length of words ej and fi in
R(ej, fi) are the number of morphemes of ej and fi.
The left part, the contribution of word transla-
tion probabilities alone, equals Eqn. 3. Therefore,
canceling the contribution of morpheme translation
probabilities reduces TAM to IBM Model 1. In
our experiments, we call this reduced version of
TAM ‘word-only’ (IBM). TAM with the contribu-
tion of both word and morpheme translation proba-
bilities, as the equation above, is called ‘word-and-
morpheme’. Finally, we also cancel out the con-
tribution of word translation probabilities, which is
called ‘morpheme-only’. In the ‘morpheme-only’
version of TAM, t(ej|fi) equals 1. Bellow is the
equation of p(e|f) in the morpheme-only model.
</bodyText>
<equation confidence="0.997877">
p(e|f) =
R(ej, fi)t(ekj |fni ) (4)
</equation>
<bodyText confidence="0.997001454545455">
Please note that, although this version of the two-
level alignment model does not use word translation
probabilities, it is also a word-aware model, as mor-
pheme alignments are restricted to correspond to a
valid word alignment according to Eqn. 1. When
presented with words that exhibit no morphology,
the morpheme-only version of TAM is equivalent to
IBM Model 1, as every single-morpheme word is it-
self a morpheme.
Deficiency and Non-Deficiency of TAM We
present two versions of TAM, the word-and-
</bodyText>
<equation confidence="0.999762958333333">
p(e|f) = R(e, f)
Y |e|
j=1
|f|
X
i=0
R(e, f) Y |e ||f |t(ej|fi)
j=1 X
i=0
R(ej, fi) |ej ||fi|
Y X
k=1 n=0
Y |e|
j=1
R(e, f)
|fi|
X
n=0
|f|
X
i=0
|ej |
Y
k=1
</equation>
<page confidence="0.983982">
34
</page>
<bodyText confidence="0.99979775">
morpheme and the morpheme-only versions. The
word-and-morpheme version of the model is defi-
cient whereas the morpheme-only model is not.
The word-and-morpheme version is deficient, be-
cause some probability is allocated to cases where
the morphemes generated by the morpheme model
do not match the words generated by the word
model. Moreover, although most languages exhibit
morphology to some extent, they can be input to the
algorithm without morpheme boundaries. This also
causes deficiency in the word-and-morpheme ver-
sion, as single morpheme words are generated twice,
as a word and as a morpheme.
Nevertheless, we observed that the deficient ver-
sion of TAM can perform as good as the non-
deficient version of TAM, and sometimes performs
better. This is not surprising, as deficient word align-
ment models such as IBM Model 3 or discriminative
word alignment models work well in practice.
Goldwater and McClosky (2005) proposed a mor-
pheme aware word alignment model for language
pairs in which the source language words corre-
spond to only one morpheme. Their word alignment
model is:
</bodyText>
<equation confidence="0.985255333333333">
K
P(e|f) = H P(ek|f)
k=0
</equation>
<bodyText confidence="0.999981166666667">
where ek is the kth morpheme of the word e. The
morpheme-only version of our model is a general-
ization of this model. However, there are major dif-
ferences in their and our implementation and exper-
imentation. Their model assumes a fixed number of
possible morphemes associated with any stem in the
language, and if the morpheme ek is not present, it
is assigned a null value.
The null word on the source side is also a null
morpheme, since every single morpheme word is it-
self a morpheme. In TAM, the null word is the null
morpheme that all unaligned morphemes align to.
</bodyText>
<subsectionHeader confidence="0.94688">
2.2 Second-Order Counts
</subsectionHeader>
<bodyText confidence="0.999816">
In TAM, we collect counts for both word translations
and morpheme translations. Unlike IBM Model 1,
</bodyText>
<equation confidence="0.986344333333333">
P(le|lf)
_
R(e, f) — (lf +1)le
</equation>
<bodyText confidence="0.9983034">
of TAM. To compute the conditional probability
P(le|lf), we assume that the length of word e (the
number of morphemes of word e) varies according
to a Poisson distribution with a mean that is linear
with length of the word f.
</bodyText>
<equation confidence="0.990243">
P(le|lf) = FPoisson(le, r · lf)
exp(−r · lf)(r · lf)le
=
lel
</equation>
<bodyText confidence="0.598980666666667">
FPoisson(le, r · lf) expresses the probability that there
are le morphemes in e if the expected number of
morphemes in e is r · lf, where r = ��le]
</bodyText>
<equation confidence="0.839889">
��lf ] is the rate
</equation>
<bodyText confidence="0.9976472">
parameter. Since lf is undefined for null words, we
omit R(e, f) for null words.
We introduce T (e|f), the translation probability
of e given f with all possible morpheme alignments,
as it will occur frequently in the counts of TAM:
</bodyText>
<equation confidence="0.98206">
T(e|f) = t(e|f)R(e,f)
</equation>
<bodyText confidence="0.999856">
The role of T(e|f) in TAM is very similar to the
role of t(e|f) in IBM Model 1. In finding the Viterbi
alignments, we do not take max over the values in
the summation in T (e|f).
</bodyText>
<subsectionHeader confidence="0.592154">
2.2.1 Word Counts
</subsectionHeader>
<bodyText confidence="0.999648375">
Similar to IBM Model 1, we collect counts for
word translations over all possible alignments,
weighted by their probability. In Eqn. 5, the count
function collects evidence from a sentence pair
(e, f) as follows: For all words ej of the sentence e
and for all word alignments aw(j), we collect counts
for a particular input word f and an output word e
iff ej = e and faw(j) = f.
</bodyText>
<equation confidence="0.9865234">
�cw(e|f�e,f,aw) =
1≤j≤|e|
s.t.
e=ej
f=faw(j)
</equation>
<subsectionHeader confidence="0.482316">
2.2.2 Morpheme Counts
</subsectionHeader>
<bodyText confidence="0.999954111111111">
As for morpheme translations, we collect counts
over all possible word and morpheme alignments,
weighted by their probability. The morpheme count
function below collects evidence from a word pair
(e, f) in a sentence pair (e, f) as follows: For all
words ej of the sentence e and for all word align-
ments aw(j), for all morphemes ekj of the word ej
and for all morpheme alignments am(j, k), we col-
lect counts for a particular input morpheme g and an
</bodyText>
<equation confidence="0.808778153846154">
does not cancel out in the counts
|e|
H
k=1
t(ek|fn)
� |f|
n=0
T(e|f)
 |f |
E
i=0
T(e|fi)
(5)
</equation>
<page confidence="0.985545">
35
</page>
<table confidence="0.908940666666667">
output morpheme h iff ej = e and faw(j) = f and
h = ekj and g = fam(j,k).
2lf − 1 words for the HMM model, the positions
&gt; lf stand for null positions between the words of f
(Och and Ney, 2003). We do not allow null to null
jumps. In sum, we enforce the following constraints:
</table>
<equation confidence="0.972868818181818">
cm(h|g; e, f, aw, am) =
T(e|f)
 |f |
0
t(h|g)
� |f |
i=1
T(e|fi)
t(h|fi)
E
1≤j≤|e|
s.t.
e=ej
f=faw(j)
E
1≤k≤|e|
s.t.
h=ekj
g=fam(j,k)
P(i + lf + 1|i0) = p(null|i0)
P(i + lf + 1|i0 + lf + 1) = 0
P(i|i0 + lf + 1) = p(i|i0)
</equation>
<bodyText confidence="0.9996904">
The left part of the morpheme count function is the
same as the word-counts in Eqn. 5. Since it does not
contain h or g, it needs to be computed only once for
each word. The right part of the equation is familiar
from the IBM Model 1 counts.
</bodyText>
<subsectionHeader confidence="0.961751">
2.3 IIMM Extension
</subsectionHeader>
<bodyText confidence="0.998961888888889">
We implemented TAM with the HMM extension
(Vogel et al., 1996) at the word level. We redefine
p(e|f) as follows:
In the HMM extension of TAM, we perform
forward-backward training using the word counts in
Eqn. 5 as the emission probabilities. We calculate
the posterior word translation probabilities for each
ej and fi such that 1 ≤ j ≤ le and 1 ≤ i ≤ 2lf − 1
as follows:
</bodyText>
<equation confidence="0.962301625">
αj(i)βj(i)
γj(i) =
αj(m)βj(m)
2lf−1
E
m=1
where α is the forward and β is the backward prob-
|e |abilities of the HMM. The HMM word counts, in
R(e, f) E H p(s(7) |C(faw(1−1))) t(ej|faw(j)) turn, are the posterior word translation probabilities
aw j=1 obtained from the forward-backward training:
E
R(ej, faw(j))
|ej|
H
am k=1
�t(ekj |fam(j,k))
</equation>
<bodyText confidence="0.999977285714286">
where the distortion probability depends on the rel-
ative jump width s(j) = aw(j − 1) − aw(j),
as opposed to absolute positions. The distortion
probability is conditioned on class of the previous
aligned word C(faw(j−1)). We used the mkcls
tool in GIZA (Och and Ney, 2003) to learn the word
classes.
We formulated the HMM extension of TAM only
at the word level. Nevertheless, the morpheme-only
version of TAM also has an HMM extension, as it
is also a word-aware model. To obtain the HMM
extension of the morpheme-only version, substitute
t(ej|faw(j)) with 1 in the equation above.
For the HMM to work correctly, we must han-
dle jumping to and jumping from null positions. We
learn the probabilities of jumping to a null position
from the data. To compute the jump probability from
a null position, we keep track of the nearest previous
source word that does not align to null, and use the
position of the previous non-null word to calculate
the jump width. For this reason, we use a total of
</bodyText>
<equation confidence="0.994483">
Ecw(e|f;e,f,aw) = γj(aw(j))
1≤j≤|e|
s.t.
e=ej
f=faw(j)
</equation>
<bodyText confidence="0.963925">
Likewise, we use the posterior probabilities in HMM
morpheme counts:
</bodyText>
<equation confidence="0.951927">
cm(h|g; e, f, aw, am) =
γj(aw(j)) t(h|g)
 |f |
�
t(h|fi)
i=1
</equation>
<bodyText confidence="0.999943333333333">
The complexity of the HMM extension of TAM is
O(n3m2), where n is the number of words, and m
is the number of morphemes per word.
</bodyText>
<subsectionHeader confidence="0.986964">
2.4 Variational Bayes
</subsectionHeader>
<bodyText confidence="0.9997584">
Moore (2004) showed that the EM algorithm is par-
ticularly susceptible to overfitting in the case of rare
words when training IBM Model 1. In order to pre-
vent overfitting, we use the Variational Bayes ex-
tension of the EM algorithm (Beal, 2003). This
</bodyText>
<figure confidence="0.9607255">
E
1≤j≤|e|
s.t.
e=ej
f=faw(j)
E
1≤k≤|e|
s.t.
h=ekj
g=fam(j,k)
36
(a) Kasım 1996’da, T¨urk makamları, ˙Ic¸is¸leri Bakanlı˘gı b¨unyesinde bir kayıp kis¸ileri arama birimi olus¸turdu.
(b) Kasım+Noun 1996+Num–Loc ,+Punc T¨urk+Noun makam+Noun–A3pl–P3sg ,+Punc ˙Ic¸is¸i+Noun–A3pl–
P3sg Bakanlık+Noun–P3sg b¨unye+Noun–P3sg–Loc bir+Det kayıp+Adj kis¸i+Noun–A3pl–Acc ara+Verb–
Inf2 birim+Noun–P3sg olus¸+Verb–Caus–Past .+Punc
(c) In November 1996 the Turkish authorities set up a missing persons search unit within the Ministry of the
Interior.
(d) in+IN November+NNP 1996+CD the+DT Turkish+JJ author+NN–ity+NDN.–NNS set+VB–VBD up+RP
a+DT miss+VB–VBG+JJ person+NN–NNS search+NN unit+NN within+IN the+DT minister+NN–
y+NDN. of+IN the+DT interior+NN .+.
(e) In+IN November+NNP 1996+CD the+DT Turkish+JJ authorities+NNS set+VBD up+RP a+DT missing+JJ
persons+NNS search+NN unit+NN within+IN the+DT Ministry+NNP of+IN the+DT Interior+NNP .+.
</figure>
<figureCaption confidence="0.999924">
Figure 3: Turkish-English data examples
</figureCaption>
<bodyText confidence="0.999734166666667">
amounts to a small change to the M step of the orig-
inal EM algorithm. We introduce Dirichlet priors α
to perform an inexact normalization by applying the
function f(v) = exp(o(v)) to the expected counts
collected in the E step, where 0 is the digamma
function (Johnson, 2007).
</bodyText>
<equation confidence="0.980963">
f(E[c(xDy)] + α)
</equation>
<bodyText confidence="0.999654">
We set α to 10−20, a very low value, to have the ef-
fect of anti-smoothing, as low values of α cause the
algorithm to favor words which co-occur frequently
and to penalize words that co-occur rarely.
</bodyText>
<sectionHeader confidence="0.99414" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.997087">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999248130434783">
We trained our model on a Turkish-English paral-
lel corpus of approximately 50K sentences, which
have a maximum of 80 morphemes. Our parallel
data consists mainly of documents in international
relations and legal documents from sources such as
the Turkish Ministry of Foreign Affairs, EU, etc. We
followed a heavily supervised approach in morpho-
logical analysis. The Turkish data was first morpho-
logically parsed (Oflazer, 1994), then disambiguated
(Sak et al., 2007) to select the contextually salient in-
terpretation of words. In addition, we removed mor-
phological features that are not explicitly marked by
an overt morpheme — thus each feature symbol be-
yond the root part-of-speech corresponds to a mor-
pheme. Line (b) of Figure 3 shows an example of
a segmented Turkish sentence. The root is followed
by its part-of-speech tag separated by a ‘+’. The
derivational and inflectional morphemes that follow
the root are separated by ‘–’s. In all experiments,
we used the same segmented version of the Turkish
data, because Turkish is an agglutinative language.
For English, we used the CELEX database
(Baayen et al., 1995) to segment English words into
morphemes. We created two versions of the data:
a segmented version that involves both derivational
and inflectional morphology, and an unsegmented
POS tagged version. The CELEX database provides
tags for English derivational morphemes, which in-
dicate their function: the part-of-speech category the
morpheme attaches to and the part-of-speech cate-
gory it returns. For example, in ‘sparse+ity’ = ‘spar-
sity’, the morpheme -ity attaches to an adjective to
the right and returns a noun. This behavior is repre-
sented as ‘NDA.’ in CELEX, where ‘.’ indicates the
attachment position. We used these tags in addition
to the surface forms of the English morphemes, in
order to disambiguate multiple functions of a single
surface morpheme.
The English sentence in line (d) of Figure 3 ex-
hibits both derivational and inflectional morphology.
For example, ‘author+ity+s’=‘authorities’ has both
an inflectional suffix -s and a derivational suffix -ity,
whereas ‘person+s’ has only an inflectional suffix -s.
For both English and Turkish data, the dashes in
Figure 3 stand for morpheme boundaries, therefore
the strings between the dashes are treated as indi-
</bodyText>
<equation confidence="0.885843">
Bx|y = f(Ej E[c(xjDy)] + α)
</equation>
<page confidence="0.989438">
37
</page>
<table confidence="0.9993098">
Words Morphemes
Tokens Types Tokens Types
English Der+Inf 1,033,726 27,758 1,368,188 19,448
English POS 1,033,726 28,647 1,033,726 28,647
Turkish Der+Inf 812,374 57,249 1,484,673 16,713
</table>
<tableCaption confidence="0.999783">
Table 1: Data statistics
</tableCaption>
<bodyText confidence="0.9992172">
visible units. Table 1 shows the number of words,
the number of morphemes and the respective vocab-
ulary sizes. The average number of morphemes in
segmented Turkish words is 2.69, and the average
length of segmented English words is 1.57.
</bodyText>
<subsectionHeader confidence="0.987228">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999926642857143">
We initialized our baseline word-only model with 5
iterations of IBM Model 1, and further trained the
HMM extension (Vogel et al., 1996) for 5 iterations.
We call this model ‘baseline HMM’ in the discus-
sions. Similarly, we initialized the two versions of
TAM with 5 iterations of the model explained in
Section 2.2, and then trained the HMM extension of
it as explained in Section 2.3 for 5 iterations.
To obtain BLEU scores for TAM models and
our implementation of the word-only model, i.e.
baseline-HMM, we bypassed GIZA++ in the Moses
toolkit (Och and Ney, 2003). We also ran GIZA++
(IBM Model 1–4) on the data. We translated 1000
sentence test sets.
</bodyText>
<sectionHeader confidence="0.999912" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999956823529412">
We evaluated the performance of our model in two
different ways. First, we evaluated against gold
word alignments for 75 Turkish-English sentences.
Second, we used the word Viterbi alignments of our
algorithm to obtain BLEU scores.
Table 2 shows the AER (Och and Ney, 2003) of
the word alignments of the Turkish-English pair and
the translation performance of the word alignments
learned by our models. We report the grow-diag-
final (Koehn et al., 2003) of the Viterbi alignments.
In Table 2, results obtained with different versions
of the English data are represented as follows: ‘Der’
stands for derivational morphology, ‘Inf’ for inflec-
tional morphology, and ‘POS’ for part-of-speech
tags. ‘Der+Inf’ corresponds to the example sen-
tence in line (d) in Figure 3, and ‘POS’ to line (e).
‘DIR’ stands for models with Dirichlet priors, and
‘NO DIR’ stands for models without Dirichlet pri-
ors. All reported results are of the HMM extension
of respective models.
Table 2 shows that using Dirichlet priors hurts
the AER performance of the word-and-morpheme
model in all experiment settings, and benefits the
morpheme-only model in the POS tagged experi-
ment settings.
In order to reduce the effect of nondeterminism,
we run Moses three times per experiment setting,
and report the highest BLEU scores obtained. Since
the BLEU scores we obtained are close, we did a sig-
nificance test on the scores (Koehn, 2004). Table 2
visualizes the partition of the BLEU scores into sta-
tistical significance groups. If two scores within the
same column have the same background color, or the
border between their cells is removed, then the dif-
ference between their scores is not statistically sig-
nificant. For example, the best BLEU scores, which
are in bold, have white background. All scores in a
given experiment setting without white background
are significantly worse than the best score in that ex-
periment setting, unless there is no border separating
them from the best score.
In all experiment settings, the TAM Models per-
form better than the baseline-HMM. Our experi-
ments showed that the baseline-HMM benefits from
Dirichlet priors to a larger extent than the TAM mod-
els. Dirichlet priors help reduce the overfitting in
the case of rare words. The size of the word vo-
cabulary is larger than the size of the morpheme
vocabulary. Therefore the number of rare words is
larger for words than it is for morphemes. Conse-
quently, baseline-HMM, using only the word vocab-
</bodyText>
<page confidence="0.996021">
38
</page>
<table confidence="0.99957680952381">
BLEU BLEU AER
EN to TR TR to EN
Der+Inf POS Der+Inf POS Der+Inf POS
NO Morph only 22.57 22.54 29.30 29.45 0.293 0.276
DIR TAM
Word &amp; Morph
21.95 22.37 28.81 29.01 0.286 0.282
IBM 4 21.82 21.82 27.91 27.91 0.357 0.370
WORD Base-HMM
IBM 4 Morph
21.78 21.38 28.22 28.02 0.381 0.375
17.15 17.94 25.70 26.33 N/A N/A
DIR Morph only 22.18 22.52 29.32 29.98 0.304 0.256
TAM 22.43 29.21
Word &amp; Morph
21.62 29.11 0.338 0.317
IBM 4 21.82 21.82 27.91 27.91 0.357 0.370
WORD Base-HMM
IBM 4 Morph
21.69 21.95 28.76 29.13 0.381 0.377
17.15 17.94 25.70 26.33 N/A N/A
</table>
<tableCaption confidence="0.998787">
Table 2: AER and BLEU Scores
</tableCaption>
<bodyText confidence="0.9995474">
ulary, benefits from the use of Dirichlet priors more
than the TAM models.
In four out of eight experiment settings, the
morpheme-only model performs better than the
word-and-morpheme version of TAM. However,
please note that our extensive experimentation
with TAM models revealed that the superiority
of the morpheme-only model over the word-and-
morpheme model is highly dependent on segmenta-
tion accuracy, degree of segmentation, and morpho-
logical richness of languages.
Finally, we treated morphemes as words and
trained IBM Model 4 on the morpheme segmented
versions of the data. To obtain BLEU scores, we
had to unsegment the translation output: we con-
catenated the prefixes to the morpheme to the right,
and suffixes to the morpheme to the left. Since this
process creates malformed words, the BLEU scores
obtained are much lower than the scores obtained by
IBM Model 4, the baseline and the TAM Models.
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999482777777778">
We presented two versions of a two-level alignment
model for morphologically rich languages. We ob-
served that information provided by word transla-
tions and morpheme translations interact in a way
that enables the model to be receptive to the par-
tial information in rarely occurring words through
their frequently occurring morphemes. We obtained
significant improvement of BLEU scores over IBM
Model 4. In conclusion, morphologically aware
word alignment models prove to be superior to their
word-only counterparts.
Acknowledgments Funded by NSF award IIS-
0910611. Kemal Oflazer acknowledges the gen-
erous support of the Qatar Foundation through
Carnegie Mellon University’s Seed Research pro-
gram. The statements made herein are solely the
responsibility of this author(s), and not necessarily
that of Qatar Foundation.
</bodyText>
<sectionHeader confidence="0.999614" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99940975">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation.
</reference>
<page confidence="0.988196">
39
</page>
<reference confidence="0.999859720588236">
Technical report, Final Report, JHU Summer Work-
shop.
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2) [CD-ROM].
Linguistic Data Consortium, University of Pennsylva-
nia [Distributor], Philadelphia, PA.
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Univer-
sity College London.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
EMNLP, pages 718–726.
Martin ˇCmejrek, Jan Cuˇr´ın, and Jiˇr´ı Havelka. 2003.
Czech-English dependency-based machine transla-
tion. In EACL, pages 83–90, Morristown, NJ, USA.
Association for Computational Linguistics.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
HLT-EMNLP.
Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In EMNLP-CoNLL, pages 296–305,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388–395.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In HLT-NAACL, pages 57–
60.
Robert C. Moore. 2004. Improving IBM word alignment
model 1. In ACL, pages 518–525, Barcelona, Spain,
July.
Jason Naradowsky and Kristina Toutanova. 2011. Unsu-
pervised bilingual morpheme segmentation and align-
ment with context-rich Hidden Semi-Markov Models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 895–904, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Sonja Niessen and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In Computa-
tional Linguistics, pages 1081–1085, Morristown, NJ,
USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kemal Oflazer. 1994. Two-level description of Turkish
morphology. Literary and Linguistic Computing, 9(2).
Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2007.
Morphological disambiguation of Turkish text with
perceptron algorithm. In CICLing, pages 107–118,
Berlin, Heidelberg. Springer-Verlag.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836–841.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from English to Turkish. In
ACL, pages 454–464, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.998638">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.827528">
<title confidence="0.9993455">Simultaneous Word-Morpheme Alignment Statistical Machine Translation</title>
<author confidence="0.981993">Elif Eyig¨oz Daniel Gildea Kemal Oflazer</author>
<affiliation confidence="0.999876">Computer Science Computer Science Computer Science University of Rochester University of Rochester Carnegie Mellon University</affiliation>
<address confidence="0.999715">Rochester, NY 14627 Rochester, NY 14627 PO Box 24866, Doha, Qatar</address>
<abstract confidence="0.9901335625">Current word alignment models for statistical machine translation do not address morphology beyond merely splitting words. We present a two-level alignment model that distinguishes between words and morphemes, in which we embed an IBM Model 1 inside an HMM based word alignment model. The model jointly induces word and morpheme alignments using an EM algorithm. We evaluated our model on Turkish-English parallel data. We obtained significant improvement of BLEU scores over IBM Model 4. Our results indicate that utilizing information from morphology improves the quality of word alignments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<contexts>
<context position="1868" citStr="Al-Onaizan et al. (1999)" startWordPosition="275" endWordPosition="278">resorted to tweaking SMT systems by manipulating the content and the form of what should be the so-called “word”. Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find 32 word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. (1999), ˇCmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to i</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation.</rawString>
</citation>
<citation valid="false">
<tech>Technical report, Final Report, JHU Summer Workshop.</tech>
<marker></marker>
<rawString>Technical report, Final Report, JHU Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>L Gulikers</author>
</authors>
<date>1995</date>
<booktitle>The CELEX Lexical Database (Release</booktitle>
<volume>2</volume>
<institution>[CD-ROM]. Linguistic Data Consortium, University of Pennsylvania [Distributor],</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="19640" citStr="Baayen et al., 1995" startWordPosition="3391" endWordPosition="3394">t interpretation of words. In addition, we removed morphological features that are not explicitly marked by an overt morpheme — thus each feature symbol beyond the root part-of-speech corresponds to a morpheme. Line (b) of Figure 3 shows an example of a segmented Turkish sentence. The root is followed by its part-of-speech tag separated by a ‘+’. The derivational and inflectional morphemes that follow the root are separated by ‘–’s. In all experiments, we used the same segmented version of the Turkish data, because Turkish is an agglutinative language. For English, we used the CELEX database (Baayen et al., 1995) to segment English words into morphemes. We created two versions of the data: a segmented version that involves both derivational and inflectional morphology, and an unsegmented POS tagged version. The CELEX database provides tags for English derivational morphemes, which indicate their function: the part-of-speech category the morpheme attaches to and the part-of-speech category it returns. For example, in ‘sparse+ity’ = ‘sparsity’, the morpheme -ity attaches to an adjective to the right and returns a noun. This behavior is represented as ‘NDA.’ in CELEX, where ‘.’ indicates the attachment p</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1995</marker>
<rawString>R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The CELEX Lexical Database (Release 2) [CD-ROM]. Linguistic Data Consortium, University of Pennsylvania [Distributor], Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
</authors>
<title>Variational Algorithms for Approximate Bayesian Inference.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University College London.</institution>
<contexts>
<context position="17023" citStr="Beal, 2003" startWordPosition="3006" endWordPosition="3007">r this reason, we use a total of Ecw(e|f;e,f,aw) = γj(aw(j)) 1≤j≤|e| s.t. e=ej f=faw(j) Likewise, we use the posterior probabilities in HMM morpheme counts: cm(h|g; e, f, aw, am) = γj(aw(j)) t(h|g) |f | � t(h|fi) i=1 The complexity of the HMM extension of TAM is O(n3m2), where n is the number of words, and m is the number of morphemes per word. 2.4 Variational Bayes Moore (2004) showed that the EM algorithm is particularly susceptible to overfitting in the case of rare words when training IBM Model 1. In order to prevent overfitting, we use the Variational Bayes extension of the EM algorithm (Beal, 2003). This E 1≤j≤|e| s.t. e=ej f=faw(j) E 1≤k≤|e| s.t. h=ekj g=fam(j,k) 36 (a) Kasım 1996’da, T¨urk makamları, ˙Ic¸is¸leri Bakanlı˘gı b¨unyesinde bir kayıp kis¸ileri arama birimi olus¸turdu. (b) Kasım+Noun 1996+Num–Loc ,+Punc T¨urk+Noun makam+Noun–A3pl–P3sg ,+Punc ˙Ic¸is¸i+Noun–A3pl– P3sg Bakanlık+Noun–P3sg b¨unye+Noun–P3sg–Loc bir+Det kayıp+Adj kis¸i+Noun–A3pl–Acc ara+Verb– Inf2 birim+Noun–P3sg olus¸+Verb–Caus–Past .+Punc (c) In November 1996 the Turkish authorities set up a missing persons search unit within the Ministry of the Interior. (d) in+IN November+NNP 1996+CD the+DT Turkish+JJ author+NN</context>
</contexts>
<marker>Beal, 2003</marker>
<rawString>Matthew J. Beal. 2003. Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4236" citStr="Brown et al. (1993)" startWordPosition="658" endWordPosition="661">nal Linguistics Turkish-English pair both on hand-aligned data and by running end-to-end machine translation experiments. To evaluate our results, we created gold word alignments for 75 Turkish-English sentences. We obtain significant improvement of AER and BLEU scores over IBM Model 4. Section 2.1 introduces the concept of morpheme alignment in terms of its relation to word alignment. Section 2.2 presents the derivation of the EM algorithm and Section 3 presents the results of our experiments. 2 Two-level Alignment Model (TAM) 2.1 Morpheme Alignment Following the standard alignment models of Brown et al. (1993), we assume one-to-many alignment for both words and morphemes. A word alignment aw (or only a) is a function mapping a set of word positions in a source language sentence to a set of word positions in a target language sentence. A morpheme alignment am is a function mapping a set of morpheme positions in a source language sentence to a set of morpheme positions in a target language sentence. A morpheme position is a pair of integers (j, k), which defines a word position j and a relative morpheme position k in the word at position j. The alignments below are depicted in Figures 1 and 2. aw(1) </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation. In</title>
<date>2009</date>
<booktitle>EMNLP,</booktitle>
<pages>718--726</pages>
<contexts>
<context position="1691" citStr="Chung and Gildea (2009)" startWordPosition="247" endWordPosition="250">account the morpheme, the smallest unit of syntax, beyond merely splitting words. Since morphology has not been addressed explicitly in word alignment models, researchers have resorted to tweaking SMT systems by manipulating the content and the form of what should be the so-called “word”. Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find 32 word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. (1999), ˇCmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In EMNLP, pages 718–726.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin ˇCmejrek</author>
<author>Jan Cuˇr´ın</author>
<author>Jiˇr´ı Havelka</author>
</authors>
<title>Czech-English dependency-based machine translation.</title>
<date>2003</date>
<booktitle>In EACL,</booktitle>
<pages>83--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>ˇCmejrek, Cuˇr´ın, Havelka, 2003</marker>
<rawString>Martin ˇCmejrek, Jan Cuˇr´ın, and Jiˇr´ı Havelka. 2003. Czech-English dependency-based machine translation. In EACL, pages 83–90, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP.</booktitle>
<contexts>
<context position="1927" citStr="Goldwater and McClosky (2005)" startWordPosition="284" endWordPosition="287">ontent and the form of what should be the so-called “word”. Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find 32 word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. (1999), ˇCmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the transla</context>
<context position="11168" citStr="Goldwater and McClosky (2005)" startWordPosition="1916" endWordPosition="1919">h the words generated by the word model. Moreover, although most languages exhibit morphology to some extent, they can be input to the algorithm without morpheme boundaries. This also causes deficiency in the word-and-morpheme version, as single morpheme words are generated twice, as a word and as a morpheme. Nevertheless, we observed that the deficient version of TAM can perform as good as the nondeficient version of TAM, and sometimes performs better. This is not surprising, as deficient word alignment models such as IBM Model 3 or discriminative word alignment models work well in practice. Goldwater and McClosky (2005) proposed a morpheme aware word alignment model for language pairs in which the source language words correspond to only one morpheme. Their word alignment model is: K P(e|f) = H P(ek|f) k=0 where ek is the kth morpheme of the word e. The morpheme-only version of our model is a generalization of this model. However, there are major differences in their and our implementation and experimentation. Their model assumes a fixed number of possible morphemes associated with any stem in the language, and if the morpheme ek is not present, it is assigned a null value. The null word on the source side i</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers? In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>296--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="18272" citStr="Johnson, 2007" startWordPosition="3168" endWordPosition="3169"> miss+VB–VBG+JJ person+NN–NNS search+NN unit+NN within+IN the+DT minister+NN– y+NDN. of+IN the+DT interior+NN .+. (e) In+IN November+NNP 1996+CD the+DT Turkish+JJ authorities+NNS set+VBD up+RP a+DT missing+JJ persons+NNS search+NN unit+NN within+IN the+DT Ministry+NNP of+IN the+DT Interior+NNP .+. Figure 3: Turkish-English data examples amounts to a small change to the M step of the original EM algorithm. We introduce Dirichlet priors α to perform an inexact normalization by applying the function f(v) = exp(o(v)) to the expected counts collected in the E step, where 0 is the digamma function (Johnson, 2007). f(E[c(xDy)] + α) We set α to 10−20, a very low value, to have the effect of anti-smoothing, as low values of α cause the algorithm to favor words which co-occur frequently and to penalize words that co-occur rarely. 3 Experimental Setup 3.1 Data We trained our model on a Turkish-English parallel corpus of approximately 50K sentences, which have a maximum of 80 morphemes. Our parallel data consists mainly of documents in international relations and legal documents from sources such as the Turkish Ministry of Foreign Affairs, EU, etc. We followed a heavily supervised approach in morphological </context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In EMNLP-CoNLL, pages 296–305, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="22442" citStr="Koehn et al., 2003" startWordPosition="3840" endWordPosition="3843">assed GIZA++ in the Moses toolkit (Och and Ney, 2003). We also ran GIZA++ (IBM Model 1–4) on the data. We translated 1000 sentence test sets. 4 Results and Discussion We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Second, we used the word Viterbi alignments of our algorithm to obtain BLEU scores. Table 2 shows the AER (Och and Ney, 2003) of the word alignments of the Turkish-English pair and the translation performance of the word alignments learned by our models. We report the grow-diagfinal (Koehn et al., 2003) of the Viterbi alignments. In Table 2, results obtained with different versions of the English data are represented as follows: ‘Der’ stands for derivational morphology, ‘Inf’ for inflectional morphology, and ‘POS’ for part-of-speech tags. ‘Der+Inf’ corresponds to the example sentence in line (d) in Figure 3, and ‘POS’ to line (e). ‘DIR’ stands for models with Dirichlet priors, and ‘NO DIR’ stands for models without Dirichlet priors. All reported results are of the HMM extension of respective models. Table 2 shows that using Dirichlet priors hurts the AER performance of the word-and-morpheme </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="23394" citStr="Koehn, 2004" startWordPosition="3996" endWordPosition="3997"> stands for models with Dirichlet priors, and ‘NO DIR’ stands for models without Dirichlet priors. All reported results are of the HMM extension of respective models. Table 2 shows that using Dirichlet priors hurts the AER performance of the word-and-morpheme model in all experiment settings, and benefits the morpheme-only model in the POS tagged experiment settings. In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). Table 2 visualizes the partition of the BLEU scores into statistical significance groups. If two scores within the same column have the same background color, or the border between their cells is removed, then the difference between their scores is not statistically significant. For example, the best BLEU scores, which are in bold, have white background. All scores in a given experiment setting without white background are significantly worse than the best score in that experiment setting, unless there is no border separating them from the best score. In all experiment settings, the TAM Mode</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="2008" citStr="Lee (2004)" startWordPosition="295" endWordPosition="296">ranslation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find 32 word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. (1999), ˇCmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, an</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-suk Lee. 2004. Morphological analysis for statistical machine translation. In HLT-NAACL, pages 57– 60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving IBM word alignment model 1. In</title>
<date>2004</date>
<booktitle>ACL,</booktitle>
<pages>518--525</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="16793" citStr="Moore (2004)" startWordPosition="2965" endWordPosition="2966"> from the data. To compute the jump probability from a null position, we keep track of the nearest previous source word that does not align to null, and use the position of the previous non-null word to calculate the jump width. For this reason, we use a total of Ecw(e|f;e,f,aw) = γj(aw(j)) 1≤j≤|e| s.t. e=ej f=faw(j) Likewise, we use the posterior probabilities in HMM morpheme counts: cm(h|g; e, f, aw, am) = γj(aw(j)) t(h|g) |f | � t(h|fi) i=1 The complexity of the HMM extension of TAM is O(n3m2), where n is the number of words, and m is the number of morphemes per word. 2.4 Variational Bayes Moore (2004) showed that the EM algorithm is particularly susceptible to overfitting in the case of rare words when training IBM Model 1. In order to prevent overfitting, we use the Variational Bayes extension of the EM algorithm (Beal, 2003). This E 1≤j≤|e| s.t. e=ej f=faw(j) E 1≤k≤|e| s.t. h=ekj g=fam(j,k) 36 (a) Kasım 1996’da, T¨urk makamları, ˙Ic¸is¸leri Bakanlı˘gı b¨unyesinde bir kayıp kis¸ileri arama birimi olus¸turdu. (b) Kasım+Noun 1996+Num–Loc ,+Punc T¨urk+Noun makam+Noun–A3pl–P3sg ,+Punc ˙Ic¸is¸i+Noun–A3pl– P3sg Bakanlık+Noun–P3sg b¨unye+Noun–P3sg–Loc bir+Det kayıp+Adj kis¸i+Noun–A3pl–Acc ara+Ve</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving IBM word alignment model 1. In ACL, pages 518–525, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Naradowsky</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised bilingual morpheme segmentation and alignment with context-rich Hidden Semi-Markov Models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>895--904</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1727" citStr="Naradowsky and Toutanova (2011)" startWordPosition="252" endWordPosition="255">allest unit of syntax, beyond merely splitting words. Since morphology has not been addressed explicitly in word alignment models, researchers have resorted to tweaking SMT systems by manipulating the content and the form of what should be the so-called “word”. Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find 32 word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. (1999), ˇCmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor s</context>
</contexts>
<marker>Naradowsky, Toutanova, 2011</marker>
<rawString>Jason Naradowsky and Kristina Toutanova. 2011. Unsupervised bilingual morpheme segmentation and alignment with context-rich Hidden Semi-Markov Models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 895–904, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT quality with morpho-syntactic analysis.</title>
<date>2000</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>1081--1085</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2140" citStr="Niessen and Ney (2000)" startWordPosition="315" endWordPosition="318">ch languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find 32 word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. (1999), ˇCmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, and we can legitimately talk about words and their morphemes in line with the linguistic conception of these terms. Our model avoids t</context>
</contexts>
<marker>Niessen, Ney, 2000</marker>
<rawString>Sonja Niessen and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Computational Linguistics, pages 1081–1085, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="14174" citStr="Och and Ney, 2003" startWordPosition="2480" endWordPosition="2483">ility. The morpheme count function below collects evidence from a word pair (e, f) in a sentence pair (e, f) as follows: For all words ej of the sentence e and for all word alignments aw(j), for all morphemes ekj of the word ej and for all morpheme alignments am(j, k), we collect counts for a particular input morpheme g and an does not cancel out in the counts |e| H k=1 t(ek|fn) � |f| n=0 T(e|f) |f | E i=0 T(e|fi) (5) 35 output morpheme h iff ej = e and faw(j) = f and h = ekj and g = fam(j,k). 2lf − 1 words for the HMM model, the positions &gt; lf stand for null positions between the words of f (Och and Ney, 2003). We do not allow null to null jumps. In sum, we enforce the following constraints: cm(h|g; e, f, aw, am) = T(e|f) |f | 0 t(h|g) � |f | i=1 T(e|fi) t(h|fi) E 1≤j≤|e| s.t. e=ej f=faw(j) E 1≤k≤|e| s.t. h=ekj g=fam(j,k) P(i + lf + 1|i0) = p(null|i0) P(i + lf + 1|i0 + lf + 1) = 0 P(i|i0 + lf + 1) = p(i|i0) The left part of the morpheme count function is the same as the word-counts in Eqn. 5. Since it does not contain h or g, it needs to be computed only once for each word. The right part of the equation is familiar from the IBM Model 1 counts. 2.3 IIMM Extension We implemented TAM with the HMM ext</context>
<context position="15724" citStr="Och and Ney, 2003" startWordPosition="2774" endWordPosition="2777">ows: αj(i)βj(i) γj(i) = αj(m)βj(m) 2lf−1 E m=1 where α is the forward and β is the backward prob|e |abilities of the HMM. The HMM word counts, in R(e, f) E H p(s(7) |C(faw(1−1))) t(ej|faw(j)) turn, are the posterior word translation probabilities aw j=1 obtained from the forward-backward training: E R(ej, faw(j)) |ej| H am k=1 �t(ekj |fam(j,k)) where the distortion probability depends on the relative jump width s(j) = aw(j − 1) − aw(j), as opposed to absolute positions. The distortion probability is conditioned on class of the previous aligned word C(faw(j−1)). We used the mkcls tool in GIZA (Och and Ney, 2003) to learn the word classes. We formulated the HMM extension of TAM only at the word level. Nevertheless, the morpheme-only version of TAM also has an HMM extension, as it is also a word-aware model. To obtain the HMM extension of the morpheme-only version, substitute t(ej|faw(j)) with 1 in the equation above. For the HMM to work correctly, we must handle jumping to and jumping from null positions. We learn the probabilities of jumping to a null position from the data. To compute the jump probability from a null position, we keep track of the nearest previous source word that does not align to </context>
<context position="21876" citStr="Och and Ney, 2003" startWordPosition="3746" endWordPosition="3749"> of segmented English words is 1.57. 3.2 Experiments We initialized our baseline word-only model with 5 iterations of IBM Model 1, and further trained the HMM extension (Vogel et al., 1996) for 5 iterations. We call this model ‘baseline HMM’ in the discussions. Similarly, we initialized the two versions of TAM with 5 iterations of the model explained in Section 2.2, and then trained the HMM extension of it as explained in Section 2.3 for 5 iterations. To obtain BLEU scores for TAM models and our implementation of the word-only model, i.e. baseline-HMM, we bypassed GIZA++ in the Moses toolkit (Och and Ney, 2003). We also ran GIZA++ (IBM Model 1–4) on the data. We translated 1000 sentence test sets. 4 Results and Discussion We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Second, we used the word Viterbi alignments of our algorithm to obtain BLEU scores. Table 2 shows the AER (Och and Ney, 2003) of the word alignments of the Turkish-English pair and the translation performance of the word alignments learned by our models. We report the grow-diagfinal (Koehn et al., 2003) of the Viterbi alignments. In Tab</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of Turkish morphology.</title>
<date>1994</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="18947" citStr="Oflazer, 1994" startWordPosition="3279" endWordPosition="3280">ve the effect of anti-smoothing, as low values of α cause the algorithm to favor words which co-occur frequently and to penalize words that co-occur rarely. 3 Experimental Setup 3.1 Data We trained our model on a Turkish-English parallel corpus of approximately 50K sentences, which have a maximum of 80 morphemes. Our parallel data consists mainly of documents in international relations and legal documents from sources such as the Turkish Ministry of Foreign Affairs, EU, etc. We followed a heavily supervised approach in morphological analysis. The Turkish data was first morphologically parsed (Oflazer, 1994), then disambiguated (Sak et al., 2007) to select the contextually salient interpretation of words. In addition, we removed morphological features that are not explicitly marked by an overt morpheme — thus each feature symbol beyond the root part-of-speech corresponds to a morpheme. Line (b) of Figure 3 shows an example of a segmented Turkish sentence. The root is followed by its part-of-speech tag separated by a ‘+’. The derivational and inflectional morphemes that follow the root are separated by ‘–’s. In all experiments, we used the same segmented version of the Turkish data, because Turkis</context>
</contexts>
<marker>Oflazer, 1994</marker>
<rawString>Kemal Oflazer. 1994. Two-level description of Turkish morphology. Literary and Linguistic Computing, 9(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Has¸im Sak</author>
<author>Tunga G¨ung¨or</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Morphological disambiguation of Turkish text with perceptron algorithm.</title>
<date>2007</date>
<booktitle>In CICLing,</booktitle>
<pages>107--118</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Sak, G¨ung¨or, Sarac¸lar, 2007</marker>
<rawString>Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2007. Morphological disambiguation of Turkish text with perceptron algorithm. In CICLing, pages 107–118, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="14801" citStr="Vogel et al., 1996" startWordPosition="2610" endWordPosition="2613">not allow null to null jumps. In sum, we enforce the following constraints: cm(h|g; e, f, aw, am) = T(e|f) |f | 0 t(h|g) � |f | i=1 T(e|fi) t(h|fi) E 1≤j≤|e| s.t. e=ej f=faw(j) E 1≤k≤|e| s.t. h=ekj g=fam(j,k) P(i + lf + 1|i0) = p(null|i0) P(i + lf + 1|i0 + lf + 1) = 0 P(i|i0 + lf + 1) = p(i|i0) The left part of the morpheme count function is the same as the word-counts in Eqn. 5. Since it does not contain h or g, it needs to be computed only once for each word. The right part of the equation is familiar from the IBM Model 1 counts. 2.3 IIMM Extension We implemented TAM with the HMM extension (Vogel et al., 1996) at the word level. We redefine p(e|f) as follows: In the HMM extension of TAM, we perform forward-backward training using the word counts in Eqn. 5 as the emission probabilities. We calculate the posterior word translation probabilities for each ej and fi such that 1 ≤ j ≤ le and 1 ≤ i ≤ 2lf − 1 as follows: αj(i)βj(i) γj(i) = αj(m)βj(m) 2lf−1 E m=1 where α is the forward and β is the backward prob|e |abilities of the HMM. The HMM word counts, in R(e, f) E H p(s(7) |C(faw(1−1))) t(ej|faw(j)) turn, are the posterior word translation probabilities aw j=1 obtained from the forward-backward traini</context>
<context position="21447" citStr="Vogel et al., 1996" startWordPosition="3672" endWordPosition="3675">[c(xjDy)] + α) 37 Words Morphemes Tokens Types Tokens Types English Der+Inf 1,033,726 27,758 1,368,188 19,448 English POS 1,033,726 28,647 1,033,726 28,647 Turkish Der+Inf 812,374 57,249 1,484,673 16,713 Table 1: Data statistics visible units. Table 1 shows the number of words, the number of morphemes and the respective vocabulary sizes. The average number of morphemes in segmented Turkish words is 2.69, and the average length of segmented English words is 1.57. 3.2 Experiments We initialized our baseline word-only model with 5 iterations of IBM Model 1, and further trained the HMM extension (Vogel et al., 1996) for 5 iterations. We call this model ‘baseline HMM’ in the discussions. Similarly, we initialized the two versions of TAM with 5 iterations of the model explained in Section 2.2, and then trained the HMM extension of it as explained in Section 2.3 for 5 iterations. To obtain BLEU scores for TAM models and our implementation of the word-only model, i.e. baseline-HMM, we bypassed GIZA++ in the Moses toolkit (Och and Ney, 2003). We also ran GIZA++ (IBM Model 1–4) on the data. We translated 1000 sentence test sets. 4 Results and Discussion We evaluated the performance of our model in two differen</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reyyan Yeniterzi</author>
<author>Kemal Oflazer</author>
</authors>
<title>Syntax-tomorphology mapping in factored phrase-based statistical machine translation from English to Turkish. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>454--464</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2280" citStr="Yeniterzi and Oflazer (2010)" startWordPosition="337" endWordPosition="340">ne translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find 32 word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. (1999), ˇCmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, and we can legitimately talk about words and their morphemes in line with the linguistic conception of these terms. Our model avoids the problem of collapsing words and morphemes into one single category. We adopt a twolevel representation of alignment: the first level invo</context>
</contexts>
<marker>Yeniterzi, Oflazer, 2010</marker>
<rawString>Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-tomorphology mapping in factored phrase-based statistical machine translation from English to Turkish. In ACL, pages 454–464, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>