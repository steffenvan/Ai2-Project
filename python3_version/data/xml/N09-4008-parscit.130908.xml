<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007804">
<title confidence="0.943571">
Writing Systems, Transliteration and Decipherment
</title>
<author confidence="0.976301">
Kevin Knight (USC/ISI)
Richard Sproat (CSLU/OHSU)
</author>
<sectionHeader confidence="0.378686" genericHeader="abstract">
Description
</sectionHeader>
<bodyText confidence="0.989925818181818">
Nearly all of the core data that computational linguists deal with is in the
form of text, which is to say that it consists of language data written (usually) in
the standard writing system for the language in question. Yet surprisingly little
is generally understood about how writing systems work. This tutorial will be
divided into three parts. In the first part we discuss the history of writing and
introduce a wide variety of writing systems, explaining their structure and how
they encode language. We end this section with a brief review of how some of
the properties of writing systems are handled in modern encoding systems, such
as Unicode, and some of the continued pitfalls that can occur despite the best
intentions of standardization. The second section of the tutorial will focus on the
problem of transcription between scripts (often termed “transliteration”), and
how this problem—which is important both for machine translation and named
entity recognition—has been addressed. The third section is more theoretical
and, at the same time we hope, more fun. We will discuss the problem of
decipherment and how computational methods might be brought to bear on
the problem of unlocking the mysteries of as yet undeciphered ancient scripts.
We start with a brief review of three famous cases of decipherment. We then
discuss how techniques that have been used in speech recognition and machine
translation might be applied to the problem of decipherment. We end with a
survey of the as-yet undeciphered ancient scripts and give some sense of the
prospects of deciphering them given currently available data.
Outline
</bodyText>
<listItem confidence="0.948352428571429">
First hour:
• History of writing
• Survey of writing systems and how they work
• Modern encodings
Second hour:
• Problems of transcription (transliteration)
• Generative models of transcription
</listItem>
<bodyText confidence="0.330236">
Break
</bodyText>
<page confidence="0.970107">
15
</page>
<note confidence="0.4154855">
Proceedings of NAACL HLT 2009: Tutorials, pages 15–16,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<listItem confidence="0.988969">
• More on generative models of transcription
• Discriminative models
Third Hour
• Famous cases of decipherment
• Prospects for “autodecipherment”
• What’s left to decipher?
</listItem>
<subsectionHeader confidence="0.93703">
Target Audience
</subsectionHeader>
<bodyText confidence="0.9707323">
This tutorial will be of interest to anyone who wishes to have a better under-
standing of how writing (the form of language that most computational linguists
deal with) works, and how such problems as transcription (transliteration) and
decipherment are approached computationally.
Bios
Kevin Knight is a Research Associate Professor in Computer Science at
the University of Southern California, a Senior Research Scientist and Fellow
at the USC/Information Sciences Institute, and Chief Scientist at Language
Weaver. Dr. Knight received a Ph.D. from Carnegie Mellon University in 1992,
and a bachelor’s degree from Harvard University. His current interests include
better statistical machine translation through linguistics, and he is also working
on exploiting cryptographic techniques to solve hard translation problems.
Richard Sproat received his Ph.D. in Linguistics from the Massachusetts
Institute of Technology in 1985. Since then he has worked at AT&amp;T Bell Labs,
at Lucent’s Bell Labs and at AT&amp;T Labs – Research, before joining the faculty
of the University of Illinois, and subsequently the Oregon Health &amp; Science Uni-
versity. Sproat has worked in numerous areas relating to language and compu-
tational linguistics, including syntax, morphology, computational morphology,
articulatory and acoustic phonetics, text processing, text-to-speech synthesis,
writing systems, and text-to-scene conversion.
</bodyText>
<page confidence="0.997141">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.011858">
<title confidence="0.99988">Writing Systems, Transliteration and Decipherment</title>
<author confidence="0.9549745">Kevin Knight Richard Sproat</author>
<title confidence="0.362262">Description</title>
<abstract confidence="0.996791428571428">Nearly all of the core data that computational linguists deal with is in the form of text, which is to say that it consists of language data written (usually) in the standard writing system for the language in question. Yet surprisingly little is generally understood about how writing systems work. This tutorial will be divided into three parts. In the first part we discuss the history of writing and introduce a wide variety of writing systems, explaining their structure and how they encode language. We end this section with a brief review of how some of the properties of writing systems are handled in modern encoding systems, such as Unicode, and some of the continued pitfalls that can occur despite the best intentions of standardization. The second section of the tutorial will focus on the problem of transcription between scripts (often termed “transliteration”), and how this problem—which is important both for machine translation and named entity recognition—has been addressed. The third section is more theoretical and, at the same time we hope, more fun. We will discuss the problem of decipherment and how computational methods might be brought to bear on the problem of unlocking the mysteries of as yet undeciphered ancient scripts. We start with a brief review of three famous cases of decipherment. We then discuss how techniques that have been used in speech recognition and machine translation might be applied to the problem of decipherment. We end with a survey of the as-yet undeciphered ancient scripts and give some sense of the prospects of deciphering them given currently available data.</abstract>
<title confidence="0.6372088">Outline First hour: • History of writing • Survey of writing systems and how they work • Modern encodings</title>
<note confidence="0.736599571428571">Second hour: • Problems of transcription (transliteration) • Generative models of transcription Break 15 of NAACL HLT 2009: pages 15–16, Colorado, June 2009. Association for Computational Linguistics</note>
<title confidence="0.672264">More on generative models of transcription • Discriminative models</title>
<author confidence="0.533975">Third Hour</author>
<abstract confidence="0.958120333333334">Famous cases of decipherment • Prospects for “autodecipherment” • What’s left to decipher? Target Audience This tutorial will be of interest to anyone who wishes to have a better understanding of how writing (the form of language that most computational linguists deal with) works, and how such problems as transcription (transliteration) and decipherment are approached computationally. Bios Knight a Research Associate Professor in Computer Science at the University of Southern California, a Senior Research Scientist and Fellow at the USC/Information Sciences Institute, and Chief Scientist at Language Weaver. Dr. Knight received a Ph.D. from Carnegie Mellon University in 1992, and a bachelor’s degree from Harvard University. His current interests include better statistical machine translation through linguistics, and he is also working on exploiting cryptographic techniques to solve hard translation problems. Sproat his Ph.D. in Linguistics from the Massachusetts Institute of Technology in 1985. Since then he has worked at AT&amp;T Bell Labs, at Lucent’s Bell Labs and at AT&amp;T Labs – Research, before joining the faculty of the University of Illinois, and subsequently the Oregon Health &amp; Science University. Sproat has worked in numerous areas relating to language and computational linguistics, including syntax, morphology, computational morphology, articulatory and acoustic phonetics, text processing, text-to-speech synthesis, writing systems, and text-to-scene conversion.</abstract>
<intro confidence="0.838714">16</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>