<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004141">
<title confidence="0.995669">
A Unified Framework for Automatic Evaluation using
N-gram Co-Occurrence Statistics
</title>
<author confidence="0.993177">
Radu SORICUT Eric BRILL
</author>
<affiliation confidence="0.996708">
Information Sciences Institute Microsoft Research
University of Southern California One Microsoft Way
</affiliation>
<address confidence="0.772996">
4676 Admiralty Way Redmond, WA 98052, USA
Marina del Rey, CA 90292, USA brill@microsoft.com
</address>
<email confidence="0.995371">
radu@isi.edu
</email>
<sectionHeader confidence="0.993772" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999034133333333">
In this paper we propose a unified framework
for automatic evaluation of NLP applications
using N-gram co-occurrence statistics. The
automatic evaluation metrics proposed to date
for Machine Translation and Automatic
Summarization are particular instances from
the family of metrics we propose. We show
that different members of the same family of
metrics explain best the variations obtained
with human evaluations, according to the
application being evaluated (Machine
Translation, Automatic Summarization, and
Automatic Question Answering) and the
evaluation guidelines used by humans for
evaluating such applications.
</bodyText>
<sectionHeader confidence="0.998695" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997908">
With the introduction of the BLEU metric for
machine translation evaluation (Papineni et al,
2002), the advantages of doing automatic
evaluation for various NLP applications have
become increasingly appreciated: they allow for
faster implement-evaluate cycles (by by-passing
the human evaluation bottleneck), less variation in
evaluation performance due to errors in human
assessor judgment, and, not least, the possibility of
hill-climbing on such metrics in order to improve
system performance (Och 2003). Recently, a
second proposal for automatic evaluation has come
from the Automatic Summarization community
(Lin and Hovy, 2003), with an automatic
evaluation metric called ROUGE, inspired by
BLEU but twisted towards the specifics of the
summarization task.
An automatic evaluation metric is said to be
successful if it is shown to have high agreement
with human-performed evaluations. Human
evaluations, however, are subject to specific
guidelines given to the human assessors when
performing the evaluation task; the variation in
human judgment is therefore highly influenced by
these guidelines. It follows that, in order for an
automatic evaluation to agree with a human-
performed evaluation, the evaluation metric used
by the automatic method must be able to account,
at least to some degree, for the bias induced by the
human evaluation guidelines. None of the
automatic evaluation methods proposed to date,
however, explicitly accounts for the different
criteria followed by the human assessors, as they
are defined independently of the guidelines used in
the human evaluations.
In this paper, we propose a framework for
automatic evaluation of NLP applications which is
able to account for the variation in the human
evaluation guidelines. We define a family of
metrics based on N-gram co-occurrence statistics,
for which the automatic evaluation metrics
proposed to date for Machine Translation and
Automatic Summarization can be seen as particular
instances. We show that different members of the
same family of metrics explain best the variations
obtained with human evaluations, according to the
application being evaluated (Machine Translation,
Automatic Summarization, and Question
Answering) and the guidelines used by humans
when evaluating such applications.
</bodyText>
<sectionHeader confidence="0.89695" genericHeader="method">
2 An Evaluation Plane for NLP
</sectionHeader>
<bodyText confidence="0.999962818181818">
In this section we describe an evaluation plane
on which we place various NLP applications
evaluated using various guideline packages. This
evaluation plane is defined by two orthogonal axes
(see Figure 1): an Application Axis, on which we
order NLP applications according to the
faithfulness/compactness ratio that characterizes
the application’s input and output; and a Guideline
Axis, on which we order various human guideline
packages, according to the precision/recall ratio
that characterizes the evaluation guidelines.
</bodyText>
<figureCaption confidence="0.999802">
Figure 1: Evaluation plane for NLP applications
</figureCaption>
<figure confidence="0.99736044">
high
Application
Axis
faithfulness
low
compactness
faithfulness
compactness
MT
QA
adequacy evaluation
TIDES−MT(2002)
AS
fluency evaluation
TIDES−MT(2002)
correctness evaluation
QA(2004)
coverage evaluation
DUC−AS (2001)
low
precision
high
recall Guideline Axis
precision
recall
</figure>
<subsectionHeader confidence="0.995553">
2.1 An Application Axis for Evaluation
</subsectionHeader>
<bodyText confidence="0.99986168">
When trying to define what translating and
summarizing means, one can arguably suggest that
a translation is some “as-faithful-as-possible”
rendering of some given input, whereas a summary
is some “as-compact-as-possible” rendering of
some given input. As such, Machine Translation
(MT) and Automatic Summarization (AS) are on
the extremes of a faithfulness/compactness (f/c)
ratio between inputs and outputs. In between these
two extremes lie various other NLP applications: a
high f/c ratio, although lower than MT’s,
characterizes Automatic Paraphrasing (paraphrase:
To express, interpret, or translate with latitude);
close to the other extreme, a low f/c ratio, although
higher than AS’s, characterizes Automatic
Summarization with view-points (summarization
which needs to focus on a given point of view,
extern to the document(s) to be summarized).
Another NLP application, Automatic Question
Answering (QA), has arguably a close-to-1 f/c
ratio: the task is to render an answer about the
thing(s) inquired for in a question (the faithfulness
side), in a manner that is concise enough to be
regarded as a useful answer (the compactness
side).
</bodyText>
<subsectionHeader confidence="0.999927">
2.2 An Guideline Axis for Evaluation
</subsectionHeader>
<bodyText confidence="0.99993095">
Formal human evaluations make use of various
guidelines that specify what particular aspects of
the output being evaluated are considered
important, for the particular application being
evaluated. For example, human evaluations of MT
(e.g., TIDES 2002 evaluation, performed by NIST)
have traditionally looked at two different aspects
of a translation: adequacy (how much of the
content of the original sentence is captured by the
proposed translation) and fluency (how correct is
the proposed translation sentence in the target
language). In many instances, evaluation
guidelines can be linearly ordered according to the
precision/recall (p/r) ratio they specify. For
example, evaluation guidelines for adequacy
evaluation of MT have a low p/r ratio, because of
the high emphasis on recall (i.e., content is
rewarded) and low emphasis on precision (i.e.,
verbosity is not penalized); on the other hand,
evaluation guidelines for fluency of MT have a
high p/r ratio, because of the low emphasis on
recall (i.e., content is not rewarded) and high
emphasis on wording (i.e., extraneous words are
penalized). Another evaluation we consider in this
paper, the DUC 2001 evaluation for Automatic
Summarization (also performed by NIST), had
specific guidelines for coverage evaluation, which
means a low p/r ratio, because of the high
emphasis on recall (i.e., content is rewarded). Last
but not least, the QA evaluation for correctness we
discuss in Section 4 has a close-to-1 p/r ratio for
evaluation guidelines (i.e., both correct content and
precise answer wording are rewarded).
When combined, the application axis and the
guideline axis define a plane in which particular
evaluations are placed according to their
application/guideline coordinates. In Figure 1 we
illustrate this evaluation plane, and the evaluation
examples mentioned above are placed in this plane
according to their coordinates.
</bodyText>
<sectionHeader confidence="0.9910995" genericHeader="method">
3 A Unified Framework for Automatic
Evaluation
</sectionHeader>
<bodyText confidence="0.999798571428571">
In this section we propose a family of evaluation
metrics based on N-gram co-occurrence statistics.
Such a family of evaluation metrics provides
flexibility in terms of accommodating both various
NLP applications and various values of
precision/recall ratio in the human guideline
packages used to evaluate such applications.
</bodyText>
<subsectionHeader confidence="0.99757">
3.1 A Precision-focused Family of Metrics
</subsectionHeader>
<bodyText confidence="0.999880666666667">
Inspired by the work of Papineni et al. (2002) on
BLEU, we define a precision-focused family of
metrics, using as parameter a non-negative integer
N. Part of the definition includes a list of stop-
words (SW) and a function for extracting the stem
of a given word (ST).
Suppose we have a given NLP application for
which we want to evaluate the candidate answer
set Candidates for some input sequences, given a
reference answer set References. For each
individual candidate answer C, we define S(C,n)
as the multi-set of n-grams obtained from the
candidate answer C after stemming the unigrams
using ST and eliminating the unigrams found in
SW. We therefore define a precision score:
</bodyText>
<equation confidence="0.96530625">
∑ ∑
C Candidates
∈ { } ngram S C n
∈ ( , )
∑ ∑ Count (ngram )
C Candidates
∈ { } ngram S C n
∈ ( , )
</equation>
<bodyText confidence="0.999715083333333">
where Count(ngram) is the number of n-gram
counts, and Countclip(ngram) is the maximum
number of co-occurrences of ngram in the
candidate answer and its reference answer.
Because the denominator in the P(n) formula
consists of a sum over the proposed candidate
answers, this formula is a precision-oriented
formula, penalizing verbose candidates. This
precision score, however, can be made artificially
higher when proposing shorter and shorter
candidate answers. This is offset by adding a
brevity penalty, BP:
</bodyText>
<equation confidence="0.998410333333333">
1, if B⋅|c|≥|r|
=
− r B c
e ( 1   ||/  ||) , if B c r
⋅ &lt;
    |
</equation>
<bodyText confidence="0.999311142857143">
where |c |equals the sum of the lengths of the
proposed answers, |r |equals the sum of the lengths
of the reference answers, and B is a brevity
constant.
We define now a precision-focused family of
metrics, parameterized by a non-negative integer
N, as:
</bodyText>
<equation confidence="0.9815945">
N
log( P(n)))
</equation>
<bodyText confidence="0.999977176470588">
This family of metrics can be interpreted as a
weighted linear average of precision scores for
increasingly longer n-grams. As the values of the
precision scores decrease roughly exponentially
with the increase of N, the logarithm is needed to
obtain a linear average. Note that the metrics of
this family are well-defined only for N’s small
enough to yield non-zero P(n) scores. For test
corpora of reasonable size, the metrics are usually
well-defined for N≤4.
The BLEU proposed by Papineni et al. (2002)
for automatic evaluation of machine translation is
part of the family of metrics PS(N), as the
particular metric obtained when N=4, wn–s are 1/N,
the brevity constant B=1, the list of stop-words SW
is empty, and the stemming function ST is the
identity function.
</bodyText>
<subsectionHeader confidence="0.998034">
3.2 A Recall-focused Family of Metrics
</subsectionHeader>
<bodyText confidence="0.999936444444444">
As proposed by Lin and Hovy (2003), a
precision-focused metric such as BLEU can be
twisted such that it yields a recall-focused metric.
In a similar manner, we define a recall-focused
family of metrics, using as parameter a non-
negative integer N, with a list of stop-words (SW)
and a function for extracting the stem of a given
word (ST) as part of the definition.
As before, suppose we have a given NLP
application for which we want to evaluate the
candidate answer set Candidates for some input
sequences, given a reference answer set
References. For each individual reference answer
R, we define S(R,n) as the multi-set of n-grams
obtained from the reference answer R after
stemming the unigrams using ST and eliminating
the unigrams found in SW. We therefore define a
recall score as:
</bodyText>
<equation confidence="0.97812125">
∑ ∑
R ferences
∈ {Re }ngram S R n
∈ ( , )
∑ ∑ Count (ngram )
R ferences
∈ {Re } ngram S R n
∈ ( , )
</equation>
<bodyText confidence="0.99976">
where, as before, Count(ngram) is the number of
n-gram counts, and Countclip(ngram) is the
maximum number of co-occurrences of ngram in
the reference answer and its corresponding
candidate answer. Because the denominator in the
R(n) formula consists of a sum over the reference
answers, this formula is essentially a recall-
oriented formula, which penalizes incomplete
candidates. This recall score, however, can be
made artificially higher when proposing longer and
longer candidate answers. This is offset by adding
a wordiness penalty, WP:
</bodyText>
<equation confidence="0.996288666666667">
1, if W⋅|c|≤|r|
−W c r
e(1   ||/  ||) , if W⋅  |c |&gt; |r |
</equation>
<bodyText confidence="0.997358">
where |c |and |r |are defined as before, and W is a
wordiness constant.
We define now a recall-focused family of
metrics, parameterized by a non-negative integer
N, as:
</bodyText>
<equation confidence="0.998115">
log( ( )))
R n
</equation>
<bodyText confidence="0.999950538461538">
This family of metrics can be interpreted as a
weighted linear average of recall scores for
increasingly longer n-grams. For test corpora of
reasonable size, the metrics are usually well-
defined for N≤4.
The ROUGE metric proposed by Lin and Hovy
(2003) for automatic evaluation of machine-
produced summaries is part of the family of
metrics RS(N), as the particular metric obtained
when N=1, wn–s are 1/N, the wordiness constant
W=∞, the list of stop-words SW is their own , and
the stemming function ST is the one defined by the
Porter stemmer (Porter 1980).
</bodyText>
<subsectionHeader confidence="0.540013">
3.3 A Unified Framework for Automatic
Evaluation
</subsectionHeader>
<bodyText confidence="0.995421">
The precision-focused metric family PS(N) and
the recall-focused metric family RS(N) defined in
</bodyText>
<figure confidence="0.967629392857143">
)
Count
clip
(ngram
=
P(n)
BP



wn
PS N BP
( ) =
⋅ exp(
1
∑=
n
)
Count
clip
(ngram
=
R(n)
WP


= 1

</figure>
<equation confidence="0.783099444444444">
N
wn
=
⋅ exp(
( )
RS N WP
1
∑=
n
</equation>
<bodyText confidence="0.5466785">
the previous sections are unified under the metric
family AEv(α,N), defined as:
</bodyText>
<equation confidence="0.966">
RS(N)PS(N)
AEv N
( , )
α _ α • RS(N) + (1−α) • PS(N)
</equation>
<bodyText confidence="0.999951098039216">
This formula extends the well-known F-measure
that combines recall and precision numbers into a
single number (van Rijsbergen, 1979), by
combining recall and precision metric families into
a single metric family. For α=0, AEv(α,N) is the
same as the recall-focused family of metrics
RS(N); for α=1, AEv(α,�N) is the same as the
precision-focused family of metrics PS(N). For α
in between 0 and 1, AEv(α,N) are metrics that
balance recall and precision according to α. For the
rest of the paper, we restrict the parameters of the
AEv(α,N) family as follows: α varies continuously
in [0,1], N varies discretely in {1,2,3,4}, the linear
weights wn are 1/N, the brevity constant is 1, the
wordiness constant is 2, the list of stop-words SW
is our own 626 stop-word list, and the stemming
function ST is the one defined by the Porter
stemmer (Porter 1980).
We establish a correspondence between the
parameters of the family of metrics AEv(α,N) and
the evaluation plane in Figure 1 as follows: α
parameterizes the guideline axis (x-axis) of the
plane, such that α=0 corresponds to a low
precision/recall (p/r) ratio, and α=1 corresponds to
a high p/r ratio; N parameterizes the application
axis (y-axis) of the plane, such that N=1
corresponds to a low faithfulness/compactness (f/c)
ratio (unigram statistics allow for a low
representation of faithfulness, but a high
representation of compactness), and N=4
corresponds to a high f/c ratio (n-gram statistics up
to 4-grams allow for a high representation of
faithfulness, but a low representation of
compactness).
This framework enables us to predict that a
human-performed evaluation is best approximated
by metrics that have similar f/c ratio as the
application being evaluated and similar p/r ratio as
the evaluation package used by the human
assessors. For example, an application with a high
f/c ratio, evaluated using a low p/r ratio evaluation
guideline package (an example of this is the
adequacy evaluation for MT in TIDES 2002), is
best approximated by the automatic evaluation
metric defined by a low α and a high N; an
application with a close-to-1 f/c ratio, evaluated
using an evaluation guideline package
characterized by a close-to-1 p/r ratio (such as the
correctness evaluation for Question Answering in
Section 4.3) is best approximated by an automatic
metric defined by a median α and a median N.
</bodyText>
<sectionHeader confidence="0.977428" genericHeader="method">
4 Evaluating the Evaluation Framework
</sectionHeader>
<bodyText confidence="0.999934482758621">
In this section, we present empirical results
regarding the ability of our family of metrics to
approximate human evaluations of various
applications under various evaluation guidelines.
We measure the amount of approximation of a
human evaluation by an automatic evaluation as
the value of the coefficient of determination R2
between the human evaluation scores and the
automatic evaluation scores for various systems
implementing Machine Translation,
Summarization, and Question Answering
applications. In this framework, the coefficient of
determination R2 is to be interpreted as the
percentage from the total variation of the human
evaluation (that is, why some system’s output is
better than some other system’s output, from the
human evaluator’s perspective) that is captured by
the automatic evaluation (that is, why some
system’s output is better than some other system’s
output, from the automatic evaluation perspective).
The values of R2 vary between 0 and 1, with a
value of 1 indicating that the automatic evaluation
explains perfectly the human evaluation variation,
and a value of 0 indicating that the automatic
evaluation explains nothing from the human
evaluation variation. All the results for the values
of R2 for the family of metrics AEv(α,N) are
reported with α varying from 0 to 1 in 0.1
increments, and N varying from 1 to 4.
</bodyText>
<subsectionHeader confidence="0.971578">
4.1 Machine Translation Evaluation
</subsectionHeader>
<bodyText confidence="0.999955166666667">
The Machine Translation evaluation carried out
by NIST in 2002 for DARPA’s TIDES programme
involved 7 systems that participated in the
Chinese-English track. Each system was evaluated
by a human judge, using one reference extracted
from a list of 4 available reference translations.
Each of the 878 test sentences was evaluated both
for adequacy (how much of the content of the
original sentence is captured by the proposed
translation) and fluency (how correct is the
proposed translation sentence in the target
language). From the publicly available data for this
evaluation (TIDES 2002), we compute the values
of R2 for 7 data points (corresponding to the 7
systems participating in the Chinese-English
track), using as a reference set one of the 4 sets of
reference translations available.
In Table 1, we present the values of the
coefficient of determination R2 for the family of
metrics AEv(α,N), when considering only the
fluency scores from the human evaluation. As
mentioned in Section 2, the evaluation guidelines
for fluency have a high precision/recall ratio,
whereas MT is an application with a high
</bodyText>
<table confidence="0.5804822">
4 76.10 76.45 76.78 77.10 77.40 77.69 77.96 78.21 78.45 78.67 78.87
3 76.11 76.6 77.04 77.44 77.80 78.11 78.38 78.61 78.80 78.94 79.04
2 73.19 74.21 75.07 75.78 76.32 76.72 76.96 77.06 77.03 76.87 76.58
1 31.71 38.22 44.82 51.09 56.59 60.99 64.10 65.90 66.50 66.12 64.99
N/α 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</table>
<tableCaption confidence="0.991009">
Table 1: R2 values for the family of metrics AEv(α,N), for fluency scores in MT evaluation
</tableCaption>
<table confidence="0.8851394">
4 83.04 82.58 82.11 81.61 81.10 80.56 80.01 79.44 78.86 78.26 77.64
3 81.80 81.00 80.16 79.27 78.35 77.39 76.40 75.37 74.31 73.23 72.11
2 80.84 79.46 77.94 76.28 74.51 72.63 70.67 68.64 66.55 64.42 62.26
1 62.16 66.26 69.18 70.59 70.35 68.48 65.24 60.98 56.11 50.98 45.88
N/α 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</table>
<tableCaption confidence="0.99597">
Table 2: R2 values for the family of metrics AEv(α,N), for adequacy scores in MT evaluation
</tableCaption>
<bodyText confidence="0.99993108">
faithfulness/compactness ratio. In this case, our
evaluation framework predicts that the automatic
evaluation metrics that explain most of the
variation in the human evaluation must have a high
α and a high N. As seen in Table 1, our evaluation
framework correctly predicts the automatic
evaluation metrics that explain most of the
variation in the human evaluation: metrics
AEv(1,3), AEv(0.9,3), and AEv(1,4) capture most
of the variation: 79.04%, 78.94%, and 78.87%,
respectively. Since metric AEv(1,4) is almost the
same as the BLEU metric (modulo stemming and
stop word elimination for unigrams), our results
confirm the current practice in the Machine
Translation community, which commonly uses
BLEU for automatic evaluation. For comparison
purposes, we also computed the value of R2 for
fluency using the BLEU score formula given in
(Papineni et al., 2002), for the 7 systems using the
same one reference, and we obtained a similar
value, 78.52%; computing the value of R2 for
fluency using the BLEU scores computed with all 4
references available yielded a lower value for R2,
64.96%, although BLEU scores obtained with
multiple references are usually considered more
reliable.
In Table 2, we present the values of the
coefficient of determination R2 for the family of
metrics AEv(α,N), when considering only the
adequacy scores from the human evaluation. As
mentioned in Section 2, the evaluation guidelines
for adequacy have a low precision/recall ratio,
whereas MT is an application with high
faithfulness/compactness ratio. In this case, our
evaluation framework predicts that the automatic
evaluation metrics that explain most of the
variation in the human evaluation must have a low
α and a high N. As seen in Table 2, our evaluation
framework correctly predicts the automatic
evaluation metric that explains most of the
variation in the human evaluation: metric AEv(0,4)
captures most of the variation, 83.04%. For
comparison purposes, we also computed the value
of R2 for adequacy using the BLEU score formula
given in (Papineni et al., 2002), for the 7 systems
using the same one reference, and we obtain a
similar value, 83.91%; computing the value of R2
for adequacy using the BLEU scores computed
with all 4 references available also yielded a lower
value for R2, 62.21%.
</bodyText>
<subsectionHeader confidence="0.994797">
4.2 Automatic Summarization Evaluation
</subsectionHeader>
<bodyText confidence="0.999724785714286">
The Automatic Summarization evaluation
carried out by NIST for the DUC 2001 conference
involved 15 participating systems. We focus here
on the multi-document summarization task, in
which 4 generic summaries (of 50, 100, 200, and
400 words) were required for a given set of
documents on a single subject. For this evaluation
30 test sets were used, and each system was
evaluated by a human judge using one reference
extracted from a list of 2 reference summaries.
One of the evaluations required the assessors to
judge the coverage of the summaries. The
coverage of a summary was measured by
comparing a system’s units versus the units of a
reference summary, and assessing whether each
system unit expresses all, most, some, hardly any,
or none of the current reference unit. A final
evaluation score for coverage was obtained using a
coverage score computed as a weighted recall
score (see (Lin and Hovy 2003) for more
information on the human summary evaluation).
From the publicly available data for this evaluation
(DUC 2001), we compute the values of R2 for 15
data points available (corresponding to the 15
participating systems).
In Tables 3-4 we present the values of the
coefficient of determination R2 for the family of
metrics AEv(α,N), when considering the coverage
</bodyText>
<table confidence="0.9312556">
4 67.10 66.51 65.91 65.29 64.65 64.00 63.34 62.67 61.99 61.30 60.61
3 69.55 68.81 68.04 67.24 66.42 65.57 64.69 63.79 62.88 61.95 61.00
2 74.43 73.29 72.06 70.74 69.35 67.87 66.33 64.71 63.03 61.30 59.51
1 90.77 90.77 90.66 90.42 90.03 89.48 88.74 87.77 86.55 85.05 83.21
N/α 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</table>
<tableCaption confidence="0.861371">
Table 3: R2 for the family of metrics AEv(α,N), for coverage scores in AS evaluation (200 words)
</tableCaption>
<table confidence="0.9923582">
4 81.24 81.04 80.78 80.47 80.12 79.73 79.30 78.84 78.35 77.84 77.31
3 84.72 84.33 83.86 83.33 82.73 82.08 81.39 80.65 79.88 79.07 78.24
2 89.54 88.56 87.47 86.26 84.96 83.59 82.14 80.65 79.10 77.53 75.92
1 92.28 91.11 89.70 88.07 86.24 84.22 82.05 79.74 77.30 74.77 72.15
N/α 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</table>
<tableCaption confidence="0.995571">
Table 4: R2 for the family of metrics AEv(α,N), for coverage scores in AS evaluation (400 words)
</tableCaption>
<bodyText confidence="0.999897739130435">
scores from the human evaluation, for summaries
of 200 and 400 words, respectively (the values of
R2 for summaries of 50 and 100 words show
similar patterns). As mentioned in Section 2, the
evaluation guidelines for coverage have a low
precision/recall ratio, whereas AS is an application
with low faithfulness/compactness ratio. In this
case, our evaluation framework predicts that the
automatic evaluation metrics that explain most of
the variation in the human evaluation must have a
low α and a low N. As seen in Tables 3-4, our
evaluation framework correctly predicts the
automatic evaluation metric that explain most of
the variation in the human evaluation: metric
AEv(0,1) explains 90.77% and 92.28% of the
variation in the human evaluation of summaries of
length 200 and 400, respectively. Since metric
AEv(0, 1) is almost the same as the ROUGE metric
proposed by Lin and Hovy (2003) (they only differ
in the stop-word list they use), our results also
confirm the proposal for such metrics to be used
for automatic evaluation by the Automatic
Summarization community.
</bodyText>
<subsectionHeader confidence="0.999781">
4.3 Question Answering Evaluation
</subsectionHeader>
<bodyText confidence="0.999943150943397">
One of the most common approaches to
automatic question answering (QA) restricts the
domain of questions to be handled to so-called
factoid questions. Automatic evaluation of factoid
QA is often straightforward, as the number of
correct answers is most of the time limited, and
exhaustive lists of correct answers are available.
When removing the factoid constraint, however,
the set of possible answer to a (complex, beyond-
factoid) question becomes unfeasibly large, and
consequently automatic evaluation becomes a
challenge.
In this section, we focus on an evaluation carried
out in order to assess the performance of a QA
system for answering questions from the
Frequently-Asked-Question (FAQ) domain
(Soricut and Brill, 2004). These are generally
questions requiring a more elaborated answer than
a simple factoid (e.g., questions such as: “How
does a film qualify for an Academy Award?”).
In order to evaluate such a system a human-
performed evaluation was performed, in which 11
versions of the QA system (various modules were
implemented using various algorithms) were
separately evaluated. Each version was evaluated
by a human evaluator, with no reference answer
available. For this evaluation 115 test questions
were used, and the human evaluator was asked to
assess whether the proposed answer was correct,
somehow related, or wrong. A unique ranking
number was achieved using a weighted average of
the scored answers. (See (Soricut and Brill, 2004)
for more details concerning the QA task and the
evaluation procedure.)
One important aspect in the evaluation procedure
was devising criteria for assigning a rating to an
answer which was not neither correct nor wrong.
One of such cases involved so-called flooded
answers: answers which contain the correct
information, along with several other unrelated
pieces of information. A first evaluation has been
carried with a guideline package asking the human
assessor to assign the rating correct to flooded
answers. In Table 5, we present the values of the
coefficient of determination R2 for the family of
metrics AEv(α,N) for this first QA evaluation. On
the guideline side, the guideline package used in
this first QA evaluation has a low precision/recall
ratio, because the human judge is asked to evaluate
based on the content provided by a given answer
(high recall), but is asked to disregard the
conciseness (or lack thereof) of the answer (low
precision); consequently, systems that focus on
</bodyText>
<table confidence="0.9973856">
4 63.40 57.62 51.86 46.26 40.96
3 81.39 76.38 70.76 64.76 58.61
2 91.72 89.21 85.54 80.78 75.14
1 61.61 58.83 55.25 51.04 46.39
N/α 0 0.1 0.2 0.3 0.4
36.02 31.51 27.43 23.78 20.54 17.70
52.51 46.63 41.09 35.97 31.33 27.15
68.87 62.25 55.56 49.04 42.88 37.20
41.55 36.74 32.12 27.85 23.97 20.54
0.5 0.6 0.7 0.8 0.9 1
</table>
<tableCaption confidence="0.754235">
Table 5: R2 for the family of metrics AEv(α,N), for correctness scores, first QA evaluation
</tableCaption>
<table confidence="0.997922">
4 79.94 79.18 75.80 70.63 64.58
3 76.15 80.44 81.19 78.45 73.07
2 67.76 77.48 84.34 86.26 82.75
1 56.55 60.81 59.60 53.56 45.38
N/α 0 0.1 0.2 0.3 0.4
58.35 52.39 46.95 42.11 37.87 34.19
66.27 59.11 52.26 46.08 40.68 36.04
75.24 65.94 56.65 48.32 41.25 35.42
37.40 30.68 25.36 21.26 18.12 15.69
0.5 0.6 0.7 0.8 0.9 1
</table>
<tableCaption confidence="0.986507">
Table 6: R2 for the family of metrics AEv(α,N), for correctness scores, second QA evaluation
</tableCaption>
<bodyText confidence="0.999990264705883">
giving correct and concise answers are not
distinguished from systems that give correct
answers, but have no regard for concision. On the
application side, as mentioned in Section 2, QA is
arguably an application characterized by a close-
to-1 faithfulness/compactness ratio. In this case,
our evaluation framework predicts that the
automatic evaluation metrics that explain most of
the variation in the human evaluation must have a
low α and a median N. As seen in Table 5, our
evaluation framework correctly predicts the
automatic evaluation metric that explain most of
the variation in the human evaluation: metric
AEv(0,2) explains most of the human variation,
91.72%. Note that other members of the AEv(α,N)
family do not explain nearly as well the variation
in the human evaluation. For example, the
ROUGE-like metric AEv(0,1) explains only
61.61% of the human variation, while the BLEU-
like metric AEv(1,4) explains a mere 17.7% of the
human variation (to use such a metric in order to
automatically emulate the human QA evaluation is
close to performing an evaluation assigning
random ratings to the output answers).
In order to further test the prediction power of
our evaluation framework, we carried out a second
QA evaluation, using a different evaluation
guideline package: a flooded answer was rated
only somehow-related. In Table 6, we present the
values of the coefficient of determination R2 for
the family of metrics AEv(α,N) for this second QA
evaluation. Instead of performing this second
evaluation from scratch, we actually simulated it
using the following methodology: 2/3 of the output
answers rated correct of the systems ranked 1st, 2nd,
3rd, and 6th by the previous human evaluation have
been intentionally over-flooded using two long and
out-of-context sentences, while their ratings were
changed from correct to somehow-related. Such a
change simulated precisely the change in the
guideline package, by downgrading flooded
answers. This means that, on the guideline side, the
guideline package used in this second QA
evaluation has a close-to-1 precision/recall ratio,
because the human judge evaluates now based both
on the content and the conciseness of a given
answer. At the same time, the application remains
unchanged, which means that on the application
side we still have a close-to-1
faithfulness/compactness ratio. In this case, our
evaluation framework predicts that the automatic
evaluation metrics that explain most of the
variation in the human evaluation must have a
median α and a median N. As seen in Table 6, our
evaluation framework correctly predicts the
automatic evaluation metric that explain most of
the variation in the human evaluation: metric
AEv(0.3,2) explains most of the variation in the
human evaluation, 86.26%. Also note that, while
the R2 values around AEv(0.3,2) are still
reasonable, evaluation metrics that are further and
further away from it have increasingly lower R2
values, meaning that they are more and more
unreliable for this task. The high correlation of
metric AEv(0.3,2) with human judgment, however,
suggests that such a metric is a good candidate for
performing automatic evaluation of QA systems
that go beyond answering factoid questions.
</bodyText>
<sectionHeader confidence="0.9998" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999818411764706">
In this paper, we propose a unified framework
for automatic evaluation based on N-gram co-
occurrence statistics, for NLP applications for
which a correct answer is usually an unfeasibly
large set (e.g., Machine Translation, Paraphrasing,
Question Answering, Summarization, etc.). The
success of BLEU in doing automatic evaluation of
machine translation output has often led
researchers to blindly try to use this metric for
evaluation tasks for which it was more or less
appropriate (see, e.g., the paper of Lin and Hovy
(2003), in which the authors start with the
assumption that BLEU might work for
summarization evaluation, and discover after
several trials a better candidate).
Our unifying framework facilitates the
understanding of when various automatic
evaluation metrics are able to closely approximate
human evaluations for various applications. Given
an application app and an evaluation guideline
package eval, the faithfulness/compactness ratio of
the application and the precision/recall ratio of the
evaluation guidelines determine a restricted area in
the evaluation plane in Figure 1 which best
characterizes the (app, eval) pair. We have
empirically demonstrated that the metrics from the
AEv(α,N) family that best approximate human
judgment are those that have the α and N
parameters in the determined restricted area. To
our knowledge, this is the first proposal regarding
automatic evaluation in which the automatic
evaluation metrics are able to account for the
variation in human judgment due to specific
evaluation guidelines.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999773708333333">
DUC. 2001. The Document Understanding
Conference. http://duc.nist.gov.
C.Y. Lin and E. H. Hovy. 2003. Automatic
Evaluation of Summaries Using N-gram Co-
Occurrence Statistics. In Proceedings of the
HLT/NAACL 2003: Main Conference, 150-156.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu.
2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In
Proceedings of the ACL 2002, 311-318.
M. F. Porter. 1980. An algorithm for Suffix
Stripping. Program, 14: 130-137.
F. J. Och. 2003. Minimum Error Rate Training for
Statistical Machine Translation. In Proceedings
of the ACL 2003, 160-167.
R. Soricut and E. Brill. 2004. Automatic Question
Answering: Beyond the Factoid. In Proceedings
of the HLT/NAACL 2004: Main Conference, 57-
64.
TIDES. 2002. The Translingual Information
Detection, Extraction, and Summarization
programme. http://tides.nist.gov.
C. J. van Rijsbergen. 1979. Information Retrieval.
London: Butterworths. Second Edition.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.658828">
<title confidence="0.9961395">A Unified Framework for Automatic Evaluation using N-gram Co-Occurrence Statistics</title>
<author confidence="0.993484">Radu SORICUT Eric BRILL</author>
<affiliation confidence="0.990618">Information Sciences Institute Microsoft Research University of Southern California One Microsoft Way</affiliation>
<address confidence="0.999892">4676 Admiralty Way Redmond, WA 98052, USA</address>
<author confidence="0.697632">Marina del Rey</author>
<author confidence="0.697632">CA</author>
<email confidence="0.998286">radu@isi.edu</email>
<abstract confidence="0.9983430625">In this paper we propose a unified framework for automatic evaluation of NLP applications using N-gram co-occurrence statistics. The automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization are particular instances from the family of metrics we propose. We show that different members of the same family of metrics explain best the variations obtained with human evaluations, according to the application being evaluated (Machine Translation, Automatic Summarization, and Automatic Question Answering) and the evaluation guidelines used by humans for evaluating such applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>The Document Understanding Conference.</title>
<date>2001</date>
<note>http://duc.nist.gov.</note>
<contexts>
<context position="6510" citStr="DUC 2001" startWordPosition="942" endWordPosition="943">s, evaluation guidelines can be linearly ordered according to the precision/recall (p/r) ratio they specify. For example, evaluation guidelines for adequacy evaluation of MT have a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded) and low emphasis on precision (i.e., verbosity is not penalized); on the other hand, evaluation guidelines for fluency of MT have a high p/r ratio, because of the low emphasis on recall (i.e., content is not rewarded) and high emphasis on wording (i.e., extraneous words are penalized). Another evaluation we consider in this paper, the DUC 2001 evaluation for Automatic Summarization (also performed by NIST), had specific guidelines for coverage evaluation, which means a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded). Last but not least, the QA evaluation for correctness we discuss in Section 4 has a close-to-1 p/r ratio for evaluation guidelines (i.e., both correct content and precise answer wording are rewarded). When combined, the application axis and the guideline axis define a plane in which particular evaluations are placed according to their application/guideline coordinates. In Figure 1 we i</context>
<context position="20851" citStr="DUC 2001" startWordPosition="3337" endWordPosition="3338">ion metric that explains most of the variation in the human evaluation: metric AEv(0,4) captures most of the variation, 83.04%. For comparison purposes, we also computed the value of R2 for adequacy using the BLEU score formula given in (Papineni et al., 2002), for the 7 systems using the same one reference, and we obtain a similar value, 83.91%; computing the value of R2 for adequacy using the BLEU scores computed with all 4 references available also yielded a lower value for R2, 62.21%. 4.2 Automatic Summarization Evaluation The Automatic Summarization evaluation carried out by NIST for the DUC 2001 conference involved 15 participating systems. We focus here on the multi-document summarization task, in which 4 generic summaries (of 50, 100, 200, and 400 words) were required for a given set of documents on a single subject. For this evaluation 30 test sets were used, and each system was evaluated by a human judge using one reference extracted from a list of 2 reference summaries. One of the evaluations required the assessors to judge the coverage of the summaries. The coverage of a summary was measured by comparing a system’s units versus the units of a reference summary, and assessing wh</context>
</contexts>
<marker>DUC, 2001</marker>
<rawString>DUC. 2001. The Document Understanding Conference. http://duc.nist.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram CoOccurrence Statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL 2003: Main Conference,</booktitle>
<pages>150--156</pages>
<contexts>
<context position="1593" citStr="Lin and Hovy, 2003" startWordPosition="214" endWordPosition="217">duction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by BLEU but twisted towards the specifics of the summarization task. An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations. Human evaluations, however, are subject to specific guidelines given to the human assessors when performing the evaluation task; the variation in human judgment is therefore highly influenced by these guidelines. It follows that, in order for an automatic evaluation to agree with a humanperformed evaluation, the evaluation metric used by th</context>
<context position="10121" citStr="Lin and Hovy (2003)" startWordPosition="1541" endWordPosition="1544">arithm is needed to obtain a linear average. Note that the metrics of this family are well-defined only for N’s small enough to yield non-zero P(n) scores. For test corpora of reasonable size, the metrics are usually well-defined for N≤4. The BLEU proposed by Papineni et al. (2002) for automatic evaluation of machine translation is part of the family of metrics PS(N), as the particular metric obtained when N=4, wn–s are 1/N, the brevity constant B=1, the list of stop-words SW is empty, and the stemming function ST is the identity function. 3.2 A Recall-focused Family of Metrics As proposed by Lin and Hovy (2003), a precision-focused metric such as BLEU can be twisted such that it yields a recall-focused metric. In a similar manner, we define a recall-focused family of metrics, using as parameter a nonnegative integer N, with a list of stop-words (SW) and a function for extracting the stem of a given word (ST) as part of the definition. As before, suppose we have a given NLP application for which we want to evaluate the candidate answer set Candidates for some input sequences, given a reference answer set References. For each individual reference answer R, we define S(R,n) as the multi-set of n-grams </context>
<context position="12007" citStr="Lin and Hovy (2003)" startWordPosition="1874" endWordPosition="1877">be made artificially higher when proposing longer and longer candidate answers. This is offset by adding a wordiness penalty, WP: 1, if W⋅|c|≤|r| −W c r e(1 ||/ ||) , if W⋅ |c |&gt; |r | where |c |and |r |are defined as before, and W is a wordiness constant. We define now a recall-focused family of metrics, parameterized by a non-negative integer N, as: log( ( ))) R n This family of metrics can be interpreted as a weighted linear average of recall scores for increasingly longer n-grams. For test corpora of reasonable size, the metrics are usually welldefined for N≤4. The ROUGE metric proposed by Lin and Hovy (2003) for automatic evaluation of machineproduced summaries is part of the family of metrics RS(N), as the particular metric obtained when N=1, wn–s are 1/N, the wordiness constant W=∞, the list of stop-words SW is their own , and the stemming function ST is the one defined by the Porter stemmer (Porter 1980). 3.3 A Unified Framework for Automatic Evaluation The precision-focused metric family PS(N) and the recall-focused metric family RS(N) defined in ) Count clip (ngram = P(n) BP    wn PS N BP ( ) = ⋅ exp( 1 ∑= n ) Count clip (ngram = R(n) WP   = 1  N wn = ⋅ exp( ( ) RS N WP 1 ∑= n the prev</context>
<context position="21686" citStr="Lin and Hovy 2003" startWordPosition="3475" endWordPosition="3478">le subject. For this evaluation 30 test sets were used, and each system was evaluated by a human judge using one reference extracted from a list of 2 reference summaries. One of the evaluations required the assessors to judge the coverage of the summaries. The coverage of a summary was measured by comparing a system’s units versus the units of a reference summary, and assessing whether each system unit expresses all, most, some, hardly any, or none of the current reference unit. A final evaluation score for coverage was obtained using a coverage score computed as a weighted recall score (see (Lin and Hovy 2003) for more information on the human summary evaluation). From the publicly available data for this evaluation (DUC 2001), we compute the values of R2 for 15 data points available (corresponding to the 15 participating systems). In Tables 3-4 we present the values of the coefficient of determination R2 for the family of metrics AEv(α,N), when considering the coverage 4 67.10 66.51 65.91 65.29 64.65 64.00 63.34 62.67 61.99 61.30 60.61 3 69.55 68.81 68.04 67.24 66.42 65.57 64.69 63.79 62.88 61.95 61.00 2 74.43 73.29 72.06 70.74 69.35 67.87 66.33 64.71 63.03 61.30 59.51 1 90.77 90.77 90.66 90.42 90</context>
<context position="23772" citStr="Lin and Hovy (2003)" startWordPosition="3832" endWordPosition="3835"> an application with low faithfulness/compactness ratio. In this case, our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a low α and a low N. As seen in Tables 3-4, our evaluation framework correctly predicts the automatic evaluation metric that explain most of the variation in the human evaluation: metric AEv(0,1) explains 90.77% and 92.28% of the variation in the human evaluation of summaries of length 200 and 400, respectively. Since metric AEv(0, 1) is almost the same as the ROUGE metric proposed by Lin and Hovy (2003) (they only differ in the stop-word list they use), our results also confirm the proposal for such metrics to be used for automatic evaluation by the Automatic Summarization community. 4.3 Question Answering Evaluation One of the most common approaches to automatic question answering (QA) restricts the domain of questions to be handled to so-called factoid questions. Automatic evaluation of factoid QA is often straightforward, as the number of correct answers is most of the time limited, and exhaustive lists of correct answers are available. When removing the factoid constraint, however, the s</context>
<context position="31027" citStr="Lin and Hovy (2003)" startWordPosition="4988" endWordPosition="4991">automatic evaluation of QA systems that go beyond answering factoid questions. 5 Conclusions In this paper, we propose a unified framework for automatic evaluation based on N-gram cooccurrence statistics, for NLP applications for which a correct answer is usually an unfeasibly large set (e.g., Machine Translation, Paraphrasing, Question Answering, Summarization, etc.). The success of BLEU in doing automatic evaluation of machine translation output has often led researchers to blindly try to use this metric for evaluation tasks for which it was more or less appropriate (see, e.g., the paper of Lin and Hovy (2003), in which the authors start with the assumption that BLEU might work for summarization evaluation, and discover after several trials a better candidate). Our unifying framework facilitates the understanding of when various automatic evaluation metrics are able to closely approximate human evaluations for various applications. Given an application app and an evaluation guideline package eval, the faithfulness/compactness ratio of the application and the precision/recall ratio of the evaluation guidelines determine a restricted area in the evaluation plane in Figure 1 which best characterizes t</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.Y. Lin and E. H. Hovy. 2003. Automatic Evaluation of Summaries Using N-gram CoOccurrence Statistics. In Proceedings of the HLT/NAACL 2003: Main Conference, 150-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL 2002,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1058" citStr="Papineni et al, 2002" startWordPosition="140" endWordPosition="143">ce statistics. The automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization are particular instances from the family of metrics we propose. We show that different members of the same family of metrics explain best the variations obtained with human evaluations, according to the application being evaluated (Machine Translation, Automatic Summarization, and Automatic Question Answering) and the evaluation guidelines used by humans for evaluating such applications. 1 Introduction With the introduction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by B</context>
<context position="7705" citStr="Papineni et al. (2002)" startWordPosition="1118" endWordPosition="1121">ordinates. In Figure 1 we illustrate this evaluation plane, and the evaluation examples mentioned above are placed in this plane according to their coordinates. 3 A Unified Framework for Automatic Evaluation In this section we propose a family of evaluation metrics based on N-gram co-occurrence statistics. Such a family of evaluation metrics provides flexibility in terms of accommodating both various NLP applications and various values of precision/recall ratio in the human guideline packages used to evaluate such applications. 3.1 A Precision-focused Family of Metrics Inspired by the work of Papineni et al. (2002) on BLEU, we define a precision-focused family of metrics, using as parameter a non-negative integer N. Part of the definition includes a list of stopwords (SW) and a function for extracting the stem of a given word (ST). Suppose we have a given NLP application for which we want to evaluate the candidate answer set Candidates for some input sequences, given a reference answer set References. For each individual candidate answer C, we define S(C,n) as the multi-set of n-grams obtained from the candidate answer C after stemming the unigrams using ST and eliminating the unigrams found in SW. We t</context>
<context position="9784" citStr="Papineni et al. (2002)" startWordPosition="1484" endWordPosition="1487">. We define now a precision-focused family of metrics, parameterized by a non-negative integer N, as: N log( P(n))) This family of metrics can be interpreted as a weighted linear average of precision scores for increasingly longer n-grams. As the values of the precision scores decrease roughly exponentially with the increase of N, the logarithm is needed to obtain a linear average. Note that the metrics of this family are well-defined only for N’s small enough to yield non-zero P(n) scores. For test corpora of reasonable size, the metrics are usually well-defined for N≤4. The BLEU proposed by Papineni et al. (2002) for automatic evaluation of machine translation is part of the family of metrics PS(N), as the particular metric obtained when N=4, wn–s are 1/N, the brevity constant B=1, the list of stop-words SW is empty, and the stemming function ST is the identity function. 3.2 A Recall-focused Family of Metrics As proposed by Lin and Hovy (2003), a precision-focused metric such as BLEU can be twisted such that it yields a recall-focused metric. In a similar manner, we define a recall-focused family of metrics, using as parameter a nonnegative integer N, with a list of stop-words (SW) and a function for </context>
<context position="19308" citStr="Papineni et al., 2002" startWordPosition="3087" endWordPosition="3090">tion framework correctly predicts the automatic evaluation metrics that explain most of the variation in the human evaluation: metrics AEv(1,3), AEv(0.9,3), and AEv(1,4) capture most of the variation: 79.04%, 78.94%, and 78.87%, respectively. Since metric AEv(1,4) is almost the same as the BLEU metric (modulo stemming and stop word elimination for unigrams), our results confirm the current practice in the Machine Translation community, which commonly uses BLEU for automatic evaluation. For comparison purposes, we also computed the value of R2 for fluency using the BLEU score formula given in (Papineni et al., 2002), for the 7 systems using the same one reference, and we obtained a similar value, 78.52%; computing the value of R2 for fluency using the BLEU scores computed with all 4 references available yielded a lower value for R2, 64.96%, although BLEU scores obtained with multiple references are usually considered more reliable. In Table 2, we present the values of the coefficient of determination R2 for the family of metrics AEv(α,N), when considering only the adequacy scores from the human evaluation. As mentioned in Section 2, the evaluation guidelines for adequacy have a low precision/recall ratio</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the ACL 2002, 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for Suffix Stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<pages>130--137</pages>
<contexts>
<context position="12312" citStr="Porter 1980" startWordPosition="1929" endWordPosition="1930">terized by a non-negative integer N, as: log( ( ))) R n This family of metrics can be interpreted as a weighted linear average of recall scores for increasingly longer n-grams. For test corpora of reasonable size, the metrics are usually welldefined for N≤4. The ROUGE metric proposed by Lin and Hovy (2003) for automatic evaluation of machineproduced summaries is part of the family of metrics RS(N), as the particular metric obtained when N=1, wn–s are 1/N, the wordiness constant W=∞, the list of stop-words SW is their own , and the stemming function ST is the one defined by the Porter stemmer (Porter 1980). 3.3 A Unified Framework for Automatic Evaluation The precision-focused metric family PS(N) and the recall-focused metric family RS(N) defined in ) Count clip (ngram = P(n) BP    wn PS N BP ( ) = ⋅ exp( 1 ∑= n ) Count clip (ngram = R(n) WP   = 1  N wn = ⋅ exp( ( ) RS N WP 1 ∑= n the previous sections are unified under the metric family AEv(α,N), defined as: RS(N)PS(N) AEv N ( , ) α _ α • RS(N) + (1−α) • PS(N) This formula extends the well-known F-measure that combines recall and precision numbers into a single number (van Rijsbergen, 1979), by combining recall and precision metric famil</context>
<context position="13582" citStr="Porter 1980" startWordPosition="2166" endWordPosition="2167">same as the recall-focused family of metrics RS(N); for α=1, AEv(α,�N) is the same as the precision-focused family of metrics PS(N). For α in between 0 and 1, AEv(α,N) are metrics that balance recall and precision according to α. For the rest of the paper, we restrict the parameters of the AEv(α,N) family as follows: α varies continuously in [0,1], N varies discretely in {1,2,3,4}, the linear weights wn are 1/N, the brevity constant is 1, the wordiness constant is 2, the list of stop-words SW is our own 626 stop-word list, and the stemming function ST is the one defined by the Porter stemmer (Porter 1980). We establish a correspondence between the parameters of the family of metrics AEv(α,N) and the evaluation plane in Figure 1 as follows: α parameterizes the guideline axis (x-axis) of the plane, such that α=0 corresponds to a low precision/recall (p/r) ratio, and α=1 corresponds to a high p/r ratio; N parameterizes the application axis (y-axis) of the plane, such that N=1 corresponds to a low faithfulness/compactness (f/c) ratio (unigram statistics allow for a low representation of faithfulness, but a high representation of compactness), and N=4 corresponds to a high f/c ratio (n-gram statist</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for Suffix Stripping. Program, 14: 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="1466" citStr="Och 2003" startWordPosition="198" endWordPosition="199">swering) and the evaluation guidelines used by humans for evaluating such applications. 1 Introduction With the introduction of the BLEU metric for machine translation evaluation (Papineni et al, 2002), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors in human assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance (Och 2003). Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (Lin and Hovy, 2003), with an automatic evaluation metric called ROUGE, inspired by BLEU but twisted towards the specifics of the summarization task. An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations. Human evaluations, however, are subject to specific guidelines given to the human assessors when performing the evaluation task; the variation in human judgment is therefore highly influenced by these guidelines. It</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum Error Rate Training for Statistical Machine Translation. In Proceedings of the ACL 2003, 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>E Brill</author>
</authors>
<title>Automatic Question Answering: Beyond the Factoid.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT/NAACL 2004: Main Conference,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="24720" citStr="Soricut and Brill, 2004" startWordPosition="3975" endWordPosition="3978">to be handled to so-called factoid questions. Automatic evaluation of factoid QA is often straightforward, as the number of correct answers is most of the time limited, and exhaustive lists of correct answers are available. When removing the factoid constraint, however, the set of possible answer to a (complex, beyondfactoid) question becomes unfeasibly large, and consequently automatic evaluation becomes a challenge. In this section, we focus on an evaluation carried out in order to assess the performance of a QA system for answering questions from the Frequently-Asked-Question (FAQ) domain (Soricut and Brill, 2004). These are generally questions requiring a more elaborated answer than a simple factoid (e.g., questions such as: “How does a film qualify for an Academy Award?”). In order to evaluate such a system a humanperformed evaluation was performed, in which 11 versions of the QA system (various modules were implemented using various algorithms) were separately evaluated. Each version was evaluated by a human evaluator, with no reference answer available. For this evaluation 115 test questions were used, and the human evaluator was asked to assess whether the proposed answer was correct, somehow rela</context>
</contexts>
<marker>Soricut, Brill, 2004</marker>
<rawString>R. Soricut and E. Brill. 2004. Automatic Question Answering: Beyond the Factoid. In Proceedings of the HLT/NAACL 2004: Main Conference, 57-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TIDES</author>
</authors>
<title>The Translingual Information Detection, Extraction, and Summarization programme.</title>
<date>2002</date>
<note>http://tides.nist.gov.</note>
<contexts>
<context position="5598" citStr="TIDES 2002" startWordPosition="800" endWordPosition="801">cument(s) to be summarized). Another NLP application, Automatic Question Answering (QA), has arguably a close-to-1 f/c ratio: the task is to render an answer about the thing(s) inquired for in a question (the faithfulness side), in a manner that is concise enough to be regarded as a useful answer (the compactness side). 2.2 An Guideline Axis for Evaluation Formal human evaluations make use of various guidelines that specify what particular aspects of the output being evaluated are considered important, for the particular application being evaluated. For example, human evaluations of MT (e.g., TIDES 2002 evaluation, performed by NIST) have traditionally looked at two different aspects of a translation: adequacy (how much of the content of the original sentence is captured by the proposed translation) and fluency (how correct is the proposed translation sentence in the target language). In many instances, evaluation guidelines can be linearly ordered according to the precision/recall (p/r) ratio they specify. For example, evaluation guidelines for adequacy evaluation of MT have a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded) and low emphasis on precision (i.</context>
<context position="14714" citStr="TIDES 2002" startWordPosition="2344" endWordPosition="2345">tation of compactness), and N=4 corresponds to a high f/c ratio (n-gram statistics up to 4-grams allow for a high representation of faithfulness, but a low representation of compactness). This framework enables us to predict that a human-performed evaluation is best approximated by metrics that have similar f/c ratio as the application being evaluated and similar p/r ratio as the evaluation package used by the human assessors. For example, an application with a high f/c ratio, evaluated using a low p/r ratio evaluation guideline package (an example of this is the adequacy evaluation for MT in TIDES 2002), is best approximated by the automatic evaluation metric defined by a low α and a high N; an application with a close-to-1 f/c ratio, evaluated using an evaluation guideline package characterized by a close-to-1 p/r ratio (such as the correctness evaluation for Question Answering in Section 4.3) is best approximated by an automatic metric defined by a median α and a median N. 4 Evaluating the Evaluation Framework In this section, we present empirical results regarding the ability of our family of metrics to approximate human evaluations of various applications under various evaluation guideli</context>
<context position="17108" citStr="TIDES 2002" startWordPosition="2719" endWordPosition="2720">ranslation Evaluation The Machine Translation evaluation carried out by NIST in 2002 for DARPA’s TIDES programme involved 7 systems that participated in the Chinese-English track. Each system was evaluated by a human judge, using one reference extracted from a list of 4 available reference translations. Each of the 878 test sentences was evaluated both for adequacy (how much of the content of the original sentence is captured by the proposed translation) and fluency (how correct is the proposed translation sentence in the target language). From the publicly available data for this evaluation (TIDES 2002), we compute the values of R2 for 7 data points (corresponding to the 7 systems participating in the Chinese-English track), using as a reference set one of the 4 sets of reference translations available. In Table 1, we present the values of the coefficient of determination R2 for the family of metrics AEv(α,N), when considering only the fluency scores from the human evaluation. As mentioned in Section 2, the evaluation guidelines for fluency have a high precision/recall ratio, whereas MT is an application with a high 4 76.10 76.45 76.78 77.10 77.40 77.69 77.96 78.21 78.45 78.67 78.87 3 76.11 </context>
</contexts>
<marker>TIDES, 2002</marker>
<rawString>TIDES. 2002. The Translingual Information Detection, Extraction, and Summarization programme. http://tides.nist.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval. London: Butterworths. Second Edition.</title>
<date>1979</date>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. London: Butterworths. Second Edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>