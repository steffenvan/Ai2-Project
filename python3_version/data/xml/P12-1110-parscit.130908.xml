<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9990935">
Incremental Joint Approach to Word Segmentation, POS Tagging, and
Dependency Parsing in Chinese
</title>
<author confidence="0.998347">
Jun Hatori&apos; Takuya Matsuzaki&apos; Yusuke Miyao&apos; Jun’ichi Tsujii&apos;
</author>
<affiliation confidence="0.988960666666667">
&apos;University of Tokyo / 7-3-1 Hongo, Bunkyo, Tokyo, Japan
&apos;National Institute of Informatics / 2-1-2 Hitotsubashi, Chiyoda, Tokyo, Japan
&apos;Microsoft Research Asia / 5 Danling Street, Haidian District, Beijing, P.R. China
</affiliation>
<email confidence="0.9929545">
hatori@is.s.u-tokyo.ac.jp
{takuya-matsuzaki,yusuke}@nii.ac.jp jtsujii@microsoft.com
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974941176471">
We propose the first joint model for word segmen-
tation, POS tagging, and dependency parsing for
Chinese. Based on an extension of the incremental
joint model for POS tagging and dependency pars-
ing (Hatori et al., 2011), we propose an efficient
character-based decoding method that can combine
features from state-of-the-art segmentation, POS
tagging, and dependency parsing models. We also
describe our method to align comparable states in
the beam, and how we can combine features of dif-
ferent characteristics in our incremental framework.
In experiments using the Chinese Treebank (CTB),
we show that the accuracies of the three tasks can
be improved significantly over the baseline models,
particularly by 0.6% for POS tagging and 2.4% for
dependency parsing. We also perform comparison
experiments with the partially joint models.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989170036363637">
In processing natural languages that do not include
delimiters (e.g. spaces) between words, word seg-
mentation is the crucial first step that is necessary
to perform virtually all NLP tasks. Furthermore, the
word-level information is often augmented with the
POS tags, which, along with segmentation, form the
basic foundation of statistical NLP.
Because the tasks of word segmentation and POS
tagging have strong interactions, many studies have
been devoted to the task of joint word segmenta-
tion and POS tagging for languages such as Chi-
nese (e.g. Kruengkrai et al. (2009)). This is because
some of the segmentation ambiguities cannot be re-
solved without considering the surrounding gram-
matical constructions encoded in a sequence of POS
tags. The joint approach to word segmentation and
POS tagging has been reported to improve word seg-
mentation and POS tagging accuracies by more than
1% in Chinese (Zhang and Clark, 2008). In addition,
some researchers recently proposed a joint approach
to Chinese POS tagging and dependency parsing (Li
et al., 2011; Hatori et al., 2011); particularly, Ha-
tori et al. (2011) proposed an incremental approach
to this joint task, and showed that the joint approach
improves the accuracies of these two tasks.
In this context, it is natural to consider further
a question regarding the joint framework: how
strongly do the tasks of word segmentation and de-
pendency parsing interact? In the following Chinese
sentences:
❙✃ ➀AF❱ ✍ ➀AF ❿✚ �current peace-prize and peace operation related
The current peace prize and peace operations are related.
❙✃ ➀AF ❱✍ ➀AF ❿✚ tH Q❙
current peace award peace operation related group
The currentpeace is awarded to peace-operation-related groups.
the only difference is the existence of the last word
Q ❙; however, whether or not this word exists
changes the whole syntactic structure and segmen-
tation of the sentence. This is an example in which
word segmentation cannot be handled properly with-
out considering long-range syntactic information.
Syntactic information is also considered ben-
eficial to improve the segmentation of out-of-
vocabulary (OOV) words. Unlike languages such
as Japanese that use a distinct character set (i.e.
katakana) for foreign words, the transliterated words
in Chinese, many of which are OOV words, fre-
quently include characters that are also used as com-
mon or function words. In the current systems, the
existence of these characters causes numerous over-
segmentation errors for OOV words.
Based on these observations, we aim at build-
ing a joint model that simultaneously processes
word segmentation, POS tagging, and dependency
parsing, trying to capture global interaction among
</bodyText>
<page confidence="0.939572">
1045
</page>
<note confidence="0.9855425">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1045–1053,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988375">
these three tasks. To handle the increased computa-
tional complexity, we adopt the incremental parsing
framework with dynamic programming (Huang and
Sagae, 2010), and propose an efficient method of
character-based decoding over candidate structures.
Two major challenges exist in formalizing the
joint segmentation and dependency parsing task in
the character-based incremental framework. First,
we must address the problem of how to align com-
parable states effectively in the beam. Because the
number of dependency arcs varies depending on
how words are segmented, we devise a step align-
ment scheme using the number of character-based
arcs, which enables effective joint decoding for the
three tasks.
Second, although the feature set is fundamen-
tally a combination of those used in previous works
(Zhang and Clark, 2010; Huang and Sagae, 2010), to
integrate them in a single incremental framework is
not straightforward. Because we must perform de-
cisions of three kinds (segmentation, tagging, and
parsing) in an incremental framework, we must ad-
just which features are to be activated when, and
how they are combined with which action labels. We
have also found that we must balance the learning
rate between features for segmentation and tagging
decisions, and those for dependency parsing.
We perform experiments using the Chinese Tree-
bank (CTB) corpora, demonstrating that the accura-
cies of the three tasks can be improved significantly
over the pipeline combination of the state-of-the-art
joint segmentation and POS tagging model, and the
dependency parser. We also perform comparison ex-
periments with partially joint models, and investi-
gate the tradeoff between the running speed and the
model performance.
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999978161290323">
In Chinese, Luo (2003) proposed a joint con-
stituency parser that performs segmentation, POS
tagging, and parsing within a single character-based
framework. They reported that the POS tags con-
tribute to segmentation accuracies by more than 1%,
but the syntactic information has no substantial ef-
fect on the segmentation accuracies. In contrast,
we built a joint model based on a dependency-based
framework, with a rich set of structural features. Us-
ing it, we show the first positive result in Chinese
that the segmentation accuracies can be improved
using the syntactic information.
Another line of work exists on lattice-based pars-
ing for Semitic languages (Cohen and Smith, 2007;
Goldberg and Tsarfaty, 2008). These methods first
convert an input sentence into a lattice encoding
the morphological ambiguities, and then conduct
joint morphological segmentation and PCFG pars-
ing. However, the segmentation possibilities consid-
ered in those studies are limited to those output by
an existing morphological analyzer. In addition, the
lattice does not include word segmentation ambigu-
ities crossing boundaries of space-delimited tokens.
In contrast, because the Chinese language does not
have spaces between words, we fundamentally need
to consider the lattice structure of the whole sen-
tence. Therefore, we place no restriction on the seg-
mentation possibilities to consider, and we assess the
full potential of the joint segmentation and depen-
dency parsing model.
Among the many recent works on joint segmen-
tation and POS tagging for Chinese, the linear-time
incremental models by Zhang and Clark (2008) and
Zhang and Clark (2010) largely inspired our model.
Zhang and Clark (2008) proposed an incremental
joint segmentation and POS tagging model, with an
effective feature set for Chinese. However, it re-
quires to computationally expensive multiple beams
to compare words of different lengths using beam
search. More recently, Zhang and Clark (2010) pro-
posed an efficient character-based decoder for their
word-based model. In their new model, a single
beam suffices for decoding; hence, they reported that
their model is practically ten times as fast as their
original model. To incorporate the word-level fea-
tures into the character-based decoder, the features
are decomposed into substring-level features, which
are effective for incomplete words to have compara-
ble scores to complete words in the beam. Because
we found that even an incremental approach with
beam search is intractable if we perform the word-
based decoding, we take a character-based approach
to produce our joint model.
The incremental framework of our model is based
on the joint POS tagging and dependency parsing
model for Chinese (Hatori et al., 2011), which is an
extension of the shift-reduce dependency parser with
dynamic programming (Huang and Sagae, 2010).
They specifically modified the shift action so that it
assigns the POS tag when a word is shifted onto the
stack. However, because they regarded word seg-
mentation as given, their model did not consider the
</bodyText>
<page confidence="0.988792">
1046
</page>
<bodyText confidence="0.923365">
interaction between segmentation and POS tagging.
</bodyText>
<sectionHeader confidence="0.999064" genericHeader="method">
3 Model
</sectionHeader>
<subsectionHeader confidence="0.993399">
3.1 Incremental Joint Segmentation, POS
Tagging, and Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.9991704">
Based on the joint POS tagging and dependency
parsing model by Hatori et al. (2011), we build our
joint model to solve word segmentation, POS tag-
ging, and dependency parsing within a single frame-
work. Particularly, we change the role of the shift ac-
tion and additionally use the append action, inspired
by the character-based actions used in the joint seg-
mentation and POS tagging model by Zhang and
Clark (2010).
The list of actions used is the following:
</bodyText>
<listItem confidence="0.978923166666667">
• A: append the first character in the queue to the
word on top of the stack.
• SH(t): shift the first character in the input queue
as a new word onto the stack, with POS tag t.
• RL/RR: reduce the top two trees on the stack,
(s0, s1), into a subtree sy0 s1 / sx0 s1, respectively.
</listItem>
<bodyText confidence="0.998901666666667">
Although SH(t) is similar to the one used in Hatori
et al. (2011), now it shifts the first character in the
queue as a new word, instead of shifting a word. Fol-
lowing Zhang and Clark (2010), the POS tag is as-
signed to the word when its first character is shifted,
and the word–tag pairs observed in the training data
and the closed-set tags (Xia, 2000) are used to prune
unlikely derivations. Because 33 tags are defined in
the CTB tag set (Xia, 2000), our model exploits a
total of 36 actions.
To train the model, we use the averaged percep-
tron with the early update (Collins and Roark, 2004).
In our joint model, the early update is invoked by
mistakes in any of word segmentation, POS tagging,
or dependency parsing.
</bodyText>
<subsectionHeader confidence="0.999993">
3.2 Alignment of States
</subsectionHeader>
<bodyText confidence="0.997534333333333">
When dependency parsing is integrated into the task
of joint word segmentation and POS tagging, it is
not straightforward to define a scheme to align (syn-
chronize) the states in the beam. In beam search, we
use the step index that is associated with each state:
the parser states in process are aligned according to
the index, and the beam search pruning is applied
to those states with the same index. Consequently,
for the beam search to function effectively, all states
with the same index must be comparable, and all
terminal states should have the same step index.
We can first think of using the number of shifted
characters as the step index, as Zhang and Clark
(2010) does. However, because RL/RR actions can
be performed without incrementing the step index,
the decoder tends to prefer states with more de-
pendency arcs, resulting more likely in premature
choice of ‘reduce’ actions or oversegmentation of
words. Alternatively, we can consider using the
number of actions that have been applied as the step
index, as Hatori et al. (2011) does. However, this
results in inconsistent numbers of actions to reach
the terminal states: some states that segment words
into larger chunks reach a terminal state earlier than
other states with smaller chunks. For these reasons,
we have found that both approaches yield poor mod-
els that are not at all competitive with the baseline
(pipeline) models1.
To address this issue, we propose an indexing
scheme using the number of character-based arcs.
We presume that in addition to the word-to-word de-
pendency arcs, each word (of length M) implicitly
has M − 1 inter-character arcs, as in: AxBxC ,
(each rectangle de-
notes a word). Then we can define the step index as
the sum of the number of shifted characters and the
total number of (inter-word and intra-word) depen-
dency arcs, which thereby meets all the following
conditions:
</bodyText>
<listItem confidence="0.999859666666667">
(1) All subtrees spanning M consecutive characters
have the same index 2M − 1.
(2) All terminal states have the same step index 2N
(including the root arc), where N is the number
of characters in the sentence.
(3) Every action increases the index.
</listItem>
<bodyText confidence="0.934377533333333">
Note that the number of shifted characters is also
necessary to meet condition (3). Otherwise, it allows
an unlimited number of SH(t) actions without incre-
menting the step index. Figure 1 portrays how the
states are aligned using the proposed scheme, where
a subtree is denoted as a rectangle with its partial
index shown inside it.
In our framework, because an action increases the
step index by 1 (for SH(t) or RL/RR) or 2 (for A), we
need to use two beams to store new states at each
step. The computational complexity of the entire
process is O(B(T + 3) · 2N), where B is the beam
1For example, in our preliminary experiment on CTB-5, the
step indexing according to the number of actions underperforms
the baseline model by 0.2–0.3% in segmentation accuracy.
</bodyText>
<figure confidence="0.9936166">
C
AxB x
A x
B x
C , and
</figure>
<page confidence="0.552816">
1047
</page>
<figureCaption confidence="0.999974">
Figure 1: Illustration of the alignment of steps.
</figureCaption>
<bodyText confidence="0.9998152">
size, T is the number of POS tags (= 33), and N
is the number of characters in the sentence. Theo-
retically, the computational time is greater than that
with the character-based joint segmentation and tag-
ging model by Zhang and Clark (2010) by a factor
</bodyText>
<equation confidence="0.639969">
of T+3 2N
</equation>
<bodyText confidence="0.807868">
T+1 N — 11, when the same beam size is used.
</bodyText>
<subsectionHeader confidence="0.937096">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999961890909091">
The feature set of our model is fundamentally a com-
bination of the features used in the state-of-the-art
joint segmentation and POS tagging model (Zhang
and Clark, 2010) and dependency parser (Huang and
Sagae, 2010), both of which are used as baseline
models in our experiment. However, we must care-
fully adjust which features are to be activated and
when, and how they are combined with which ac-
tion labels, depending on the type of the features be-
cause we intend to perform three tasks in a single
incremental framework.
The list of the features used in our joint model
is presented in Table 1, where S01–S05, W01–
W21, and T01–05 are taken from Zhang and Clark
(2010), and P01–P28 are taken from Huang and
Sagae (2010). Note that not all features are always
considered: each feature is only considered if the
action to be performed is included in the list of ac-
tions in the “When to apply” column. Because S01–
S05 are used to represent the likelihood score of
substring sequences, they are only used for A and
SH(t) without being combined with any action la-
bel. Because T01–T05 are used to determine the
POS tag of the word being shifted, they are only ap-
plied for SH(t). Because W01–W21 are used to de-
termine whether to segment at the current position
or not, they are only used for those actions involved
in boundary determination decisions (A, SH(t), RL0,
and RR0). The action labels RL0/RR0 are used to
denote the ‘reduce’ actions that determine the word
boundary2, whereas RL1/RR1 denote those ‘reduce’
actions that are applied when the word boundary has
already been fixed. In addition, to capture the shared
nature of boundary determination actions (SH(t),
RL0/RR0), we use a generalized action label SH’ to
represent any of them when combined with W01–
W21. We also propose to use the features U01–U03,
which we found are effective to adjust the character-
level and substring-level scores.
Regarding the parsing features P01–P28, because
we found that P01–P17 are also useful for segmen-
tation decisions, these features are applied to all ac-
tions including A, with an explicit distinction of ac-
tion labels RL0/RR0 from RL1/RR1. On the other
hand, P18–P28 are only used when one of the parser
actions (SH(t), RL, or RR) is applied. Note that P07–
P09 and P18–P21 (look-ahead features) require the
look-ahead information of the next word form and
POS tags, which cannot be incorporated straightfor-
wardly in an incremental framework. Although we
have found that these features can be incorporated
using the delayed features proposed by Hatori et al.
(2011), we did not use them in our current model
because it results in the significant increase of com-
putational time.
</bodyText>
<subsectionHeader confidence="0.742406">
3.3.1 Dictionary features
</subsectionHeader>
<bodyText confidence="0.999994166666667">
Because segmentation using a dictionary alone
can serve as a strong baseline in Chinese word seg-
mentation (Sproat et al., 1996), the use of dictio-
naries is expected to make our joint model more ro-
bust and enables us to investigate the contribution of
the syntactic dependency in a more realistic setting.
Therefore, we optionally use four features D01–D04
associated with external dictionaries. These features
distinguish each dictionary source, reflecting the fact
that different dictionaries have different characteris-
tics. These features will also be used in our reimple-
mentation of the model by Zhang and Clark (2010).
</bodyText>
<subsectionHeader confidence="0.99931">
3.4 Adjusting the Learning Rate of Features
</subsectionHeader>
<bodyText confidence="0.940655875">
In formulating the three tasks in the incremental
framework, we found that adjusting the update rate
depending on the type of the features (segmenta-
tion/tagging vs. parsing) crucially impacts the final
performance of the model. To investigate this point,
we define the feature vector 0 and score 4) of the
2A reduce action has an additional effect of fixing the bound-
ary of the top word on the stack if the last action was A or SHM.
</bodyText>
<figure confidence="0.976022875">
step 8
1 1 1 1 1 1 1 1
3
3
5 1 1 1
5
7
1 1 1 1 1
3
3
1 1
1
step 3
1 1 1
3
step 4
1 1 1
3
1
1
step 5
1 1 1
3
5
1 1
1 1
step 1
1
step 2
1 1
1 1
1
1 1 1 1
step 6
1 1 1 1 1 1
3
3 3
5
1 1 1
1
step 7
3
3 3
5
7
1 1 1 1
1 1
1
</figure>
<page confidence="0.586918">
1048
</page>
<table confidence="0.988657793103448">
Id Feature template Label When to apply
U01 q_1.e o q_1.t φ A, SH(t)
S02 q_1.t o c0 φ A, SH(t)
S03 q_1.t o q_1.b o c0 φ)
A
S04 q_1.t o c0 o C(q_1.b φ A
S05 q_1.t o c0 o c1 φ A
D01 len(q_1.w) o i A,SH’ A, SH(t), RR/RL0
D02 len(q_1.w) o q_1.t o i A,SH’ A, SH(t), RR/RL0
D03 len(q_1.w) o i A,SH’ A, SH(t), RR/RL0
D04 len(q_1.w) o q_1.t o i A,SH’ A, SH(t), RR/RL0
(D01,02: if q_1.w E Di; D03,04: if q_1.w E/ Di)
W01,02 q_1.w q_2.w o q_1.w A,SH’ A, SH(t): RR/RL0
W03 q_1.w (for single-char word) A,SH’ A, SH(t) RR/RL0
W04 q_1.b o len(q_1.w) A,SH’ A, SH(t), RR/RL0
W05 q_1.e o len(q_1.w) A,SH’ A, SH(t), RR/RL0
W06,07 q_1.e o c0 q_1.b o q_1.e A,SH’ A, SH(t), RR/RL0
W08,09 q_1.w o c0 q_2.e o q_1.w A,SH’ A, SH(t), RR/RL0
W10,11 q_1.b o c0 q_2.e o q_1.e A,SH’ A, SH(t):
H t , RR/RL0
W12 q_2.w o len(q_1.w) A,SH’ A, SH(t), RR/RL0
W13 len(q_2.w) o q_1.w A,SH’ A, SH(t), RR/RL0
W14 q_1.w o q_1.t A,SH’ A, SH(t), RR/RL0
W15 q_2.t o q_1.w A,SH’ A, SH(t), RR/RL0
W16 q_1.t o q_1.w o q_2.e A,SH’ A, SH(t), RR/RL0
W17 q_1.t o q_1.w o c0 A,SH’ A, SH(t), RR/RL0
W18 q_2.e o q_1.w o c0 o q1.t A,SH’ A, SH(t), RR/RL0
W19 q_1.t o q_1.e A,SH’ A, SH(t), RR/RL0
W20 q_1.t o q_1.e o c A,SH’ A, SH(t), RR/RL0
</table>
<equation confidence="0.9657985">
W21 q_1.t o c o cat(q_1.e) \ A,SH’ A, SH(t), RR/RL0
(W20, W21: c E q_1.w e)
T01,02 q_1.t q_2.t o q_1.t SH(t) SH(t)
T03,04 q_1.w c0 SH(t) SH ((t))
T05 c0 o q_1.t o q_1.e SH(t) SH t
P01,02 s0.w s0.t A, SH(t), RR/RL0/1 any
P03,04 s0.w o s0.t s1.w A, SH(t), RR/RL0/1 any
P05,06 s1.t s1.w o s1.t A, SH(t), RR/RL0/1 any
P07,08 q0.w q0.t A, SH(t), RR/RL0/1 any
P09,10 q0.w o q0.t s0.w o s1.w A, SH(t), RR/RL0/1 any
P11,12 s0.t o s1.t s0.t o q0.t A, SH(t), RR/RL0/1 any
P13 s0.w o s0.t o s1.t A, SH(t), RR/RL0/1 any
P14 s0.t o s1.w o s1.t A, SH(t), RR/RL0/1 any
P15
</equation>
<bodyText confidence="0.937757823529412">
and q_2 respectively denote the last-shifted word and the
word shifted before q_1. q.w and q.t respectively denote the
(root) word form and POS tag of a subtree (word) q, and q.b and
q.e the beginning and ending characters of q.w.
and c1 are
the first and second characters in the queue. q.w\e denotes the
set of characters excluding the ending character of q.w.
denotes the length of the word, capped at 16 if longer.
de-
notes the category of the character, which is the set of POS tags
observed in the training data. Di is a dictionary, a set of words.
The action label
means that the feature is not combined with
any label;
denotes the use of the default action set
SH(t), and
as
</bodyText>
<figure confidence="0.342888142857143">
q_1
c0
len(·)
cat(·)
φ
“as-is”
“A,
RR/RL”
is.
joint model.
Training Development Test
#snt #wrd #snt #wrd #oov #snt #wrd #oov
CTB-5d 16k 438k 804 21k 1.2k 1.9k
3.1k
CTB-5j 18k 494k 352 6.8k 553 348
278
15k 423k - - - - -
23k 641k 2.1k 60k 3.3k 2.8k 82k 4.6k
50k
8.0k
CTB-5c
-
CTB-6
CTB-7 31k 718k 10k 237k 13k 10k 245k 13k
Table 2: Statistics of datasets.
ψ as
4)(ψ, a) = λ~ ·
where
corresponds to the segmentation and tag-
ging features (those starting with
or
and
is the set of the parsing features (start-
ing with
Then, if we set
</figure>
<figureCaption confidence="0.3252802">
to a number smaller
than 1, perceptron updates for the parsing features
will be kept small at the early stage of training be-
cause the update is proportional to the values of the
feature vector. However, even if
</figureCaption>
<bodyText confidence="0.9538516">
is initially small,
the global weights for the parsing features will in-
crease as needed and compensate for the small
as the training proceeds. In this way, we can con-
trol the contribution of syntactic dependencies at the
early stage of training. Section 4.3 shows that the
best setting we found is
= 0.5: this result sug-
gests that we probably should resolve remaining er-
rors by preferentiall
</bodyText>
<equation confidence="0.689208888888889">
~φst
‘U’,‘S’,‘T’,
‘D’),
~φp
‘P’).
σp
σp
σp
σp
</equation>
<bodyText confidence="0.934263">
y using the local n-gram based
features at the early stage of training. Otherwise,
the premature incorporation of the non-local syntac-
tic dependencies might engender overfitting to the
training data.
</bodyText>
<equation confidence="0.978616222222222">
P16
P17
P18
P20
P23
P25
P27
P28
*
</equation>
<tableCaption confidence="0.851333">
Table 1: Feature templates for the full
action a being applied to the state
</tableCaption>
<equation confidence="0.9330955">
{ }
~φ(ψ, a) = λ~ · ~φst(ψ, a) + σp~φp(ψ, a) ,
</equation>
<sectionHeader confidence="0.999675" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998236">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.989813">
We use the Chinese Penn Treebank ver. 5.1, 6.0,
and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7)
for evaluation. These corpora are split into train-
ing, development, and test sets, according to previ-
ous works. For CTB-5, we refer to the split by Duan
et al. (2007) as CTB-5d, and to the split by Jiang
et al. (2008) as CTB-5j. We also prepare a dataset
for cross validation: the dataset
consists of
sentences from CTB-5 excluding the development
and test sets of CTB-5d and CTB-5j. We split
into five sets
and alternatively use
four of these as the training set and the rest as the
test set. CTB-6 is split according to the official spli
</bodyText>
<figure confidence="0.982438562962963">
CTB-5c
CTB-
5c
(CTB-5c-n),
t
U02,03
q_1.e q_1.e o q_1.t as-is any
S01 q_1.e o c0 φ
A
o
o
A, SH(t),
any
o
o
A, SH(t),
any
o
o
o
A, SH(t),
any
o
o q1.t as-is
RR, RL
P19 s1.t o
o
as-is
RR, RL
o
o q1.t as-is
RR, RL
P21 s1.t o
o
as-is
RR, RL
P22 s1.t o s1.rc.t o
as-is
RR, RL
s1.lc.t o
as-is
RR, RL
P24 s1.t o s1.rc.t o
as-is
RR, RL
o
o
as-is
RR, RL
P26 s1.t o
o
as-is
RR, RL
o
o
as-is
RR, RL
o
o
as
s0.w
s1.w
s1.t
RR/RL0/1
s0.w
s0.t
s1.w
RR/RL0/1
s0.w
s0.t
s1.w
s1.t
RR/RL0/1
s0.t
q0.t
SH(t),
s0.t
q0.t
SH(t),
s0.w
q0.t
SH(t),
s0.w
q0.t
SH(t),
s0.t
SH(t),
s1.t o
s0.t
SH(t),
s0.w
SH(t),
s1.t
s1.lc.t
s0.w
SH(t),
s0.t
s0.rc.t
SH(t),
s1.t
s0.w
s0.lc.t
SH(t),
s2.t
s1.t
s0.t
-is SH(t), RR, RL
1049
96
94
92
90
0 10 20 30 40 50 60 70 80
Seg (σ_p=0.1)
Seg (σ_p=0.2)
Seg (σ_p=0.5)
Seg (σ_p=1.0)
Tag (σ_p=0.1)
Tag (σ_p=0.2)
Tag (σ_p=0.5)
Tag (σ_p=1.0)
76
74
72
70
68
66
64
62
60
0 10 20 30 40 50 60 70 80
Dep (σ_p=0.1)
Dep (σ_p=0.2)
Dep (σ_p=0.5)
Dep (σ_p=1.0)
</figure>
<figureCaption confidence="0.969004666666667">
Figure 2: F1 scores (in %) of SegTagDep on CTB-
5c-1 w.r.t. the training epoch (x-axis) and parsing
feature weights (in legend).
</figureCaption>
<bodyText confidence="0.9854958">
tagging (Zhang and Clark, 2008; Zhang and Clark,
2010) and dependency parsing (Huang and Sagae,
2010). Therefore, we can investigate the contribu-
tion of the joint approach through comparison with
the pipeline and joint models.
</bodyText>
<page confidence="0.8934175">
88
86
</page>
<bodyText confidence="0.999380055555556">
described in the documentation, and CTB-7 is split
according to Wang et al. (2011). The statistics of
these splits are shown in Table 2. As external dic-
tionaries, we use the HowNet Word List3, consist-
ing of 91,015 words, and page names from the Chi-
nese Wikipedia4 as of Oct 26, 2011, consisting of
709,352 words. These dictionaries only consist of
word forms with no frequency or POS information.
We use standard measures of word-level preci-
sion, recall, and F1 score, for evaluating each task.
The output of dependencies cannot be correct unless
the syntactic head and dependent of the dependency
relation are both segmented correctly. Following the
standard setting in dependency parsing works, we
evaluate the task of dependency parsing with the un-
labeled attachment scores excluding punctuations.
Statistical significance is tested by McNemar’s test
(t : p &lt; 0.05, t : p &lt; 0.01).
</bodyText>
<subsectionHeader confidence="0.989776">
4.2 Baseline and Proposed Models
</subsectionHeader>
<bodyText confidence="0.9999285">
We use the following baseline and proposed models
for evaluation.
</bodyText>
<listItem confidence="0.9885146">
• SegTag: our reimplementation of the joint seg-
mentation and POS tagging model by Zhang and
Clark (2010). Table 5 shows that this reimple-
mentation almost reproduces the accuracy of their
implementation. We used the beam of 16, which
they reported to achieve the best accuracies.
• Dep’: the state-of-the-art dependency parser by
Huang and Sagae (2010). We used our reimple-
mentation, which is used in Hatori et al. (2011).
• Dep: Dep’ without look-ahead features.
• TagDep: the joint POS tagging and dependency
parsing model (Hatori et al., 2011), where the
look-ahead features are omitted.5
• SegTag+Dep/SegTag+Dep’: a pipeline combina-
tion of SegTag and Dep or Dep’.
• SegTag+TagDep: a pipeline combination of Seg-
Tag and TagDep, where only the segmentation
output of SegTag is used as input to TagDep; the
output tags of TagDep are used for evaluation.
• SegTagDep: the proposed full joint model.
</listItem>
<bodyText confidence="0.944651">
All of the models described above except Dep’ are
based on the same feature sets for segmentation and
</bodyText>
<footnote confidence="0.999404666666667">
3http://www.keenage.com/html/e index.html
4http://zh.wikipedia.org/wiki
5We used the original implementation used in Hatori et al.
(2011). In Hatori et al. (2011), we confirmed that omission of
the look-ahead features results in a 0.26% decrease in the pars-
ing accuracy on CTB-5d (dev).
</footnote>
<subsectionHeader confidence="0.999434">
4.3 Development Results
</subsectionHeader>
<bodyText confidence="0.999791703703703">
We have some parameters to tune: parsing feature
weight up, beam size, and training epoch. All these
parameters are set based on experiments on CTB-5c.
For experiments on CTB-5j, CTB-6, and CTB-7, the
training epoch is set using the development set.
Figure 2 shows the F1 scores of the proposed
model (SegTagDep) on CTB-5c-1 with respect to the
training epoch and different parsing feature weights,
where “Seg”, “Tag”, and “Dep” respectively denote
the F1 scores of word segmentation, POS tagging,
and dependency parsing. In this experiment, the ex-
ternal dictionaries are not used, and the beam size
of 32 is used. Interestingly, if we simply set up to
1, the accuracies seem to converge at lower levels.
The up = 0.2 setting seems to reach almost identi-
cal segmentation and tagging accuracies as the best
setting up = 0.5, but the convergence occurs more
slowly. Based on this experiment, we set up to 0.5
throughout the experiments in this paper.
Table 3 shows the performance and speed of the
full joint model (with no dictionaries) on CTB-5c-1
with respect to the beam size. Although even the
beam size of 32 results in competitive accuracies
for word segmentation and POS tagging, the depen-
dency accuracy is affected most by the increase of
the beam size. Based on this experiment, we set the
beam size of SegTagDep to 64 throughout the exper-
</bodyText>
<page confidence="0.955646">
1050
</page>
<table confidence="0.9626718">
System Seg Tag
Kruengkrai ’09 97.87 93.67
Zhang ’10 97.78 93.67
Sun ’11 98.17 94.02
Wang ’11 98.11 94.18
SegTag 97.66 93.61
SegTagDep 97.73 94.46
SegTag(d) 98.18 94.08
SegTagDep(d) 98.26 94.64
Table 5: Final results on CTB-5j
</table>
<page confidence="0.590277">
97
</page>
<figure confidence="0.9909106">
96
95
94
93
92
91
90
0.05 0.1 0.2 0.5 1 2
SegTag (Seg)
SegTagDep (Seg)
SegTag (Tag)
SegTag+TagDep (Tag)
SegTagDep (Tag)
76
75
74
73
72
71
70
69
0.05 0.1 0.2 0.5 1 2
SegTag+Dep (Dep)
SegTag+TagDep (Dep)
SegTagDep (Dep)
</figure>
<figureCaption confidence="0.8596584">
Figure 3: Performance of baseline and joint models
w.r.t. the average processing time (in sec.) per sen-
tence. Each point corresponds to the beam size of
4, 8, 16, 32, (64). The beam size of 16 is used for
SegTag in SegTag+Dep and SegTag+TagDep.
</figureCaption>
<bodyText confidence="0.987358428571429">
tionaries); these differences can be attributed to the
combination of the relieved error propagation and
the incorporation of the syntactic dependencies. In
addition, SegTag+TagDep has OOV tagging accura-
cies consistently lower than SegTag, suggesting that
the syntactic dependency has a negative effect on the
POS tagging accuracy of OOV words7. In contrast,
this negative effect is not observed for SegTagDep:
both the overall tagging accuracy and the OOV accu-
racy are improved, demonstrating the effectiveness
of the proposed model.
Figure 3 shows the performance and processing
time comparison of various models and their com-
binations. Although SegTagDep takes a few times
longer to achieve accuracies comparable to those of
SegTag+Dep/TagDep, it seems to present potential
7This is consistent with Hatori et al. (2011)’s observation
that although the joint POS tagging and dependency parsing im-
proves the accuracy of syntactically influential POS tags, it has
a slight side effect of increasing the confusion between general
and proper nouns (NN vs. NR).
</bodyText>
<table confidence="0.997515333333333">
Beam Seg Tag Dep Speed
4 94.96 90.19 70.29 5.7
8 95.78 91.53 72.81 3.2
16 96.09 92.09 74.20 1.8
32 96.18 92.24 74.57 0.95
64 96.28 92.37 74.96 0.48
</table>
<tableCaption confidence="0.986308">
Table 3: F1 scores and speed (in sentences per sec.)
of SegTagDep on CTB-5c-1 w.r.t. the beam size.
</tableCaption>
<bodyText confidence="0.921245">
iments in this paper, unless otherwise noted.
</bodyText>
<subsectionHeader confidence="0.994839">
4.4 Main Results
</subsectionHeader>
<bodyText confidence="0.999997371428571">
In this section, we present experimentally obtained
results using the proposed and baseline models. Ta-
ble 4 shows the segmentation, POS tagging, and
dependency parsing F1 scores of these models on
CTB-5c. Irrespective of the existence of the dic-
tionary features, the joint model SegTagDep largely
increases the POS tagging and dependency pars-
ing accuracies (by 0.56–0.63% and 2.34–2.44%);
the improvements in parsing accuracies are still
significant even compared with SegTag+Dep’ (the
pipeline model with the look-ahead features). How-
ever, when the external dictionaries are not used
(“wo/dict”), no substantial improvements for seg-
mentation accuracies were observed. In contrast,
when the dictionaries are used (“w/dict”), the seg-
mentation accuracies are now improved over the
baseline model SegTag consistently (on every trial).
Although the overall improvement in segmentation
is only around 0.1%, more than 1% improvement is
observed if we specifically examine OOV6 words.
The difference between “wo/dict” and “w/dict” re-
sults suggests that the syntactic dependencies might
work as a noise when the segmentation model is in-
sufficiently stable, but the model does improve when
it is stable, not receiving negative effects from the
syntactic dependencies.
The partially joint model SegTag+TagDep is
shown to perform reasonably well in dependency
parsing: with dictionaries, it achieved the 2.02% im-
provement over SegTag+Dep, which is only 0.32%
lower than SegTagDep. However, whereas Seg-
Tag+TagDep showed no substantial improvement in
tagging accuracies over SegTag (when the dictionar-
ies are used), SegTagDep achieved consistent im-
provements of 0.46% and 0.58% (without/with dic-
</bodyText>
<footnote confidence="0.958287">
6We define the OOV words as the words that have not seen in
the training data, even when the external dictionaries are used.
</footnote>
<page confidence="0.959885">
1051
</page>
<table confidence="0.998684">
Segmentation POS Tagging
Model Dependency
ALL OOV ALL OOV
SegTag+Dep 91.74 59.82 72.58
wo/dict SegTag+Dep’ 96.22 72.24 72.94 (+0.36$)
SegTag+TagDep 91.86 (+0.12$) 58.89 (-0.93$) 74.60 (+2.02$)
SegTagDep 96.19 (-0.03) 72.24 (+0.00) 92.30 (+0.56$) 61.03 (+1.21$) 74.92 (+2.34$)
SegTag+Dep 92.34 65.44 73.53
w/dict SegTag+Dep’ 96.82 78.32 73.90 (+0.37$)
SegTag+TagDep 92.35 (+0.01) 63.20 (-2.24$) 75.45 (+1.92$)
SegTagDep 96.90 (+0.08$) 79.38 (+1.06$) 92.97 (+0.63$) 67.40 (+1.96$) 75.97 (+2.44$)
</table>
<tableCaption confidence="0.9509245">
Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five
trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep (� : p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.981861">
for greater improvement, especially for tagging and
parsing accuracies, when a larger beam can be used.
</bodyText>
<subsectionHeader confidence="0.995587">
4.5 Comparison with Other Systems
</subsectionHeader>
<bodyText confidence="0.999932217391304">
Table 5 and Table 6 show a comparison of the seg-
mentation and POS tagging accuracies with other
state-of-the-art models. “Kruengkrai+ ’09” is a
lattice-based model by Kruengkrai et al. (2009).
“Zhang ’10” is the incremental model by Zhang and
Clark (2010). These two systems use no external re-
sources other than the CTB corpora. “Sun+ ’11” is a
CRF-based model (Sun, 2011) that uses a combina-
tion of several models, with a dictionary of idioms.
“Wang+ ’11” is a semi-supervised model by Wang
et al. (2011), which additionally uses the Chinese
Gigaword Corpus.
Our models with dictionaries (those marked with
‘(d)’) have competitive accuracies to other state-of-
the-art systems, and SegTagDep(d) achieved the best
reported segmentation and POS tagging accuracies,
using no additional corpora other than the dictio-
naries. Particularly, the POS tagging accuracy is
more than 0.4% higher than the previous best sys-
tem thanks to the contribution of syntactic depen-
dencies. These results also suggest that the use of
readily available dictionaries can be more effective
than semi-supervised approaches.
</bodyText>
<sectionHeader confidence="0.99973" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.995528285714286">
In this paper, we proposed the first joint model
for word segmentation, POS tagging, and depen-
dency parsing in Chinese. The model demonstrated
substantial improvements on the three tasks over
the pipeline combination of the state-of-the-art joint
segmentation and POS tagging model, and depen-
dency parser. Particularly, results showed that the
</bodyText>
<table confidence="0.947070454545454">
CTB-6 Test CTB-7 Test
Model
Seg Tag Dep Seg Tag Dep
Kruengkrai ’09 95.50 90.50 - 95.40 89.86 -
Wang ’11 95.79 91.12 - 95.65 90.46 -
SegTag+Dep 95.46 90.64 72.57 95.49 90.11 71.25
SegTagDep 95.45 91.27 74.88 95.42 90.62 73.58
(diff.) -0.01 +0.63$ +2.31$ -0.07 +0.51$ +2.33$
SegTag+Dep(d) 96.13 91.38 73.62 95.98 90.68 72.06
SegTagDep(d) 96.18 91.95 75.76 96.07 91.28 74.58
(diff.) +0.05 +0.57$ +2.14$ +0.09$ +0.60$ +2.52$
</table>
<tableCaption confidence="0.994328">
Table 6: Final results on CTB-6 and CTB-7
</tableCaption>
<bodyText confidence="0.963527">
accuracies of POS tagging and dependency pars-
ing were remarkably improved by 0.6% and 2.4%,
respectively corresponding to 8.3% and 10.2% er-
ror reduction. For word segmentation, although
the overall improvement was only around 0.1%,
greater than 1% improvements was observed for
OOV words. We conducted some comparison ex-
periments of the partially joint and full joint mod-
els. Compared to SegTagDep, SegTag+TagDep per-
forms reasonably well in terms of dependency pars-
ing accuracy, whereas the POS tagging accuracies
are more than 0.5% lower.
In future work, probabilistic pruning techniques
such as the one based on a maximum entropy model
are expected to improve the efficiency of the joint
model further because the accuracies are apparently
still improved if a larger beam can be used. More
efficient decoding would also allow the use of the
look-ahead features (Hatori et al., 2011) and richer
parsing features (Zhang and Nivre, 2011).
Acknowledgement We are grateful to the anony-
mous reviewers for their comments and suggestions, and
to Xianchao Wu, Kun Yu, Pontus Stenetorp, and Shin-
suke Mori for their helpful feedback.
</bodyText>
<page confidence="0.995742">
1052
</page>
<sectionHeader confidence="0.998333" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999639755813954">
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL 2004).
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilis-
tic parsing action models for multi-lingual dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of the 46th Annual
Meeting of the Association of Computational Linguis-
tics (ACL-2008).
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint POS tagging
and dependency parsing in Chinese. In Proceedings
of the Fifth International Joint Conference on Natural
Language Processing (IJCNLP-2011).
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.
2008. A cascaded linear model for joint Chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Haizhou. 2011. Joint mod-
els for Chinese POS tagging and dependency parsing.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing.
Xiaoqiang Luo. 2003. A maximum entropy Chinese
character-based parser. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2003).
Richard Sproat, Chilin Shih, William Gale, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22.
Weiwei Sun. 2011. A stacked sub-word model for joint
Chinese word segmentation and part-of-speech tag-
ging. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving Chinese word segmentation and POS
tagging with semi-supervised methods using large
auto-analyzed data. In Proceedings of the Fifth Inter-
national Joint Conference on Natural Language Pro-
cessing (IJCNLP-2011).
Fei Xia. 2000. The part-of-speech tagging guidelines
for the Penn Chinese treebank (3.0). Technical Report
IRCS-00-07, University of Pennsylvania Institute for
Research in Cognitive Science Technical Report, Oc-
tober.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single perceptron.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (short pa-
pers).
</reference>
<page confidence="0.979341">
1053
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608556">
<title confidence="0.901052">Incremental Joint Approach to Word Segmentation, POS Tagging,</title>
<author confidence="0.791452">Dependency Parsing in Chinese</author>
<affiliation confidence="0.83667">of Tokyo / 7-3-1 Hongo, Bunkyo, Tokyo, Institute of Informatics / 2-1-2 Hitotsubashi, Chiyoda, Tokyo,</affiliation>
<address confidence="0.984068">Research Asia / 5 Danling Street, Haidian District, Beijing, P.R.</address>
<email confidence="0.999705">jtsujii@microsoft.com</email>
<abstract confidence="0.997919055555556">We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partially joint models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Joint morphological and syntactic disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="6607" citStr="Cohen and Smith, 2007" startWordPosition="1004" endWordPosition="1007"> that performs segmentation, POS tagging, and parsing within a single character-based framework. They reported that the POS tags contribute to segmentation accuracies by more than 1%, but the syntactic information has no substantial effect on the segmentation accuracies. In contrast, we built a joint model based on a dependency-based framework, with a rich set of structural features. Using it, we show the first positive result in Chinese that the segmentation accuracies can be improved using the syntactic information. Another line of work exists on lattice-based parsing for Semitic languages (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). These methods first convert an input sentence into a lattice encoding the morphological ambiguities, and then conduct joint morphological segmentation and PCFG parsing. However, the segmentation possibilities considered in those studies are limited to those output by an existing morphological analyzer. In addition, the lattice does not include word segmentation ambiguities crossing boundaries of space-delimited tokens. In contrast, because the Chinese language does not have spaces between words, we fundamentally need to consider the lattice structure of the whol</context>
</contexts>
<marker>Cohen, Smith, 2007</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2007. Joint morphological and syntactic disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="10438" citStr="Collins and Roark, 2004" startWordPosition="1637" endWordPosition="1640">sy0 s1 / sx0 s1, respectively. Although SH(t) is similar to the one used in Hatori et al. (2011), now it shifts the first character in the queue as a new word, instead of shifting a word. Following Zhang and Clark (2010), the POS tag is assigned to the word when its first character is shifted, and the word–tag pairs observed in the training data and the closed-set tags (Xia, 2000) are used to prune unlikely derivations. Because 33 tags are defined in the CTB tag set (Xia, 2000), our model exploits a total of 36 actions. To train the model, we use the averaged perceptron with the early update (Collins and Roark, 2004). In our joint model, the early update is invoked by mistakes in any of word segmentation, POS tagging, or dependency parsing. 3.2 Alignment of States When dependency parsing is integrated into the task of joint word segmentation and POS tagging, it is not straightforward to define a scheme to align (synchronize) the states in the beam. In beam search, we use the step index that is associated with each state: the parser states in process are aligned according to the index, and the beam search pruning is applied to those states with the same index. Consequently, for the beam search to function </context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Probabilistic parsing action models for multi-lingual dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<contexts>
<context position="22158" citStr="Duan et al. (2007)" startWordPosition="3795" endWordPosition="3798">s at the early stage of training. Otherwise, the premature incorporation of the non-local syntactic dependencies might engender overfitting to the training data. P16 P17 P18 P20 P23 P25 P27 P28 * Table 1: Feature templates for the full action a being applied to the state { } ~φ(ψ, a) = λ~ · ~φst(ψ, a) + σp~φp(ψ, a) , 4 Experiment 4.1 Experimental Settings We use the Chinese Penn Treebank ver. 5.1, 6.0, and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7) for evaluation. These corpora are split into training, development, and test sets, according to previous works. For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j. We also prepare a dataset for cross validation: the dataset consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j. We split into five sets and alternatively use four of these as the training set and the rest as the test set. CTB-6 is split according to the official spli CTB-5c CTB5c (CTB-5c-n), t U02,03 q_1.e q_1.e o q_1.t as-is any S01 q_1.e o c0 φ A o o A, SH(t), any o o A, SH(t), any o o o A, SH(t), any o o q1.t as-is RR, RL P19 s1.t o o as-is RR, RL o o q1.t as-is RR, RL P21 s1.t o o as-i</context>
</contexts>
<marker>Duan, Zhao, Xu, 2007</marker>
<rawString>Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic parsing action models for multi-lingual dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics (ACL-2008).</booktitle>
<contexts>
<context position="6637" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="1008" endWordPosition="1011">tion, POS tagging, and parsing within a single character-based framework. They reported that the POS tags contribute to segmentation accuracies by more than 1%, but the syntactic information has no substantial effect on the segmentation accuracies. In contrast, we built a joint model based on a dependency-based framework, with a rich set of structural features. Using it, we show the first positive result in Chinese that the segmentation accuracies can be improved using the syntactic information. Another line of work exists on lattice-based parsing for Semitic languages (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). These methods first convert an input sentence into a lattice encoding the morphological ambiguities, and then conduct joint morphological segmentation and PCFG parsing. However, the segmentation possibilities considered in those studies are limited to those output by an existing morphological analyzer. In addition, the lattice does not include word segmentation ambiguities crossing boundaries of space-delimited tokens. In contrast, because the Chinese language does not have spaces between words, we fundamentally need to consider the lattice structure of the whole sentence. Therefore, we plac</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics (ACL-2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental joint POS tagging and dependency parsing in Chinese.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP-2011).</booktitle>
<contexts>
<context position="687" citStr="Hatori et al., 2011" startWordPosition="89" endWordPosition="92">ependency Parsing in Chinese Jun Hatori&apos; Takuya Matsuzaki&apos; Yusuke Miyao&apos; Jun’ichi Tsujii&apos; &apos;University of Tokyo / 7-3-1 Hongo, Bunkyo, Tokyo, Japan &apos;National Institute of Informatics / 2-1-2 Hitotsubashi, Chiyoda, Tokyo, Japan &apos;Microsoft Research Asia / 5 Danling Street, Haidian District, Beijing, P.R. China hatori@is.s.u-tokyo.ac.jp {takuya-matsuzaki,yusuke}@nii.ac.jp jtsujii@microsoft.com Abstract We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese. Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models. We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework. In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing. We also perform comparison experiments with the partial</context>
<context position="2395" citStr="Hatori et al., 2011" startWordPosition="354" endWordPosition="357">d to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al. (2009)). This is because some of the segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a sequence of POS tags. The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1% in Chinese (Zhang and Clark, 2008). In addition, some researchers recently proposed a joint approach to Chinese POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011); particularly, Hatori et al. (2011) proposed an incremental approach to this joint task, and showed that the joint approach improves the accuracies of these two tasks. In this context, it is natural to consider further a question regarding the joint framework: how strongly do the tasks of word segmentation and dependency parsing interact? In the following Chinese sentences: ❙✃ AF❱ ✍ AF ✚ �current peace-prize and peace operation related The current peace prize and peace operations are related. ❙✃ AF ❱✍ AF ✚ tH Q❙ current peace award peace operation related group The currentpeace is award</context>
<context position="8653" citStr="Hatori et al., 2011" startWordPosition="1318" endWordPosition="1321"> their model is practically ten times as fast as their original model. To incorporate the word-level features into the character-based decoder, the features are decomposed into substring-level features, which are effective for incomplete words to have comparable scores to complete words in the beam. Because we found that even an incremental approach with beam search is intractable if we perform the wordbased decoding, we take a character-based approach to produce our joint model. The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al., 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). They specifically modified the shift action so that it assigns the POS tag when a word is shifted onto the stack. However, because they regarded word segmentation as given, their model did not consider the 1046 interaction between segmentation and POS tagging. 3 Model 3.1 Incremental Joint Segmentation, POS Tagging, and Dependency Parsing Based on the joint POS tagging and dependency parsing model by Hatori et al. (2011), we build our joint model to solve word segmentation, POS taggi</context>
<context position="9910" citStr="Hatori et al. (2011)" startWordPosition="1539" endWordPosition="1542">ingle framework. Particularly, we change the role of the shift action and additionally use the append action, inspired by the character-based actions used in the joint segmentation and POS tagging model by Zhang and Clark (2010). The list of actions used is the following: • A: append the first character in the queue to the word on top of the stack. • SH(t): shift the first character in the input queue as a new word onto the stack, with POS tag t. • RL/RR: reduce the top two trees on the stack, (s0, s1), into a subtree sy0 s1 / sx0 s1, respectively. Although SH(t) is similar to the one used in Hatori et al. (2011), now it shifts the first character in the queue as a new word, instead of shifting a word. Following Zhang and Clark (2010), the POS tag is assigned to the word when its first character is shifted, and the word–tag pairs observed in the training data and the closed-set tags (Xia, 2000) are used to prune unlikely derivations. Because 33 tags are defined in the CTB tag set (Xia, 2000), our model exploits a total of 36 actions. To train the model, we use the averaged perceptron with the early update (Collins and Roark, 2004). In our joint model, the early update is invoked by mistakes in any of </context>
<context position="11634" citStr="Hatori et al. (2011)" startWordPosition="1839" endWordPosition="1842">beam search to function effectively, all states with the same index must be comparable, and all terminal states should have the same step index. We can first think of using the number of shifted characters as the step index, as Zhang and Clark (2010) does. However, because RL/RR actions can be performed without incrementing the step index, the decoder tends to prefer states with more dependency arcs, resulting more likely in premature choice of ‘reduce’ actions or oversegmentation of words. Alternatively, we can consider using the number of actions that have been applied as the step index, as Hatori et al. (2011) does. However, this results in inconsistent numbers of actions to reach the terminal states: some states that segment words into larger chunks reach a terminal state earlier than other states with smaller chunks. For these reasons, we have found that both approaches yield poor models that are not at all competitive with the baseline (pipeline) models1. To address this issue, we propose an indexing scheme using the number of character-based arcs. We presume that in addition to the word-to-word dependency arcs, each word (of length M) implicitly has M − 1 inter-character arcs, as in: AxBxC , (e</context>
<context position="16438" citStr="Hatori et al. (2011)" startWordPosition="2671" endWordPosition="2674">cause we found that P01–P17 are also useful for segmentation decisions, these features are applied to all actions including A, with an explicit distinction of action labels RL0/RR0 from RL1/RR1. On the other hand, P18–P28 are only used when one of the parser actions (SH(t), RL, or RR) is applied. Note that P07– P09 and P18–P21 (look-ahead features) require the look-ahead information of the next word form and POS tags, which cannot be incorporated straightforwardly in an incremental framework. Although we have found that these features can be incorporated using the delayed features proposed by Hatori et al. (2011), we did not use them in our current model because it results in the significant increase of computational time. 3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting. Therefore, we optionally use four features D01–D04 associated with external dictionaries. These features distinguish each dictionary source, reflecting the </context>
<context position="25243" citStr="Hatori et al. (2011)" startWordPosition="4361" endWordPosition="4364">ing punctuations. Statistical significance is tested by McNemar’s test (t : p &lt; 0.05, t : p &lt; 0.01). 4.2 Baseline and Proposed Models We use the following baseline and proposed models for evaluation. • SegTag: our reimplementation of the joint segmentation and POS tagging model by Zhang and Clark (2010). Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation. We used the beam of 16, which they reported to achieve the best accuracies. • Dep’: the state-of-the-art dependency parser by Huang and Sagae (2010). We used our reimplementation, which is used in Hatori et al. (2011). • Dep: Dep’ without look-ahead features. • TagDep: the joint POS tagging and dependency parsing model (Hatori et al., 2011), where the look-ahead features are omitted.5 • SegTag+Dep/SegTag+Dep’: a pipeline combination of SegTag and Dep or Dep’. • SegTag+TagDep: a pipeline combination of SegTag and TagDep, where only the segmentation output of SegTag is used as input to TagDep; the output tags of TagDep are used for evaluation. • SegTagDep: the proposed full joint model. All of the models described above except Dep’ are based on the same feature sets for segmentation and 3http://www.keenage.c</context>
<context position="28999" citStr="Hatori et al. (2011)" startWordPosition="4978" endWordPosition="4981">acies consistently lower than SegTag, suggesting that the syntactic dependency has a negative effect on the POS tagging accuracy of OOV words7. In contrast, this negative effect is not observed for SegTagDep: both the overall tagging accuracy and the OOV accuracy are improved, demonstrating the effectiveness of the proposed model. Figure 3 shows the performance and processing time comparison of various models and their combinations. Although SegTagDep takes a few times longer to achieve accuracies comparable to those of SegTag+Dep/TagDep, it seems to present potential 7This is consistent with Hatori et al. (2011)’s observation that although the joint POS tagging and dependency parsing improves the accuracy of syntactically influential POS tags, it has a slight side effect of increasing the confusion between general and proper nouns (NN vs. NR). Beam Seg Tag Dep Speed 4 94.96 90.19 70.29 5.7 8 95.78 91.53 72.81 3.2 16 96.09 92.09 74.20 1.8 32 96.18 92.24 74.57 0.95 64 96.28 92.37 74.96 0.48 Table 3: F1 scores and speed (in sentences per sec.) of SegTagDep on CTB-5c-1 w.r.t. the beam size. iments in this paper, unless otherwise noted. 4.4 Main Results In this section, we present experimentally obtained </context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2011</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2011. Incremental joint POS tagging and dependency parsing in Chinese. In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4357" citStr="Huang and Sagae, 2010" startWordPosition="655" endWordPosition="658">ese characters causes numerous oversegmentation errors for OOV words. Based on these observations, we aim at building a joint model that simultaneously processes word segmentation, POS tagging, and dependency parsing, trying to capture global interaction among 1045 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1045–1053, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics these three tasks. To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework. First, we must address the problem of how to align comparable states effectively in the beam. Because the number of dependency arcs varies depending on how words are segmented, we devise a step alignment scheme using the number of character-based arcs, which enables effective joint decoding for the three tasks. Second, although the feature set is fundamentally a combina</context>
<context position="8763" citStr="Huang and Sagae, 2010" startWordPosition="1334" endWordPosition="1337">s into the character-based decoder, the features are decomposed into substring-level features, which are effective for incomplete words to have comparable scores to complete words in the beam. Because we found that even an incremental approach with beam search is intractable if we perform the wordbased decoding, we take a character-based approach to produce our joint model. The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al., 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). They specifically modified the shift action so that it assigns the POS tag when a word is shifted onto the stack. However, because they regarded word segmentation as given, their model did not consider the 1046 interaction between segmentation and POS tagging. 3 Model 3.1 Incremental Joint Segmentation, POS Tagging, and Dependency Parsing Based on the joint POS tagging and dependency parsing model by Hatori et al. (2011), we build our joint model to solve word segmentation, POS tagging, and dependency parsing within a single framework. Particularly, we change the role of the shift action and</context>
<context position="14090" citStr="Huang and Sagae, 2010" startWordPosition="2271" endWordPosition="2274">curacy. C AxB x A x B x C , and 1047 Figure 1: Illustration of the alignment of steps. size, T is the number of POS tags (= 33), and N is the number of characters in the sentence. Theoretically, the computational time is greater than that with the character-based joint segmentation and tagging model by Zhang and Clark (2010) by a factor of T+3 2N T+1 N — 11, when the same beam size is used. 3.3 Features The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. However, we must carefully adjust which features are to be activated and when, and how they are combined with which action labels, depending on the type of the features because we intend to perform three tasks in a single incremental framework. The list of the features used in our joint model is presented in Table 1, where S01–S05, W01– W21, and T01–05 are taken from Zhang and Clark (2010), and P01–P28 are taken from Huang and Sagae (2010). Note that not all features are always considered: each feature is only considered if the act</context>
<context position="23708" citStr="Huang and Sagae, 2010" startWordPosition="4109" endWordPosition="4112"> o s0.t SH(t), s0.w SH(t), s1.t s1.lc.t s0.w SH(t), s0.t s0.rc.t SH(t), s1.t s0.w s0.lc.t SH(t), s2.t s1.t s0.t -is SH(t), RR, RL 1049 96 94 92 90 0 10 20 30 40 50 60 70 80 Seg (σ_p=0.1) Seg (σ_p=0.2) Seg (σ_p=0.5) Seg (σ_p=1.0) Tag (σ_p=0.1) Tag (σ_p=0.2) Tag (σ_p=0.5) Tag (σ_p=1.0) 76 74 72 70 68 66 64 62 60 0 10 20 30 40 50 60 70 80 Dep (σ_p=0.1) Dep (σ_p=0.2) Dep (σ_p=0.5) Dep (σ_p=1.0) Figure 2: F1 scores (in %) of SegTagDep on CTB5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). tagging (Zhang and Clark, 2008; Zhang and Clark, 2010) and dependency parsing (Huang and Sagae, 2010). Therefore, we can investigate the contribution of the joint approach through comparison with the pipeline and joint models. 88 86 described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3, consisting of 91,015 words, and page names from the Chinese Wikipedia4 as of Oct 26, 2011, consisting of 709,352 words. These dictionaries only consist of word forms with no frequency or POS information. We use standard measures of word-level precision, recall, and F1 score,</context>
<context position="25174" citStr="Huang and Sagae (2010)" startWordPosition="4348" endWordPosition="4351"> task of dependency parsing with the unlabeled attachment scores excluding punctuations. Statistical significance is tested by McNemar’s test (t : p &lt; 0.05, t : p &lt; 0.01). 4.2 Baseline and Proposed Models We use the following baseline and proposed models for evaluation. • SegTag: our reimplementation of the joint segmentation and POS tagging model by Zhang and Clark (2010). Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation. We used the beam of 16, which they reported to achieve the best accuracies. • Dep’: the state-of-the-art dependency parser by Huang and Sagae (2010). We used our reimplementation, which is used in Hatori et al. (2011). • Dep: Dep’ without look-ahead features. • TagDep: the joint POS tagging and dependency parsing model (Hatori et al., 2011), where the look-ahead features are omitted.5 • SegTag+Dep/SegTag+Dep’: a pipeline combination of SegTag and Dep or Dep’. • SegTag+TagDep: a pipeline combination of SegTag and TagDep, where only the segmentation output of SegTag is used as input to TagDep; the output tags of TagDep are used for evaluation. • SegTagDep: the proposed full joint model. All of the models described above except Dep’ are base</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Lu</author>
</authors>
<title>A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of</booktitle>
<contexts>
<context position="22209" citStr="Jiang et al. (2008)" startWordPosition="3806" endWordPosition="3809">remature incorporation of the non-local syntactic dependencies might engender overfitting to the training data. P16 P17 P18 P20 P23 P25 P27 P28 * Table 1: Feature templates for the full action a being applied to the state { } ~φ(ψ, a) = λ~ · ~φst(ψ, a) + σp~φp(ψ, a) , 4 Experiment 4.1 Experimental Settings We use the Chinese Penn Treebank ver. 5.1, 6.0, and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7) for evaluation. These corpora are split into training, development, and test sets, according to previous works. For CTB-5, we refer to the split by Duan et al. (2007) as CTB-5d, and to the split by Jiang et al. (2008) as CTB-5j. We also prepare a dataset for cross validation: the dataset consists of sentences from CTB-5 excluding the development and test sets of CTB-5d and CTB-5j. We split into five sets and alternatively use four of these as the training set and the rest as the test set. CTB-6 is split according to the official spli CTB-5c CTB5c (CTB-5c-n), t U02,03 q_1.e q_1.e o q_1.t as-is any S01 q_1.e o c0 φ A o o A, SH(t), any o o A, SH(t), any o o o A, SH(t), any o o q1.t as-is RR, RL P19 s1.t o o as-is RR, RL o o q1.t as-is RR, RL P21 s1.t o o as-is RR, RL P22 s1.t o s1.rc.t o as-is RR, RL s1.lc.t </context>
</contexts>
<marker>Jiang, Huang, Liu, Lu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu. 2008. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="1892" citStr="Kruengkrai et al. (2009)" startWordPosition="273" endWordPosition="276">ts with the partially joint models. 1 Introduction In processing natural languages that do not include delimiters (e.g. spaces) between words, word segmentation is the crucial first step that is necessary to perform virtually all NLP tasks. Furthermore, the word-level information is often augmented with the POS tags, which, along with segmentation, form the basic foundation of statistical NLP. Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al. (2009)). This is because some of the segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a sequence of POS tags. The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1% in Chinese (Zhang and Clark, 2008). In addition, some researchers recently proposed a joint approach to Chinese POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011); particularly, Hatori et al. (2011) proposed an incremental approach to this joint task, and sho</context>
<context position="32379" citStr="Kruengkrai et al. (2009)" startWordPosition="5491" endWordPosition="5494">.45 (+1.92$) SegTagDep 96.90 (+0.08$) 79.38 (+1.06$) 92.97 (+0.63$) 67.40 (+1.96$) 75.97 (+2.44$) Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep (� : p &lt; 0.01). for greater improvement, especially for tagging and parsing accuracies, when a larger beam can be used. 4.5 Comparison with Other Systems Table 5 and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models. “Kruengkrai+ ’09” is a lattice-based model by Kruengkrai et al. (2009). “Zhang ’10” is the incremental model by Zhang and Clark (2010). These two systems use no external resources other than the CTB corpora. “Sun+ ’11” is a CRF-based model (Sun, 2011) that uses a combination of several models, with a dictionary of idioms. “Wang+ ’11” is a semi-supervised model by Wang et al. (2011), which additionally uses the Chinese Gigaword Corpus. Our models with dictionaries (those marked with ‘(d)’) have competitive accuracies to other state-ofthe-art systems, and SegTagDep(d) achieved the best reported segmentation and POS tagging accuracies, using no additional corpora o</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Wenliang Chen</author>
<author>Haizhou Haizhou</author>
</authors>
<title>Joint models for Chinese POS tagging and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2373" citStr="Li et al., 2011" startWordPosition="350" endWordPosition="353"> have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al. (2009)). This is because some of the segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a sequence of POS tags. The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1% in Chinese (Zhang and Clark, 2008). In addition, some researchers recently proposed a joint approach to Chinese POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011); particularly, Hatori et al. (2011) proposed an incremental approach to this joint task, and showed that the joint approach improves the accuracies of these two tasks. In this context, it is natural to consider further a question regarding the joint framework: how strongly do the tasks of word segmentation and dependency parsing interact? In the following Chinese sentences: ❙✃ AF❱ ✍ AF ✚ �current peace-prize and peace operation related The current peace prize and peace operations are related. ❙✃ AF ❱✍ AF ✚ tH Q❙ current peace award peace operation related group The</context>
</contexts>
<marker>Li, Zhang, Che, Liu, Chen, Haizhou, 2011</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Haizhou. 2011. Joint models for Chinese POS tagging and dependency parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>A maximum entropy Chinese character-based parser.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP-2003).</booktitle>
<contexts>
<context position="5949" citStr="Luo (2003)" startWordPosition="903" endWordPosition="904">. We have also found that we must balance the learning rate between features for segmentation and tagging decisions, and those for dependency parsing. We perform experiments using the Chinese Treebank (CTB) corpora, demonstrating that the accuracies of the three tasks can be improved significantly over the pipeline combination of the state-of-the-art joint segmentation and POS tagging model, and the dependency parser. We also perform comparison experiments with partially joint models, and investigate the tradeoff between the running speed and the model performance. 2 Related Works In Chinese, Luo (2003) proposed a joint constituency parser that performs segmentation, POS tagging, and parsing within a single character-based framework. They reported that the POS tags contribute to segmentation accuracies by more than 1%, but the syntactic information has no substantial effect on the segmentation accuracies. In contrast, we built a joint model based on a dependency-based framework, with a rich set of structural features. Using it, we show the first positive result in Chinese that the segmentation accuracies can be improved using the syntactic information. Another line of work exists on lattice-</context>
</contexts>
<marker>Luo, 2003</marker>
<rawString>Xiaoqiang Luo. 2003. A maximum entropy Chinese character-based parser. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state wordsegmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<contexts>
<context position="16704" citStr="Sproat et al., 1996" startWordPosition="2715" endWordPosition="2718"> (SH(t), RL, or RR) is applied. Note that P07– P09 and P18–P21 (look-ahead features) require the look-ahead information of the next word form and POS tags, which cannot be incorporated straightforwardly in an incremental framework. Although we have found that these features can be incorporated using the delayed features proposed by Hatori et al. (2011), we did not use them in our current model because it results in the significant increase of computational time. 3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting. Therefore, we optionally use four features D01–D04 associated with external dictionaries. These features distinguish each dictionary source, reflecting the fact that different dictionaries have different characteristics. These features will also be used in our reimplementation of the model by Zhang and Clark (2010). 3.4 Adjusting the Learning Rate of Features In formulating the three tasks in the incremental framework,</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state wordsegmentation algorithm for Chinese. Computational Linguistics, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="32560" citStr="Sun, 2011" startWordPosition="5525" endWordPosition="5526"> five trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep (� : p &lt; 0.01). for greater improvement, especially for tagging and parsing accuracies, when a larger beam can be used. 4.5 Comparison with Other Systems Table 5 and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models. “Kruengkrai+ ’09” is a lattice-based model by Kruengkrai et al. (2009). “Zhang ’10” is the incremental model by Zhang and Clark (2010). These two systems use no external resources other than the CTB corpora. “Sun+ ’11” is a CRF-based model (Sun, 2011) that uses a combination of several models, with a dictionary of idioms. “Wang+ ’11” is a semi-supervised model by Wang et al. (2011), which additionally uses the Chinese Gigaword Corpus. Our models with dictionaries (those marked with ‘(d)’) have competitive accuracies to other state-ofthe-art systems, and SegTagDep(d) achieved the best reported segmentation and POS tagging accuracies, using no additional corpora other than the dictionaries. Particularly, the POS tagging accuracy is more than 0.4% higher than the previous best system thanks to the contribution of syntactic dependencies. These</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP-2011).</booktitle>
<contexts>
<context position="23922" citStr="Wang et al. (2011)" startWordPosition="4144" endWordPosition="4147"> (σ_p=1.0) Tag (σ_p=0.1) Tag (σ_p=0.2) Tag (σ_p=0.5) Tag (σ_p=1.0) 76 74 72 70 68 66 64 62 60 0 10 20 30 40 50 60 70 80 Dep (σ_p=0.1) Dep (σ_p=0.2) Dep (σ_p=0.5) Dep (σ_p=1.0) Figure 2: F1 scores (in %) of SegTagDep on CTB5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). tagging (Zhang and Clark, 2008; Zhang and Clark, 2010) and dependency parsing (Huang and Sagae, 2010). Therefore, we can investigate the contribution of the joint approach through comparison with the pipeline and joint models. 88 86 described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3, consisting of 91,015 words, and page names from the Chinese Wikipedia4 as of Oct 26, 2011, consisting of 709,352 words. These dictionaries only consist of word forms with no frequency or POS information. We use standard measures of word-level precision, recall, and F1 score, for evaluating each task. The output of dependencies cannot be correct unless the syntactic head and dependent of the dependency relation are both segmented correctly. Following the standard setting in dependency </context>
<context position="32693" citStr="Wang et al. (2011)" startWordPosition="5547" endWordPosition="5550">especially for tagging and parsing accuracies, when a larger beam can be used. 4.5 Comparison with Other Systems Table 5 and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models. “Kruengkrai+ ’09” is a lattice-based model by Kruengkrai et al. (2009). “Zhang ’10” is the incremental model by Zhang and Clark (2010). These two systems use no external resources other than the CTB corpora. “Sun+ ’11” is a CRF-based model (Sun, 2011) that uses a combination of several models, with a dictionary of idioms. “Wang+ ’11” is a semi-supervised model by Wang et al. (2011), which additionally uses the Chinese Gigaword Corpus. Our models with dictionaries (those marked with ‘(d)’) have competitive accuracies to other state-ofthe-art systems, and SegTagDep(d) achieved the best reported segmentation and POS tagging accuracies, using no additional corpora other than the dictionaries. Particularly, the POS tagging accuracy is more than 0.4% higher than the previous best system thanks to the contribution of syntactic dependencies. These results also suggest that the use of readily available dictionaries can be more effective than semi-supervised approaches. 5 Conclus</context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of the Fifth International Joint Conference on Natural Language Processing (IJCNLP-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>The part-of-speech tagging guidelines for the Penn Chinese treebank (3.0).</title>
<date>2000</date>
<tech>Technical Report IRCS-00-07,</tech>
<institution>University of Pennsylvania Institute for Research in Cognitive Science</institution>
<contexts>
<context position="10197" citStr="Xia, 2000" startWordPosition="1595" endWordPosition="1596">aracter in the queue to the word on top of the stack. • SH(t): shift the first character in the input queue as a new word onto the stack, with POS tag t. • RL/RR: reduce the top two trees on the stack, (s0, s1), into a subtree sy0 s1 / sx0 s1, respectively. Although SH(t) is similar to the one used in Hatori et al. (2011), now it shifts the first character in the queue as a new word, instead of shifting a word. Following Zhang and Clark (2010), the POS tag is assigned to the word when its first character is shifted, and the word–tag pairs observed in the training data and the closed-set tags (Xia, 2000) are used to prune unlikely derivations. Because 33 tags are defined in the CTB tag set (Xia, 2000), our model exploits a total of 36 actions. To train the model, we use the averaged perceptron with the early update (Collins and Roark, 2004). In our joint model, the early update is invoked by mistakes in any of word segmentation, POS tagging, or dependency parsing. 3.2 Alignment of States When dependency parsing is integrated into the task of joint word segmentation and POS tagging, it is not straightforward to define a scheme to align (synchronize) the states in the beam. In beam search, we u</context>
</contexts>
<marker>Xia, 2000</marker>
<rawString>Fei Xia. 2000. The part-of-speech tagging guidelines for the Penn Chinese treebank (3.0). Technical Report IRCS-00-07, University of Pennsylvania Institute for Research in Cognitive Science Technical Report, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association</booktitle>
<contexts>
<context position="2244" citStr="Zhang and Clark, 2008" startWordPosition="330" endWordPosition="333">m the basic foundation of statistical NLP. Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al. (2009)). This is because some of the segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a sequence of POS tags. The joint approach to word segmentation and POS tagging has been reported to improve word segmentation and POS tagging accuracies by more than 1% in Chinese (Zhang and Clark, 2008). In addition, some researchers recently proposed a joint approach to Chinese POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011); particularly, Hatori et al. (2011) proposed an incremental approach to this joint task, and showed that the joint approach improves the accuracies of these two tasks. In this context, it is natural to consider further a question regarding the joint framework: how strongly do the tasks of word segmentation and dependency parsing interact? In the following Chinese sentences: ❙✃ AF❱ ✍ AF ✚ �current peace-prize and peace operation related The c</context>
<context position="7529" citStr="Zhang and Clark (2008)" startWordPosition="1142" endWordPosition="1145">an existing morphological analyzer. In addition, the lattice does not include word segmentation ambiguities crossing boundaries of space-delimited tokens. In contrast, because the Chinese language does not have spaces between words, we fundamentally need to consider the lattice structure of the whole sentence. Therefore, we place no restriction on the segmentation possibilities to consider, and we assess the full potential of the joint segmentation and dependency parsing model. Among the many recent works on joint segmentation and POS tagging for Chinese, the linear-time incremental models by Zhang and Clark (2008) and Zhang and Clark (2010) largely inspired our model. Zhang and Clark (2008) proposed an incremental joint segmentation and POS tagging model, with an effective feature set for Chinese. However, it requires to computationally expensive multiple beams to compare words of different lengths using beam search. More recently, Zhang and Clark (2010) proposed an efficient character-based decoder for their word-based model. In their new model, a single beam suffices for decoding; hence, they reported that their model is practically ten times as fast as their original model. To incorporate the word-l</context>
<context position="23637" citStr="Zhang and Clark, 2008" startWordPosition="4098" endWordPosition="4101">), s0.t q0.t SH(t), s0.w q0.t SH(t), s0.w q0.t SH(t), s0.t SH(t), s1.t o s0.t SH(t), s0.w SH(t), s1.t s1.lc.t s0.w SH(t), s0.t s0.rc.t SH(t), s1.t s0.w s0.lc.t SH(t), s2.t s1.t s0.t -is SH(t), RR, RL 1049 96 94 92 90 0 10 20 30 40 50 60 70 80 Seg (σ_p=0.1) Seg (σ_p=0.2) Seg (σ_p=0.5) Seg (σ_p=1.0) Tag (σ_p=0.1) Tag (σ_p=0.2) Tag (σ_p=0.5) Tag (σ_p=1.0) 76 74 72 70 68 66 64 62 60 0 10 20 30 40 50 60 70 80 Dep (σ_p=0.1) Dep (σ_p=0.2) Dep (σ_p=0.5) Dep (σ_p=1.0) Figure 2: F1 scores (in %) of SegTagDep on CTB5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). tagging (Zhang and Clark, 2008; Zhang and Clark, 2010) and dependency parsing (Huang and Sagae, 2010). Therefore, we can investigate the contribution of the joint approach through comparison with the pipeline and joint models. 88 86 described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3, consisting of 91,015 words, and page names from the Chinese Wikipedia4 as of Oct 26, 2011, consisting of 709,352 words. These dictionaries only consist of word forms with no frequency or POS information. </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5016" citStr="Zhang and Clark, 2010" startWordPosition="756" endWordPosition="759">haracter-based decoding over candidate structures. Two major challenges exist in formalizing the joint segmentation and dependency parsing task in the character-based incremental framework. First, we must address the problem of how to align comparable states effectively in the beam. Because the number of dependency arcs varies depending on how words are segmented, we devise a step alignment scheme using the number of character-based arcs, which enables effective joint decoding for the three tasks. Second, although the feature set is fundamentally a combination of those used in previous works (Zhang and Clark, 2010; Huang and Sagae, 2010), to integrate them in a single incremental framework is not straightforward. Because we must perform decisions of three kinds (segmentation, tagging, and parsing) in an incremental framework, we must adjust which features are to be activated when, and how they are combined with which action labels. We have also found that we must balance the learning rate between features for segmentation and tagging decisions, and those for dependency parsing. We perform experiments using the Chinese Treebank (CTB) corpora, demonstrating that the accuracies of the three tasks can be i</context>
<context position="7556" citStr="Zhang and Clark (2010)" startWordPosition="1147" endWordPosition="1150">nalyzer. In addition, the lattice does not include word segmentation ambiguities crossing boundaries of space-delimited tokens. In contrast, because the Chinese language does not have spaces between words, we fundamentally need to consider the lattice structure of the whole sentence. Therefore, we place no restriction on the segmentation possibilities to consider, and we assess the full potential of the joint segmentation and dependency parsing model. Among the many recent works on joint segmentation and POS tagging for Chinese, the linear-time incremental models by Zhang and Clark (2008) and Zhang and Clark (2010) largely inspired our model. Zhang and Clark (2008) proposed an incremental joint segmentation and POS tagging model, with an effective feature set for Chinese. However, it requires to computationally expensive multiple beams to compare words of different lengths using beam search. More recently, Zhang and Clark (2010) proposed an efficient character-based decoder for their word-based model. In their new model, a single beam suffices for decoding; hence, they reported that their model is practically ten times as fast as their original model. To incorporate the word-level features into the char</context>
<context position="9518" citStr="Zhang and Clark (2010)" startWordPosition="1459" endWordPosition="1462">they regarded word segmentation as given, their model did not consider the 1046 interaction between segmentation and POS tagging. 3 Model 3.1 Incremental Joint Segmentation, POS Tagging, and Dependency Parsing Based on the joint POS tagging and dependency parsing model by Hatori et al. (2011), we build our joint model to solve word segmentation, POS tagging, and dependency parsing within a single framework. Particularly, we change the role of the shift action and additionally use the append action, inspired by the character-based actions used in the joint segmentation and POS tagging model by Zhang and Clark (2010). The list of actions used is the following: • A: append the first character in the queue to the word on top of the stack. • SH(t): shift the first character in the input queue as a new word onto the stack, with POS tag t. • RL/RR: reduce the top two trees on the stack, (s0, s1), into a subtree sy0 s1 / sx0 s1, respectively. Although SH(t) is similar to the one used in Hatori et al. (2011), now it shifts the first character in the queue as a new word, instead of shifting a word. Following Zhang and Clark (2010), the POS tag is assigned to the word when its first character is shifted, and the w</context>
<context position="11264" citStr="Zhang and Clark (2010)" startWordPosition="1780" endWordPosition="1783">f joint word segmentation and POS tagging, it is not straightforward to define a scheme to align (synchronize) the states in the beam. In beam search, we use the step index that is associated with each state: the parser states in process are aligned according to the index, and the beam search pruning is applied to those states with the same index. Consequently, for the beam search to function effectively, all states with the same index must be comparable, and all terminal states should have the same step index. We can first think of using the number of shifted characters as the step index, as Zhang and Clark (2010) does. However, because RL/RR actions can be performed without incrementing the step index, the decoder tends to prefer states with more dependency arcs, resulting more likely in premature choice of ‘reduce’ actions or oversegmentation of words. Alternatively, we can consider using the number of actions that have been applied as the step index, as Hatori et al. (2011) does. However, this results in inconsistent numbers of actions to reach the terminal states: some states that segment words into larger chunks reach a terminal state earlier than other states with smaller chunks. For these reason</context>
<context position="13794" citStr="Zhang and Clark (2010)" startWordPosition="2217" endWordPosition="2220">to store new states at each step. The computational complexity of the entire process is O(B(T + 3) · 2N), where B is the beam 1For example, in our preliminary experiment on CTB-5, the step indexing according to the number of actions underperforms the baseline model by 0.2–0.3% in segmentation accuracy. C AxB x A x B x C , and 1047 Figure 1: Illustration of the alignment of steps. size, T is the number of POS tags (= 33), and N is the number of characters in the sentence. Theoretically, the computational time is greater than that with the character-based joint segmentation and tagging model by Zhang and Clark (2010) by a factor of T+3 2N T+1 N — 11, when the same beam size is used. 3.3 Features The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. However, we must carefully adjust which features are to be activated and when, and how they are combined with which action labels, depending on the type of the features because we intend to perform three tasks in a single incremental framewo</context>
<context position="17198" citStr="Zhang and Clark (2010)" startWordPosition="2791" endWordPosition="2794"> Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting. Therefore, we optionally use four features D01–D04 associated with external dictionaries. These features distinguish each dictionary source, reflecting the fact that different dictionaries have different characteristics. These features will also be used in our reimplementation of the model by Zhang and Clark (2010). 3.4 Adjusting the Learning Rate of Features In formulating the three tasks in the incremental framework, we found that adjusting the update rate depending on the type of the features (segmentation/tagging vs. parsing) crucially impacts the final performance of the model. To investigate this point, we define the feature vector 0 and score 4) of the 2A reduce action has an additional effect of fixing the boundary of the top word on the stack if the last action was A or SHM. step 8 1 1 1 1 1 1 1 1 3 3 5 1 1 1 5 7 1 1 1 1 1 3 3 1 1 1 step 3 1 1 1 3 step 4 1 1 1 3 1 1 step 5 1 1 1 3 5 1 1 1 1 ste</context>
<context position="23661" citStr="Zhang and Clark, 2010" startWordPosition="4102" endWordPosition="4105">w q0.t SH(t), s0.w q0.t SH(t), s0.t SH(t), s1.t o s0.t SH(t), s0.w SH(t), s1.t s1.lc.t s0.w SH(t), s0.t s0.rc.t SH(t), s1.t s0.w s0.lc.t SH(t), s2.t s1.t s0.t -is SH(t), RR, RL 1049 96 94 92 90 0 10 20 30 40 50 60 70 80 Seg (σ_p=0.1) Seg (σ_p=0.2) Seg (σ_p=0.5) Seg (σ_p=1.0) Tag (σ_p=0.1) Tag (σ_p=0.2) Tag (σ_p=0.5) Tag (σ_p=1.0) 76 74 72 70 68 66 64 62 60 0 10 20 30 40 50 60 70 80 Dep (σ_p=0.1) Dep (σ_p=0.2) Dep (σ_p=0.5) Dep (σ_p=1.0) Figure 2: F1 scores (in %) of SegTagDep on CTB5c-1 w.r.t. the training epoch (x-axis) and parsing feature weights (in legend). tagging (Zhang and Clark, 2008; Zhang and Clark, 2010) and dependency parsing (Huang and Sagae, 2010). Therefore, we can investigate the contribution of the joint approach through comparison with the pipeline and joint models. 88 86 described in the documentation, and CTB-7 is split according to Wang et al. (2011). The statistics of these splits are shown in Table 2. As external dictionaries, we use the HowNet Word List3, consisting of 91,015 words, and page names from the Chinese Wikipedia4 as of Oct 26, 2011, consisting of 709,352 words. These dictionaries only consist of word forms with no frequency or POS information. We use standard measures</context>
<context position="24927" citStr="Zhang and Clark (2010)" startWordPosition="4309" endWordPosition="4312">re, for evaluating each task. The output of dependencies cannot be correct unless the syntactic head and dependent of the dependency relation are both segmented correctly. Following the standard setting in dependency parsing works, we evaluate the task of dependency parsing with the unlabeled attachment scores excluding punctuations. Statistical significance is tested by McNemar’s test (t : p &lt; 0.05, t : p &lt; 0.01). 4.2 Baseline and Proposed Models We use the following baseline and proposed models for evaluation. • SegTag: our reimplementation of the joint segmentation and POS tagging model by Zhang and Clark (2010). Table 5 shows that this reimplementation almost reproduces the accuracy of their implementation. We used the beam of 16, which they reported to achieve the best accuracies. • Dep’: the state-of-the-art dependency parser by Huang and Sagae (2010). We used our reimplementation, which is used in Hatori et al. (2011). • Dep: Dep’ without look-ahead features. • TagDep: the joint POS tagging and dependency parsing model (Hatori et al., 2011), where the look-ahead features are omitted.5 • SegTag+Dep/SegTag+Dep’: a pipeline combination of SegTag and Dep or Dep’. • SegTag+TagDep: a pipeline combinati</context>
<context position="32443" citStr="Zhang and Clark (2010)" startWordPosition="5502" endWordPosition="5505">) 67.40 (+1.96$) 75.97 (+2.44$) Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over five trials on CTB-5c. Figures in parentheses show the differences over SegTag+Dep (� : p &lt; 0.01). for greater improvement, especially for tagging and parsing accuracies, when a larger beam can be used. 4.5 Comparison with Other Systems Table 5 and Table 6 show a comparison of the segmentation and POS tagging accuracies with other state-of-the-art models. “Kruengkrai+ ’09” is a lattice-based model by Kruengkrai et al. (2009). “Zhang ’10” is the incremental model by Zhang and Clark (2010). These two systems use no external resources other than the CTB corpora. “Sun+ ’11” is a CRF-based model (Sun, 2011) that uses a combination of several models, with a dictionary of idioms. “Wang+ ’11” is a semi-supervised model by Wang et al. (2011), which additionally uses the Chinese Gigaword Corpus. Our models with dictionaries (those marked with ‘(d)’) have competitive accuracies to other state-ofthe-art systems, and SegTagDep(d) achieved the best reported segmentation and POS tagging accuracies, using no additional corpora other than the dictionaries. Particularly, the POS tagging accura</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</booktitle>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (short papers).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>