<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9976325">
SemantiKLUE: Semantic Textual Similarity
with Maximum Weight Matching
</title>
<author confidence="0.492027">
Nataliia Plotnikova and Gabriella Lapesa and Thomas Proisl and Stefan Evert
</author>
<affiliation confidence="0.390697">
Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg (FAU)
Professur f¨ur Korpuslinguistik
</affiliation>
<address confidence="0.821337">
Bismarckstr. 6, 91054 Erlangen, Germany
</address>
<email confidence="0.997473">
{nataliia.plotnikova,gabriella.lapesa,thomas.proisl,stefan.evert}@fau.de
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999364157894737">
This paper describes the SemantiKLUE sys-
tem (Proisl et al., 2014) used for the SemEval-
2015 shared task on Semantic Textual Simi-
larity (STS) for English. The system was de-
veloped for SemEval-2013 and extended for
SemEval-2014, where it participated in three
tasks and ranked 13th out of 38 submissions
for the English STS task. While this year’s
submission ranks 46th out of 73, further ex-
periments on the selection of training data
led to notable improvements showing that the
system could have achieved rank 22 out of
73. We report a detailed analysis of those
training selection experiments in which we
tested different combinations of all the avail-
able STS datasets, as well as results of a qual-
itative analysis conducted on a sample of the
sentence pairs for which SemantiKLUE gave
wrong STS predictions.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919055555556">
The SemEval-2015 task on ”Semantic Textual Sim-
ilarity for English” (Agirre et al., 2015) is a rerun
of the corresponding task from SemEval-2014 with
new test data and updated categories. The predic-
tions of participating systems were evaluated against
manually annotated and subsequently filtered data.
STS was measured on a scale ranging from 0 (no
similarity at all) to 5 (total equivalence). SemantiK-
LUE, developed in 2014, uses a distributional bag-
of-words model as well as a word-to-word align-
ment for each pair of sentences based on a maximum
weight matching algorithm.
Our SemEval-2015 submission for all 5 test cat-
egories (headlines, images, belief, answers-forums,
answers-students) was based on the training data set
from 2014 with 2234 sentence pairs from 3 cate-
gories, namely paraphrase sentence pairs (MSRpar),
sentence pairs from video descriptions (MSRvid)
and MT evaluation sentence pairs (SMTeuroparl).
Follow up experiments conducted after the submis-
sion deadline showed us that this training config-
uration was far from optimal, and that our system
would have benefited a lot from a better training,
as we managed to significantly improve the overall
scores. With the best training configuration, Seman-
tiKLUE would have ranked 22nd out of 73 submis-
sions (11th out of 28 teams), with a weighted mean
of Pearson correlation coefficients over all test cate-
gories of 0.7508 (best system: 0.8015)
In the following sections, we first give a short
overview of the system (Section 2), and then we de-
scribe the follow-up experiments that allowed us to
define the best training data set in terms of its sub-
sets (Section 3); finally, we present the results of a
qualitative analysis of the performance of our sys-
tem (Section 4).
</bodyText>
<sectionHeader confidence="0.993886" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.99899775">
SemantiKLUE combines supervised and unsuper-
vised approaches for the computation of textual sim-
ilarity: a number of similarity measures are com-
puted and passed to a support vector regression
learner, which is trained on the available training
data and test sets of previous years. The learnt
weights are then used to generate semantic similar-
ity scores for the test data in the desired range.
</bodyText>
<page confidence="0.983376">
111
</page>
<note confidence="0.9693075">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 111–116,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.985011">
2.1 Training Data and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999968642857143">
The system was trained on manually annotated sen-
tence pairs from the STS task at SemEval 2014.
All sentence pairs were preprocessed with Stan-
ford CoreNLP1 for part-of-speech annotation and
lemmatization. Each sentence was represented as a
graph using the CCprocessed variant of the Stanford
Dependencies (collapsed dependencies with propa-
gation of conjunct dependencies) implemented with
the NetworkX2 module. This graph representation
was involved in the computation of all 39 similar-
ity measures for words and tokens in each sentence.
Prepositions, articles, conjunctions as well as auxil-
iary verbs like be and have were ignored in the com-
putation of token-based measures.
</bodyText>
<subsectionHeader confidence="0.981874">
2.2 Similarity Measures: Overview
</subsectionHeader>
<bodyText confidence="0.9998775">
A detailed description of all 39 similarity measures
used as features in SemantiKLUE is provided in
Proisl et al., 2014 (Sections 2.2 - 2.7). Similarity
measures used by our system include:
</bodyText>
<listItem confidence="0.968129272727273">
• Heuristic similarity measures: word form over-
lap and lemma overlap between two texts com-
puted with Jaccard coefficient; difference in text
length used by Gale and Church (1993); a binary
feature to treat negation in each sentence pair.
• Document similarity measures based on two
distributional models: a model based on non-
lemmatized information, built from the second re-
lease of the Google Books N-Grams database (Lin
et al., 2012); a lemmatized model, built from a 10-
billion word Web corpus3.
• Alignment-based measures: one-to-one align-
ment and one-to-many alignment for both words
and lemmata, computed via maximum weight
matching, based on cosine similarity between two
words in paired sentences as edge weight. Figure
1 visualizes a one-to-many alignment based on
lemmatized data. The colors of the connections
correspond to different cosine ranges, reported in
the legend to the right of the plot.
• WordNet-based similarity measures: Leacock
and Chodorow’s (1998) normalized path length
</listItem>
<footnote confidence="0.9967178">
1http://nlp.stanford.edu/software/corenlp.shtml
2http://networkx.github.com
3Wackypedia and UkWaC (Baroni et al., 2009), UMBC
WebBase (Han et al., 2013), and UKCOW 2012 (Sch¨afer and
Bildhauer, 2012).
</footnote>
<figureCaption confidence="0.996885">
Figure 1: One-to-many alignment plot. Sentences: “A
black and white dog is jumping into the water” , “A white
dog runs across the water”; Subset: Images; Gold Score:
2.8; SemantiKLUE score: 2.93.
</figureCaption>
<bodyText confidence="0.99837025">
and Lin’s (1998) universal similarity measure.
Using these similarity measures, the best one-to-
one and the best one-to-many alignment are com-
puted. After that, the arithmetic mean of the simi-
larities between the aligned words from text A and
text B with and without identical word pairs is cal-
culated. An additional WordNet-based feature is
the number of unknown words in both texts.
</bodyText>
<listItem confidence="0.954279076923077">
• Dependency-based heuristic measures: overlap
of dependency relation labels between the two
texts; arithmetic mean of the similarities between
the best aligned one-to-one dependency relations
based on Leacock and Chodorow’s normalized
path lengths; average overlap of neighbors for all
aligned word pairs based on one-to-one alignment
created with similarity scores from the lemma-
based DSM.
• Experimental features: cosine similarities for
each pair of sentences; average neighbor rank
based on the rank of text A among the nearest
neighbors from text B and vice versa.
</listItem>
<bodyText confidence="0.999148111111111">
The feature set described above was processed
by the support vector regressor implemented in the
scikit-learn4 (Pedregosa et al., 2011) library. All
the experiments presented in this paper rely on the
best support vector setting identified by Proisl et al.
(2014), namely: RBF kernel of degree 2 and penalty
C = 0.7. In what follows, we describe the proce-
dures adopted to adjust training data and find the
best training configurations.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9901265">
This section describes all post-hoc experiments on
the STS 2015 test data performed to improve the
</bodyText>
<footnote confidence="0.98101">
4http://scikit-learn.org/
</footnote>
<page confidence="0.997831">
112
</page>
<bodyText confidence="0.973139333333333">
predictions of the system. The abbreviations used
in the following tables reporting experiment results
are listed in Table 1.
</bodyText>
<table confidence="0.844092857142857">
short full name source
mp MSRpar5 train 2014
mv MSRvid6 train 2014
smt SMTeuroparl7 train 2014
img images8 test 2014
hl headlines9 test 2014
ow OnWN10 test 2014
df deft-forum11 test 2014
dn deft-news12 test 2014
tn tweet-news test 2014
fn FNWN 13 test 2013
ans-f answers-forums test 2015
ans-s answers-students test 2015
head headlines test 2015
</table>
<tableCaption confidence="0.999979">
Table 1: Training set categories: abbreviations.
</tableCaption>
<bodyText confidence="0.99967547826087">
All 39 similarity measures were used by the re-
gression learner to train the system. SemantiKLUE
was tested on different training data with various
combinations of training and test sets from 2013 and
2014. Results for the submitted system are typeset
in italics in Table 2, the best results in each column
are typeset in bold font.
The best results would have been obtained by
training on the MSR data from SemEval 2014 for
all test sets. Considerable improvements can be
achieved removing the SMTeuroparl category from
the training set. This category consists of MT pairs
of sentences whose exclusion would have given the
system rank 37 (weighted mean of .7148) instead of
46 (.6717) out of 73 submissions.
We turned the test data from SemEval 2014 into a
training set for the 2015 test data (see Table 3). The
figures in Table 3 show that training sets for images
and headlines perform best with the corresponding
categories of the test set (images and headlines) from
SemEval 2014.
STS results appear to be extremely sensitive to the
choice of the training dataset. For this reason, we
</bodyText>
<footnote confidence="0.999246888888889">
5Microsoft Research Paraphrase Corpus.
6Microsoft Research Video Description Corpus.
7WMT2008 development dataset.
8Image descriptions from the Flickr dataset.
9Headlines mined from news sources.
10Sense definitions from OntoNotes and WordNet .
11Forum posts.
12News summaries.
13Sense definitions from FrameNet and WordNet
</footnote>
<table confidence="0.99942">
ans-f ans-s head belief images mean
-.2533 .5944 .4515 .3102 .6497 .4310
.3262 .5990 .6044 .5021 .7879 .6014
.2603 .5263 .4073 .3177 .4715 .4235
.5509 .7259 .7009 .6961 .8088 .7148
.4891 .6849 .6822 .5658 .7991 .6734
-.0893 .4989 .2947 .1296 .3781 .2980
.4913 .7005 .6681 .5617 .7915 .6717
</table>
<tableCaption confidence="0.986087">
Table 2: Evaluation results for different training sets from
</tableCaption>
<table confidence="0.998084375">
2014.
ans-f ans-s head belief images mean
img .2673 .6549 .6574 .5669 .8180 .6367
hl .5760 .6760 .7734 .6439 .7249 .6960
ow .3446 .6661 .5960 .5386 .7334 .6093
df .3743 .5884 .5618 .6023 .5818 .5551
dn .2620 .6746 .5765 .5804 .7246 .5992
tn .6484 .6134 .6968 .6858 .7018 .6698
</table>
<tableCaption confidence="0.981689">
Table 3: Evaluation results for different training sets
based on the 2014 test categories.
</tableCaption>
<bodyText confidence="0.9858536">
conducted more fine-grained experiments to look for
the best combination of training data for the 2015
test sets. We combined training and test data of
SemEval 2014 with the best training categories of
SemEval 2013 (see Table 4) to test the performance
of the system on the optimal training subset defined
for SemEval 201414. That optimal training configu-
ration consists of the FNWN, headlines, MSR and
OnWN data sets: the corresponding performance
is typeset in italics. Comparable or even better re-
sults can be achieved with a combination of test and
train categories of SemEval 2014 only. Thus, com-
bining the training category MSR (mp + mv) with
another test category of 2014 (such as tweets or
headlines) results in about 1.5%-2% improvement.
A more precise investigation helped us to find the
best test combination with MSR, headlines, images,
and tweet-news categories. This brought our sys-
tem to the weighted mean of .7508, corresponding
to the 11th place out of 28 teams. We tried to fur-
ther improve these results, by adding the optimal
categories for training found in 2014 and extended
the best training set defined for 2015 with FNWN
(mp+mv+hl+img+tn+fn), but this led to slightly
worse results in all test categories.
A further set of experiments was aimed at testing
different subsets of similarity measures used at the
14For space reasons we list only the combinations resulting in
the best scores. Combinations with SMTeuroparl, for example,
led to consistently worse results and are therefore left out.
</bodyText>
<equation confidence="0.701202">
mp
mv
smt
mp + mv
mv + smt
smt + mp
mp + mv + smt
</equation>
<page confidence="0.995465">
113
</page>
<table confidence="0.9966275">
answers-forums answers-students headlines belief images mean
.5119 .6995 .7663 .6296 .8262 .7157
.6158 .6949 .7354 .6982 .8187 .7265
.6313 .6625 .7736 .6887 .7350 .7078
.6460 .7213 .7462 .7118 .8136 .7400
.6223 .7028 .7682 .7004 .8247 .7392
.4853 .7297 .7110 .6596 .8302 .7108
.6246 .7336 .7766 .7057 .8210 .7491
.5426 .7335 .7775 .6664 .8147 .7326
.6458 .6961 .7734 .7106 .8180 .7414
.6319 .7292 .7434 .7076 .8269 .7423
.5891 .7212 .7459 .6895 .8087 .7288
.3337 .6693 .4005 .5791 .7756 .5755
.5906 .7225 .7600 .6762 .8135 .7324
.6341 .7325 .7686 .7067 .8315 .7508
.5931 .7313 .7684 .6869 .8291 .7422
</table>
<figure confidence="0.533744866666667">
img+hl
tn+img
tn+hl
tn+mp+mv
tn+hl+img
mp+mv+img
mp+mv+hl
mp+mv+hl+fn
mp+mv+tn+hl
mp+mv+tn+img
mp+mv+tn+fn
mp+mv+img+fn
mp+mv+ow+fn+hl
mp+mv+hl+img+tn
mp+mv+hl+img+tn+fn
</figure>
<tableCaption confidence="0.998719">
Table 4: Evaluation results for different training sets based on train and test categories of 2014 and 2013.
</tableCaption>
<table confidence="0.9864032">
answers-forums answers-students headlines belief images mean
.5377 .6483 .6393 .6663 .6608 .6375
.3930 .6566 .5744 .5449 .5901 .5725
.6423 .6484 .6610 .7075 .7774 .6904
.6043 .6749 .6082 .6777 .7469 .6677
</table>
<construct confidence="0.94857375">
token (one to one)
token (one to many)
lemma (one to one)
lemma (one to many)
</construct>
<tableCaption confidence="0.9905">
Table 5: Single-feature experiments with different alignments: correlation based on cosine similarity.
</tableCaption>
<bodyText confidence="0.856877">
img hl ow df dn tn
img .8689 .6141 .6767 .3363 .4479 .5183
hl .7249 .8173 .6754 .4179 .6028 .6763
ow .7039 .5707 .8926 .3790 .5666 .5760
df .5497 .4931 .5969 .7818 .5193 .4836
dn .6957 .5582 .6428 .4008 .8588 .3935
tn .6823 .6453 .6321 .3816 .5222 .8697
</bodyText>
<tableCaption confidence="0.981876">
Table 6: Test data categories of 2014 against each other
(columns = training sets, lines = test sets).
</tableCaption>
<bodyText confidence="0.999875888888888">
machine learning stage. Results showed that the use
of fewer similarity features (exclusion of all identi-
cal words in each pair of sentences from the calcu-
lation of similarity scores) resulted in worse perfor-
mance of the whole system.
Our system is based on a relatively large feature
set, but we were also interested in discovering how
well SemantiKLUE would have performed if trained
on a single feature. We tested a feature based on co-
sine similarity between the two centroid vectors as
a measure of semantic similarity for each sentence
pair as suggested by Sch¨utze (1998) using either to-
kens or lemmas (see Table 5). We selected cosine
between centroid vectors as a candidate feature, be-
cause it is most intuitive and naturally connects to
the representation of topical information, crucial in
capturing textual similarity.
We found that regardless of the alignment (one
to one or one to many both for lemma and to-
kens), the weighted mean of Pearson correlation
coefficients is low (.6904 for the one-to-one align-
ment) for the cosine similarity value calculated with
lemma based centroid vectors, but still higher than
what is achieved by the more complex system with
a large set of features with a poor training set (.6717)
in the submission with mp+mv+smt used for the
training set (see Table 4 for comparison).
As we were interested in identifying the most bal-
anced training sets in the test categories of 2014, we
tested all categories against each other. Results are
shown in Table 6: the rows of the table correspond
to test subsets, while columns represent training sets.
The results typeset in italics show that there is a high
level of overtraining for the cases in which training
and test data are identical. The most balanced and
robust test data are those of the image and OnWN
categories: they can be used as training data for fu-
ture experiments.
To sum up, our results show that the best train-
ing configuration for SemEval 2015 involves MSR,
headlines, images, and tweet-news categories (see
Table 4). The scatter plots in Figures 2 to 4 relate
the similarity score in the gold standard (x-axis) to
the relatedness score produced by SemantiKLUE (y-
axis) in its best training configuration, for three of
</bodyText>
<page confidence="0.979429">
114
</page>
<figure confidence="0.973882103448276">
●
●
●
●
● ●
● ●
●
●
0 1 2 3 4 5
●
●
●
●
●
●
0 1 2 3 4
●
●
●
● ●● ● ●
● ●●
●
● ● ●
●
● ● ● ● ●
● ●
● ●
●
●
</figure>
<equation confidence="0.840390405405406">
● ● ● ●
● ● ●
● ● ●
● ●
● ● ●
● ●
● ● ● ● ● ● ●
● ●
● ●
● ● ●
● ● ● ● ● ● ●
● ● ● ●
● ● ● ● ● ● ● ● ● ● ●
● ● ●
● ●
● ● ● ●
● ● ●
● ● ●
● ● ● ● ●
● ● ●
● ● ●●
● ●
● ● ●●
● ● ● ● ● ● ● ●
● ● ●
● ● ● ● ●
● ● ●
● ● ●
● ● ●
● ●
● ● ● ●●
● ● ● ●
●●
● ● ●●
● ●● ●●
● ● ●
● ● ●
</equation>
<figure confidence="0.93800504285714">
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
0 1 2 3 4
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ● ● ●
● ● ●
● ●
● ● ● ●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
● ● ●
● ● ●
● ● ● ● ● ● ●
● ● ●
●
● ● ● ● ● ●
● ● ● ● ● ●
● ● ● ● ● ●
● ● ● ●
● ●
● ● ●
● ●
● ●
● ●
● ● ● ●
● ● ● ● ●
● ● ● ●
● ● ●
● ● ●
● ● ● ●
● ●
●
●
● ● ● ●
● ● ● ●
● ●
● ●●
● ● ●●
● ● ● ● ● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
● ●● ●
● ●
● ●
● ●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
● ● ●
● ●
●
●●●
● ● ● ● ● ● ●●
● ●
● ● ●
●
●
● ●
● ●
● ● ●
● ● ● ●
● ● ●
●
●●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
● ● ● ●
● ● ● ●
● ● ●
● ●
● ●
● ● ● ● ●
● ● ● ● ●
● ●
● ●
● ●
● ● ●
● ●
●
●
●
●
●●
●
●
●●
●
● ●
● ●
● ●
●●
●
● ●
●
●
●
●
●
●
●
●●
●
●
●
●
● ● ● ● ●●●
● ● ● ●
● ● ●
● ●●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
0 1 2 3 4 0 1 2 3 4 5 0 1 2 3 4 5
0 1 2 3 4
0 1 2 3 4 5
0 1 2 3 4 5
</figure>
<figureCaption confidence="0.99963025">
Figure 2: Answers Forums.
Figure 3: Belief.
Figure 4: Images.
Figure 2: Answers Forums. Figure 3: Belief. Figure 4: Images.
</figureCaption>
<bodyText confidence="0.997624454545455">
with negative difference (overestimation)16.
Let us start with the cases in which SemantiK-
LUE overestimated STS. We list the identified mis-
take categories, providing a short description for the
cases in which the label is not self-explanatory, and
report the percentage of affected sentences. Each
item can be affected by more than one mistake type.
• One or two words (often very frequent and with
generic meaning) dominate the alignment, or
one sentence is practically a subset of the other:
56% of the items.
</bodyText>
<listItem confidence="0.992073">
• Wrong alignments: 25% of the items.
• Modification: presence of identical modifiers
</listItem>
<bodyText confidence="0.985957714285714">
with different heads boosts overall similarity. This
mistake type affects 7% of the cases.
• Same frame, different participants: the sen-
tences depict the same event, but the participants
(or the background) determine a significant differ-
ence in meaning that our system fails to capture.
This problem affects 8% of the items.
</bodyText>
<listItem confidence="0.990262666666667">
• Same participants, different frames: 11% of
the items.
• Negation: 10% of the items.
• (Near) Antonyms: 8% of the items.
• Proper Names: 18% of the items.
• Amounts: when building the alignment, Seman-
</listItem>
<bodyText confidence="0.997476">
tiKLUE ignores numerical values, which are in
some cases crucial in determining (dis)similarities
between sentences otherwise near identical (e.g.,
“2 people killed..” vs. “100 people killed”). This
problem affects 18% of the items.
the five Semeval 2015 test sets. For each plot we
show the regression line (drawn in red) as well as a
smoother, drawn (in blue) with the LOWESS func-
tion from R15. Smoothed lines show different non-
linear patterns for the different subsets.
</bodyText>
<sectionHeader confidence="0.993901" genericHeader="method">
4 Qualitative Analysis
</sectionHeader>
<bodyText confidence="0.9998674">
In this section, we report the results of a qualita-
tive analysis conducted on sentence pairs for which
SemantiKLUE, in the optimal training configuration
identified in Section 2.2, made wrong predictions.
Our goal was to identify a taxonomy of Seman-
tiKLUE’s problems. Broadly speaking, there are
two possibilities for SemantiKLUE to make a wrong
similarity guess: the system can overestimate the
similarity between the two sentences - thus gener-
ating a relatedness score higher than the speakers’
judgments - or it can underestimate similarity -
generating a score lower than the gold standard. In
the process of interpretation/classification, we relied
on the inspection of alignment plots (cf. Figure 1)
and on our knowledge of the dynamics of the fea-
tures within SemantiKLUE.
The analysis was conducted manually on a se-
lected sample of sentence pairs from the test data.
We selected sentences for which the absolute dif-
ference between the similarity score in the gold
standard and the relatedness score produced by Se-
mantiKLUE was between 1.5 and 2.5 points. That
range was identified by inspecting the distribution
of gold standard/relatedness score differences in the
five subsets (corresponding plots are not shown here
for reasons of space). Within this range, we ran-
domly picked 40 items (sentence pairs) per subset,
20 with positive difference (underestimation), 20
We now proceed to cases of underestimation, for
which we identified the following mistake types:
</bodyText>
<footnote confidence="0.66320175">
16In two cases, we had to enlarge the range to ensure that
at least 20 items would have been selected: belief/positive, be-
tween 1.4 and 3.5; answers-forums/negative, between 1 and 2.5.
15http://www.r-project.org/
</footnote>
<page confidence="0.979644">
115
</page>
<listItem confidence="0.6080685">
• Collocations (e.g, “heads up”, “make sense”)
negatively affect the alignment process: Seman-
</listItem>
<bodyText confidence="0.9496175">
tiKLUE would have performed better if multi-
words had entered the alignment process as a
whole, and not as individual edges. This mistake
type affects 10% of the items.
</bodyText>
<listItem confidence="0.998970111111111">
• Crucial alignments missing or weaker than ex-
pected: 17% of the items.
• The similarity between the sentences is due to
logical form, compositionality or world knowl-
edge. This problem affects 16% of the items.
• Different register makes alignment problematic,
even if the sentences are content-wise similar:
12% of the items.
• Displacement of different pieces of information
</listItem>
<bodyText confidence="0.546927">
between two sentences otherwise centered on
the same topic makes them less similar for Se-
mantiKLUE then for the raters: 28% of the items.
</bodyText>
<listItem confidence="0.960694666666667">
• Spelling mistakes prevent otherwise straightfor-
ward alignments: 10% of the items.
• Difficult cases, for which the alignment would
simply suggest a score higher than the one pre-
dicted by the regressor. Such cases, (15%), re-
quire further investigation.
</listItem>
<sectionHeader confidence="0.998852" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999987333333334">
In this paper, we presented the results of our evalua-
tion experiments on the performance of the Seman-
tiKLUE system (Proisl et al., 2014) on the SemEval-
2015 STS task. Our experiments showed that the
performance of our system is heavily dependent on
the choice of the training set, as we managed to sig-
nificantly improve the performance of our system
with respect to the original submission. The qual-
itative evaluation sketched in Section 4 provided in-
teresting insights into specific features of the STS
data and it allowed us to identify some idiosyncra-
cies (e.g., the behavior of the system in case of align-
ment of identical words) and weaknesses (e.g., the
treatment of multiwords in the process of alignment)
that we are already working on improving.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999950614035088">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mi-
halcea, German Rigau, Larraitz Uria, Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Sim-
ilarity, English, Spanish and Pilot on Interpretability.
In Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015). Denver, CO.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: A
collection of very large linguistically processed Web-
crawled corpora. Language Resources and Evaluation,
43(3):209–226.
William A. Gale and Kenneth W. Church. 1993. A pro-
gram for aligning sentences in bilingual corpora. Com-
putational Linguistics, 19(1):75–102.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013.
UMBC EBIQUITY-CORE: Semantic textual sim-
ilarity systems. In Proceedings of the 2nd Joint
Conference on Lexical and Computational Semantics.
Atlanta, GA.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for word
sense identification. In Christiane Fellbaum, editor,
WordNet: An Electronic Lexical Database, pages 265–
283. MIT Press, Cambridge, MA.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296–
304. San Francisco, CA.
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the Google Books Ngram
Corpus. In Proceedings of the ACL 2012 Syst. Demon-
strations, pages 169–174, Jeju Island, Korea.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and Douard Duchesnay. 2011. Scikit-learn: Ma-
chine learning in Python. Journal ofMachine Learning
Research, 12:2825–2830.
Thomas Proisl, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 532–540. Dublin, Ireland.
Roland Sch¨afer and Felix Bildhauer. 2012. Build-
ing large corpora from the web using a new efficient
tool chain. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC ’12), pages 486–493. Istanbul, Turkey.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–123.
</reference>
<page confidence="0.999026">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.190693">
<title confidence="0.92435425">SemantiKLUE: Semantic Textual with Maximum Weight Matching Plotnikova Lapesa Proisl Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg</title>
<note confidence="0.4881605">Professur f¨ur Bismarckstr. 6, 91054 Erlangen,</note>
<abstract confidence="0.9962583">This paper describes the SemantiKLUE system (Proisl et al., 2014) used for the SemEval- 2015 shared task on Semantic Textual Similarity (STS) for English. The system was developed for SemEval-2013 and extended for SemEval-2014, where it participated in three tasks and ranked 13th out of 38 submissions for the English STS task. While this year’s submission ranks 46th out of 73, further experiments on the selection of training data led to notable improvements showing that the system could have achieved rank 22 out of 73. We report a detailed analysis of those training selection experiments in which we tested different combinations of all the available STS datasets, as well as results of a qualitative analysis conducted on a sample of the sentence pairs for which SemantiKLUE gave wrong STS predictions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Mona Diab Cer</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar,</title>
<location>Rada Mihalcea, German Rigau, Larraitz Uria, Janyce Wiebe.</location>
<marker>Cer, </marker>
<rawString>Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, Janyce Wiebe.</rawString>
</citation>
<citation valid="true">
<title>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</booktitle>
<location>Denver, CO.</location>
<marker>2015</marker>
<rawString>2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A collection of very large linguistically processed Webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="5570" citStr="Baroni et al., 2009" startWordPosition="843" endWordPosition="846"> • Alignment-based measures: one-to-one alignment and one-to-many alignment for both words and lemmata, computed via maximum weight matching, based on cosine similarity between two words in paired sentences as edge weight. Figure 1 visualizes a one-to-many alignment based on lemmatized data. The colors of the connections correspond to different cosine ranges, reported in the legend to the right of the plot. • WordNet-based similarity measures: Leacock and Chodorow’s (1998) normalized path length 1http://nlp.stanford.edu/software/corenlp.shtml 2http://networkx.github.com 3Wackypedia and UkWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Sch¨afer and Bildhauer, 2012). Figure 1: One-to-many alignment plot. Sentences: “A black and white dog is jumping into the water” , “A white dog runs across the water”; Subset: Images; Gold Score: 2.8; SemantiKLUE score: 2.93. and Lin’s (1998) universal similarity measure. Using these similarity measures, the best one-toone and the best one-to-many alignment are computed. After that, the arithmetic mean of the similarities between the aligned words from text A and text B with and without identical word pairs is calculated. An additional WordNe</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed Webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="4630" citStr="Gale and Church (1993)" startWordPosition="708" endWordPosition="711"> computation of all 39 similarity measures for words and tokens in each sentence. Prepositions, articles, conjunctions as well as auxiliary verbs like be and have were ignored in the computation of token-based measures. 2.2 Similarity Measures: Overview A detailed description of all 39 similarity measures used as features in SemantiKLUE is provided in Proisl et al., 2014 (Sections 2.2 - 2.7). Similarity measures used by our system include: • Heuristic similarity measures: word form overlap and lemma overlap between two texts computed with Jaccard coefficient; difference in text length used by Gale and Church (1993); a binary feature to treat negation in each sentence pair. • Document similarity measures based on two distributional models: a model based on nonlemmatized information, built from the second release of the Google Books N-Grams database (Lin et al., 2012); a lemmatized model, built from a 10- billion word Web corpus3. • Alignment-based measures: one-to-one alignment and one-to-many alignment for both words and lemmata, computed via maximum weight matching, based on cosine similarity between two words in paired sentences as edge weight. Figure 1 visualizes a one-to-many alignment based on lemm</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>William A. Gale and Kenneth W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Johnathan Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic textual similarity systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics.</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="5603" citStr="Han et al., 2013" startWordPosition="849" endWordPosition="852">one alignment and one-to-many alignment for both words and lemmata, computed via maximum weight matching, based on cosine similarity between two words in paired sentences as edge weight. Figure 1 visualizes a one-to-many alignment based on lemmatized data. The colors of the connections correspond to different cosine ranges, reported in the legend to the right of the plot. • WordNet-based similarity measures: Leacock and Chodorow’s (1998) normalized path length 1http://nlp.stanford.edu/software/corenlp.shtml 2http://networkx.github.com 3Wackypedia and UkWaC (Baroni et al., 2009), UMBC WebBase (Han et al., 2013), and UKCOW 2012 (Sch¨afer and Bildhauer, 2012). Figure 1: One-to-many alignment plot. Sentences: “A black and white dog is jumping into the water” , “A white dog runs across the water”; Subset: Images; Gold Score: 2.8; SemantiKLUE score: 2.93. and Lin’s (1998) universal similarity measure. Using these similarity measures, the best one-toone and the best one-to-many alignment are computed. After that, the arithmetic mean of the similarities between the aligned words from text A and text B with and without identical word pairs is calculated. An additional WordNet-based feature is the number of </context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Johnathan Weese. 2013. UMBC EBIQUITY-CORE: Semantic textual similarity systems. In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics. Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database,</booktitle>
<pages>265--283</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 265– 283. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<location>San Francisco, CA.</location>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning, pages 296– 304. San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Lin</author>
<author>Jean-Baptiste Michel</author>
<author>Erez Lieberman Aiden</author>
<author>Jon Orwant</author>
<author>Will Brockman</author>
<author>Slav Petrov</author>
</authors>
<title>Syntactic annotations for the Google Books Ngram Corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 Syst. Demonstrations,</booktitle>
<pages>169--174</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4886" citStr="Lin et al., 2012" startWordPosition="750" endWordPosition="753">led description of all 39 similarity measures used as features in SemantiKLUE is provided in Proisl et al., 2014 (Sections 2.2 - 2.7). Similarity measures used by our system include: • Heuristic similarity measures: word form overlap and lemma overlap between two texts computed with Jaccard coefficient; difference in text length used by Gale and Church (1993); a binary feature to treat negation in each sentence pair. • Document similarity measures based on two distributional models: a model based on nonlemmatized information, built from the second release of the Google Books N-Grams database (Lin et al., 2012); a lemmatized model, built from a 10- billion word Web corpus3. • Alignment-based measures: one-to-one alignment and one-to-many alignment for both words and lemmata, computed via maximum weight matching, based on cosine similarity between two words in paired sentences as edge weight. Figure 1 visualizes a one-to-many alignment based on lemmatized data. The colors of the connections correspond to different cosine ranges, reported in the legend to the right of the plot. • WordNet-based similarity measures: Leacock and Chodorow’s (1998) normalized path length 1http://nlp.stanford.edu/software/c</context>
</contexts>
<marker>Lin, Michel, Aiden, Orwant, Brockman, Petrov, 2012</marker>
<rawString>Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden, Jon Orwant, Will Brockman, and Slav Petrov. 2012. Syntactic annotations for the Google Books Ngram Corpus. In Proceedings of the ACL 2012 Syst. Demonstrations, pages 169–174, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<date>2011</date>
<booktitle>Scikit-learn: Machine learning in Python. Journal ofMachine Learning Research,</booktitle>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Douard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal ofMachine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Proisl</author>
<author>Stefan Evert</author>
<author>Paul Greiner</author>
<author>Besim Kabashi</author>
</authors>
<title>SemantiKLUE: Robust semantic similarity at multiple levels using maximum weight matching.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>532--540</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="4381" citStr="Proisl et al., 2014" startWordPosition="668" endWordPosition="671">sentence was represented as a graph using the CCprocessed variant of the Stanford Dependencies (collapsed dependencies with propagation of conjunct dependencies) implemented with the NetworkX2 module. This graph representation was involved in the computation of all 39 similarity measures for words and tokens in each sentence. Prepositions, articles, conjunctions as well as auxiliary verbs like be and have were ignored in the computation of token-based measures. 2.2 Similarity Measures: Overview A detailed description of all 39 similarity measures used as features in SemantiKLUE is provided in Proisl et al., 2014 (Sections 2.2 - 2.7). Similarity measures used by our system include: • Heuristic similarity measures: word form overlap and lemma overlap between two texts computed with Jaccard coefficient; difference in text length used by Gale and Church (1993); a binary feature to treat negation in each sentence pair. • Document similarity measures based on two distributional models: a model based on nonlemmatized information, built from the second release of the Google Books N-Grams database (Lin et al., 2012); a lemmatized model, built from a 10- billion word Web corpus3. • Alignment-based measures: on</context>
<context position="7066" citStr="Proisl et al. (2014)" startWordPosition="1077" endWordPosition="1080">Chodorow’s normalized path lengths; average overlap of neighbors for all aligned word pairs based on one-to-one alignment created with similarity scores from the lemmabased DSM. • Experimental features: cosine similarities for each pair of sentences; average neighbor rank based on the rank of text A among the nearest neighbors from text B and vice versa. The feature set described above was processed by the support vector regressor implemented in the scikit-learn4 (Pedregosa et al., 2011) library. All the experiments presented in this paper rely on the best support vector setting identified by Proisl et al. (2014), namely: RBF kernel of degree 2 and penalty C = 0.7. In what follows, we describe the procedures adopted to adjust training data and find the best training configurations. 3 Experiments This section describes all post-hoc experiments on the STS 2015 test data performed to improve the 4http://scikit-learn.org/ 112 predictions of the system. The abbreviations used in the following tables reporting experiment results are listed in Table 1. short full name source mp MSRpar5 train 2014 mv MSRvid6 train 2014 smt SMTeuroparl7 train 2014 img images8 test 2014 hl headlines9 test 2014 ow OnWN10 test 20</context>
<context position="21562" citStr="Proisl et al., 2014" startWordPosition="3921" endWordPosition="3924">e similar: 12% of the items. • Displacement of different pieces of information between two sentences otherwise centered on the same topic makes them less similar for SemantiKLUE then for the raters: 28% of the items. • Spelling mistakes prevent otherwise straightforward alignments: 10% of the items. • Difficult cases, for which the alignment would simply suggest a score higher than the one predicted by the regressor. Such cases, (15%), require further investigation. 5 Conclusion In this paper, we presented the results of our evaluation experiments on the performance of the SemantiKLUE system (Proisl et al., 2014) on the SemEval2015 STS task. Our experiments showed that the performance of our system is heavily dependent on the choice of the training set, as we managed to significantly improve the performance of our system with respect to the original submission. The qualitative evaluation sketched in Section 4 provided interesting insights into specific features of the STS data and it allowed us to identify some idiosyncracies (e.g., the behavior of the system in case of alignment of identical words) and weaknesses (e.g., the treatment of multiwords in the process of alignment) that we are already work</context>
</contexts>
<marker>Proisl, Evert, Greiner, Kabashi, 2014</marker>
<rawString>Thomas Proisl, Stefan Evert, Paul Greiner, and Besim Kabashi. 2014. SemantiKLUE: Robust semantic similarity at multiple levels using maximum weight matching. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 532–540. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Building large corpora from the web using a new efficient tool chain.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC ’12),</booktitle>
<pages>486--493</pages>
<location>Istanbul, Turkey.</location>
<marker>Sch¨afer, Bildhauer, 2012</marker>
<rawString>Roland Sch¨afer and Felix Bildhauer. 2012. Building large corpora from the web using a new efficient tool chain. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC ’12), pages 486–493. Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>