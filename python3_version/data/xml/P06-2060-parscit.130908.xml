<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003365">
<title confidence="0.9950755">
Minority Vote: At-Least-N Voting
Improves Recall for Extracting Relations
</title>
<author confidence="0.932549">
Nanda Kambhatla
</author>
<affiliation confidence="0.728232">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.9578075">
1101 Kitchawan Road Rt 134
Yorktown, NY 10598
</address>
<email confidence="0.998612">
nanda@us.ibm.com
</email>
<sectionHeader confidence="0.993878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999714625">
Several NLP tasks are characterized by
asymmetric data where one class label
NONE, signifying the absence of any
structure (named entity, coreference, re-
lation, etc.) dominates all other classes.
Classifiers built on such data typically
have a higher precision and a lower re-
call and tend to overproduce the NONE
class. We present a novel scheme for vot-
ing among a committee of classifiers that
can significantly boost the recall in such
situations. We demonstrate results show-
ing up to a 16% relative improvement in
ACE value for the 2004 ACE relation ex-
traction task for English, Arabic and Chi-
nese.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922255319149">
Statistical classifiers are widely used for diverse
NLP applications such as part of speech tagging
(Ratnaparkhi, 1999), chunking (Zhang et al., 2002),
semantic parsing (Magerman, 1993), named entity
extraction (Borthwick, 1999; Bikel et al., 1997; Flo-
rian et al., 2004), coreference resolution (Soon et al.,
2001), relation extraction (Kambhatla, 2004), etc. A
number of these applications are characterized by a
dominance of a NONE class in the training exam-
ples. For example, for coreference resolution, classi-
fiers might classify whether a given pair of mentions
are references to the same entity or not. In this case,
we typically have a lot more examples of mention
pairs that are not coreferential (i.e. the NONE class)
than otherwise. Similarly, if a classifier is predicting
the presence/absence of a semantic relation between
two mentions, there are typically far more examples
signifying an absence of a relation.
Classifiers built with asymmetric data dominated
by one class (a NONE class donating absence of a
relation or coreference or a named entity etc.) can
overgenerate the NONE class. This often results in a
unbalanced classifier where precision is higher than
recall.
In this paper, we present a novel approach for
improving the recall of such classifiers by using a
new voting scheme from a committee of classifiers.
There are a plethora of algorithms for combining
classifiers (e.g. see (Xu et al., 1992)). A widely
used approach is a majority voting scheme, where
each classifier in the committee gets a vote and the
class with the largest number of votes ’wins’ (i.e. the
corresponding class is output as the prediction of the
committee).
We are interested in improving overall recall and
reduce the overproduction of the class NONE. Our
scheme predicts the class label C obtaining the sec-
ond highest number of votes when NONE gets the
highest number of votes, provided C gets at least
N votes. Thus, we predict a label other than NONE
when there is some evidence of the presense of the
structure we are looking for (relations, coreference,
named entities, etc.) even in the absense of a clear
majority.
This paper is organized as follows. In section 2,
we give an overview of the various schemes for com-
bining classifiers. In section 3, we present our vot-
</bodyText>
<page confidence="0.987316">
460
</page>
<note confidence="0.7245985">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 460–466,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998137">
ing algorithm. In section 4, we describe the ACE
relation extraction task. In section 5, we present em-
pirical results for relation extraction and we discuss
our results and conclude in section 6.
</bodyText>
<sectionHeader confidence="0.965731" genericHeader="method">
2 Combining Classifiers
</sectionHeader>
<bodyText confidence="0.998939275862069">
Numerous methods for combining classifiers have
been proposed and utlized to improve the perfor-
mance of different NLP tasks such as part of speech
tagging (Brill and Wu, 1998), identifying base noun
phrases (Tjong Kim Sang et al., 2000), named en-
tity extraction (Florian et al., 2003), etc. Ho et al
(1994) investigated different approaches for rerank-
ing the outputs of a committee of classifiers and also
explored union and intersection methods for reduc-
ing the set of predicted categories. Florian et al
(2002) give a broad overview of methods for com-
bining classifiers and present empirical results for
word sense disambiguation.
Xu et al (1992) and Florian et al (2002) consider
three approaches for combining classifiers. In the
first approach, individual classifiers output posterior
probabilities that are merged (e.g. by taking an av-
erage) to arrive at a composite posterior probability
of each class. In the second scheme, each classifier
outputs a ranked list of classes instead of a proba-
bility distribution and the different ranked lists are
merged to arrive at a final ranking. Methods us-
ing the third approach, often called voting methods,
treat each classifier as a black box that outputs only
the top ranked class and combines these to arrive at
the final decision (class). The choice of approach
and the specific method of combination may be con-
strained by the specific classification algorithms in
use.
In this paper, we focus on voting methods, since
for small data sets, it is hard to reliably estimate
probability distributions or even a complete order-
ing of classes especially when the number of classes
is large.
A widely used voting method for combining clas-
sifiers is a Majority Vote scheme (e.g. (Brill and
Wu, 1998; Tjong Kim Sang et al., 2000)). Each
classifier gets to vote for its top ranked class and
the class with the highest number of votes ’wins’.
Henderson et al (1999) use a Majority Vote scheme
where different parsers vote on constituents’ mem-
bership in a hypothesized parse. Halteren et al
(1998) compare a number of voting methods includ-
ing a Majority Vote scheme with other combination
methods for part of speech tagging.
In this paper, we induce multiple classifiers by us-
ing bagging (Breiman, 1996). Following Breiman’s
approach, we obtain multiple classifiers by first
making bootstrap replicates of the training data and
training different classifiers on each of the replicates.
The bootstrap replicates are induced by repeatedly
sampling with replacement training events from the
original training data to arrive at replicate data sets
of the same size as the training data set. Breiman
(1996) uses a Majority Vote scheme for combining
the output of the classifiers. In the next section, we
will describe the different voting schemes we ex-
plored in our work.
</bodyText>
<sectionHeader confidence="0.99852" genericHeader="method">
3 At-Least-N Voting
</sectionHeader>
<bodyText confidence="0.998072964285714">
We are specifically interested in NLP tasks char-
acterized by asymmetric data where, typically, we
have far more occurances of a NONE class that sig-
inifies the absense of structure (e.g. a named en-
tity, or a coreference relation or a semantic relation).
Classifiers trained on such data sets can overgener-
ate the NONE class, and thus have a higher preci-
sion and lower recall in discovering the underlying
structure (i.e. the named entities or coreference links
etc.). With such tasks, the benefits yielded by a Ma-
jority Vote is limited, since, because of the asym-
metry in the data, a majority of the classifiers might
predict NONE most of the time.
We propose alternative voting schemes, dubbed
At-Least-N Voting, to deal with the overproduction
of NONE. Given a committee of classifiers (obtained
by bagging or some other mechanism), the classi-
fiers first cast their vote. If the majority vote is for a
class C other than NONE, we simply output C as the
prediction. If the majority vote is for NONE, we out-
put the class label obtaining the second highest num-
ber of votes, provided it has at least N votes. Thus,
we choose to defer to the minority vote of classifiers
which agree on finding some structure even when
the majority of classifiers vote for NONE. We expect
this voting scheme to increase recall at the expense
of precision.
At-Least-N Voting induces a spectrum of combi-
</bodyText>
<page confidence="0.999039">
461
</page>
<bodyText confidence="0.99995525">
nation methods ranging from a Majority Vote (when
N is more than half of the total number of classifiers)
to a scheme, where the evidence of any structure by
even one classifier is believed (At-Least-1 Voting).
The exact choice of N is an empirical one and de-
pends on the amount of asymmetry in the data and
the imbalance between precision and recall in the
classifiers.
</bodyText>
<sectionHeader confidence="0.991625" genericHeader="method">
4 The ACE Relation Extraction Task
</sectionHeader>
<bodyText confidence="0.999415">
Automatic Content Extraction (ACE) is an annual
evaluation conducted by NIST (NIST, 2004) on in-
formation extraction, focusing on extraction of en-
tities, events, and relations. The Entity Detection
and Recognition task entails detection of mentions
of entities and grouping together the mentions that
are references to the same entity. In ACE terminol-
ogy, mentions are references in text (or audio, chats,
...) to real world entities. Similarly relation men-
tions are references in text to semantic relations be-
tween entity mentions and relations group together
all relation mentions that identify the same semantic
relation between the same entities.
In the frament of text:
John’s son, Jim went for a walk. Jim liked
his father.
all the underlined words are mentions referring to
two entities, John, and Jim. Morover, John and
Jim have a family relation evidenced as two relation
mentions ”John’s son” between the entity mentions
”John” and ”son” and ”his father” between the entity
mentions ”his” and ”father”.
In the relation extraction task, systems must pre-
dict the presence of a predetermined set of binary
relations among mentions of entities, label the rela-
tion, and identify the two arguments. In the 2004
ACE evaluation, systems were evaluated on their ef-
ficacy in correctly identifying relations among both
system output entities and with ’true’ entities (i.e. as
annotated by human annotators as opposed to sys-
tem output). In this paper, we present results for ex-
tracting relations between ’true’ entities.
Table 1 shows the set of relation types, subtypes,
and their frequency counts in the training data for the
2004 ACE evaluation. For training classifiers, the
great paucity of positive training events (where rela-
tions exist) compared to the negative events (where
</bodyText>
<table confidence="0.9996965">
Type Subtype Count
ART user-or-owner 140
(agent artifact) inventor/manufacturer 3
other 6
EMP-ORG employ-executive 420
employ-staff 416
employ-undetermined 62
member-of-group 126
partner 11
subsidiary 213
other 37
GPE-AFF citizen-or-resident 173
(GPE affiliation) based-in 225
other 63
DISCOURSE -none- 122
PHYSICAL located 516
near 81
part-whole 333
PER-SOC business 119
(personal/social) family 115
other 28
OTHER-AFF ethnic 28
(PER/ORG ideology 26
affiliation) other 27
</table>
<tableCaption confidence="0.82571">
Table 1: The set of types and subtypes of relations
used in the 2004 ACE evaluation.
</tableCaption>
<bodyText confidence="0.7112195">
relations do not exist) suggest that schemes for im-
proving recall might benefit this task.
</bodyText>
<sectionHeader confidence="0.994173" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.998653666666667">
In this section, we present results of experiments
comparing three different methods of combining
classifiers for ACE relation extraction:
</bodyText>
<listItem confidence="0.9927045">
• At-Least-N for different values of N,
• Majority Voting, and
• a simple algorithm, called summing, where we
add the posterior scores for each class from all
the classifiers and select the class with the max-
imum summed score.
</listItem>
<bodyText confidence="0.99508725">
Since the official ACE evaluation set is not pub-
licly available, to facilitate comparison with our re-
sults and for internal testing of our algorithms, for
each language (English, Arabic, and Chinese), we
</bodyText>
<page confidence="0.993731">
462
</page>
<table confidence="0.9997162">
En Ar Ch
Training Set (documents) 227 511 480
Training Set (rel-mentions) 3290 4126 4347
Test Set (documents) 114 178 166
Test Set (rel-mentions) 1381 1894 1774
</table>
<tableCaption confidence="0.9853985">
Table 2: The Division of LDC annotated data into
training and development test sets.
</tableCaption>
<bodyText confidence="0.998666">
divided the ACE 2004 training data provided by
LDC in a roughly 75%:25% ratio into a training set
and a test set. Table 2 summarizes the number of
documents and the number of relation mentions in
each data set. The test sets were deliberately chosen
to be the most recent 25% of documents in chrono-
logical order, since entities and relations in news
tend to repeat and random shuffles can greatly re-
duce the out-of-vocabulary problem.
</bodyText>
<subsectionHeader confidence="0.990184">
5.1 Maximum Entropy Classifiers
</subsectionHeader>
<bodyText confidence="0.9999864">
We used bagging (Breiman, 1996) to create replicate
training sets of the same size as the original training
set by repeatedly sampling with replacement from
the training set. We created 25 replicate training sets
(bags) for each language (Arabic, Chinese, English)
and trained separate maximum entropy classifiers on
each bag. We then applied At-Least-N (N = 1,2,5),
Majority Vote, and Summing algorithms with the
trained classifiers and measured the resulting perfor-
mance on our development set.
For each bag, we built maximum entropy models
to predict the presence of relation mentions and the
type and subtype of relations, when their presence
is predicted. Our models operate on every pair of
mentions in a document that are not references to
the same entity, to extract relation mentions. Since
there are 23 unique type-subtype pairs in Table 1,
our classifiers have 47 classes: two classes for each
pair corresponding to the two argument orderings
(e.g. ”John’s son” vs. ”his father”) and a NONE
class signifying no relation.
Similar to our earlier work (Kambhatla, 2004),
we used a combination of lexical, syntactic, and se-
mantic features including all the words in between
the two mentions, the entity types and subtypes of
the two mentions, the number of words in between
the two mentions, features derived from the small-
est parse fragment connecting the two mentions, etc.
These features were held constant throughout these
experiments.
</bodyText>
<subsectionHeader confidence="0.707624">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999951585365853">
We report the F-measure, precision and recall for
extracting relation mentions for all three languages.
We also report ACE value1, the official metric used
by NIST that assigns 0% value to a system that pro-
duces no output and a 100% value to a system that
extracts all relations without generating any false
alarms. Note that the ACE value counts each rela-
tion only once even if it is expressed in text many
times as different relation mentions. The reader is
referred to the NIST web site (NIST, 2004) for more
details on the ACE value computation.
Figures 1(a), 1(b), and 1(c) show the F-measure,
precision, and recall respectively for the English test
set obtained by different classifier combination tech-
niques as we vary the number of bags. Figures 2(a),
2(b), and 2(c) show similar curves for Chinese, and
Figures 3(a), 3(b), and 3(c) show similar curves for
Arabic. All these figures show the performance of a
single classifier as a straight line.
From the plots, it is clear that our hope of increas-
ing recall by combining classifiers is realized for all
three languages. As expected, the recall rises fastest
for At-Least-N when N is small, i.e when small mi-
nority opinion or even a single dissenting opinion is
being trusted. Of course, the rise in recall is at the
expense of a loss of precision. Overall, At-Least-N
for intermediate ranges of N (N=5 for English and
Chinese and N=2 for Arabic) performs best where
the moderate loss in precision is more than offset by
a rise in recall.
Both the Majority Vote method and the Summing
method succeed in avoiding a sharp loss of preci-
sion. However, they fail to increase the recall signif-
icantly either.
Table 3 summarizes the best results (F-measure)
for each classifier combination method for all three
languages compared with the result for a single clas-
sifier. At their best operating points, all three combi-
nation methods handily outperform the single clas-
sifier. At-Least-N seems to have a slight edge over
the other two methods, but the difference is small.
</bodyText>
<footnote confidence="0.9154445">
1Here we use the ACE value metric used for the ACE 2004
evaluation
</footnote>
<page confidence="0.999609">
463
</page>
<figure confidence="0.995499805555556">
0 5 10 15 20 25
Number of Bags
(a) F-measure
66
64
62
60
58
56
46
0 5 10 15 20 25
Number of Bags
(b) Precision
44
42
40
0 5 10 15 20 25
Number of Bags
(c) Recall
46
38
36
34
68
Precision
Recall
54
52
50
48
50
49
48
47
F
46
</figure>
<figureCaption confidence="0.9814405">
Figure 1: Comparing F-measure, precision, and recall of different voting schemes for English relation
extraction.
</figureCaption>
<figure confidence="0.994665066666667">
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
45
44
43
0 5 10 15 20 25
Number of Bags
(a) F-measure
0 5 10 15 20 25
Number of Bags
(b) Precision
0 5 10 15 20 25
Number of Bags
(c) Recall
</figure>
<figureCaption confidence="0.9850085">
Figure 2: Comparing F-measure, precision, and recall of different voting schemes for Chinese relation
extraction.
</figureCaption>
<figure confidence="0.993415">
70
76
74
68
72
66
70
64
Precision
68
Recall
62
66
F
60
64
58
62
56
60
54
58
52
56
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
67
66
65
64
63
62
61
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
F
31
30
29
28
27
26
25
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
Precision
44
42
40
38
36
34
32
30
28
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
Recall
30
28
26
24
22
20
18
At-Least-1
At-Least-2
At-Least-5
Majority Vote
Summing
Single
0 5 10 15 20 25
0 5 10 15 20 25 0 5 10 15 20 25 0 5 10 15 20 25
0 5 10 15 20 25
0 5 10 15 20 25
Number of Bags
Number of Bags
Number of Bags
Number of Bags Number of Bags Number of Bags
(b) Precision
(a) F-measure
(c) Recall
(a) F-measure (b) Precision (c) Recall
</figure>
<figureCaption confidence="0.992417">
Figure 3: Comparing F-measure, precision, and recall of different voting schemes for Arabic relation ex-
traction.
</figureCaption>
<page confidence="0.989899">
464
</page>
<table confidence="0.9990032">
English Arabic Chinese
Single 46.87 27.47 63.75
At-Least-N 49.52 30.41 66.79
Majority Vote 49.24 29.02 66.21
Summing 48.66 29.02 66.40
</table>
<tableCaption confidence="0.893221">
Table 3: Comparing the best F-measure obtained by
At-Least-N Voting with Majority Voting, Summing
and the single best classifier.
</tableCaption>
<table confidence="0.999599333333333">
English Arabic Chinese
Single 59.6 37.3 69.6
At-Least-N 63.9 43.5 71.0
</table>
<tableCaption confidence="0.859828666666667">
Table 4: Comparing the ACE Value obtained by At-
Least-N Voting with the single best classifier for the
operating points used in Table 3.
</tableCaption>
<bodyText confidence="0.99018436">
Table 4 shows the ACE value obtained by our
best performing classifier combination method (At-
Least-N at the operating points in Table 3) compared
with a single classifier. Note that while the improve-
ment for Chinese is slight, for Arabic performance
improves by over 16% relative and for English, the
improvement is over 7% relative over the single clas-
sifier2. Since the ACE value collapses relation men-
tions referring to the same relation, finding new re-
lations (i.e. recall) is more important. This might
explain the relatively larger difference in ACE value
between the single classifier performance and At-
Least-N.
The rules of the ACE evaluation prohibit us from
presenting a detailed comparison of our relation ex-
traction system with the other participants. How-
ever, our relation extraction system (using the At-
Least-N classifier combination scheme as described
here) performed very competitively in 2004 ACE
evaluation both in the system output relation ex-
traction task (RDR) and the relation extraction task
where the ’true’ mentions and entities are given.
Due to time limitations, we did not try At-Least-N
with N &gt; 5. From the plots, there is a potential for
getting greater gains by experimenting with a larger
</bodyText>
<footnote confidence="0.6349872">
2Note that ACE value metric used in the ACE 2004 eval-
uation weights entitites differently based on their type. Thus,
relations with PERSON-NAME arguments end up contribut-
ing a lot more the overall score than relations with FACILITY-
PRONOUN arguments.
</footnote>
<bodyText confidence="0.843757">
number of bags and with a larger N.
</bodyText>
<sectionHeader confidence="0.998595" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999946">
Several NLP problems exhibit a dominance of a
NONE class that typically signifies a lack of struc-
ture like a named entity, coreference, etc. Especially
when coupled with small training sets, this results in
classifiers with unbalanced precision and recall. We
have argued that a classifier voting scheme that is fo-
cused on improving recall can help increase overall
performance in such situations.
We have presented a class of voting methods,
dubbed At-Least-N that defer to the opinion of a mi-
nority of classifiers (consisting of N members) even
when the majority predicts NONE. This can boost
recall at the expense of precision. However, by vary-
ing N and the number of classifiers, we can pick an
operating point that improves the overall F-measure.
We have presented results for ACE relation ex-
traction for three languages comparing At-Least-N
with Majority Vote and Summing methods for com-
bining classifiers. All three classifier combination
methods significantly outperform a single classifier.
Also, At-Least-N consistently gave us the best per-
formance across different languages.
We used bagging to induce multiple classifiers for
our task. Because of the random bootstrap sam-
pling, different replicate training sets might tilt to-
wards one class or another. Thus, if we have many
classifiers trained on the replicate training sets, some
of them are likely to be better at predicting certain
classes than others. In future, we plan to experi-
ment with other methods for collecting a committee
of classifiers.
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999964636363636">
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings ofANLP-97, pages 194–201.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York
University.
L. Breiman. 1996. Bagging predictors. In Machine
Learning, volume 24, page 123.
E. Brill and J. Wu. 1998. Classifier combination
for improved lexical disambiguation. Proceedings of
COLING-ACL’98, pages 191–195, August.
</reference>
<page confidence="0.998653">
465
</page>
<bodyText confidence="0.536297285714286">
T. Zhang, F. Damerau, and D. E. Johnson. 2002. Text
chunking based on a generalization of Winnow. Jour-
nal ofMachine Learning Research, 2:615–637.
Radu Florian and David Yarowsky. 2002. Modeling con-
sensus: Classifier combination for word sense disam-
biguation. In Proceedings of EMNLP’02, pages 25–
32.
</bodyText>
<reference confidence="0.999880914893617">
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In Proceedings of CoNNL’03, pages 168–171.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics:
HLT-NAACL 2004, pages 1–8.
J. Henderson and E. Brill. 1999. Exploiting diversity in
natural language processing: Combining parsers. In
Proceedings on EMNLP99, pages 187–194.
T. K. Ho, J. J. Hull, and S. N. Srihari. 1994. Deci-
sion combination in multiple classifier systems. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 16(1):66–75, January.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy mod-
els for information extraction. In The Proceedings of
42st Annual Meeting of the Association for Computa-
tional Linguistics, pages 178–181, Barcelona, Spain,
July. Association for Computational Linguistics.
D. Magerman. 1993. Parsing as statistical pattern recog-
nition.
NIST. 2004. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151–178.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521–
544.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean,
R. Koeling, Y. Krymolowsky, V. Punyakanok, and
D. Roth. 2000. Applying system combination to base
noun phrase identification. In Proceedings of COL-
ING 2000, pages 857–863.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Im-
proving data driven wordclass tagging by system com-
bination. In Proceedings of COLING-ACL’98, pages
491–497.
L. Xu, A. Krzyzak, and C. Suen. 1992. Methods of
combining multiple classifiers and their applications
to handwriting recognition. IEEE Trans. on Systems,
Man. Cybernet, 22(3):418–435.
</reference>
<page confidence="0.999598">
466
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.719991">
<title confidence="0.999304">Minority Vote: At-Least-N Voting Improves Recall for Extracting Relations</title>
<author confidence="0.999414">Nanda Kambhatla</author>
<affiliation confidence="0.999967">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.99703">1101 Kitchawan Road Rt 134 Yorktown, NY 10598</address>
<email confidence="0.999432">nanda@us.ibm.com</email>
<abstract confidence="0.982602647058823">Several NLP tasks are characterized by asymmetric data where one class label signifying the absence of any structure (named entity, coreference, relation, etc.) dominates all other classes. Classifiers built on such data typically have a higher precision and a lower reand tend to overproduce the class. We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a high-performance learning namefinder.</title>
<date>1997</date>
<booktitle>In Proceedings ofANLP-97,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="1058" citStr="Bikel et al., 1997" startWordPosition="160" endWordPosition="163">gher precision and a lower recall and tend to overproduce the NONE class. We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese. 1 Introduction Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging (Ratnaparkhi, 1999), chunking (Zhang et al., 2002), semantic parsing (Magerman, 1993), named entity extraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al., 2001), relation extraction (Kambhatla, 2004), etc. A number of these applications are characterized by a dominance of a NONE class in the training examples. For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not. In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class) than otherwise. Similarly, if a classifier is predicting the presence/absence of a semantic relation between two mention</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: a high-performance learning namefinder. In Proceedings ofANLP-97, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="1038" citStr="Borthwick, 1999" startWordPosition="158" endWordPosition="159">pically have a higher precision and a lower recall and tend to overproduce the NONE class. We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese. 1 Introduction Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging (Ratnaparkhi, 1999), chunking (Zhang et al., 2002), semantic parsing (Magerman, 1993), named entity extraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al., 2001), relation extraction (Kambhatla, 2004), etc. A number of these applications are characterized by a dominance of a NONE class in the training examples. For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not. In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class) than otherwise. Similarly, if a classifier is predicting the presence/absence of a semantic relation</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>A. Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>In Machine Learning,</booktitle>
<volume>24</volume>
<pages>123</pages>
<contexts>
<context position="5709" citStr="Breiman, 1996" startWordPosition="930" endWordPosition="931"> large. A widely used voting method for combining classifiers is a Majority Vote scheme (e.g. (Brill and Wu, 1998; Tjong Kim Sang et al., 2000)). Each classifier gets to vote for its top ranked class and the class with the highest number of votes ’wins’. Henderson et al (1999) use a Majority Vote scheme where different parsers vote on constituents’ membership in a hypothesized parse. Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging. In this paper, we induce multiple classifiers by using bagging (Breiman, 1996). Following Breiman’s approach, we obtain multiple classifiers by first making bootstrap replicates of the training data and training different classifiers on each of the replicates. The bootstrap replicates are induced by repeatedly sampling with replacement training events from the original training data to arrive at replicate data sets of the same size as the training data set. Breiman (1996) uses a Majority Vote scheme for combining the output of the classifiers. In the next section, we will describe the different voting schemes we explored in our work. 3 At-Least-N Voting We are specifica</context>
<context position="11854" citStr="Breiman, 1996" startWordPosition="1932" endWordPosition="1933"> (rel-mentions) 1381 1894 1774 Table 2: The Division of LDC annotated data into training and development test sets. divided the ACE 2004 training data provided by LDC in a roughly 75%:25% ratio into a training set and a test set. Table 2 summarizes the number of documents and the number of relation mentions in each data set. The test sets were deliberately chosen to be the most recent 25% of documents in chronological order, since entities and relations in news tend to repeat and random shuffles can greatly reduce the out-of-vocabulary problem. 5.1 Maximum Entropy Classifiers We used bagging (Breiman, 1996) to create replicate training sets of the same size as the original training set by repeatedly sampling with replacement from the training set. We created 25 replicate training sets (bags) for each language (Arabic, Chinese, English) and trained separate maximum entropy classifiers on each bag. We then applied At-Least-N (N = 1,2,5), Majority Vote, and Summing algorithms with the trained classifiers and measured the resulting performance on our development set. For each bag, we built maximum entropy models to predict the presence of relation mentions and the type and subtype of relations, when</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. In Machine Learning, volume 24, page 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL’98,</booktitle>
<pages>191--195</pages>
<contexts>
<context position="3642" citStr="Brill and Wu, 1998" startWordPosition="585" endWordPosition="588">s schemes for combining classifiers. In section 3, we present our vot460 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 460–466, Sydney, July 2006. c�2006 Association for Computational Linguistics ing algorithm. In section 4, we describe the ACE relation extraction task. In section 5, we present empirical results for relation extraction and we discuss our results and conclude in section 6. 2 Combining Classifiers Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al., 2000), named entity extraction (Florian et al., 2003), etc. Ho et al (1994) investigated different approaches for reranking the outputs of a committee of classifiers and also explored union and intersection methods for reducing the set of predicted categories. Florian et al (2002) give a broad overview of methods for combining classifiers and present empirical results for word sense disambiguation. Xu et al (1992) and Florian et al (2002) consider three approaches for combining classifiers. In the first approach, individual classifiers ou</context>
<context position="5208" citStr="Brill and Wu, 1998" startWordPosition="843" endWordPosition="846">, often called voting methods, treat each classifier as a black box that outputs only the top ranked class and combines these to arrive at the final decision (class). The choice of approach and the specific method of combination may be constrained by the specific classification algorithms in use. In this paper, we focus on voting methods, since for small data sets, it is hard to reliably estimate probability distributions or even a complete ordering of classes especially when the number of classes is large. A widely used voting method for combining classifiers is a Majority Vote scheme (e.g. (Brill and Wu, 1998; Tjong Kim Sang et al., 2000)). Each classifier gets to vote for its top ranked class and the class with the highest number of votes ’wins’. Henderson et al (1999) use a Majority Vote scheme where different parsers vote on constituents’ membership in a hypothesized parse. Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging. In this paper, we induce multiple classifiers by using bagging (Breiman, 1996). Following Breiman’s approach, we obtain multiple classifiers by first making bootstrap replicates</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. Proceedings of COLING-ACL’98, pages 191–195, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>T Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNNL’03,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="3751" citStr="Florian et al., 2003" startWordPosition="603" endWordPosition="606"> Main Conference Poster Sessions, pages 460–466, Sydney, July 2006. c�2006 Association for Computational Linguistics ing algorithm. In section 4, we describe the ACE relation extraction task. In section 5, we present empirical results for relation extraction and we discuss our results and conclude in section 6. 2 Combining Classifiers Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al., 2000), named entity extraction (Florian et al., 2003), etc. Ho et al (1994) investigated different approaches for reranking the outputs of a committee of classifiers and also explored union and intersection methods for reducing the set of predicted categories. Florian et al (2002) give a broad overview of methods for combining classifiers and present empirical results for word sense disambiguation. Xu et al (1992) and Florian et al (2002) consider three approaches for combining classifiers. In the first approach, individual classifiers output posterior probabilities that are merged (e.g. by taking an average) to arrive at a composite posterior p</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003. Named entity recognition through classifier combination. In Proceedings of CoNNL’03, pages 168–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1081" citStr="Florian et al., 2004" startWordPosition="164" endWordPosition="168"> lower recall and tend to overproduce the NONE class. We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese. 1 Introduction Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging (Ratnaparkhi, 1999), chunking (Zhang et al., 2002), semantic parsing (Magerman, 1993), named entity extraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al., 2001), relation extraction (Kambhatla, 2004), etc. A number of these applications are characterized by a dominance of a NONE class in the training examples. For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not. In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class) than otherwise. Similarly, if a classifier is predicting the presence/absence of a semantic relation between two mentions, there are typically </context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N Nicolov, and S Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>E Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings on EMNLP99,</booktitle>
<pages>187--194</pages>
<marker>Henderson, Brill, 1999</marker>
<rawString>J. Henderson and E. Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings on EMNLP99, pages 187–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Ho</author>
<author>J J Hull</author>
<author>S N Srihari</author>
</authors>
<title>Decision combination in multiple classifier systems.</title>
<date>1994</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="3773" citStr="Ho et al (1994)" startWordPosition="608" endWordPosition="611">ions, pages 460–466, Sydney, July 2006. c�2006 Association for Computational Linguistics ing algorithm. In section 4, we describe the ACE relation extraction task. In section 5, we present empirical results for relation extraction and we discuss our results and conclude in section 6. 2 Combining Classifiers Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al., 2000), named entity extraction (Florian et al., 2003), etc. Ho et al (1994) investigated different approaches for reranking the outputs of a committee of classifiers and also explored union and intersection methods for reducing the set of predicted categories. Florian et al (2002) give a broad overview of methods for combining classifiers and present empirical results for word sense disambiguation. Xu et al (1992) and Florian et al (2002) consider three approaches for combining classifiers. In the first approach, individual classifiers output posterior probabilities that are merged (e.g. by taking an average) to arrive at a composite posterior probability of each cla</context>
</contexts>
<marker>Ho, Hull, Srihari, 1994</marker>
<rawString>T. K. Ho, J. J. Hull, and S. N. Srihari. 1994. Decision combination in multiple classifier systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(1):66–75, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction.</title>
<date>2004</date>
<booktitle>In The Proceedings of 42st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>178--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1164" citStr="Kambhatla, 2004" startWordPosition="177" endWordPosition="178">ng among a committee of classifiers that can significantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese. 1 Introduction Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging (Ratnaparkhi, 1999), chunking (Zhang et al., 2002), semantic parsing (Magerman, 1993), named entity extraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al., 2001), relation extraction (Kambhatla, 2004), etc. A number of these applications are characterized by a dominance of a NONE class in the training examples. For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not. In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class) than otherwise. Similarly, if a classifier is predicting the presence/absence of a semantic relation between two mentions, there are typically far more examples signifying an absence of a relation. Classifiers built with asymm</context>
<context position="12900" citStr="Kambhatla, 2004" startWordPosition="2099" endWordPosition="2100">performance on our development set. For each bag, we built maximum entropy models to predict the presence of relation mentions and the type and subtype of relations, when their presence is predicted. Our models operate on every pair of mentions in a document that are not references to the same entity, to extract relation mentions. Since there are 23 unique type-subtype pairs in Table 1, our classifiers have 47 classes: two classes for each pair corresponding to the two argument orderings (e.g. ”John’s son” vs. ”his father”) and a NONE class signifying no relation. Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the smallest parse fragment connecting the two mentions, etc. These features were held constant throughout these experiments. 5.2 Results We report the F-measure, precision and recall for extracting relation mentions for all three languages. We also report ACE value1, the official metric used by NIST that assigns 0% value to a system that produces no out</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for information extraction. In The Proceedings of 42st Annual Meeting of the Association for Computational Linguistics, pages 178–181, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Parsing as statistical pattern recognition.</title>
<date>1993</date>
<contexts>
<context position="996" citStr="Magerman, 1993" startWordPosition="153" endWordPosition="154">classes. Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class. We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese. 1 Introduction Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging (Ratnaparkhi, 1999), chunking (Zhang et al., 2002), semantic parsing (Magerman, 1993), named entity extraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al., 2001), relation extraction (Kambhatla, 2004), etc. A number of these applications are characterized by a dominance of a NONE class in the training examples. For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not. In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class) than otherwise. Similarly, if a classifier is predicting t</context>
</contexts>
<marker>Magerman, 1993</marker>
<rawString>D. Magerman. 1993. Parsing as statistical pattern recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE evaluation plan.</title>
<date>2004</date>
<note>www.nist.gov/speech/tests/ace/index.htm.</note>
<contexts>
<context position="8172" citStr="NIST, 2004" startWordPosition="1346" endWordPosition="1347">this voting scheme to increase recall at the expense of precision. At-Least-N Voting induces a spectrum of combi461 nation methods ranging from a Majority Vote (when N is more than half of the total number of classifiers) to a scheme, where the evidence of any structure by even one classifier is believed (At-Least-1 Voting). The exact choice of N is an empirical one and depends on the amount of asymmetry in the data and the imbalance between precision and recall in the classifiers. 4 The ACE Relation Extraction Task Automatic Content Extraction (ACE) is an annual evaluation conducted by NIST (NIST, 2004) on information extraction, focusing on extraction of entities, events, and relations. The Entity Detection and Recognition task entails detection of mentions of entities and grouping together the mentions that are references to the same entity. In ACE terminology, mentions are references in text (or audio, chats, ...) to real world entities. Similarly relation mentions are references in text to semantic relations between entity mentions and relations group together all relation mentions that identify the same semantic relation between the same entities. In the frament of text: John’s son, Jim</context>
<context position="13784" citStr="NIST, 2004" startWordPosition="2249" endWordPosition="2250">onnecting the two mentions, etc. These features were held constant throughout these experiments. 5.2 Results We report the F-measure, precision and recall for extracting relation mentions for all three languages. We also report ACE value1, the official metric used by NIST that assigns 0% value to a system that produces no output and a 100% value to a system that extracts all relations without generating any false alarms. Note that the ACE value counts each relation only once even if it is expressed in text many times as different relation mentions. The reader is referred to the NIST web site (NIST, 2004) for more details on the ACE value computation. Figures 1(a), 1(b), and 1(c) show the F-measure, precision, and recall respectively for the English test set obtained by different classifier combination techniques as we vary the number of bags. Figures 2(a), 2(b), and 2(c) show similar curves for Chinese, and Figures 3(a), 3(b), and 3(c) show similar curves for Arabic. All these figures show the performance of a single classifier as a straight line. From the plots, it is clear that our hope of increasing recall by combining classifiers is realized for all three languages. As expected, the recal</context>
</contexts>
<marker>NIST, 2004</marker>
<rawString>NIST. 2004. The ACE evaluation plan. www.nist.gov/speech/tests/ace/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--151</pages>
<contexts>
<context position="930" citStr="Ratnaparkhi, 1999" startWordPosition="144" endWordPosition="145">ture (named entity, coreference, relation, etc.) dominates all other classes. Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class. We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese. 1 Introduction Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging (Ratnaparkhi, 1999), chunking (Zhang et al., 2002), semantic parsing (Magerman, 1993), named entity extraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al., 2001), relation extraction (Kambhatla, 2004), etc. A number of these applications are characterized by a dominance of a NONE class in the training examples. For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not. In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>Adwait Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34:151–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>544</pages>
<contexts>
<context position="1125" citStr="Soon et al., 2001" startWordPosition="171" endWordPosition="174">class. We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations. We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese. 1 Introduction Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging (Ratnaparkhi, 1999), chunking (Zhang et al., 2002), semantic parsing (Magerman, 1993), named entity extraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al., 2001), relation extraction (Kambhatla, 2004), etc. A number of these applications are characterized by a dominance of a NONE class in the training examples. For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not. In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class) than otherwise. Similarly, if a classifier is predicting the presence/absence of a semantic relation between two mentions, there are typically far more examples signifying an absence of a</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521– 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>W Daelemans</author>
<author>H Dejean</author>
<author>R Koeling</author>
<author>Y Krymolowsky</author>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>Applying system combination to base noun phrase identification.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>857--863</pages>
<contexts>
<context position="3703" citStr="Sang et al., 2000" startWordPosition="595" endWordPosition="598">our vot460 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 460–466, Sydney, July 2006. c�2006 Association for Computational Linguistics ing algorithm. In section 4, we describe the ACE relation extraction task. In section 5, we present empirical results for relation extraction and we discuss our results and conclude in section 6. 2 Combining Classifiers Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al., 2000), named entity extraction (Florian et al., 2003), etc. Ho et al (1994) investigated different approaches for reranking the outputs of a committee of classifiers and also explored union and intersection methods for reducing the set of predicted categories. Florian et al (2002) give a broad overview of methods for combining classifiers and present empirical results for word sense disambiguation. Xu et al (1992) and Florian et al (2002) consider three approaches for combining classifiers. In the first approach, individual classifiers output posterior probabilities that are merged (e.g. by taking </context>
<context position="5238" citStr="Sang et al., 2000" startWordPosition="849" endWordPosition="852"> treat each classifier as a black box that outputs only the top ranked class and combines these to arrive at the final decision (class). The choice of approach and the specific method of combination may be constrained by the specific classification algorithms in use. In this paper, we focus on voting methods, since for small data sets, it is hard to reliably estimate probability distributions or even a complete ordering of classes especially when the number of classes is large. A widely used voting method for combining classifiers is a Majority Vote scheme (e.g. (Brill and Wu, 1998; Tjong Kim Sang et al., 2000)). Each classifier gets to vote for its top ranked class and the class with the highest number of votes ’wins’. Henderson et al (1999) use a Majority Vote scheme where different parsers vote on constituents’ membership in a hypothesized parse. Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging. In this paper, we induce multiple classifiers by using bagging (Breiman, 1996). Following Breiman’s approach, we obtain multiple classifiers by first making bootstrap replicates of the training data and trai</context>
</contexts>
<marker>Sang, Daelemans, Dejean, Koeling, Krymolowsky, Punyakanok, Roth, 2000</marker>
<rawString>E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling, Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Applying system combination to base noun phrase identification. In Proceedings of COLING 2000, pages 857–863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98,</booktitle>
<pages>491--497</pages>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of COLING-ACL’98, pages 491–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Xu</author>
<author>A Krzyzak</author>
<author>C Suen</author>
</authors>
<title>Methods of combining multiple classifiers and their applications to handwriting recognition.</title>
<date>1992</date>
<journal>IEEE Trans. on Systems, Man. Cybernet,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="2238" citStr="Xu et al., 1992" startWordPosition="351" endWordPosition="354">emantic relation between two mentions, there are typically far more examples signifying an absence of a relation. Classifiers built with asymmetric data dominated by one class (a NONE class donating absence of a relation or coreference or a named entity etc.) can overgenerate the NONE class. This often results in a unbalanced classifier where precision is higher than recall. In this paper, we present a novel approach for improving the recall of such classifiers by using a new voting scheme from a committee of classifiers. There are a plethora of algorithms for combining classifiers (e.g. see (Xu et al., 1992)). A widely used approach is a majority voting scheme, where each classifier in the committee gets a vote and the class with the largest number of votes ’wins’ (i.e. the corresponding class is output as the prediction of the committee). We are interested in improving overall recall and reduce the overproduction of the class NONE. Our scheme predicts the class label C obtaining the second highest number of votes when NONE gets the highest number of votes, provided C gets at least N votes. Thus, we predict a label other than NONE when there is some evidence of the presense of the structure we ar</context>
<context position="4115" citStr="Xu et al (1992)" startWordPosition="662" endWordPosition="665">assifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al., 2000), named entity extraction (Florian et al., 2003), etc. Ho et al (1994) investigated different approaches for reranking the outputs of a committee of classifiers and also explored union and intersection methods for reducing the set of predicted categories. Florian et al (2002) give a broad overview of methods for combining classifiers and present empirical results for word sense disambiguation. Xu et al (1992) and Florian et al (2002) consider three approaches for combining classifiers. In the first approach, individual classifiers output posterior probabilities that are merged (e.g. by taking an average) to arrive at a composite posterior probability of each class. In the second scheme, each classifier outputs a ranked list of classes instead of a probability distribution and the different ranked lists are merged to arrive at a final ranking. Methods using the third approach, often called voting methods, treat each classifier as a black box that outputs only the top ranked class and combines these</context>
</contexts>
<marker>Xu, Krzyzak, Suen, 1992</marker>
<rawString>L. Xu, A. Krzyzak, and C. Suen. 1992. Methods of combining multiple classifiers and their applications to handwriting recognition. IEEE Trans. on Systems, Man. Cybernet, 22(3):418–435.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>