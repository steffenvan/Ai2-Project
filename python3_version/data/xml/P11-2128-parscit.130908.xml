<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000502">
<title confidence="0.979496">
Entity Set Expansion using Topic information
</title>
<author confidence="0.830479">
Kugatsu Sadamitsu, Kuniko Saito, Kenji Imamura and Genichiro Kikui*
</author>
<affiliation confidence="0.593416">
NTT Cyber Space Laboratories, NTT Corporation
</affiliation>
<address confidence="0.793753">
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
</address>
<email confidence="0.9911525">
{sadamitsu.kugatsu, saito.kuniko, imamura.kenji}@lab.ntt.co.jp
kikui@cse.oka-pu.ac.jp
</email>
<sectionHeader confidence="0.995595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997191416666667">
This paper proposes three modules based on
latent topics of documents for alleviating “se-
mantic drift” in bootstrapping entity set ex-
pansion. These new modules are added to a
discriminative bootstrapping algorithm to re-
alize topic feature generation, negative exam-
ple selection and entity candidate pruning. In
this study, we model latent topics with LDA
(Latent Dirichlet Allocation) in an unsuper-
vised way. Experiments show that the accu-
racy of the extracted entities is improved by
6.7 to 28.2% depending on the domain.
</bodyText>
<sectionHeader confidence="0.998947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988227609756097">
The task of this paper is entity set expansion in
which the lexicons are expanded from just a few
seed entities (Pantel et al., 2009). For example,
the user inputs a few words “Apple”, “Google” and
“IBM” , and the system outputs “Microsoft”, “Face-
book” and “Intel”.
Many set expansion algorithms are based on boot-
strapping algorithms, which iteratively acquire new
entities. These algorithms suffer from the general
problem of “semantic drift”. Semantic drift moves
the extraction criteria away from the initial criteria
demanded by the user and so reduces the accuracy
of extraction. Pantel and Pennacchiotti (2006) pro-
posed Espresso, a relation extraction method based
on the co-training bootstrapping algorithm with en-
tities and attributes. Espresso alleviates semantic-
drift by a sophisticated scoring system based on
* Presently with Okayama Prefectural University
pointwise mutual information (PMI). Thelen and
Riloff (2002), Ghahramani and Heller (2005) and
Sarmento et al. (2007) also proposed original score
functions with the goal of reducing semantic-drift.
Our purpose is also to reduce semantic drift. For
achieving this goal, we use a discriminative method
instead of a scoring function and incorporate topic
information into it. Topic information means the
genre of each document as estimated by statisti-
cal topic models. In this paper, we effectively uti-
lize topic information in three modules: the first
generates the features of the discriminative mod-
els; the second selects negative examples; the third
prunes incorrect examples from candidate examples
for new entities. Our experiments show that the pro-
posal improves the accuracy of the extracted entities.
The remainder of this paper is organized as fol-
lows. In Section 2, we illustrate discriminative boot-
strapping algorithms and describe their problems.
Our proposal is described in Section 3 and experi-
mental results are shown in Section 4. Related works
are described in Section 5. Finally, Section 6 pro-
vides our conclusion and describes future works.
</bodyText>
<subsectionHeader confidence="0.514571">
2 Problems of the previous Discriminative
Bootstrapping method
</subsectionHeader>
<bodyText confidence="0.999934625">
Some previous works introduced discriminative
methods based on the logistic sigmoid classifier,
which can utilize arbitrary features for the relation
extraction task instead of a scoring function such as
Espresso (Bellare et al., 2006; Mintz et al., 2009).
Bellare et al. reported that the discriminative ap-
proach achieves better accuracy than Espresso when
the number of extracted pairs is increased because
</bodyText>
<page confidence="0.977253">
726
</page>
<note confidence="0.583591">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726–731,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.86258206185567">
multiple features are used to support the evidence.
However, three problems exist in their methods.
First, they use only local context features. The dis-
criminative approach is useful for using arbitrary
features, however, they did not identify which fea-
ture or features are effective for the methods. Al-
though the context features and attributes partly re-
duce entity word sense ambiguity, some ambiguous
entities remain. For example, consider the domain
broadcast program (PRG) and assume that PRG’s
attribute is advertisement. A false example is shown
here: “Android ’s advertisement employs Japanese
popular actors. The attractive smartphone begins to
target new users who are ordinary people.” The en-
tity Android belongs to the cell-phone domain, not
PRG, but appears with positive attributes or contexts
because many cell-phones are introduced in adver-
tisements as same as broadcast program. By us-
ing topic, i.e. the genre of the document, we can
distinguish “Android” from PRG and remove such
false examples even if the false entity appeared with
positive context strings or attributes. Second, they
did not solve the problem of negative example se-
lection. Because negative examples are necessary
for discriminative training, they used all remaining
examples, other than positive examples, as negative
examples. Although this is the simplest technique,
it is impossible to use all of the examples provided
by a large-scale corpus for discriminative training.
Third, their methods discriminate all candidates for
new entities. This principle increases the risk of gen-
erating many false-positive examples and is ineffi-
cient. We solve these three problems by using topic
information.
3 Set expansion using Topic information
3.1 Basic bootstrapping methods
In this section, we describe the basic method
adopted from Bellare (Bellare et al., 2006). Our
system’s configuration diagram is shown in Figure
1. In Figure 1, arrows with solid lines indicate the
basic process described in this section. The other
parts are described in the following sections. After
N3 positive seed entities are manually given, every
noun co-occurring with the seed entities is ranked
by PMI scores and then selected manually as Na
positive attributes. N3 and Na are predefined ba-
727
Figure 1: The structure of our system.
sic adjustment numbers. The entity-attribute pairs
are obtained by taking the cross product of seed en-
tity lists and attribute lists. The pairs are used as
queries for retrieving the positive documents, which
include positive pairs. The document set De,a in-
cluding same entity-attribute pair {e, a} is regarded
as one example Ee,a to alleviate over-fitting for con-
text features. These are called positive examples in
Figure 1. Once positive examples are constructed,
discriminative models can be trained by randomly
selecting negative examples.
Candidate entities are restricted to only the
Named Entities that lie in the close proximity to the
positive attributes. These candidates of documents,
including Named Entity and positive attribute pairs,
are regarded as one example the same as the train-
ing data. The discriminative models are used to cal-
culate the discriminative positive score, s(e, a), of
each candidate pair, {e, a}. Our system extracts N,,,
types of new entities with high scores at each iter-
ation as defined by the summation of s(e, a) of all
positive attributes (AP); ∑acA, s(e, a). Note that
we do not iteratively extract new attributes because
our purpose is entity set expansion.
3.2 Topic features and Topic models
In previous studies, context information is only used
as the features of discriminative models as we de-
scribed in Section 2. Our method utilizes not only
context features but also topic features. By utiliz-
ing topic information, our method can disambiguate
the entity word sense and alleviate semantic drift.
In order to derive the topic information, we utilize
statistical topic models, which represent the relation
between documents and words through hidden top-
ics. The topic models can calculate the posterior
probability p(z|d) of topic z in document d. For
example, the topic models give high probability to
topic z =”cell-phone” in the above example sen-
tences 1. This posterior probability is useful as a
global feature for discrimination. The topic feature
value Ot(z, e, a) is calculated as follows.
In this paper, we use Latent Dirichlet Allocation
(LDA) as the topic models (Blei et al., 2003). LDA
represents the latent topics of the documents and the
co-occurrence between each topic.
In Figure 1, shaded part and the arrows with bro-
ken lines indicate our proposed method with its use
of topic information including the following sec-
tions.
</bodyText>
<subsectionHeader confidence="0.99951">
3.3 Negative example selection
</subsectionHeader>
<bodyText confidence="0.9999887">
If we choose negative examples randomly, such ex-
amples are harmful for discrimination because some
examples include the same contexts or topics as the
positive examples. By contrast, negative examples
belonging to broad genres are needed to alleviate se-
mantic drift. We use topic information to efficiently
select such negative examples.
In our method, the negative examples are cho-
sen far from the positive examples according to the
measure of topic similarity. For calculating topic
similarity, we use a ranking score called “positive
topic score”, PT (z), defined as follows, PT (z) =
EdED, p(z|d), where DP indicates the set of pos-
itive documents and p(z|d) is topic posterior prob-
ability for a given positive document. The bottom
50% of the topics sorted in decreasing order of pos-
itive topic score are used as the negative topics.
Our system picks up as many negative documents
as there are positive documents with each selected
negative topic being equally represented.
</bodyText>
<subsectionHeader confidence="0.975256">
3.4 Candidate Pruning
</subsectionHeader>
<bodyText confidence="0.9982085">
Previous works discriminate all candidates for ex-
tracting new entities. Our basic system can constrain
</bodyText>
<footnote confidence="0.584452">
1z is a random variable whose sample space is represented
as a discrete variable, not explicit words.
</footnote>
<bodyText confidence="0.99995635">
the candidate set by positive attributes, however, this
is not enough as described in Section 2. Our candi-
date pruning module, described below, uses the mea-
sure of topic similarity to remove obviously incor-
rect documents.
This pruning module is similar to negative exam-
ple selection described in the previous section. The
positive topic score, PT, is used as a candidate con-
straint. Taking all positive examples, we select the
positive topics, PZ, which including all topics z sat-
isfying the condition PT (z) &gt; th. At least one
topic with the largest score is chosen as a positive
topic when PT (z) ≤ th about all topics. After se-
lecting this positive topic, the documents including
entity candidates are removed if the posterior prob-
ability satisfy p(z|d) ≤ th for all topics z. In this
paper, we set the threshold to th = 0.2. This con-
straint means that the topic of the document matches
that of the positive entities and can be regarded as a
hard constraint for topic features.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997738">
4.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999660666666667">
We use 30M Japanese blog articles crawled in May
2008. The documents were tokenized by JTAG
(Fuchi and Takagi, 1998), chunked, and labeled with
IREX 8 Named Entity types by CRFs using Mini-
mum Classification Error rate (Suzuki et al., 2006),
and transformed into features. The context features
were defined using the template “(head) entity (mid.)
attribute (tail)”. The words included in each part
were used as surface, part-of-speech and Named En-
tity label features added position information. Max-
imum word number of each part was set at 2 words.
The features have to appear in both the positive and
negative training data at least 5 times.
In the experiments, we used three domains, car
(“CAR”), broadcast program (“PRG”) and sports or-
ganization (“SPT”). The adjustment numbers for ba-
sic settings are N3 = 10, Na = 10, N,,, = 100. Af-
ter running 10 iterations, we obtained 1000 entities
in total. 5VM&amp;quot;ght (Joachims, 1999) with second
order polynomial kernel was used as the discrimina-
tive model. Parallel LDA, which is LDA with MPI
(Liu et al., 2011), was used for training 100 mix-
ture topic models and inference. Training corpus for
topic models consisted of the content gathered from
</bodyText>
<equation confidence="0.960302666666667">
Ot(z, e, a) = E EdED�,a p(z�|d).
z&apos;
E dED,,a p(z|d)
</equation>
<page confidence="0.967804">
728
</page>
<table confidence="0.6670024">
CAR PRG SPT
1. Baseline 0.249 0.717 0.781
2. Topic features + 1. 0.483 0.727 0.844
3. Negative selection + 2. 0.509 0.762 0.846
4. Candidate pruning + 3. 0.531 0.824 0.848
</table>
<tableCaption confidence="0.89008475">
Table 1: The experimental results for the three domains.
Bold font indicates that the difference between accuracy
of the methods in the row and the previous row is signifi-
cant (P &lt; 0.05 by binomial test) and italic font indicates
</tableCaption>
<bodyText confidence="0.984610619047619">
(P &lt; 0.1).
14 days of blog articles. In the Markov-chain Monte
Carlo (MCMC) method, sampling was iterated 200
times for training with a burn-in taking 50 iterations.
These parameters were selected based on the results
of a preliminary experiment.
Four experimental settings were examined. First
is Baseline; it is described in Section 3.1. Second is
the first method with the addition of topic features.
Third is the second method with the addition of a
negative example selection module. Fourth is the
third method with the addition of a candidate prun-
ing module (equals the entire shaded part in Fig-
ure 1). Each extracted entity is labeled with cor-
rect or incorrect by two evaluators based on the re-
sults of a commercial search engine. The n score for
agreement between evaluators was 0.895. Because
the third evaluator checked the two evaluations and
confirmed that the examples which were judged as
correct by either one of the evaluators were correct,
those examples were counted as correct.
</bodyText>
<subsectionHeader confidence="0.983685">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999880714285714">
Table 1 shows the accuracy and significance for each
domain. Using topic features significantly improves
accuracy in the CAR and SPT domains. The nega-
tive example selection module improves accuracy in
the CAR and PRG domains. This means the method
could reduce the risk of selecting false-negative ex-
amples. Also, the candidate pruning method is ef-
fective for the CAR and PRG domains. The CAR
domain has lower accuracy than the others. This
is because similar entities such as motorcycles are
extracted; they have not only the same context but
also the same topic as the CAR domain. In the SPT
domain, the method with topic features offer signif-
icant improvements in accuracy and no further im-
provement was achieved by the other two modules.
To confirm whether our modules work properly,
we show some characteristic words belonging to
each topic that is similar and not similar to target do-
main in Table 2. Table 2 shows characteristic words
for one positive topic zh and two negative topics zl
and ze, defined as follow.
</bodyText>
<listItem confidence="0.998236">
• zh (the second row) is the topic that maximizes
PT (z), which is used as a positive topic.
• zl (the fourth row) is the topic that minimizes
PT (z), which is used as a negative topic.
• ze (the fifth row) is a topic that, we consider, ef-
fectively eliminates “drifted entities” extracted
by the baseline method. ze is eventually in-
cluded in the lower half of topic list sorted by
PT (z).
</listItem>
<bodyText confidence="0.99998752631579">
For a given topic, z, we chose topmost three words
in terms of topic-word score. The topic-word score
of a word, v, is defined as p(v|z)/p(v), where p(v)
is the unigram probability of v, which was estimated
by maximum likelihood estimation. For utilizing
candidate pruning, near topics including zh must be
similar to the domain. By contrast, for utilizing neg-
ative example selection, the lower half of topics, zl,
ze and other negative topics, must be far from the
domain. Our system succeeded in achieving this.
As shown in “CAR” in Table 2, the nearest topic
includes “shaken” (automobile inspection) and the
farthest topic includes “naika” (internal medicine)
which satisfies our expectation. Furthermore, the ef-
fective negative topic is similar to the topic of drifted
entity sets (digital device). This indicates that our
method successfully eliminated drifted entities. We
can confirm that the other domains trend in the same
direction as “CAR” domain.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="related work">
5 Related Works
</sectionHeader>
<bodyText confidence="0.999933142857143">
Some prior studies use every word in a docu-
ment/sentence as the features, such as the distribu-
tional approaches (Pantel et al., 2009). These meth-
ods are regarded as using global information, how-
ever, the space of word features are sparse, even if
the amount of data available is large. Our approach
can avoid this problem by using topic models which
</bodyText>
<page confidence="0.995019">
729
</page>
<table confidence="0.997948714285714">
domain CAR PRG SPT
words of the shaken Mari YAMADA, toshu (pitcher),
nearest topic zh (automobile inspection), Tohru KUSANO, senpatsu
(highest PT score) nosha (delivering a car), Reiko TOKITA (starting member),
daisha (loaner car) (Japanese stars) shiai (game)
drifted entities iPod, mac PS2, XBOX360 B’z, CHAGE&amp;ASKA
(using baseline) (digital device) (video game) (music)
words of effective PT score) Lv. (level), sinpu (new release),
negative topic ze gaso (pixel), kariba (hunting area), X JAPAN,
(Lower half of kido (brightness), girumen (guild member) Kazuyoshi Saito
mazabodo (mother board) (Japanese musicians)
words of naika (internal medicine), tsure (hook a fish), toritomento (treatment),
the farthest topic zl hairan (ovulation), choka (result of hooking), keana (pore),
(Lowest PT score) shujii (attending doctor) choko (diary of hooking) hoshitsu (moisture retention)
</table>
<tableCaption confidence="0.998364">
Table 2: The characteristic words belonging to three topics, zh, zl and ze. zh is the nearest topic and zl is the farthest
topic for positive entity-attribute seed pairs. ze is an effective negative topic for eliminating “drifted entities” extracted
by the baseline system.
</tableCaption>
<bodyText confidence="0.999156476190476">
are clustering methods based on probabilistic mea-
sures. By contrast, Pas¸ca and Durme (2008) pro-
posed clustering methods that are effective in terms
of extraction, even though their clustering target is
only the surrounding context. Ritter and Etzioni
(2010) proposed a generative approach to use ex-
tended LDA to model selectional preferences. Al-
though their approach is similar to ours, our ap-
proach is discriminative and so can treat arbitrary
features; it is applicable to bootstrapping methods.
The accurate selection of negative examples is a
major problem for positive and unlabeled learning
methods or general bootstrapping methods and some
previous works have attempted to reach a solution
(Liu et al., 2002; Li et al., 2010). However, their
methods are hard to apply to the Bootstrapping al-
gorithms because the positive seed set is too small
to accurately select negative examples. Our method
uses topic information to efficiently solve both the
problem of extracting global information and the
problem of selecting negative examples.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999971">
We proposed an approach to set expansion that uses
topic information in three modules and showed that
it can improve expansion accuracy. The remaining
problem is that the grain size of topic models is not
always the same as the target domain. To resolve
this problem, we will incorporate the active learning
or the distributional approaches. Also, comparisons
with the previous works are remaining work. From
another perspective, we are considering the use of
graph-based approaches (Komachi et al., 2008) in-
corporated with the topic information using PHITS
(Cohn and Chang, 2000), to further enhance entity
extraction accuracy.
</bodyText>
<sectionHeader confidence="0.998184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987110038461539">
Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2006. Lightly-supervised at-
tribute extraction. In Proceedings of the Advances in
Neural Information Processing Systems Workshop on
Machine Learning for Web Search.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of Ma-
chine Learning Research, 3:993–1022.
David Cohn and Huau Chang. 2000. Learning to prob-
abilistically identify authoritative documents. In Pro-
ceedings of the 17th International Conference on Ma-
chine Learning, pages 167–174.
Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese
Morphological Analyzer using Word Co-occurrence-
JTAG. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics, pages 409–413.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian sets. In Proceedings of the Advances in Neu-
ral Information Processing Systems.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning. Software available at
http://svmlight.joachims.org/.
</reference>
<page confidence="0.985666">
730
</page>
<reference confidence="0.998515587301587">
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1011–1020.
Xiao-Li Li, Bing Liu, and See-Kiong Ng. 2010. Neg-
ative Training Data can be Harmful to Text Classi-
fication. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 218–228.
Bing Liu, Wee S. Lee, Philip S. Yu, and Xiaoli Li. 2002.
Partially supervised classification of text documents.
In Proceedings of the 19th International Conference
on Machine Learning, pages 387–394.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. PLDA+: Parallel latent dirich-
let allocation with data placement and pipeline pro-
cessing. ACM Transactions on Intelligent Systems
and Technology, special issue on Large Scale Machine
Learning. Software available at http://code.
google.com/p/plda.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003–1011.
Marius Pas¸ca and Benjamin Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, pages 19–27.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 113–120.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 938–
947.
Alan Ritter and Oren Etzioni. 2010. A Latent Dirich-
let Allocation method for Selectional Preferences. In
Proceedings of the 48th ACL Conference, pages 424–
434.
Luis Sarmento, Valentin Jijkuon, Maarten de Rijke, and
Eugenio Oliveira. 2007. More like these: grow-
ing entity classes from seeds. In Proceedings of the
16th ACM Conference on Information and Knowledge
Management, pages 959–962.
Jun Suzuki, Erik McDermott, and Hideki Isozaki. 2006.
Training Conditional Random Fields with Multivari-
ate Evaluation Measures. In Proceedings of the 21st
COLING and 44th ACL Conference, pages 217–224.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using ex-
traction pattern contexts. In Proceedings of the 2002
conference on Empirical methods in natural language
processing, pages 214–221.
</reference>
<page confidence="0.998002">
731
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.491812">
<title confidence="0.997426">Entity Set Expansion using Topic information</title>
<author confidence="0.992612">Kuniko Saito Sadamitsu</author>
<author confidence="0.992612">Kenji Imamura</author>
<affiliation confidence="0.982375">NTT Cyber Space Laboratories, NTT</affiliation>
<address confidence="0.983754">1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847,</address>
<email confidence="0.9493595">saito.kuniko,kikui@cse.oka-pu.ac.jp</email>
<abstract confidence="0.954519769230769">This paper proposes three modules based on latent topics of documents for alleviating “semantic drift” in bootstrapping entity set expansion. These new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and entity candidate pruning. In this study, we model latent topics with LDA (Latent Dirichlet Allocation) in an unsupervised way. Experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2% depending on the domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Partha P Talukdar</author>
<author>Giridhar Kumaran</author>
<author>Fernando Pereira</author>
<author>Mark Liberman</author>
<author>Andrew McCallum</author>
<author>Mark Dredze</author>
</authors>
<title>Lightly-supervised attribute extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems Workshop on Machine Learning</booktitle>
<institution>for Web Search.</institution>
<contexts>
<context position="3174" citStr="Bellare et al., 2006" startWordPosition="468" endWordPosition="471">is organized as follows. In Section 2, we illustrate discriminative bootstrapping algorithms and describe their problems. Our proposal is described in Section 3 and experimental results are shown in Section 4. Related works are described in Section 5. Finally, Section 6 provides our conclusion and describes future works. 2 Problems of the previous Discriminative Bootstrapping method Some previous works introduced discriminative methods based on the logistic sigmoid classifier, which can utilize arbitrary features for the relation extraction task instead of a scoring function such as Espresso (Bellare et al., 2006; Mintz et al., 2009). Bellare et al. reported that the discriminative approach achieves better accuracy than Espresso when the number of extracted pairs is increased because 726 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726–731, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics multiple features are used to support the evidence. However, three problems exist in their methods. First, they use only local context features. The discriminative approach is useful for using arbitrary features, however</context>
<context position="5401" citStr="Bellare et al., 2006" startWordPosition="802" endWordPosition="805">ing, they used all remaining examples, other than positive examples, as negative examples. Although this is the simplest technique, it is impossible to use all of the examples provided by a large-scale corpus for discriminative training. Third, their methods discriminate all candidates for new entities. This principle increases the risk of generating many false-positive examples and is inefficient. We solve these three problems by using topic information. 3 Set expansion using Topic information 3.1 Basic bootstrapping methods In this section, we describe the basic method adopted from Bellare (Bellare et al., 2006). Our system’s configuration diagram is shown in Figure 1. In Figure 1, arrows with solid lines indicate the basic process described in this section. The other parts are described in the following sections. After N3 positive seed entities are manually given, every noun co-occurring with the seed entities is ranked by PMI scores and then selected manually as Na positive attributes. N3 and Na are predefined ba727 Figure 1: The structure of our system. sic adjustment numbers. The entity-attribute pairs are obtained by taking the cross product of seed entity lists and attribute lists. The pairs ar</context>
</contexts>
<marker>Bellare, Talukdar, Kumaran, Pereira, Liberman, McCallum, Dredze, 2006</marker>
<rawString>Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran, Fernando Pereira, Mark Liberman, Andrew McCallum, and Mark Dredze. 2006. Lightly-supervised attribute extraction. In Proceedings of the Advances in Neural Information Processing Systems Workshop on Machine Learning for Web Search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="7995" citStr="Blei et al., 2003" startWordPosition="1221" endWordPosition="1224">leviate semantic drift. In order to derive the topic information, we utilize statistical topic models, which represent the relation between documents and words through hidden topics. The topic models can calculate the posterior probability p(z|d) of topic z in document d. For example, the topic models give high probability to topic z =”cell-phone” in the above example sentences 1. This posterior probability is useful as a global feature for discrimination. The topic feature value Ot(z, e, a) is calculated as follows. In this paper, we use Latent Dirichlet Allocation (LDA) as the topic models (Blei et al., 2003). LDA represents the latent topics of the documents and the co-occurrence between each topic. In Figure 1, shaded part and the arrows with broken lines indicate our proposed method with its use of topic information including the following sections. 3.3 Negative example selection If we choose negative examples randomly, such examples are harmful for discrimination because some examples include the same contexts or topics as the positive examples. By contrast, negative examples belonging to broad genres are needed to alleviate semantic drift. We use topic information to efficiently select such n</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Huau Chang</author>
</authors>
<title>Learning to probabilistically identify authoritative documents.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<pages>167--174</pages>
<marker>Cohn, Chang, 2000</marker>
<rawString>David Cohn and Huau Chang. 2000. Learning to probabilistically identify authoritative documents. In Proceedings of the 17th International Conference on Machine Learning, pages 167–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Fuchi</author>
<author>Shinichiro Takagi</author>
</authors>
<title>Japanese Morphological Analyzer using Word Co-occurrenceJTAG.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>409--413</pages>
<contexts>
<context position="10616" citStr="Fuchi and Takagi, 1998" startWordPosition="1656" endWordPosition="1659"> At least one topic with the largest score is chosen as a positive topic when PT (z) ≤ th about all topics. After selecting this positive topic, the documents including entity candidates are removed if the posterior probability satisfy p(z|d) ≤ th for all topics z. In this paper, we set the threshold to th = 0.2. This constraint means that the topic of the document matches that of the positive entities and can be regarded as a hard constraint for topic features. 4 Experiments 4.1 Experimental Settings We use 30M Japanese blog articles crawled in May 2008. The documents were tokenized by JTAG (Fuchi and Takagi, 1998), chunked, and labeled with IREX 8 Named Entity types by CRFs using Minimum Classification Error rate (Suzuki et al., 2006), and transformed into features. The context features were defined using the template “(head) entity (mid.) attribute (tail)”. The words included in each part were used as surface, part-of-speech and Named Entity label features added position information. Maximum word number of each part was set at 2 words. The features have to appear in both the positive and negative training data at least 5 times. In the experiments, we used three domains, car (“CAR”), broadcast program </context>
</contexts>
<marker>Fuchi, Takagi, 1998</marker>
<rawString>Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese Morphological Analyzer using Word Co-occurrenceJTAG. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 409–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Katherine A Heller</author>
</authors>
<title>Bayesian sets.</title>
<date>2005</date>
<booktitle>In Proceedings of the Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1808" citStr="Ghahramani and Heller (2005)" startWordPosition="258" endWordPosition="261">ms, which iteratively acquire new entities. These algorithms suffer from the general problem of “semantic drift”. Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Pantel and Pennacchiotti (2006) proposed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with entities and attributes. Espresso alleviates semanticdrift by a sophisticated scoring system based on * Presently with Okayama Prefectural University pointwise mutual information (PMI). Thelen and Riloff (2002), Ghahramani and Heller (2005) and Sarmento et al. (2007) also proposed original score functions with the goal of reducing semantic-drift. Our purpose is also to reduce semantic drift. For achieving this goal, we use a discriminative method instead of a scoring function and incorporate topic information into it. Topic information means the genre of each document as estimated by statistical topic models. In this paper, we effectively utilize topic information in three modules: the first generates the features of the discriminative models; the second selects negative examples; the third prunes incorrect examples from candida</context>
</contexts>
<marker>Ghahramani, Heller, 2005</marker>
<rawString>Zoubin Ghahramani and Katherine A. Heller. 2005. Bayesian sets. In Proceedings of the Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical. Advances in Kernel Methods -Support Vector Learning. Software available at http://svmlight.joachims.org/.</title>
<date>1999</date>
<contexts>
<context position="11422" citStr="Joachims, 1999" startWordPosition="1793" endWordPosition="1794">sing the template “(head) entity (mid.) attribute (tail)”. The words included in each part were used as surface, part-of-speech and Named Entity label features added position information. Maximum word number of each part was set at 2 words. The features have to appear in both the positive and negative training data at least 5 times. In the experiments, we used three domains, car (“CAR”), broadcast program (“PRG”) and sports organization (“SPT”). The adjustment numbers for basic settings are N3 = 10, Na = 10, N,,, = 100. After running 10 iterations, we obtained 1000 entities in total. 5VM&amp;quot;ght (Joachims, 1999) with second order polynomial kernel was used as the discriminative model. Parallel LDA, which is LDA with MPI (Liu et al., 2011), was used for training 100 mixture topic models and inference. Training corpus for topic models consisted of the content gathered from Ot(z, e, a) = E EdED�,a p(z�|d). z&apos; E dED,,a p(z|d) 728 CAR PRG SPT 1. Baseline 0.249 0.717 0.781 2. Topic features + 1. 0.483 0.727 0.844 3. Negative selection + 2. 0.509 0.762 0.846 4. Candidate pruning + 3. 0.531 0.824 0.848 Table 1: The experimental results for the three domains. Bold font indicates that the difference between ac</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-Scale SVM Learning Practical. Advances in Kernel Methods -Support Vector Learning. Software available at http://svmlight.joachims.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mamoru Komachi</author>
<author>Taku Kudo</author>
<author>Masashi Shimbo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1011--1020</pages>
<marker>Komachi, Kudo, Shimbo, Matsumoto, 2008</marker>
<rawString>Mamoru Komachi, Taku Kudo, Masashi Shimbo, and Yuji Matsumoto. 2008. Graph-based analysis of semantic drift in Espresso-like bootstrapping algorithms. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1011–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao-Li Li</author>
<author>Bing Liu</author>
<author>See-Kiong Ng</author>
</authors>
<title>Negative Training Data can be Harmful to Text Classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>218--228</pages>
<contexts>
<context position="17796" citStr="Li et al., 2010" startWordPosition="2839" endWordPosition="2842">ods that are effective in terms of extraction, even though their clustering target is only the surrounding context. Ritter and Etzioni (2010) proposed a generative approach to use extended LDA to model selectional preferences. Although their approach is similar to ours, our approach is discriminative and so can treat arbitrary features; it is applicable to bootstrapping methods. The accurate selection of negative examples is a major problem for positive and unlabeled learning methods or general bootstrapping methods and some previous works have attempted to reach a solution (Liu et al., 2002; Li et al., 2010). However, their methods are hard to apply to the Bootstrapping algorithms because the positive seed set is too small to accurately select negative examples. Our method uses topic information to efficiently solve both the problem of extracting global information and the problem of selecting negative examples. 6 Conclusion We proposed an approach to set expansion that uses topic information in three modules and showed that it can improve expansion accuracy. The remaining problem is that the grain size of topic models is not always the same as the target domain. To resolve this problem, we will </context>
</contexts>
<marker>Li, Liu, Ng, 2010</marker>
<rawString>Xiao-Li Li, Bing Liu, and See-Kiong Ng. 2010. Negative Training Data can be Harmful to Text Classification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 218–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Wee S Lee</author>
<author>Philip S Yu</author>
<author>Xiaoli Li</author>
</authors>
<title>Partially supervised classification of text documents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Machine Learning,</booktitle>
<pages>387--394</pages>
<contexts>
<context position="17778" citStr="Liu et al., 2002" startWordPosition="2835" endWordPosition="2838">ed clustering methods that are effective in terms of extraction, even though their clustering target is only the surrounding context. Ritter and Etzioni (2010) proposed a generative approach to use extended LDA to model selectional preferences. Although their approach is similar to ours, our approach is discriminative and so can treat arbitrary features; it is applicable to bootstrapping methods. The accurate selection of negative examples is a major problem for positive and unlabeled learning methods or general bootstrapping methods and some previous works have attempted to reach a solution (Liu et al., 2002; Li et al., 2010). However, their methods are hard to apply to the Bootstrapping algorithms because the positive seed set is too small to accurately select negative examples. Our method uses topic information to efficiently solve both the problem of extracting global information and the problem of selecting negative examples. 6 Conclusion We proposed an approach to set expansion that uses topic information in three modules and showed that it can improve expansion accuracy. The remaining problem is that the grain size of topic models is not always the same as the target domain. To resolve this</context>
</contexts>
<marker>Liu, Lee, Yu, Li, 2002</marker>
<rawString>Bing Liu, Wee S. Lee, Philip S. Yu, and Xiaoli Li. 2002. Partially supervised classification of text documents. In Proceedings of the 19th International Conference on Machine Learning, pages 387–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Yuzhou Zhang</author>
<author>Edward Y Chang</author>
<author>Maosong Sun</author>
</authors>
<title>PLDA+: Parallel latent dirichlet allocation with data placement and pipeline processing.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent</journal>
<booktitle>Systems and Technology, special issue on Large Scale Machine Learning. Software available</booktitle>
<contexts>
<context position="11551" citStr="Liu et al., 2011" startWordPosition="1814" endWordPosition="1817"> and Named Entity label features added position information. Maximum word number of each part was set at 2 words. The features have to appear in both the positive and negative training data at least 5 times. In the experiments, we used three domains, car (“CAR”), broadcast program (“PRG”) and sports organization (“SPT”). The adjustment numbers for basic settings are N3 = 10, Na = 10, N,,, = 100. After running 10 iterations, we obtained 1000 entities in total. 5VM&amp;quot;ght (Joachims, 1999) with second order polynomial kernel was used as the discriminative model. Parallel LDA, which is LDA with MPI (Liu et al., 2011), was used for training 100 mixture topic models and inference. Training corpus for topic models consisted of the content gathered from Ot(z, e, a) = E EdED�,a p(z�|d). z&apos; E dED,,a p(z|d) 728 CAR PRG SPT 1. Baseline 0.249 0.717 0.781 2. Topic features + 1. 0.483 0.727 0.844 3. Negative selection + 2. 0.509 0.762 0.846 4. Candidate pruning + 3. 0.531 0.824 0.848 Table 1: The experimental results for the three domains. Bold font indicates that the difference between accuracy of the methods in the row and the previous row is significant (P &lt; 0.05 by binomial test) and italic font indicates (P &lt; 0</context>
</contexts>
<marker>Liu, Zhang, Chang, Sun, 2011</marker>
<rawString>Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and Maosong Sun. 2011. PLDA+: Parallel latent dirichlet allocation with data placement and pipeline processing. ACM Transactions on Intelligent Systems and Technology, special issue on Large Scale Machine Learning. Software available at http://code. google.com/p/plda.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="3195" citStr="Mintz et al., 2009" startWordPosition="472" endWordPosition="475">s. In Section 2, we illustrate discriminative bootstrapping algorithms and describe their problems. Our proposal is described in Section 3 and experimental results are shown in Section 4. Related works are described in Section 5. Finally, Section 6 provides our conclusion and describes future works. 2 Problems of the previous Discriminative Bootstrapping method Some previous works introduced discriminative methods based on the logistic sigmoid classifier, which can utilize arbitrary features for the relation extraction task instead of a scoring function such as Espresso (Bellare et al., 2006; Mintz et al., 2009). Bellare et al. reported that the discriminative approach achieves better accuracy than Espresso when the number of extracted pairs is increased because 726 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726–731, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics multiple features are used to support the evidence. However, three problems exist in their methods. First, they use only local context features. The discriminative approach is useful for using arbitrary features, however, they did not identi</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Weaklysupervised acquisition of open-domain classes and class attributes from web documents and query logs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<marker>Pas¸ca, Van Durme, 2008</marker>
<rawString>Marius Pas¸ca and Benjamin Van Durme. 2008. Weaklysupervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="1465" citStr="Pantel and Pennacchiotti (2006)" startWordPosition="211" endWordPosition="214">1 Introduction The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al., 2009). For example, the user inputs a few words “Apple”, “Google” and “IBM” , and the system outputs “Microsoft”, “Facebook” and “Intel”. Many set expansion algorithms are based on bootstrapping algorithms, which iteratively acquire new entities. These algorithms suffer from the general problem of “semantic drift”. Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Pantel and Pennacchiotti (2006) proposed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with entities and attributes. Espresso alleviates semanticdrift by a sophisticated scoring system based on * Presently with Okayama Prefectural University pointwise mutual information (PMI). Thelen and Riloff (2002), Ghahramani and Heller (2005) and Sarmento et al. (2007) also proposed original score functions with the goal of reducing semantic-drift. Our purpose is also to reduce semantic drift. For achieving this goal, we use a discriminative method instead of a scoring function and incorporate </context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>938--947</pages>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 938– 947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
</authors>
<title>A Latent Dirichlet Allocation method for Selectional Preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th ACL Conference,</booktitle>
<pages>424--434</pages>
<contexts>
<context position="17321" citStr="Ritter and Etzioni (2010)" startWordPosition="2763" endWordPosition="2766">t PT score) shujii (attending doctor) choko (diary of hooking) hoshitsu (moisture retention) Table 2: The characteristic words belonging to three topics, zh, zl and ze. zh is the nearest topic and zl is the farthest topic for positive entity-attribute seed pairs. ze is an effective negative topic for eliminating “drifted entities” extracted by the baseline system. are clustering methods based on probabilistic measures. By contrast, Pas¸ca and Durme (2008) proposed clustering methods that are effective in terms of extraction, even though their clustering target is only the surrounding context. Ritter and Etzioni (2010) proposed a generative approach to use extended LDA to model selectional preferences. Although their approach is similar to ours, our approach is discriminative and so can treat arbitrary features; it is applicable to bootstrapping methods. The accurate selection of negative examples is a major problem for positive and unlabeled learning methods or general bootstrapping methods and some previous works have attempted to reach a solution (Liu et al., 2002; Li et al., 2010). However, their methods are hard to apply to the Bootstrapping algorithms because the positive seed set is too small to accu</context>
</contexts>
<marker>Ritter, Etzioni, 2010</marker>
<rawString>Alan Ritter and Oren Etzioni. 2010. A Latent Dirichlet Allocation method for Selectional Preferences. In Proceedings of the 48th ACL Conference, pages 424– 434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Sarmento</author>
<author>Valentin Jijkuon</author>
<author>Maarten de Rijke</author>
<author>Eugenio Oliveira</author>
</authors>
<title>More like these: growing entity classes from seeds.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>959--962</pages>
<marker>Sarmento, Jijkuon, de Rijke, Oliveira, 2007</marker>
<rawString>Luis Sarmento, Valentin Jijkuon, Maarten de Rijke, and Eugenio Oliveira. 2007. More like these: growing entity classes from seeds. In Proceedings of the 16th ACM Conference on Information and Knowledge Management, pages 959–962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Erik McDermott</author>
<author>Hideki Isozaki</author>
</authors>
<title>Training Conditional Random Fields with Multivariate Evaluation Measures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING and 44th ACL Conference,</booktitle>
<pages>217--224</pages>
<contexts>
<context position="10739" citStr="Suzuki et al., 2006" startWordPosition="1677" endWordPosition="1680">is positive topic, the documents including entity candidates are removed if the posterior probability satisfy p(z|d) ≤ th for all topics z. In this paper, we set the threshold to th = 0.2. This constraint means that the topic of the document matches that of the positive entities and can be regarded as a hard constraint for topic features. 4 Experiments 4.1 Experimental Settings We use 30M Japanese blog articles crawled in May 2008. The documents were tokenized by JTAG (Fuchi and Takagi, 1998), chunked, and labeled with IREX 8 Named Entity types by CRFs using Minimum Classification Error rate (Suzuki et al., 2006), and transformed into features. The context features were defined using the template “(head) entity (mid.) attribute (tail)”. The words included in each part were used as surface, part-of-speech and Named Entity label features added position information. Maximum word number of each part was set at 2 words. The features have to appear in both the positive and negative training data at least 5 times. In the experiments, we used three domains, car (“CAR”), broadcast program (“PRG”) and sports organization (“SPT”). The adjustment numbers for basic settings are N3 = 10, Na = 10, N,,, = 100. After </context>
</contexts>
<marker>Suzuki, McDermott, Isozaki, 2006</marker>
<rawString>Jun Suzuki, Erik McDermott, and Hideki Isozaki. 2006. Training Conditional Random Fields with Multivariate Evaluation Measures. In Proceedings of the 21st COLING and 44th ACL Conference, pages 217–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Thelen</author>
<author>Ellen Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 conference on Empirical methods in natural language processing,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="1778" citStr="Thelen and Riloff (2002)" startWordPosition="254" endWordPosition="257"> on bootstrapping algorithms, which iteratively acquire new entities. These algorithms suffer from the general problem of “semantic drift”. Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Pantel and Pennacchiotti (2006) proposed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with entities and attributes. Espresso alleviates semanticdrift by a sophisticated scoring system based on * Presently with Okayama Prefectural University pointwise mutual information (PMI). Thelen and Riloff (2002), Ghahramani and Heller (2005) and Sarmento et al. (2007) also proposed original score functions with the goal of reducing semantic-drift. Our purpose is also to reduce semantic drift. For achieving this goal, we use a discriminative method instead of a scoring function and incorporate topic information into it. Topic information means the genre of each document as estimated by statistical topic models. In this paper, we effectively utilize topic information in three modules: the first generates the features of the discriminative models; the second selects negative examples; the third prunes i</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>Michael Thelen and Ellen Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings of the 2002 conference on Empirical methods in natural language processing, pages 214–221.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>