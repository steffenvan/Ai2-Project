<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.994008">
An Infinite Hierarchical Bayesian Model of Phrasal Translation
</title>
<author confidence="0.999061">
Trevor Cohn
</author>
<affiliation confidence="0.984912">
Department of Computer Science
The University of Sheffield
</affiliation>
<address confidence="0.361607">
Sheffield, United Kingdom
</address>
<email confidence="0.993257">
t.cohn@sheffield.ac.uk
</email>
<author confidence="0.990594">
Gholamreza Haffari
</author>
<affiliation confidence="0.958318333333333">
Faculty of Information Technology
Monash University
Clayton, Australia
</affiliation>
<email confidence="0.99853">
reza@monash.edu
</email>
<sectionHeader confidence="0.993897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999735095238095">
Modern phrase-based machine translation
systems make extensive use of word-
based translation models for inducing
alignments from parallel corpora. This
is problematic, as the systems are inca-
pable of accurately modelling many trans-
lation phenomena that do not decompose
into word-for-word translation. This pa-
per presents a novel method for induc-
ing phrase-based translation units directly
from parallel data, which we frame as
learning an inverse transduction grammar
(ITG) using a recursive Bayesian prior.
Overall this leads to a model which learns
translations of entire sentences, while also
learning their decomposition into smaller
units (phrase-pairs) recursively, terminat-
ing at word translations. Our experiments
on Arabic, Urdu and Farsi to English
demonstrate improvements over competi-
tive baseline systems.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983929824562">
The phrase-based approach (Koehn et al., 2003)
to machine translation (MT) has transformed MT
from a narrow research topic into a truly useful
technology to end users. Leading translation sys-
tems (Chiang, 2007; Koehn et al., 2007; Marcu et
al., 2006) all use some kind of multi-word transla-
tion unit, which allows translations to be produced
from large canned units of text from the training
corpus. Larger phrases allow for the lexical con-
text to be considered in choosing the translation,
and also limit the number of reordering decisions
required to produce a full translation.
Word-based translation models (Brown et al.,
1993) remain central to phrase-based model train-
ing, where they are used to infer word-level align-
ments from sentence aligned parallel data, from
which phrasal translation units are extracted us-
ing a heuristic. Although this approach demon-
strably works, it suffers from a number of short-
comings. Firstly, many phrase-based phenomena
which do not decompose into word translations
(e.g., idioms) will be missed, as the underlying
word-based alignment model is unlikely to pro-
pose the correct alignments. Secondly, the rela-
tionship between different phrase-pairs is not con-
sidered, such as between single word translations
and larger multi-word phrase-pairs or where one
large phrase-pair subsumes another.
This paper develops a phrase-based translation
model which aims to address the above short-
comings of the phrase-based translation pipeline.
Specifically, we formulate translation using in-
verse transduction grammar (ITG), and seek to
learn an ITG from parallel corpora. The novelty
of our approach is that we develop a Bayesian
prior over the grammar, such that a nontermi-
nal becomes a ‘cache’ learning each production
and its complete yield, which in turn is recur-
sively composed of its child constituents. This is
closely related to adaptor grammars (Johnson et
al., 2007a), which also generate full tree rewrites
in a monolingual setting. Our model learns trans-
lations of entire sentences while also learning their
decomposition into smaller units (phrase-pairs) re-
cursively, terminating at word translations. The
model is richly parameterised, such that it can de-
scribe phrase-based phenomena while also explic-
itly modelling the relationships between phrase-
pairs and their component expansions, thus ame-
liorating the disconnect between the treatment of
words versus phrases in the current MT pipeline.
We develop a Bayesian approach using a Pitman-
Yor process prior, which is capable of modelling
a diverse range of geometrically decaying distri-
butions over infinite event spaces (here translation
phrase-pairs), an approach shown to be state of the
art for language modelling (Teh, 2006).
</bodyText>
<page confidence="0.950976">
780
</page>
<note confidence="0.9136245">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999945333333333">
We are not the first to consider this idea; Neu-
big et al. (2011) developed a similar approach for
learning an ITG using a form of Pitman-Yor adap-
tor grammar. However Neubig et al.’s work was
flawed in a number of respects, most notably in
terms of their heuristic beam sampling algorithm
which does not meet either of the Markov Chain
Monte Carlo criteria of ergodicity or detailed bal-
ance. Consequently their approach does not con-
stitute a valid Bayesian model. In contrast, this
paper provides a more rigorous and theoretically
sound method. Moreover our approach results in
consistent translation improvements across a num-
ber of translation tasks compared to Neubig et al.’s
method, and a competitive phrase-based baseline.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999968611764706">
Inversion transduction grammar (or ITG) (Wu,
1997) is a well studied synchronous grammar for-
malism. Terminal productions of the form X →
e/f generate a word in two languages, and non-
terminal productions allow phrasal movement in
the translation process. Straight productions, de-
noted by their non-terminals inside square brack-
ets [...], generate their symbols in the given or-
der in both languages, while inverted productions,
indicated by angled brackets (...), generate their
symbols in the reverse order in the target language.
In the context of machine translation, ITG
has been explored for statistical word alignment
in both unsupervised (Zhang and Gildea, 2005;
Cherry and Lin, 2007; Zhang et al., 2008; Pauls et
al., 2010) and supervised (Haghighi et al., 2009;
Cherry and Lin, 2006) settings, and for decoding
(Petrov et al., 2008). Our paper fits into the re-
cent line of work for jointly inducing the phrase ta-
ble and word alignment (DeNero and Klein, 2010;
Neubig et al., 2011). The work of DeNero and
Klein (2010) presents a supervised approach to
this problem, whereas our work is unsupervised
hence more closely related to Neubig et al. (2011)
which we describe in detail below.
A number of other approaches have been de-
veloped for learning phrase-based models from
bilingual data, starting with Marcu and Wong
(2002) who developed an extension to IBM model
1 to handle multi-word units. This pioneer-
ing approach suffered from intractable inference
and moreover, suffers from degenerate solutions
(DeNero and Klein, 2010). Our approach is simi-
lar to these previous works, except that we impose
additional constraints on how phrase-pairs can be
tiled to produce a sentence pair, and moreover,
we seek to model the embedding of phrase-pairs
in one another, something not considered by this
prior work. Another strand of related research is
in estimating a broader class of synchronous gram-
mars than ITGs, such as SCFGs (Blunsom et al.,
2009b; Levenberg et al., 2012). Conceptually, our
work could be readily adapted to general SCFGs
using similar techniques.
This work was inspired by adaptor grammars
(Johnson et al., 2007a), a monolingual grammar
formalism whereby a non-terminal rewrites in a
single step as a complete subtree. The model prior
allows for trees to be generated as a mixture of a
cache and a base adaptor grammar. In our case,
we have generalised to a bilingual setting using an
ITG. Additionally, we have extended the model to
allow recursive nesting of adapted non-terminals,
such that we end up with an infinitely recursive
formulation where the top-level and base distribu-
tions are explicitly linked together.
As mentioned above, ours is not the first work
attempting to generalise adaptor grammars for ma-
chine translation; (Neubig et al., 2011) also devel-
oped a similar approach based around ITG using a
Pitman-Yor Process prior. Our approach improves
upon theirs in terms of the model and inference,
and critically, this is borne out in our experiments
where we show uniform improvements in transla-
tion quality over a baseline system, as compared
to their almost entirely negative results. We be-
lieve that their approach had a number of flaws:
For inference they use a beam-search, which may
speed up processing but means that they are no
longer sampling from the true distribution, nor a
distribution with the same support as the posterior.
Moreover they include a Metropolis-Hastings cor-
rection step, which is required to correct the sam-
ples to account for repeated substructures which
will be otherwise underrepresented. Consequently
their approach does not constitute a Markov Chain
Monte Carlo sampler, but rather a complex heuris-
tic.
The other respect in which this work differs
from Neubig et al. (2011) is in terms of model for-
mulation. They develop an ITG which generates
phrase-pairs as terminals, while we employ a more
restrictive word-based model which forces the de-
composition of every phrase-pair. This is an im-
portant restriction as it means that we jointly learn
</bodyText>
<page confidence="0.995773">
781
</page>
<bodyText confidence="0.999955714285714">
a word and phrase based model, such that word
based phenomena can affect the phrasal struc-
tures. Finally our approach models separately the
three different types of ITG production (mono-
tone, swap and lexical emission), allowing for a
richer parameterisation which the model exploits
by learning different hyper-parameter values.
</bodyText>
<sectionHeader confidence="0.991747" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9977915">
The generative process of the model follows that
of ITG with the following simple grammar
</bodyText>
<equation confidence="0.99948">
X → [X X] |hX Xi
X → e/f  |e/⊥  |⊥/f ,
</equation>
<bodyText confidence="0.999785230769231">
where [·] denotes monotone ordering and h·i de-
notes a swap in one language. The symbol ⊥ de-
notes the empty string. This corresponds to a sim-
ple generative story, with each stage being a non-
terminal rewrite starting with X and terminating
when there are no frontier non-terminals.
A popular variant is a phrasal ITG, where the
leaves of the ITG tree are phrase-pairs and the
training seeks to learn a segmentation of the source
and target which yields good phrases. We would
not expect this model to do very well as it cannot
consider overlapping phrases, but instead is forced
into selecting between many competing – and of-
ten equally viable – options. Our approach im-
proves over the phrasal model by recursively gen-
erating complete phrases. This way we don’t insist
on a single tiling of phrases for a sentence pair, but
explicitly model the set of hierarchically nested
phrases as defined by an ITG derivation. This ap-
proach is closer in spirit to the phrase-extraction
heuristic, which defines a set of ‘atomic’ terminal
phrase-pairs and then extracts every combination
of these atomic phase-pairs which is contiguous in
the source and target.1
The generative process is that we draw a com-
plete ITG tree, t ∼ P2(·), as follows:
</bodyText>
<listItem confidence="0.984659142857143">
1. choose the rule type, r ∼ R, where r ∈
{mono, swap, emit}
2. for r =mono
(a) draw the complete subtree expansion,
t = X → [...] ∼ TM
3. for r = swap
(a) draw the complete subtree expansion,
</listItem>
<equation confidence="0.635025">
t = X → h...i ∼ TS
</equation>
<footnote confidence="0.808203">
1Our technique considers the subset of phrase-pairs which
are consistent with the ITG tree.
</footnote>
<listItem confidence="0.902907">
4. for r = emit
(a) draw a pair of strings, (e, f) ∼ E
(b) set t = X → e/f
</listItem>
<bodyText confidence="0.999925933333333">
Note that we split the problem of drawing a tree
into two steps: first choosing the top-level rule
type and then drawing a rule of that type. This
gives us greater control than simply drawing a tree
of any type from one distribution, due to our pa-
rameterisation of the priors over the model param-
eters TM, TS and E.
To complete the generative story, we need to
specify the prior distributions for TM, TS and
E. First, we deal with the emission distribu-
tion, E which we drawn from a Dirichlet Pro-
cess prior E ∼ DP(bE, P0). We restrict the emis-
sion rules to generate word pairs rather than phrase
pairs.2 For the base distribution, P0, we use a sim-
ple uniform distribution over word pairs,
</bodyText>
<equation confidence="0.9842795">
η2 1 e =6 ⊥, f =6 ⊥
VEVF
η(1 − η)VF1 e=⊥,f=6⊥
η(1−η)VE 1 e =6 ⊥, f = ⊥
</equation>
<bodyText confidence="0.955117444444444">
where the constant η denotes the binomial proba-
bility of a word being aligned.3
We use Pitman-Yor Process priors for the TM
and TS parameters
TM ∼ PYP(aM, bM, P1(·|r = mono))
TS ∼ PYP(aS, bS, P1(·|r = swap))
where P1(t1, t2|r) is a distribution over a pair of
trees (the left and right children of a monotone or
swap production). P1 is defined as follows:
</bodyText>
<listItem confidence="0.994548">
1. choose the complete left subtree t1 ∼ P2,
2. choose the complete right subtree t2 ∼ P2,
3. set t = X → [t1 t2] or t = X → ht1 t2i
depending on r
</listItem>
<bodyText confidence="0.99382425">
This generative process is mutually recursive: P2
makes draws from P1 and P1 makes draws from
P2. The recursion is terminated when the rule type
r = emit is drawn.
Following standard practice in Bayesian mod-
els, we integrate out R, TM, TS and E. This
means draws from P2 (or P1) are no longer iid:
for any non-trivial tree, computing its probabil-
ity under this model is complicated by the fact
2Note that we could allow phrases here, but given the
model can already reason over phrases by way of its hier-
archical formulation, this is an unnecessary complication.
3We also experimented with using word translation prob-
abilities from IBM model 1, based on the prior used by Lev-
enberg et al. (2012), however we found little empirical differ-
ence compared with this simpler uniform model.
</bodyText>
<equation confidence="0.9406346">
⎧
⎨⎪
⎪⎩
P0(e,f) =
,
</equation>
<page confidence="0.974457">
782
</page>
<bodyText confidence="0.99995796">
that the probability of its two subtrees are inter-
dependent. This is best understood in terms of
the Chinese Restaurant Franchise (CRF; Teh et al.
(2006)), which describes the posterior distribution
after integrating out the model parameters. In our
case we can consider the process of drawing a tree
from P2 as a customer entering a restaurant and
choosing where to sit, from an infinite set of ta-
bles. The seating decision is based on the number
of other customers at each table, such that popular
tables are more likely to be joined than unpopular
or empty ones. If the customer chooses an occu-
pied table, the identity of the tree is then set to
be the same as for the other customers also seated
there. For empty tables the tree must be sampled
from the base distribution P1. In the standard CRF
analogy, this leads to another customer entering
the restaurant one step up in the hierarchy, and
this process can be chained many times. In our
case, however, every new table leads to new cus-
tomers reentering the original restaurant – these
correspond to the left and right child trees of a
monotone or swap rule. The recursion terminates
when a table is shared, or a new table is labelled
with a emit rule.
</bodyText>
<subsectionHeader confidence="0.957968">
3.1 Inference
</subsectionHeader>
<bodyText confidence="0.99982">
The probability of a tree (i.e., a draw from P2) un-
der the model is
</bodyText>
<equation confidence="0.999908">
P2(t) = P(r)P2(t|r) (1)
</equation>
<bodyText confidence="0.99986">
where r is the rule type, one of mono, swap or
emit. The distribution over types, P(r), is de-
fined as
</bodyText>
<equation confidence="0.9998015">
P(r) = nr + bT 3
nT,− + bT
</equation>
<bodyText confidence="0.999714">
where nT,− are the counts over rules of types.4
The second component in (1), P2(t|r), is de-
fined separately for each rule type. For r = mono
or r = swap rules, it is defined as
where n−t,r is the count for tree t in the other train-
ing sentences, K−t,r is the table count for t and n− r
</bodyText>
<footnote confidence="0.7701515">
4The conditioning on event and table counts, n−, K− is
omitted for clarity.
</footnote>
<bodyText confidence="0.999609666666667">
and K−r are the total count of trees and tables, re-
spectively. Finally, the probability for r = emit
is given by
</bodyText>
<equation confidence="0.996406">
n−t,E + bEP0(e,f)
,
n−r + br
</equation>
<bodyText confidence="0.987746333333333">
where t = X -+ e/f.
To complete the derivation we still need to de-
fine P1, which is formulated as
</bodyText>
<equation confidence="0.997665">
P1(t1,t2) = P2(t1)P2(t2|t1),
</equation>
<bodyText confidence="0.999988138888889">
where the conditioning of the second recursive call
to P2 reflects that the counts n− and K− may
be affected by the first draw from P2. Although
these two draws are assumed iid in the prior, after
marginalising out T they are no longer indepen-
dent. For this reason, evaluating P2(t) is computa-
tionally expensive, requiring tracking of repeated
substructures in descendent sub-trees of t, which
may affect other descendants. This results in an
asymptotic complexity exponential in the number
of nodes in the tree. For this reason we consider
trees annotated with binary values denoting their
table assignment, namely whether they share a ta-
ble or are seated alone. Given this, the calculation
is greatly simplified, and has linear complexity.5
We construct an approximating ITG following
the technique used for sampling trees from mono-
lingual tree-substitution grammars (Cohn et al.,
2010). To do so we encode the first term from
(2) separately from the second term (correspond-
ing to draws from P1). Summing together these
two alternate paths – i.e., during inside inference –
we recover P2 as shown in (2). The full grammar
transform for inside inference is shown in Table 1.
The sampling algorithm closely follows the
process for sampling derivations from Bayesian
PCFGs (Johnson et al., 2007b). For each sentence-
pair, we first decrement the counts associated with
its current tree, and then sample a new deriva-
tion. This involves first constructing the inside
lattice using the productions in Table 1, and then
performing a top-down sampling pass. After
sampling each derivation from the approximating
grammar, we then convert this into its correspond-
ing ITG tree, which we then score with the full
model and accept or reject the sample using the
</bodyText>
<footnote confidence="0.9470435">
5To support this computation, we track explicit table as-
signments for every training tree and their component sub-
trees. We also sample trees labelled with seating indicator
variables.
</footnote>
<equation confidence="0.977585">
n
P2(t|r) =
− K−t,rar +
r + br
n −
P1(t1, t2|r) ,
(2)
K−r ar+br
n−r + br
−
t,r
P2(t|r = emit) =
783
X M P(r = mono)
X 5 P(r = swap)
X E P(r = emit)
For every tree, t, of type r = mono, with nt,M &gt; 0:
sig(t) --� yield(t) 1
For every tree, t, of type r = swap, with nt,S &gt; 0:
sig(t) --� yield(t) 1
</equation>
<bodyText confidence="0.998988">
For every word pair, e/f in sentence pair,
where one of e, f can be 1:
</bodyText>
<equation confidence="0.875679">
E --� e/f P2(t)
</equation>
<tableCaption confidence="0.893511">
Table 1: Grammar transformation rules for MAP
</tableCaption>
<bodyText confidence="0.944049444444444">
inside inference. The function sig(t) returns a
unique identifier for the complete tree t, and
the function yield(t) returns the pair of terminal
strings from the yield of t.
Metropolis-Hastings algorithm.6 Accepted sam-
ples then replace the old tree (otherwise the old
tree is retained) and the model counts are incre-
mented. This process is then repeated for each
sentence pair in the corpus in a random order.
</bodyText>
<sectionHeader confidence="0.999702" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998614">
Datasets We train our model across three
language pairs: Urdu-+English (UR-EN),
Farsi-+English (FA-EN), and Arabic-+English
(AR-EN). The corpora statistics of these trans-
lation tasks are summarised in Table 2. The
UR-EN corpus comes from NIST 2009 translation
evaluation.7 The AR-EN training data consists
of the eTIRR corpus (LDC2004E72), the Ara-
bic news corpus (LDC2004T17), the Ummah
corpus (LDC2004T18), and the sentences with
confidence c &gt; 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For FA-EN, we use TEP8 Tehran English-Persian
Parallel corpus (Pilevar and Faili, 2011), which
consists of conversational/informal text extracted
</bodyText>
<footnote confidence="0.960267142857143">
6The full model differs from the approximating grammar
in that it accounts for inter-dependencies between subtrees
by recursively tracking the changes in the customer and table
counts while scoring the tree. Around 98% of samples were
accepted in our experiments.
7http://www.itl.nist.gov/iad/mig/tests/mt/2009
8http://ece.ut.ac.ir/NLP/resources.htm
</footnote>
<table confidence="0.99925475">
source target sentences
UR-EN 745K 575K 148K
FA-EN 4.7M 4.4M 498K
AR-EN 1.94M 2.08M 113K
</table>
<tableCaption confidence="0.861890666666667">
Table 2: Corpora statistics showing numbers of
parallel sentences and source and target words for
the training sets.
</tableCaption>
<bodyText confidence="0.999782486486486">
from 1600 movie subtitles. We tokenized this
corpus, removed noisy single-word sentences,
randomly selected the development and test sets,
and used the rest of the corpus as the training set.
We discard sentences with length above 30 from
the datasets for all experiments.9
Sampler configuration Samplers are initialised
with trees created from GIZA++ alignments
constructed using a SCFG factorisation method
(Blunsom et al., 2009a). This algorithm repre-
sents the translation of a sentence as a large SCFG
rule, which it then factorises into lower rank SCFG
rules, a process akin to rule binarisation com-
monly used in SCFG decoding. Rules that can-
not be reduced to a rank-2 SCFG are simplified
by dropping alignment edges until they can be
factorised, the net result being an ITG derivation
largely respecting the alignments.10
The blocked sampler was run 1000 iterations
for UR-EN, 100 iterations for FA-EN and AR-
EN. After each full sampling iteration, we resam-
ple all the hyper-parameters using slice-sampling,
with the following priors: a — Beta(1,1),
b — Gamma(10, 0.1). Figure 1 shows the poste-
rior probability improves with each full sampling
iterations. The alignment probability was set to
η = 0.99. The sampling was repeated for 5 in-
dependent runs, and we present results where we
combine the outputs of these runs. This is a form
of Monte Carlo integration which allows us to rep-
resent the uncertainty in the posterior, while also
representing multiple modes, if present.
The time complexity of our inference algorithm
is O(n6), which can be prohibitive for large scale
machine translation tasks. We reduce the com-
plexity by constraining the inside inference to
consider only derivations which are compatible
</bodyText>
<footnote confidence="0.982758">
9Hence the BLEU scores we get for the baselines may
appear lower than what reported in the literature.
10Using the factorised alignments directly in a translation
system resulted in a slight loss in BLEU versus using the un-
factorised alignments. Our baseline system uses the latter.
</footnote>
<figure confidence="0.982331571428572">
K− MaM+bM
M � [XX]
n−M+bM
K−S aS+bS
Base
5 --� (XX)
n−S +bS
nt,M
M � sig(t)
n−M+bM
−K−t,Mar
n− t,S−K− t,SaS
5 � sig(t)
n−S +bS
Type
Count
Emit
784
log posterior −9100000 −9050000 −9000000 −8950000
0 100 200 300 400 500
iteration
</figure>
<figureCaption confidence="0.9622295">
Figure 1: Training progress on the UR-EN corpus,
showing the posterior probability improving with
each full sampling iteration. Different colours de-
note independent sampling runs.
</figureCaption>
<figure confidence="0.9689685">
0 5 10 15 20 25 30
average sentence length
</figure>
<figureCaption confidence="0.997948">
Figure 2: The runtime cost of bottom-up inside in-
</figureCaption>
<bodyText confidence="0.962984833333333">
ference and top-down sampling as a function of
sentence length (UR-EN), with time shown on a
logarithmic scale. Full ITG inference is shown
with red circles, and restricted inference using the
intersection constraints with blue triangles. The
average time complexity for the latter is roughly
O(l4), as plotted in green t = 2 × 10−7l4.
with high confidence alignments from GIZA++.11
Figure 2 shows the sampling time with respect
to the average sentence length, showing that our
alignment-constrained sampling algorithm is bet-
ter than the unconstrained algorithm with empir-
ical complexity of n4. However, the time com-
plexity is still high, so we set the maximum sen-
tence length to 30 to keep our experiments practi-
cable. Presumably other means of inference may
be more efficient, such as Gibbs sampling (Lev-
enberg et al., 2012) or auxiliary variable sampling
(Blunsom and Cohn, 2010); we leave these exten-
sions to future work.
Baselines. Following (Levenberg et al., 2012;
Neubig et al., 2011), we evaluate our model by
using its output word alignments to construct a
phrase table. As a baseline, we train a phrase-
based model using the moses toolkit12 based on
the word alignments obtained using GIZA++ in
both directions and symmetrized using the grow-
diag-final-and heuristic13 (Koehn et al., 2003).
This alignment is used as input to the rule fac-
torisation algorithm, producing the ITG trees with
which we initialise our sampler. To put our results
in the context of the previous work, we also com-
pare against pialign (Neubig et al., 2011), an ITG
algorithm using a Pitman-Yor process prior, as de-
scribed in Section 2.14
In the end-to-end MT pipeline we use a stan-
dard set of features: relative-frequency and lexical
translation model probabilities in both directions;
distance-based distortion model; language model
and word count. We set the distortion limit to
6 and max-phrase-length to 7 in all experiments.
We train 3-gram language models using modified
Kneser-Ney smoothing. For AR-EN experiments
the language model is trained on English data as
(Blunsom et al., 2009a), and for FA-EN and UR-
EN the English data are the target sides of the
bilingual training data. We use minimum error
rate training (Och, 2003) with nbest list size 100
to optimize the feature weights for maximum de-
velopment BLEU.
11These are taken from the final model 4 word alignments,
using the intersection of the source-target and target-source
models. These alignments are very high precision (but have
low recall), and therefore are unlikely to harm the model.
</bodyText>
<footnote confidence="0.94725675">
12http://www.statmt.org/moses
13We use the default parameter settings in both moses and
GIZA++.
14http://www.phontron.com/pialign
</footnote>
<figure confidence="0.992231735294118">
time (s)
1e−05 1e−03 1e−01
●
● ●
● ● ●
● ● ● ●
●
●
● ● ●
●
●●
●● ●
●
●●●●●
●●
●● ●
● ● ●●
● ● ●
● ●
● ● ●
●
●
●
●
●
●● ● ●● ●
●●●●●●●●●●●●
●●●●●●●●●●
●●●
●
●● ● ●
●●
●● ● ●
●
</figure>
<page confidence="0.991495">
785
</page>
<table confidence="0.998642555555556">
Baselines This paper
GIZA++ pialign individual combination
UR-EN 16.95 15.65 16.68 ± .12 16.97
FA-EN 20.69 21.41 21.36 ± .17 21.50
MT03 44.05 43.30 44.8 ± .28 45.10
MT04 38.15 37.78 38.4 ± .08 38.4
AR-EN
MT05 42.81 42.18 43.13 ± .23 43.45
MT08 32.43 33.00 32.7 ± .15 32.80
</table>
<tableCaption confidence="0.999128">
Table 3: The BLEU scores for the translation tasks of three language pairs. The individual column show
</tableCaption>
<bodyText confidence="0.593798666666667">
the average and 95% confidence intervals for 5 independent runs, whereas the combination column show
the results for combining the phrase tables of all these runs. The baselines are GIZA++ alignments and
those generated by the pialign (Neubig et al., 2011) bold: the best result.
</bodyText>
<figure confidence="0.371342">
rule frequency
</figure>
<figureCaption confidence="0.9924395">
Figure 3: Fraction of rules with a given frequency,
using a single sample grammar (UR-EN).
</figureCaption>
<subsectionHeader confidence="0.807856">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999986304347826">
Table 3 shows the BLEU scores for the three trans-
lation tasks UR/AR/FA→EN based on our method
against the baselines. For our models, we report
the average BLEU score of the 5 independent runs
as well as that of the aggregate phrase table gen-
erated by these 5 independent runs. There are
a number of interesting observations in Table 3.
Firstly, combining the phrase tables from indepen-
dent runs results in increased BLEU scores, possi-
bly due to the representation of uncertainty in the
outputs, and the representation of different modes
captured by the individual models. We believe this
type of Monte Carlo model averaging should be
considered in general when sampling techniques
are employed for grammatical inference, e.g. in
parsing and translation. Secondly, our approach
consistently improves over the Giza++ baseline
often by a large margin, whereas pialign under-
performs the GIZA++ baseline in many cases.
Thirdly, our model consistently outperforms pi-
align (except in AR-EN MT08 which is very
close). This highlights the modeling and inference
differences between our method and the pialign.
</bodyText>
<sectionHeader confidence="0.985947" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999898633333333">
In this section, we present some insights about the
learned grammar and the model hyper-parameters.
Firstly, we start by presenting various statistics
about different learned grammars. Figure 3 shows
the fraction of rules with a given frequency for
each of the three rule types. The three types of rule
exhibit differing amounts of high versus low fre-
quency rules, and all roughly follow power laws.
As expected, there is a higher tendency to reuse
high-frequency emissions (or single-word transla-
tion) compared to other rule types, which are the
basic building blocks to compose larger rules (or
phrases). Table 4 lists the high frequency mono-
tone and swap rules in the learned grammar. We
observe the high frequency swap rules capture re-
ordering in verb clusters, preposition-noun inver-
sions and adjective-noun reordering. Similar pat-
terns are seen in the monotone rules, along with
some common canned phrases. Note that “in Iraq”
appears twice, once as an inversion in UR-EN and
another time in monotone order for AR-EN.
Secondly, we analyse the values learned for
the model hyper-parameters; Figure 4.(a) shows
the posterior distribution over the hyper-parameter
values. There is very little spread in the inferred
values, suggesting the sampling chains may have
converged. Furthermore, there is a large differ-
ence between the learned hyper-parameters for the
monotone rules versus the swap rules. For the
Pitman-Yor Process prior, the values of the hyper-
</bodyText>
<figure confidence="0.950473666666667">
1 2 5 10 20 50 100
fraction of grammar
1e−05 1e−03 1e−01
monotone
swap
emit
</figure>
<page confidence="0.836275">
786
</page>
<bodyText confidence="0.826933">
FARSI-ENGLISH
pialign sure/mTm n,chief/r vs, vou sure/mTm nv, im sure/mTm nm, boss/r vs, make sure/mTm n,
are vou sure/mTm nv, anvwav/hr HAl, president/r vs jmhwr, not sure/mTm n nvstm, im
sure/mTm nm kh, i&apos;m sure/mTm nm, sure/mTm nA
our method sure/mTm }, have/dA $ th, be/b $, have/dA $ th bA $, let me/ * Ar, because of/xATr,
sure/mTm } n, do/kAr rA, come on/zwd bA, excuse me/bbx, kill/rA bk, come on/zwdbA,
more than/$ tr, behind/p $, what do/mnZwrt, what do vou/mnZwrt , kill/k $, dont
worrv/ngrAn nbA, is it/$ dh, welcome/xw $ 1, chief/r } vs, make sure/mTm, is/mv $,
make sure/mTm }, make sure/mTm} n, im sorrv/bbx, left/g * A, if/Agr $
</bodyText>
<figure confidence="0.6287755">
ARABIC-ENGLISH
pialign said ./ƛāƈ -ć, states/ƴĔŔłƢƛā, united/łāķāƛćƛā, al-wafd/I ů Ĕžćƛā, efforts/ƛĕŁ, of mass
</figure>
<figureCaption confidence="0.716879111111111">
destruction/ƛƢāŦƛā ĢāƢĔƛā, voum/Ƣćķƛā, jintao/ćāł Ʀķœ, alam al voum/Ƣćķƛā Ƣƛāŷƛā, al-ittihad
,/ĔāŔłāƛā, the field of/ƛāœƢ, islamabad/Ƣāƛťā, scheduled/ĢĢƈƢƛā ƦƢ, al-alam al-voum/Ƣćķƛā Ƣƛāŷƛā
ů, / al-havat/ů ƴāķŔƛā, peninsula/ƴĢķģœƛā ƱŁŦ, meanwhile/ƱťžƦ łƈćƛā, prime/ƹāĢģć, the
invitation/ƦƢ ƴćŷĔ, / al-havat ,/ů ƴāķŔƛā, korean peninsula/ƴķĢćƋƛā ƴĢķģœƛā ƱŁŦ, al-nahar ”/ĢāƱƦƛā
DD, department/ƴķœĢāŕƛā ƴĢāģć, cote/łćƋ, as possible/ƦƋƢƢ łƈć, al alam al voum/Ƣćķƛā Ƣƛāŷƛā, ,
al-alam al-voum ,/Ƣćķƛā Ƣƛāŷƛā, at the invitation/ƦƢ ƴćŷĔ -Ł, jacques/Ƌāœ ķťƦĢžƛā, well as/āĕƋ,
points/ķƛķ, vladimir putin/ƦķłćŁ ĢķƢķĔāƛž ķťćĢƛā, george w. bush/œĢćœ
our method the united/ƴĔŔłƢƛā, us dollars/ķƋķĢƢā, prime/ƹāĢģćƛā ťķĶĢ, china &apos;/Ʀķůƛā, spokesman/ŃĔŔłƢƛā
-Ł, manv/ƦƢ, is expected/ŷƈćłƢƛā, is expected to/ŷƈćłƢƛā, at least/ƛƈāƛā, on tuesdav/Ƣćķ, egvpt
&apos;/ĢůƢ, thursdav/Ƣćķ, the un/ƴĔŔłƢƛā, on thursdav/Ƣćķ, fridav/Ƣćķ, on fridav/Ƣćķ, to/ŁťŔ,
al-wafd ,/ů Ĕžćƛā, the us/ƴĔŔłƢƛā, for/-ƛ ƴŁťƦƛā, first time/Ŀƛćāƛā ƴĢƢƛā, further/ƦƢ, iraq
&apos;/ƈāĢŷƛā, israeli prime/ķƛķĶāĢťāƛā ƹāĢģćƛā ťķĶĢ, the two/ƦķĔƛŁƛā, on saturdav/Ƣćķ, on sundav/Ƣćķ,
u.s./ƴĔŔłƢƛā, views/ĢŵƦƛā, sharon &apos;/ƦćĢāŦ, countrv &apos;/ĔāƛŁƛā, he said/ĢƋĕ, israel &apos;/ƛķĶāĢťā, people
&apos;/ŁŷŦƛā, here/Ƣćķƛā āƦƱ, china &apos;/Ʀķůƛƛ, , he said/žāŰā -ć, earlier/łƈć Ŀž, china &apos;/ƴķƦķůƛā,
at least/ƛƈķ āƛ āƢ, the u.s./ƴĔŔłƢƛā, the gaza/ŷāŴƈ, the gaza strip/ŷāŴƈ, he added/ƛāƈ, are
expected/ŷƈćłƢƛā, are expected to/ŷƈćłƢƛā, are expected to/ŷƈćłƢƛā ƦƢ, million u.s./ƦćķƛƢ,
according/ƛāƈ, to/ƛůāćł, order/ƦƢ, in order/ƦƢ, he pointed/ĢāŦā, mfa , asharq al/ů Ŵťćāƛā, mfa
, asharq al awsat/ů Ŵťćāƛā, arafat &apos;/łāžĢŷ
</figureCaption>
<tableCaption confidence="0.920856">
Table S: Good phrase pairs in the top-100 high frequency phrase pairs specific to the phrase tables
coming from our method vs that ofpialign for FA-EN and AR-EN translation tasks.
</tableCaption>
<bodyText confidence="0.999976565217391">
parameters affects the rate at which the number of
types grows compared to the number of tokens.
Specifically, as the discount a or the concentra-
tion b parameters increases we expect for a rela-
tive increase in the number of types. If the number
of observed monotone and swap rules were equal,
then there would be a higher chance in reusing the
monotone rules. However, the number of observed
monotone and swap rules are not equal, as plotted
in Figure 4.(b). Similar results were observed for
the other language pairs (figures omitted for space
reasons).
Thirdly, we performed a manual evaluation for
the quality of the phrase-pairs learned exclusively
by our method vs pialign. For each method,
we considered the top-100 high frequency phrase-
pairs which are specific to that method. Then we
asked a bilingual human expert to identify rea-
sonably well phrase-pairs among these top-100
phrase-pairs. The results are summarized in Ta-
ble 5, and show that we learn roughly twice as
many reasonably good phrase-pairs for AR-EN
and FA-EN compared to pialign.
</bodyText>
<sectionHeader confidence="0.668382" genericHeader="conclusions">
Conclusions
</sectionHeader>
<bodyText confidence="0.999974333333333">
We have presented a novel method for learn-
ing a phrase-based model of translation directly
from parallel data which we have framed as learn-
ing an inverse transduction grammar (ITG) us-
ing a recursive Bayesian prior. This has led
to a model which learns translations of en-
tire sentences, while also learning their decom-
position into smaller units (phrase-pairs) recur-
sively, terminating at word translations. We have
presented a Metropolis-Hastings sampling algo-
rithm for blocked inference in our non-parametric
ITG. Our experiments on Urdu-English, Arabic-
English, and Farsi-English translation tasks all
demonstrate improvements over competitive base-
line systems.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997414">
The first author was supported by the EPSRC
(grant EP/I034750/1) and an Erasmus-Mundus
scholarship funding a research visit to Melbourne.
The second author was supported by an early ca-
reer research award from Monash University.
</bodyText>
<page confidence="0.9809975">
787
788
</page>
<figure confidence="0.99986735">
0 5 10 15 20 25 30
be
65000 65500 66000
bt
291000 292000 293000
monotone
(b)
176000 177000
swap
0.0000 0.0010 0.0020
0.905 0.910 0.915 0.920 0.925
1000 2000 3000 4000
am and as
bm and bs
0.0000 0.0005 0.0010 0.0015
(a)
0 200 400 600 800 1000
0.00 0.01 0.02 0.03 0.04 0.05 0.06
0.0000 0.0004 0.0008 0.0012
0.0000 0.0004 0.0008 0.0012
</figure>
<figureCaption confidence="0.998379">
Figure 4: (a) Posterior over the hyper-parameters,
</figureCaption>
<bodyText confidence="0.972456333333333">
aM, aS, bM, bS, bE, bT, measured for UR-EN us-
ing samples 400–500 for 3 independent sampling
chains, and the intersection constraints. (b) Poste-
rior over the number of monotone and swap rules
in the resultant grammars. The distribution for
emission rules was also peaked about 147k rules.
</bodyText>
<equation confidence="0.534748162162162">
key ( ( ( ?2fAM?rL ) ( 1fMu ) ) ( baB/fF?A ) )
kkk ( ( ( baB/fF?A ) ( 1fMu ) ) ( i?aifF? ) )
kRN ( ( ( ( ?2fAM?rL ) ( 1fMu ) )
( baB/fF?A ) ) ( i?aifF? ) )
R93 ( ( ( B7fA;` ) ( 1f% ) ) ( vQmfT ) )
Ry3 ( ( ?2fAM?rL ) ( ( baB/fF?A ) ( 1fMu ) ) )
R3k ( ( rBHHf;A ) ( #2f?r ) )
RkN ( ( Bbf?u ) ( MQifM?vL ) )
Rkj ( ( ?abf?u ) ( #22Mf;vA ) )
Ry9 ( ( rBHHf;A ) ( #2fDAvu ) )
Ryj ( ( BMfKvL ) ( B`a[f1`A[ ) )
Urdu-English
3Ny ( ( QM2fvFv ) ( Q7fAx ) )
39j ( ( ( v2a?f`? ) ( 1f% ) ) ( XfX ) )
dj3 ( ( rBi?f#A ) ( K2fKM ) )
e99 ( ( ( ( QFavf#A ) ( 1f$ ) ) ( 1f? ) ) ( XfX ) )
ey3 ( ( iQf#? ) ( K2fKM ) )
k8R ( ( Bbf/? ) ( Bif$ ) )
kky ( ( i2HHf#;r ) ( K2fKM ) )
RNN ( I ( Bf1 ) ( +aMfirMK ) ) ( ǶifMKv ) =
RNy ( ( ( r?QfFv ) ( a`2f?biv ) ) ( vQmfir ) )
R3d ( ( iQH/f;7i ) ( K2fKM ) )
Farsi-English
8ee ( ( BMfĿž ) ( B`a[fƈāĢŷƛā ) )
9R9 ( ( BMfķž ) ( 2;vTifĢůƢ ) )
jNR ( ( i?BbfāĕƱ ) ( v2a`fƢāŷƛā ) )
j8e ( ( ab?a`[fƈĢŦƛā ) ( aH@arbaifŴťćāƛā ) )
jyy ( ( BMfķž ) ( B`a[fƈāĢŷƛā ) )
9yk9 ( ( Xf. ) ( Ǵf&amp;quot; ) )
RjRk ( I ( i?2f ) ( mMBi2/fƴĔŔłƢƛā ) )
(
biai2bfłāķāƛćƛā ) =
ee8 ( ( mMBi2/fƴĔŔłƢƛā ) ( biai2bfłāķāƛćƛā ) )
e8y ( ( HabifķŰāƢƛā ) ( v2a`fƢāŷƛā ) )
9ed ( I ( i?2f ) ( mMBi2/fƴĔŔłƢƛā ) )
( MaiBQMbfƢƢāƛā ) =
Arabic-English
</equation>
<bodyText confidence="0.781822666666667">
Table 4: Top 5 monotone and swap productions
aQK2 �iBM i2ti ~M/ BMHBM2 ~`~#B+, ƢƋķƛŷ Ƣāƛťƛā
andtheir counts. Rules with mostly punctuation
l`/m- 6�`bB- ~~#B+
or encoding 1:many or many:1 alignments were
omitted.
</bodyText>
<sectionHeader confidence="0.975761" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996878201834862">
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238–241,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009a. A Gibbs sampler for phrasal syn-
chronous grammar induction. In ACL2009, Singa-
pore, August.
Phil Blunsom, Trevor Cohn, and Miles Osborne.
2009b. Bayesian synchronous grammar induction.
In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot-
tou, editors, Advances in Neural Information Pro-
cessing Systems 21, pages 161–168. MIT Press.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263–311.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of COLING/ACL. As-
sociation for Computational Linguistics.
Colin Cherry and Dekany Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In Proc. of the HLT-NAACL Workshop on
Syntax and Structure in Statistical Translation (SSST
2007), Rochester, USA.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, pages 3053–3096.
John DeNero and Dan Klein. 2010. Discriminative
modeling of extraction sets for machine translation.
In The 48th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies (ACL).
Aria Haghighi, John Blitzer, and Dan Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, Suntec, Singapore. Association for
Computational Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007a. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian
models. In B. Sch¨olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641–648. MIT Press, Cambridge,
MA.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007b. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of the 7th Inter-
national Conference on Human Language Technol-
ogy Research and 8th Annual Meeting of the NAACL
(HLT-NAACL 2007), pages 139–146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the 3rd International Conference on Human Lan-
guage Technology Research and 4th Annual Meet-
ing of the NAACL (HLT-NAACL 2003), pages 81–88,
Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-
2007), Prague.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 223–232, Jeju Island, Korea, July.
Association for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proc. of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2002), pages 133–139, Philadelphia, July.
Association for Computational Linguistics.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. of the 2006 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2006), pages 44–52, Sydney, Australia, July.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In The 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 632–641,
Portland, Oregon, USA, 6.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the ACL (ACL-2003), pages 160–
167, Sapporo, Japan.
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment
with inversion transduction grammars. In Proceed-
ings of the North American Conference of the Asso-
ciation for Computational Linguistics (NAACL). As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.982667">
789
</page>
<reference confidence="0.999859852941176">
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of EMNLP.
Association for Computational Linguistics.
M. T. Pilevar and H. Faili. 2011. Tep: Tehran english-
persian parallel corpus. In Proc. International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing).
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M.
Blei. 2006. Hierarchical Dirichlet processes.
Journal of the American Statistical Association,
101(476):1566–1581.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
985–992.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the 43rd Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL). Association for Computational Linguis-
tics.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proc. of the 46th Annual Conference of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08:HLT), pages 97–105,
Columbus, Ohio, June.
</reference>
<page confidence="0.997189">
790
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.124115">
<title confidence="0.999983">An Infinite Hierarchical Bayesian Model of Phrasal Translation</title>
<author confidence="0.999899">Trevor Cohn</author>
<affiliation confidence="0.9979145">Department of Computer Science The University of Sheffield</affiliation>
<address confidence="0.842515">Sheffield, United</address>
<email confidence="0.989596">t.cohn@sheffield.ac.uk</email>
<affiliation confidence="0.61402625">Gholamreza Faculty of Information Monash Clayton,</affiliation>
<email confidence="0.99994">reza@monash.edu</email>
<abstract confidence="0.999560863636363">Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Inducing synchronous grammars with slice sampling.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>238--241</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="22421" citStr="Blunsom and Cohn, 2010" startWordPosition="3741" endWordPosition="3744">plexity for the latter is roughly O(l4), as plotted in green t = 2 × 10−7l4. with high confidence alignments from GIZA++.11 Figure 2 shows the sampling time with respect to the average sentence length, showing that our alignment-constrained sampling algorithm is better than the unconstrained algorithm with empirical complexity of n4. However, the time complexity is still high, so we set the maximum sentence length to 30 to keep our experiments practicable. Presumably other means of inference may be more efficient, such as Gibbs sampling (Levenberg et al., 2012) or auxiliary variable sampling (Blunsom and Cohn, 2010); we leave these extensions to future work. Baselines. Following (Levenberg et al., 2012; Neubig et al., 2011), we evaluate our model by using its output word alignments to construct a phrase table. As a baseline, we train a phrasebased model using the moses toolkit12 based on the word alignments obtained using GIZA++ in both directions and symmetrized using the growdiag-final-and heuristic13 (Koehn et al., 2003). This alignment is used as input to the rule factorisation algorithm, producing the ITG trees with which we initialise our sampler. To put our results in the context of the previous w</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2010. Inducing synchronous grammars with slice sampling. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 238–241, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In ACL2009, Singapore,</booktitle>
<contexts>
<context position="6738" citStr="Blunsom et al., 2009" startWordPosition="1032" endWordPosition="1035">who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, except that we impose additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. This work was inspired by adaptor grammars (Johnson et al., 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recur</context>
<context position="19466" citStr="Blunsom et al., 2009" startWordPosition="3251" endWordPosition="3254">ntences UR-EN 745K 575K 148K FA-EN 4.7M 4.4M 498K AR-EN 1.94M 2.08M 113K Table 2: Corpora statistics showing numbers of parallel sentences and source and target words for the training sets. from 1600 movie subtitles. We tokenized this corpus, removed noisy single-word sentences, randomly selected the development and test sets, and used the rest of the corpus as the training set. We discard sentences with length above 30 from the datasets for all experiments.9 Sampler configuration Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method (Blunsom et al., 2009a). This algorithm represents the translation of a sentence as a large SCFG rule, which it then factorises into lower rank SCFG rules, a process akin to rule binarisation commonly used in SCFG decoding. Rules that cannot be reduced to a rank-2 SCFG are simplified by dropping alignment edges until they can be factorised, the net result being an ITG derivation largely respecting the alignments.10 The blocked sampler was run 1000 iterations for UR-EN, 100 iterations for FA-EN and AREN. After each full sampling iteration, we resample all the hyper-parameters using slice-sampling, with the followin</context>
<context position="23614" citStr="Blunsom et al., 2009" startWordPosition="3935" endWordPosition="3938">e context of the previous work, we also compare against pialign (Neubig et al., 2011), an ITG algorithm using a Pitman-Yor process prior, as described in Section 2.14 In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. We set the distortion limit to 6 and max-phrase-length to 7 in all experiments. We train 3-gram language models using modified Kneser-Ney smoothing. For AR-EN experiments the language model is trained on English data as (Blunsom et al., 2009a), and for FA-EN and UREN the English data are the target sides of the bilingual training data. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the feature weights for maximum development BLEU. 11These are taken from the final model 4 word alignments, using the intersection of the source-target and target-source models. These alignments are very high precision (but have low recall), and therefore are unlikely to harm the model. 12http://www.statmt.org/moses 13We use the default parameter settings in both moses and GIZA++. 14http://www.phontron.com/pialign t</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009a. A Gibbs sampler for phrasal synchronous grammar induction. In ACL2009, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Miles Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 21,</booktitle>
<pages>161--168</pages>
<editor>In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6738" citStr="Blunsom et al., 2009" startWordPosition="1032" endWordPosition="1035">who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, except that we impose additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. This work was inspired by adaptor grammars (Johnson et al., 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recur</context>
<context position="19466" citStr="Blunsom et al., 2009" startWordPosition="3251" endWordPosition="3254">ntences UR-EN 745K 575K 148K FA-EN 4.7M 4.4M 498K AR-EN 1.94M 2.08M 113K Table 2: Corpora statistics showing numbers of parallel sentences and source and target words for the training sets. from 1600 movie subtitles. We tokenized this corpus, removed noisy single-word sentences, randomly selected the development and test sets, and used the rest of the corpus as the training set. We discard sentences with length above 30 from the datasets for all experiments.9 Sampler configuration Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method (Blunsom et al., 2009a). This algorithm represents the translation of a sentence as a large SCFG rule, which it then factorises into lower rank SCFG rules, a process akin to rule binarisation commonly used in SCFG decoding. Rules that cannot be reduced to a rank-2 SCFG are simplified by dropping alignment edges until they can be factorised, the net result being an ITG derivation largely respecting the alignments.10 The blocked sampler was run 1000 iterations for UR-EN, 100 iterations for FA-EN and AREN. After each full sampling iteration, we resample all the hyper-parameters using slice-sampling, with the followin</context>
<context position="23614" citStr="Blunsom et al., 2009" startWordPosition="3935" endWordPosition="3938">e context of the previous work, we also compare against pialign (Neubig et al., 2011), an ITG algorithm using a Pitman-Yor process prior, as described in Section 2.14 In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. We set the distortion limit to 6 and max-phrase-length to 7 in all experiments. We train 3-gram language models using modified Kneser-Ney smoothing. For AR-EN experiments the language model is trained on English data as (Blunsom et al., 2009a), and for FA-EN and UREN the English data are the target sides of the bilingual training data. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the feature weights for maximum development BLEU. 11These are taken from the final model 4 word alignments, using the intersection of the source-target and target-source models. These alignments are very high precision (but have low recall), and therefore are unlikely to harm the model. 12http://www.statmt.org/moses 13We use the default parameter settings in both moses and GIZA++. 14http://www.phontron.com/pialign t</context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, and Miles Osborne. 2009b. Bayesian synchronous grammar induction. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 161–168. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1763" citStr="Brown et al., 1993" startWordPosition="251" endWordPosition="254">ased approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from which phrasal translation units are extracted using a heuristic. Although this approach demonstrably works, it suffers from a number of shortcomings. Firstly, many phrase-based phenomena which do not decompose into word translations (e.g., idioms) will be missed, as the underlying word-based alignment model is unlikely to propose the correct alignments. Secondly, the relationship between different phrase-pairs is not considered, such as between single wor</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5577" citStr="Cherry and Lin, 2006" startWordPosition="840" endWordPosition="843">ages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-w</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discriminative training. In Proceedings of COLING/ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekany Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In Proc. of the HLT-NAACL Workshop on Syntax and Structure in Statistical Translation (SSST 2007),</booktitle>
<location>Rochester, USA.</location>
<contexts>
<context position="5475" citStr="Cherry and Lin, 2007" startWordPosition="822" endWordPosition="825"> synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekany Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In Proc. of the HLT-NAACL Workshop on Syntax and Structure in Statistical Translation (SSST 2007), Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1341" citStr="Chiang, 2007" startWordPosition="184" endWordPosition="185">ning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems. 1 Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from which phrasal translation units are e</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3053--3096</pages>
<contexts>
<context position="15914" citStr="Cohn et al., 2010" startWordPosition="2673" endWordPosition="2676">uating P2(t) is computationally expensive, requiring tracking of repeated substructures in descendent sub-trees of t, which may affect other descendants. This results in an asymptotic complexity exponential in the number of nodes in the tree. For this reason we consider trees annotated with binary values denoting their table assignment, namely whether they share a table or are seated alone. Given this, the calculation is greatly simplified, and has linear complexity.5 We construct an approximating ITG following the technique used for sampling trees from monolingual tree-substitution grammars (Cohn et al., 2010). To do so we encode the first term from (2) separately from the second term (corresponding to draws from P1). Summing together these two alternate paths – i.e., during inside inference – we recover P2 as shown in (2). The full grammar transform for inside inference is shown in Table 1. The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al., 2007b). For each sentencepair, we first decrement the counts associated with its current tree, and then sample a new derivation. This involves first constructing the inside lattice using the producti</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. Journal of Machine Learning Research, pages 3053–3096.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative modeling of extraction sets for machine translation.</title>
<date>2010</date>
<booktitle>In The 48th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL).</booktitle>
<contexts>
<context position="5752" citStr="DeNero and Klein, 2010" startWordPosition="872" endWordPosition="875">te their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to th</context>
</contexts>
<marker>DeNero, Klein, 2010</marker>
<rawString>John DeNero and Dan Klein. 2010. Discriminative modeling of extraction sets for machine translation. In The 48th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised itg models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, Suntec, Singapore. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5554" citStr="Haghighi et al., 2009" startWordPosition="836" endWordPosition="839">ate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM mo</context>
</contexts>
<marker>Haghighi, Blitzer, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, and Dan Klein. 2009. Better word alignments with supervised itg models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor grammars: A framework for specifying compositional nonparametric bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3034" citStr="Johnson et al., 2007" startWordPosition="446" endWordPosition="449">r where one large phrase-pair subsumes another. This paper develops a phrase-based translation model which aims to address the above shortcomings of the phrase-based translation pipeline. Specifically, we formulate translation using inverse transduction grammar (ITG), and seek to learn an ITG from parallel corpora. The novelty of our approach is that we develop a Bayesian prior over the grammar, such that a nonterminal becomes a ‘cache’ learning each production and its complete yield, which in turn is recursively composed of its child constituents. This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting. Our model learns translations of entire sentences while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, whi</context>
<context position="6921" citStr="Johnson et al., 2007" startWordPosition="1060" endWordPosition="1063">ero and Klein, 2010). Our approach is similar to these previous works, except that we impose additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. This work was inspired by adaptor grammars (Johnson et al., 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the top-level and base distributions are explicitly linked together. As mentioned above, ours is not the first work attempting to generalise adaptor grammars fo</context>
<context position="16319" citStr="Johnson et al., 2007" startWordPosition="2742" endWordPosition="2745">s, the calculation is greatly simplified, and has linear complexity.5 We construct an approximating ITG following the technique used for sampling trees from monolingual tree-substitution grammars (Cohn et al., 2010). To do so we encode the first term from (2) separately from the second term (corresponding to draws from P1). Summing together these two alternate paths – i.e., during inside inference – we recover P2 as shown in (2). The full grammar transform for inside inference is shown in Table 1. The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al., 2007b). For each sentencepair, we first decrement the counts associated with its current tree, and then sample a new derivation. This involves first constructing the inside lattice using the productions in Table 1, and then performing a top-down sampling pass. After sampling each derivation from the approximating grammar, we then convert this into its corresponding ITG tree, which we then score with the full model and accept or reject the sample using the 5To support this computation, we track explicit table assignments for every training tree and their component subtrees. We also sample trees lab</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007a. Adaptor grammars: A framework for specifying compositional nonparametric bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL (HLT-NAACL</booktitle>
<pages>139--146</pages>
<contexts>
<context position="3034" citStr="Johnson et al., 2007" startWordPosition="446" endWordPosition="449">r where one large phrase-pair subsumes another. This paper develops a phrase-based translation model which aims to address the above shortcomings of the phrase-based translation pipeline. Specifically, we formulate translation using inverse transduction grammar (ITG), and seek to learn an ITG from parallel corpora. The novelty of our approach is that we develop a Bayesian prior over the grammar, such that a nonterminal becomes a ‘cache’ learning each production and its complete yield, which in turn is recursively composed of its child constituents. This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting. Our model learns translations of entire sentences while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, whi</context>
<context position="6921" citStr="Johnson et al., 2007" startWordPosition="1060" endWordPosition="1063">ero and Klein, 2010). Our approach is similar to these previous works, except that we impose additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. This work was inspired by adaptor grammars (Johnson et al., 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the top-level and base distributions are explicitly linked together. As mentioned above, ours is not the first work attempting to generalise adaptor grammars fo</context>
<context position="16319" citStr="Johnson et al., 2007" startWordPosition="2742" endWordPosition="2745">s, the calculation is greatly simplified, and has linear complexity.5 We construct an approximating ITG following the technique used for sampling trees from monolingual tree-substitution grammars (Cohn et al., 2010). To do so we encode the first term from (2) separately from the second term (corresponding to draws from P1). Summing together these two alternate paths – i.e., during inside inference – we recover P2 as shown in (2). The full grammar transform for inside inference is shown in Table 1. The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al., 2007b). For each sentencepair, we first decrement the counts associated with its current tree, and then sample a new derivation. This involves first constructing the inside lattice using the productions in Table 1, and then performing a top-down sampling pass. After sampling each derivation from the approximating grammar, we then convert this into its corresponding ITG tree, which we then score with the full model and accept or reject the sample using the 5To support this computation, we track explicit table assignments for every training tree and their component subtrees. We also sample trees lab</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007b. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL (HLT-NAACL 2007), pages 139–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 3rd International Conference on Human Language Technology Research and 4th Annual Meeting of the NAACL (HLT-NAACL</booktitle>
<pages>81--88</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1178" citStr="Koehn et al., 2003" startWordPosition="156" endWordPosition="159">compose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems. 1 Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central</context>
<context position="22837" citStr="Koehn et al., 2003" startWordPosition="3809" endWordPosition="3812"> 30 to keep our experiments practicable. Presumably other means of inference may be more efficient, such as Gibbs sampling (Levenberg et al., 2012) or auxiliary variable sampling (Blunsom and Cohn, 2010); we leave these extensions to future work. Baselines. Following (Levenberg et al., 2012; Neubig et al., 2011), we evaluate our model by using its output word alignments to construct a phrase table. As a baseline, we train a phrasebased model using the moses toolkit12 based on the word alignments obtained using GIZA++ in both directions and symmetrized using the growdiag-final-and heuristic13 (Koehn et al., 2003). This alignment is used as input to the rule factorisation algorithm, producing the ITG trees with which we initialise our sampler. To put our results in the context of the previous work, we also compare against pialign (Neubig et al., 2011), an ITG algorithm using a Pitman-Yor process prior, as described in Section 2.14 In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. We set the distortion limit to 6 and max-phrase-length to 7 in a</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of the 3rd International Conference on Human Language Technology Research and 4th Annual Meeting of the NAACL (HLT-NAACL 2003), pages 81–88, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the ACL (ACL2007),</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="1361" citStr="Koehn et al., 2007" startWordPosition="186" endWordPosition="189">e transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems. 1 Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from which phrasal translation units are extracted using a heu</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the ACL (ACL2007), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Chris Dyer</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model for learning SCFGs with discontiguous rules.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>223--232</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6764" citStr="Levenberg et al., 2012" startWordPosition="1036" endWordPosition="1039">ion to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, except that we impose additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. This work was inspired by adaptor grammars (Johnson et al., 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the</context>
<context position="12838" citStr="Levenberg et al. (2012)" startWordPosition="2117" endWordPosition="2121">d P1 makes draws from P2. The recursion is terminated when the rule type r = emit is drawn. Following standard practice in Bayesian models, we integrate out R, TM, TS and E. This means draws from P2 (or P1) are no longer iid: for any non-trivial tree, computing its probability under this model is complicated by the fact 2Note that we could allow phrases here, but given the model can already reason over phrases by way of its hierarchical formulation, this is an unnecessary complication. 3We also experimented with using word translation probabilities from IBM model 1, based on the prior used by Levenberg et al. (2012), however we found little empirical difference compared with this simpler uniform model. ⎧ ⎨⎪ ⎪⎩ P0(e,f) = , 782 that the probability of its two subtrees are interdependent. This is best understood in terms of the Chinese Restaurant Franchise (CRF; Teh et al. (2006)), which describes the posterior distribution after integrating out the model parameters. In our case we can consider the process of drawing a tree from P2 as a customer entering a restaurant and choosing where to sit, from an infinite set of tables. The seating decision is based on the number of other customers at each table, such </context>
<context position="22365" citStr="Levenberg et al., 2012" startWordPosition="3732" endWordPosition="3736">on constraints with blue triangles. The average time complexity for the latter is roughly O(l4), as plotted in green t = 2 × 10−7l4. with high confidence alignments from GIZA++.11 Figure 2 shows the sampling time with respect to the average sentence length, showing that our alignment-constrained sampling algorithm is better than the unconstrained algorithm with empirical complexity of n4. However, the time complexity is still high, so we set the maximum sentence length to 30 to keep our experiments practicable. Presumably other means of inference may be more efficient, such as Gibbs sampling (Levenberg et al., 2012) or auxiliary variable sampling (Blunsom and Cohn, 2010); we leave these extensions to future work. Baselines. Following (Levenberg et al., 2012; Neubig et al., 2011), we evaluate our model by using its output word alignments to construct a phrase table. As a baseline, we train a phrasebased model using the moses toolkit12 based on the word alignments obtained using GIZA++ in both directions and symmetrized using the growdiag-final-and heuristic13 (Koehn et al., 2003). This alignment is used as input to the rule factorisation algorithm, producing the ITG trees with which we initialise our samp</context>
</contexts>
<marker>Levenberg, Dyer, Blunsom, 2012</marker>
<rawString>Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012. A Bayesian model for learning SCFGs with discontiguous rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 223–232, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrasebased, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002),</booktitle>
<pages>133--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia,</location>
<contexts>
<context position="6117" citStr="Marcu and Wong (2002)" startWordPosition="932" endWordPosition="935"> Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, except that we impose additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs </context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrasebased, joint probability model for statistical machine translation. In Proc. of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP-2002), pages 133–139, Philadelphia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006),</booktitle>
<pages>44--52</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1382" citStr="Marcu et al., 2006" startWordPosition="190" endWordPosition="193">ar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems. 1 Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from which phrasal translation units are extracted using a heuristic. Although this</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical machine translation with syntactified target language phrases. In Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006), pages 44–52, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),</booktitle>
<pages>632--641</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4109" citStr="Neubig et al. (2011)" startWordPosition="606" endWordPosition="610">ect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). 780 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics We are not the first to consider this idea; Neubig et al. (2011) developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar. However Neubig et al.’s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently their approach does not constitute a valid Bayesian model. In contrast, this paper provides a more rigorous and theoretically sound method. Moreover our approach results in consistent translation improvements across a number of translation tasks compared to Ne</context>
<context position="5774" citStr="Neubig et al., 2011" startWordPosition="876" endWordPosition="879">given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, ex</context>
<context position="7565" citStr="Neubig et al., 2011" startWordPosition="1165" endWordPosition="1168">r formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the top-level and base distributions are explicitly linked together. As mentioned above, ours is not the first work attempting to generalise adaptor grammars for machine translation; (Neubig et al., 2011) also developed a similar approach based around ITG using a Pitman-Yor Process prior. Our approach improves upon theirs in terms of the model and inference, and critically, this is borne out in our experiments where we show uniform improvements in translation quality over a baseline system, as compared to their almost entirely negative results. We believe that their approach had a number of flaws: For inference they use a beam-search, which may speed up processing but means that they are no longer sampling from the true distribution, nor a distribution with the same support as the posterior. M</context>
<context position="22531" citStr="Neubig et al., 2011" startWordPosition="3759" endWordPosition="3762"> GIZA++.11 Figure 2 shows the sampling time with respect to the average sentence length, showing that our alignment-constrained sampling algorithm is better than the unconstrained algorithm with empirical complexity of n4. However, the time complexity is still high, so we set the maximum sentence length to 30 to keep our experiments practicable. Presumably other means of inference may be more efficient, such as Gibbs sampling (Levenberg et al., 2012) or auxiliary variable sampling (Blunsom and Cohn, 2010); we leave these extensions to future work. Baselines. Following (Levenberg et al., 2012; Neubig et al., 2011), we evaluate our model by using its output word alignments to construct a phrase table. As a baseline, we train a phrasebased model using the moses toolkit12 based on the word alignments obtained using GIZA++ in both directions and symmetrized using the growdiag-final-and heuristic13 (Koehn et al., 2003). This alignment is used as input to the rule factorisation algorithm, producing the ITG trees with which we initialise our sampler. To put our results in the context of the previous work, we also compare against pialign (Neubig et al., 2011), an ITG algorithm using a Pitman-Yor process prior,</context>
<context position="25024" citStr="Neubig et al., 2011" startWordPosition="4196" endWordPosition="4199">is paper GIZA++ pialign individual combination UR-EN 16.95 15.65 16.68 ± .12 16.97 FA-EN 20.69 21.41 21.36 ± .17 21.50 MT03 44.05 43.30 44.8 ± .28 45.10 MT04 38.15 37.78 38.4 ± .08 38.4 AR-EN MT05 42.81 42.18 43.13 ± .23 43.45 MT08 32.43 33.00 32.7 ± .15 32.80 Table 3: The BLEU scores for the translation tasks of three language pairs. The individual column show the average and 95% confidence intervals for 5 independent runs, whereas the combination column show the results for combining the phrase tables of all these runs. The baselines are GIZA++ alignments and those generated by the pialign (Neubig et al., 2011) bold: the best result. rule frequency Figure 3: Fraction of rules with a given frequency, using a single sample grammar (UR-EN). 4.1 Results Table 3 shows the BLEU scores for the three translation tasks UR/AR/FA→EN based on our method against the baselines. For our models, we report the average BLEU score of the 5 independent runs as well as that of the aggregate phrase table generated by these 5 independent runs. There are a number of interesting observations in Table 3. Firstly, combining the phrase tables from independent runs results in increased BLEU scores, possibly due to the represent</context>
</contexts>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 632–641, Portland, Oregon, USA, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the ACL (ACL-2003),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="23757" citStr="Och, 2003" startWordPosition="3963" endWordPosition="3964"> Section 2.14 In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. We set the distortion limit to 6 and max-phrase-length to 7 in all experiments. We train 3-gram language models using modified Kneser-Ney smoothing. For AR-EN experiments the language model is trained on English data as (Blunsom et al., 2009a), and for FA-EN and UREN the English data are the target sides of the bilingual training data. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the feature weights for maximum development BLEU. 11These are taken from the final model 4 word alignments, using the intersection of the source-target and target-source models. These alignments are very high precision (but have low recall), and therefore are unlikely to harm the model. 12http://www.statmt.org/moses 13We use the default parameter settings in both moses and GIZA++. 14http://www.phontron.com/pialign time (s) 1e−05 1e−03 1e−01 ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ●● ● ● ●●●●● ●● ●● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ●● ● ●●●●●●●●●●●● ●●●●●</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the ACL (ACL-2003), pages 160– 167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Unsupervised syntactic alignment with inversion transduction grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Conference of the Association for Computational Linguistics (NAACL). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5516" citStr="Pauls et al., 2010" startWordPosition="830" endWordPosition="833">roductions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002</context>
</contexts>
<marker>Pauls, Klein, Chiang, Knight, 2010</marker>
<rawString>Adam Pauls, Dan Klein, David Chiang, and Kevin Knight. 2010. Unsupervised syntactic alignment with inversion transduction grammars. In Proceedings of the North American Conference of the Association for Computational Linguistics (NAACL). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5626" citStr="Petrov et al., 2008" startWordPosition="848" endWordPosition="851">vement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of EMNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Pilevar</author>
<author>H Faili</author>
</authors>
<title>Tep: Tehran englishpersian parallel corpus.</title>
<date>2011</date>
<booktitle>In Proc. International Conference on Intelligent Text Processing and Computational Linguistics (CICLing).</booktitle>
<contexts>
<context position="18421" citStr="Pilevar and Faili, 2011" startWordPosition="3106" endWordPosition="3109">. 4 Experiments Datasets We train our model across three language pairs: Urdu-+English (UR-EN), Farsi-+English (FA-EN), and Arabic-+English (AR-EN). The corpora statistics of these translation tasks are summarised in Table 2. The UR-EN corpus comes from NIST 2009 translation evaluation.7 The AR-EN training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). For FA-EN, we use TEP8 Tehran English-Persian Parallel corpus (Pilevar and Faili, 2011), which consists of conversational/informal text extracted 6The full model differs from the approximating grammar in that it accounts for inter-dependencies between subtrees by recursively tracking the changes in the customer and table counts while scoring the tree. Around 98% of samples were accepted in our experiments. 7http://www.itl.nist.gov/iad/mig/tests/mt/2009 8http://ece.ut.ac.ir/NLP/resources.htm source target sentences UR-EN 745K 575K 148K FA-EN 4.7M 4.4M 498K AR-EN 1.94M 2.08M 113K Table 2: Corpora statistics showing numbers of parallel sentences and source and target words for the </context>
</contexts>
<marker>Pilevar, Faili, 2011</marker>
<rawString>M. T. Pilevar and H. Faili. 2011. Tep: Tehran englishpersian parallel corpus. In Proc. International Conference on Intelligent Text Processing and Computational Linguistics (CICLing).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="13104" citStr="Teh et al. (2006)" startWordPosition="2164" endWordPosition="2167">ility under this model is complicated by the fact 2Note that we could allow phrases here, but given the model can already reason over phrases by way of its hierarchical formulation, this is an unnecessary complication. 3We also experimented with using word translation probabilities from IBM model 1, based on the prior used by Levenberg et al. (2012), however we found little empirical difference compared with this simpler uniform model. ⎧ ⎨⎪ ⎪⎩ P0(e,f) = , 782 that the probability of its two subtrees are interdependent. This is best understood in terms of the Chinese Restaurant Franchise (CRF; Teh et al. (2006)), which describes the posterior distribution after integrating out the model parameters. In our case we can consider the process of drawing a tree from P2 as a customer entering a restaurant and choosing where to sit, from an infinite set of tables. The seating decision is based on the number of other customers at each table, such that popular tables are more likely to be joined than unpopular or empty ones. If the customer chooses an occupied table, the identity of the tree is then set to be the same as for the other customers also seated there. For empty tables the tree must be sampled from</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="3852" citStr="Teh, 2006" startWordPosition="570" endWordPosition="571">rminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). 780 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics We are not the first to consider this idea; Neubig et al. (2011) developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar. However Neubig et al.’s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently the</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985–992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4837" citStr="Wu, 1997" startWordPosition="724" endWordPosition="725">ork was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently their approach does not constitute a valid Bayesian model. In contrast, this paper provides a more rigorous and theoretically sound method. Moreover our approach results in consistent translation improvements across a number of translation tasks compared to Neubig et al.’s method, and a competitive phrase-based baseline. 2 Related Work Inversion transduction grammar (or ITG) (Wu, 1997) is a well studied synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic lexicalized inversion transduction grammar for alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5453" citStr="Zhang and Gildea, 2005" startWordPosition="818" endWordPosition="821"> 1997) is a well studied synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic lexicalized inversion transduction grammar for alignment. In Proceedings of the 43rd Annual Conference of the Association for Computational Linguistics (ACL). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT),</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5495" citStr="Zhang et al., 2008" startWordPosition="826" endWordPosition="829">ormalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets (...), generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proc. of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-08:HLT), pages 97–105, Columbus, Ohio, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>