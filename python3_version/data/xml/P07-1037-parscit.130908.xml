<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001118">
<title confidence="0.989979">
Supertagged Phrase-Based Statistical Machine Translation
</title>
<author confidence="0.994709">
Hany Hassan
</author>
<affiliation confidence="0.973639">
School of Computing,
Dublin City University,
</affiliation>
<address confidence="0.862873">
Dublin 9, Ireland
</address>
<email confidence="0.995371">
hhasan@computing.dcu.ie
</email>
<author confidence="0.984986">
Khalil Sima’an
</author>
<affiliation confidence="0.86396">
Language and Computation,
University of Amsterdam,
Amsterdam, The Netherlands
</affiliation>
<email confidence="0.994811">
simaan@science.uva.nl
</email>
<author confidence="0.997586">
Andy Way
</author>
<affiliation confidence="0.9737465">
School of Computing,
Dublin City University,
</affiliation>
<address confidence="0.864011">
Dublin 9, Ireland
</address>
<email confidence="0.996971">
away@computing.dcu.ie
</email>
<sectionHeader confidence="0.995614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999409296296296">
Until quite recently, extending Phrase-based
Statistical Machine Translation (PBSMT)
with syntactic structure caused system per-
formance to deteriorate. In this work we
show that incorporating lexical syntactic de-
scriptions in the form of supertags can yield
significantly better PBSMT systems. We de-
scribe a novel PBSMT model that integrates
supertags into the target language model
and the target side of the translation model.
Two kinds of supertags are employed: those
from Lexicalized Tree-Adjoining Grammar
and Combinatory Categorial Grammar. De-
spite the differences between these two ap-
proaches, the supertaggers give similar im-
provements. In addition to supertagging, we
also explore the utility of a surface global
grammaticality measure based on combina-
tory operators. We perform various experi-
ments on the Arabic to English NIST 2005
test set addressing issues such as sparseness,
scalability and the utility of system subcom-
ponents. Our best result (0.4688 BLEU)
improves by 6.1% relative to a state-of-the-
art PBSMT model, which compares very
favourably with the leading systems on the
NIST 2005 task.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966314285714">
Within the field of Machine Translation, by far the
most dominant paradigm is Phrase-based Statistical
Machine Translation (PBSMT) (Koehn et al., 2003;
Tillmann &amp; Xia, 2003). However, unlike in rule- and
example-based MT, it has proven difficult to date to
incorporate linguistic, syntactic knowledge in order
to improve translation quality. Only quite recently
have (Chiang, 2005) and (Marcu et al., 2006) shown
that incorporating some form of syntactic structure
could show improvements over a baseline PBSMT
system. While (Chiang, 2005) avails of structure
which is not linguistically motivated, (Marcu et al.,
2006) employ syntactic structure to enrich the en-
tries in the phrase table.
In this paper we explore a novel approach towards
extending a standard PBSMT system with syntactic
descriptions: we inject lexical descriptions into both
the target side of the phrase translation table and the
target language model. Crucially, the kind of lexical
descriptions that we employ are those that are com-
monly devised within lexicon-driven approaches to
linguistic syntax, e.g. Lexicalized Tree-Adjoining
Grammar (Joshi &amp; Schabes, 1992; Bangalore &amp;
Joshi, 1999) and Combinary Categorial Grammar
(Steedman, 2000). In these linguistic approaches, it
is assumed that the grammar consists of a very rich
lexicon and a tiny, impoverished1 set of combina-
tory operators that assemble lexical entries together
into parse-trees. The lexical entries consist of syn-
tactic constructs (‘supertags’) that describe informa-
tion such as the POS tag of the word, its subcatego-
rization information and the hierarchy of phrase cat-
egories that the word projects upwards. In this work
we employ the lexical entries but exchange the al-
gebraic combinatory operators with the more robust
</bodyText>
<footnote confidence="0.976474">
1These operators neither carry nor presuppose further lin-
guistic knowledge beyond what the lexicon contains.
</footnote>
<page confidence="0.863259">
288
</page>
<note confidence="0.927969">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288–295,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998926770491803">
and efficient supertagging approach: like standard English. They employ a stochastic, top-down trans-
taggers, supertaggers employ probabilities based on duction process that assigns a joint probability to
local context and can be implemented using finite a source sentence and each of its alternative trans-
state technology, e.g. Hidden Markov Models (Ban- lations when rewriting the target parse-tree into a
galore &amp; Joshi, 1999). source sentence. The rewriting/transduction process
There are currently two supertagging approaches is driven by “xRS rules”, each consisting of a pair
available: LTAG-based (Bangalore &amp; Joshi, 1999) of a source phrase and a (possibly only partially)
and CCG-based (Clark &amp; Curran, 2004). Both the lexicalized syntactified target phrase. In order to
LTAG (Chen et al., 2006) and the CCG supertag extract xRS rules, the word-to-word alignment in-
sets (Hockenmaier, 2003) were acquired from the duced from the parallel training corpus is used to
WSJ section of the Penn-II Treebank using hand- guide heuristic tree ‘cutting’ criteria.
built extraction rules. Here we test both the LTAG While the research of (Marcu et al., 2006) has
and CCG supertaggers. We interpolate (log-linearly) much in common with the approach proposed here
the supertagged components (language model and (such as the syntactified target phrases), there re-
phrase table) with the components of a standard main a number of significant differences. Firstly,
PBSMT system. Our experiments on the Arabic– rather than induce millions of xRS rules from par-
English NIST 2005 test suite show that each of the allel data, we extract phrase pairs in the standard
supertagged systems significantly improves over the way (Och &amp; Ney, 2003) and associate with each
baseline PBSMT system. Interestingly, combining phrase-pair a set of target language syntactic struc-
the two taggers together diminishes the benefits of tures based on supertag sequences. Relative to using
supertagging seen with the individual LTAG and arbitrary parse-chunks, the power of supertags lies
CCG systems. In this paper we discuss these and in the fact that they are, syntactically speaking, rich
other empirical issues. lexical descriptions. A supertag can be assigned to
The remainder of the paper is organised as fol- every word in a phrase. On the one hand, the cor-
lows: in section 2 we discuss the related work on en- rect sequence of supertags could be assembled to-
riching PBSMT with syntactic structure. In section gether, using only impoverished combinatory opera-
3, we describe the baseline PBSMT system which tors, into a small set of constituents/parses (‘almost’
our work extends. In section 4, we detail our ap- a parse). On the other hand, because supertags are
proach. Section 5 describes the experiments carried lexical entries, they facilitate robust syntactic pro-
out, together with the results obtained. Section 6 cessing (using Markov models, for instance) which
concludes, and provides avenues for further work. does not necessarily aim at building a fully con-
2 Related Work nected graph.
Until very recently, the experience with adding syn- A second major difference with xRS rules is that
tax to PBSMT systems was negative. For example, our supertag-enriched target phrases need not be
(Koehn et al., 2003) demonstrated that adding syn- generalized into (xRS or any other) rules that work
tax actually harmed the quality of their SMT system. with abstract categories. Finally, like POS tagging,
Among the first to demonstrate improvement when supertagging is more efficient than actual parsing or
adding recursive structure was (Chiang, 2005), who tree transduction.
allows for hierarchical phrase probabilities that han- 3 Baseline Phrase-Based SMT System
dle a range of reordering phenomena in the correct We present the baseline PBSMT model which we
fashion. Chiang’s derived grammar does not rely on extend with supertags in the next section. Our
any linguistic annotations or assumptions, so that the baseline PBSMT model uses GIZA++2 to obtain
‘syntax’ induced is not linguistically motivated. word-level alignments in both language directions.
Coming right up to date, (Marcu et al., 2006) The bidirectional word alignment is used to obtain
demonstrate that ‘syntactified’ target language phrase translation pairs using heuristics presented in
phrases can improve translation quality for Chinese–
289
2http://www.fjoch.com/GIZA++.html
(Och &amp; Ney, 2003) and (Koehn et al., 2003), and the
Moses decoder was used for phrase extraction and
decoding.3
Let t and s be the target and source language
sentences respectively. Any (target or source) sen-
tence x will consist of two parts: a bag of elements
(words/phrases etc.) and an order over that bag. In
other words, x = (0x� Ox), where 0x stands for the
bag of phrases that constitute x, and Ox for the order
of the phrases as given in x (Ox can be implemented
as a function from a bag of tokens 0x to a set with a
finite number of positions). Hence, we may separate
order from content:
</bodyText>
<sectionHeader confidence="0.984257" genericHeader="method">
4 Our Approach: Supertagged PBSMT
</sectionHeader>
<bodyText confidence="0.992022">
We extend the baseline model with lexical linguis-
tic representations (supertags) both in the language
model as well as in the phrase translation model. Be-
fore we describe how our model extends the base-
line, we shortly review the supertagging approaches
in Lexicalized Tree-Adjoining Grammar and Com-
binatory Categorial Grammar.
</bodyText>
<subsectionHeader confidence="0.954285">
4.1 Supertags: Lexical Syntax
</subsectionHeader>
<figure confidence="0.929940166666667">
NP
includes
D
The
NP
NP
purchase
N
NP
price
NP
N
S
NP VP
V
NP
taxes
NP
N
arg max P(t|s) = argmaxP(s  |t)P(t) (1)
t t
= arg max TM distortion LM
Ot,Ot) z } |{ z } |{ z } |{
P(0s  |0t) P(Os  |Ot) Pw(t) (2)
</figure>
<bodyText confidence="0.9951085">
Here, Pw(t) is the target language model, P(Os|Ot)
represents the conditional (order) linear distortion
probability, and P(0s|0t) stands for a probabilis-
tic translation model from target language bags of
phrases to source language bags of phrases using a
phrase translation table. As commonly done in PB-
SMT, we interpolate these models log-linearly (us-
ing different A weights) together with a word penalty
weight which allows for control over the length of
the target sentence t:
</bodyText>
<equation confidence="0.827273">
arg max
Ot,Ot) P(0s  |0t) P(Os  |Ot)λo
Pw(t)λlm exp|t|λw
</equation>
<bodyText confidence="0.9999725">
For convenience of notation, the interpolation factor
for the bag of phrases translation model is shown in
formula (3) at the phrase level (but that does not en-
tail any difference). For a bag of phrases 0t consist-
ing of phrases ti, and bag 0s consisting of phrases
si, the phrase translation model is given by:
</bodyText>
<equation confidence="0.9955085">
P(φs  |φt) = � P(si|ti)
si
ti
P(si |ti) = Pph(si|ti)λt1Pw(si|ti)λt2Pr(ti|si)λt3 (3)
</equation>
<bodyText confidence="0.99899">
where Pph and Pr are the phrase-translation proba-
bility and its reverse probability, and Pw is the lexi-
cal translation probability.
</bodyText>
<footnote confidence="0.905038">
3http://www.statmt.org/moses/
</footnote>
<figureCaption confidence="0.982048">
Figure 1: An LTAG supertag sequence for the sen-
</figureCaption>
<bodyText confidence="0.998434464285714">
tence The purchase price includes taxes. The sub-
categorization information is most clearly available
in the verb includes which takes a subject NP to its
left and an object NP to its right.
Modern linguistic theory proposes that a syntactic
parser has access to an extensive lexicon of word-
structure pairs and a small, impoverished set of oper-
ations to manipulate and combine the lexical entries
into parses. Examples of formal instantiations of this
idea include CCG and LTAG. The lexical entries are
syntactic constructs (graphs) that specify informa-
tion such as POS tag, subcategorization/dependency
information and other syntactic constraints at the
level of agreement features. One important way of
portraying such lexical descriptions is via the su-
pertags devised in the LTAG and CCG frameworks
(Bangalore &amp; Joshi, 1999; Clark &amp; Curran, 2004).
A supertag (see Figure 1) represents a complex,
linguistic word category that encodes a syntactic
structure expressing a specific local behaviour of a
word, in terms of the arguments it takes (e.g. sub-
ject, object) and the syntactic environment in which
it appears. In fact, in LTAG a supertag is an elemen-
tary tree and in CCG it is a CCG lexical category.
Both descriptions can be viewed as closely related
functional descriptions.
The term “supertagging” (Bangalore &amp; Joshi,
1999) refers to tagging the words of a sentence, each
</bodyText>
<page confidence="0.964323">
290
</page>
<bodyText confidence="0.9999249375">
with a supertag. When well-formed, an ordered se-
quence of supertags can be viewed as a compact
representation of a small set of constituents/parses
that can be obtained by assembling the supertags
together using the appropriate combinatory opera-
tors (such as substitution and adjunction in LTAG
or function application and combination in CCG).
Akin to POS tagging, the process of supertagging
an input utterance proceeds with statistics that are
based on the probability of a word-supertag pair
given their Markovian or local context (Bangalore
&amp; Joshi, 1999; Clark &amp; Curran, 2004). This is the
main difference with full parsing: supertagging the
input utterance need not result in a fully connected
graph.
The LTAG-based supertagger of (Bangalore &amp;
Joshi, 1999) is a standard HMM tagger and consists
of a (second-order) Markov language model over su-
pertags and a lexical model conditioning the proba-
bility of every word on its own supertag (just like
standard HMM-based POS taggers).
The CCG supertagger (Clark &amp; Curran, 2004) is
based on log-linear probabilities that condition a su-
pertag on features representing its context. The CCG
supertagger does not constitute a language model
nor are the Maximum Entropy estimates directly in-
terpretable as such. In our model we employ the
CCG supertagger to obtain the best sequences of su-
pertags for a corpus of sentences from which we ob-
tain language model statistics. Besides the differ-
ence in probabilities and statistical estimates, these
two supertaggers differ in the way the supertags are
extracted from the Penn Treebank, cf. (Hocken-
maier, 2003; Chen et al., 2006). Both supertaggers
achieve a supertagging accuracy of 90–92%.
Three aspects make supertags attractive in the
context of SMT. Firstly, supertags are rich syntac-
tic constructs that exist for individual words and so
they are easy to integrate into SMT models that can
be based on any level of granularity, be it word-
or phrase-based. Secondly, supertags specify the
local syntactic constraints for a word, which res-
onates well with sequential (finite state) statistical
(e.g. Markov) models. Finally, because supertags
are rich lexical descriptions that represent under-
specification in parsing, it is possible to have some
of the benefits of full parsing without imposing the
strict connectedness requirements that it demands.
</bodyText>
<subsectionHeader confidence="0.986053">
4.2 A Supertag-Based SMT model
</subsectionHeader>
<bodyText confidence="0.999562133333333">
We employ the aforementioned supertaggers to en-
rich the English side of the parallel training cor-
pus with a single supertag sequence per sentence.
Then we extract phrase-pairs together with the co-
occuring English supertag sequence from this cor-
pus via the same phrase extraction method used in
the baseline model. This way we directly extend
the baseline model described in section 3 with su-
pertags both in the phrase translation table and in
the language model. Next we define the probabilistic
model that accompanies this syntactic enrichment of
the baseline model.
Let 5T represent a supertag sequence of the same
length as a target sentence t. Equation (2) changes
as follows:
</bodyText>
<equation confidence="0.999436285714286">
P(s  |t, 5T)PST(t, 5T) �
TM w.sup.tags
z } |{
P(Os  |Ot,ST)
LM w.sup.tags
z }|{
PST(t, 5T)
</equation>
<bodyText confidence="0.999349318181818">
The approximations made in this formula are of two
kinds: the standard split into components and the
search for the most likely joint probability of a tar-
get hypothesis and a supertag sequence cooccuring
with the source sentence (a kind of Viterbi approach
to avoid the complex optimization involving the sum
over supertag sequences). The distortion and word
penalty models are the same as those used in the
baseline PBSMT model.
Supertagged Language Model The ‘language
model’ PST (t, 5T) is a supertagger assigning prob-
abilities to sequences of word–supertag pairs. The
language model is further smoothed by log-linear
interpolation with the baseline language model over
word sequences.
Supertags in Phrase Tables The supertagged
phrase translation probability consists of a combina-
tion of supertagged components analogous to their
counterparts in the baseline model (equation (3)),
i.e. it consists of P(s  |t, 5T), its reverse and
a word-level probability. We smooth this proba-
bility by log-linear interpolation with the factored
</bodyText>
<figure confidence="0.972745375">
X
arg max
t
ST
arg max
(t,ST)
distortion
z } |{
P(Os  |Ot)A
word−penalty
z } |{
exp|t|��
291
John bought quickly shares
NNP_NN VBD_(S[dcl]\NP)/NP RB|(S\NP)\(S\NP) NNS_N
2 Violations
</figure>
<figureCaption confidence="0.9891215">
Figure 2: Example CCG operator violations: V = 2
and L = 3, and so the penalty factor is 1/3.
</figureCaption>
<bodyText confidence="0.996724714285714">
backoff version P(s  |t)P(s  |ST), where we im-
port the baseline phrase table probability and ex-
ploit the probability of a source phrase given the tar-
get supertag sequence. A model in which we omit
P(s  |ST) turns out to be slightly less optimal than
this one.
As in most state-of-the-art PBSMT systems, we
use GIZA++ to obtain word-level alignments in both
language directions. The bidirectional word align-
ment is used to obtain lexical phrase translation pairs
using heuristics presented in (Och &amp; Ney, 2003) and
(Koehn et al., 2003). Given the collected phrase
pairs, we estimate the phrase translation probability
distribution by relative frequency as follows:
</bodyText>
<equation confidence="0.987146666666667">
P _
count (s, t)
ph(s |t) Es count(s, t)
</equation>
<bodyText confidence="0.997315142857143">
For each extracted lexical phrase pair, we extract the
corresponding supertagged phrase pairs from the su-
pertagged target sequence in the training corpus (cf.
section 5). For each lexical phrase pair, there is
at least one corresponding supertagged phrase pair.
The probability of the supertagged phrase pair is es-
timated by relative frequency as follows:
</bodyText>
<equation confidence="0.961137">
Pst(s|t, st) = E
s count(s, t, st)
</equation>
<subsectionHeader confidence="0.833781">
4.3 LMs with a Grammaticality Factor
</subsectionHeader>
<bodyText confidence="0.999991407407407">
The supertags usually encode dependency informa-
tion that could be used to construct an ‘almost parse’
with the help of the CCG/LTAG composition oper-
ators. The n-gram language model over supertags
applies a kind of statistical ‘compositionality check’
but due to smoothing effects this could mask cru-
cial violations of the compositionality operators of
the grammar formalism (CCG in this case). It is
interesting to observe the effect of integrating into
the language model a penalty imposed when formal
compostion operators are violated. We combine the
n-gram language model with a penalty factor that
measures the number of encountered combinatory
operator violations in a sequence of supertags (cf.
Figure 2). For a supertag sequence of length (L)
which has (V ) operator violations (as measured by
the CCG system), the language model P will be ad-
justed as P* = P x (1 − �i ). This is of course no
longer a simple smoothed maximum-likelihood es-
timate nor is it a true probability. Nevertheless, this
mechanism provides a simple, efficient integration
of a global compositionality (grammaticality) mea-
sure into the n-gram language model over supertags.
Decoder The decoder used in this work is Moses,
a log-linear decoder similar to Pharaoh (Koehn,
2004), modified to accommodate supertag phrase
probabilities and supertag language models.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999579083333333">
In this section we present a number of experiments
that demonstrate the effect of lexical syntax on trans-
lation quality. We carried out experiments on the
NIST open domain news translation task from Ara-
bic into English. We performed a number of ex-
periments to examine the effect of supertagging ap-
proaches (CCG or LTAG) with varying data sizes.
Data and Settings The experiments were con-
ducted for Arabic to English translation and tested
on the NIST 2005 evaluation set. The systems were
trained on the LDC Arabic–English parallel corpus;
we use the news part (130K sentences, about 5 mil-
lion words) to train systems with what we call the
small data set, and the news and a large part of
the UN data (2 million sentences, about 50 million
words) for experiments with large data sets.
The n-gram target language model was built us-
ing 250M words from the English GigaWord Cor-
pus using the SRILM toolkit.4 Taking 10% of the
English GigaWord Corpus used for building our tar-
get language model, the supertag-based target lan-
guage models were built from 25M words that were
supertagged. For the LTAG supertags experiments,
we used the LTAG English supertagger5 (Bangalore
</bodyText>
<footnote confidence="0.757341">
4http://www.speech.sri.com/projects/srilm/
5http://www.cis.upenn.edu/˜xtag/gramrelease.html
count(s, t, st)
</footnote>
<page confidence="0.986264">
292
</page>
<bodyText confidence="0.99970615">
&amp; Joshi, 1999) to tag the English part of the parallel
data and the supertag language model data. For the
CCG supertag experiments, we used the CCG su-
pertagger of (Clark &amp; Curran, 2004) and the Edin-
burgh CCG tools6 to tag the English part of the par-
allel corpus as well as the CCG supertag language
model data.
The NIST MT03 test set is used for development,
particularly for optimizing the interpolation weights
using Minimum Error Rate training (Och, 2003).
Baseline System The baseline system is a state-
of-the-art PBSMT system as described in sec-
tion 3. We built two baseline systems with two
different-sized training sets: ‘Base-SMALL’ (5 mil-
lion words) and ‘Base-LARGE’ (50 million words)
as described above. Both systems use a trigram lan-
guage model built using 250 million words from
the English GigaWord Corpus. Table 1 presents the
BLEU scores (Papineni et al., 2002) of both systems
on the NIST 2005 MT Evaluation test set.
</bodyText>
<table confidence="0.982407">
System BLEU Score
Base-SMALL 0.4008
Base-LARGE 0.4418
</table>
<tableCaption confidence="0.999844">
Table 1: Baseline systems’ BLEU scores
</tableCaption>
<subsectionHeader confidence="0.982276">
5.1 Baseline vs. Supertags on Small Data Sets
</subsectionHeader>
<bodyText confidence="0.993941666666667">
We compared the translation quality of the baseline
systems with the LTAG and CCG supertags systems
(LTAG-SMALL and CCG-SMALL). The results are
</bodyText>
<table confidence="0.99625675">
System BLEU Score
Base-SMALL 0.4008
LTAG-SMALL 0.4205
CCG-SMALL 0.4174
</table>
<tableCaption confidence="0.998952">
Table 2: LTAG and CCG systems on small data
</tableCaption>
<bodyText confidence="0.9999">
given in Table 2. All systems were trained on the
same parallel data. The LTAG supertag-based sys-
tem outperforms the baseline by 1.97 BLEU points
absolute (or 4.9% relative), while the CCG supertag-
based system scores 1.66 BLEU points over the
</bodyText>
<footnote confidence="0.754702">
6http://groups.inf.ed.ac.uk/ccg/software.html
</footnote>
<bodyText confidence="0.9943916">
baseline (4.1% relative). These significant improve-
ments indicate that the rich information in supertags
helps select better translation candidates.
POS Tags vs. Supertags A supertag is a complex
tag that localizes the dependency and the syntax in-
formation from the context, whereas a normal POS
tag just describes the general syntactic category of
the word without further constraints. In this experi-
ment we compared the effect of using supertags and
POS tags on translation quality. As can be seen
</bodyText>
<table confidence="0.996187">
System BLEU Score
Base-SMALL 0.4008
POS-SMALL 0.4073
LTAG-SMALL .0.4205
</table>
<tableCaption confidence="0.894465">
Table 3: Comparing the effect of supertags and POS
tags
</tableCaption>
<bodyText confidence="0.997496133333334">
in Table 3, while the POS tags help (0.65 BLEU
points, or 1.7% relative increase over the baseline),
they clearly underperform compared to the supertag
model (by 3.2%).
The Usefulness of a Supertagged LM In these
experiments we study the effect of the two added
feature (cost) functions: supertagged translation and
language models. We compare the baseline system
to the supertags system with the supertag phrase-
table probability but without the supertag LM. Ta-
ble 4 lists the baseline system (Base-SMALL), the
LTAG system without supertagged language model
(LTAG-TM-ONLY) and the LTAG-SMALL sys-
tem with both supertagged translation and language
models. The results presented in Table 4 indi-
</bodyText>
<table confidence="0.9894305">
System BLEU Score
Base-SMALL 0.4008
LTAG-TM-ONLY 0.4146
LTAG-SMALL .0.4205
</table>
<tableCaption confidence="0.999867">
Table 4: The effect of supertagged components
</tableCaption>
<bodyText confidence="0.9969206">
cate that the improvement is a shared contribution
between the supertagged translation and language
models: adding the LTAG TM improves BLEU
score by 1.38 points (3.4% relative) over the base-
line, with the LTAG LM improving BLEU score by
</bodyText>
<page confidence="0.994699">
293
</page>
<bodyText confidence="0.841785">
a further 0.59 points (a further 1.4% increase).
</bodyText>
<subsectionHeader confidence="0.971564">
5.2 Scalability: Larger Training Corpora
</subsectionHeader>
<bodyText confidence="0.999784210526316">
Outperforming a PBSMT system on small amounts
of training data is less impressive than doing so on
really large sets. The issue here is scalability as well
as whether the PBSMT system is able to bridge the
performance gap with the supertagged system when
reasonably large sizes of training data are used. To
this end, we trained the systems on 2 million sen-
tences of parallel data, deploying LTAG supertags
and CCG supertags. Table 5 presents the compari-
son between these systems and the baseline trained
on the same data. The LTAG system improves by
1.17 BLEU points (2.6% relative), but the CCG sys-
tem gives an even larger increase: 1.91 BLEU points
(4.3% relative). While this is slightly lower than
the 4.9% relative improvement with the smaller data
sets, the sustained increase is probably due to ob-
serving more data with different supertag contexts,
which enables the model to select better target lan-
guage phrases.
</bodyText>
<table confidence="0.98987825">
System BLEU Score
Base-LARGE 0.4418
LTAG-LARGE 0.4535
CCG-LARGE 0.4609
</table>
<tableCaption confidence="0.999656">
Table 5: The effect of more training data
</tableCaption>
<bodyText confidence="0.999902928571428">
Adding a grammaticality factor As described in
section 4.3, we integrate an impoverished grammat-
icality factor based on two standard CCG combi-
nation operations, namely Forward and Backward
Application. Table 6 compares the results of the
baseline, the CCG with an n-gram LM-only system
(CCG-LARGE) and CCG-LARGE with this ‘gram-
maticalized’ LM system (CCG-LARGE-GRAM).
We see that bringing the grammaticality tests to
bear onto the supertagged system gives a further im-
provement of 0.79 BLEU points, a 1.7% relative
increase, culminating in an overall increase of 2.7
BLEU points, or a 6.1% relative improvement over
the baseline system.
</bodyText>
<subsectionHeader confidence="0.762244">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.996429">
A natural question to ask is whether LTAG and CCG
supertags are playing similar (overlapping, or con-
</bodyText>
<table confidence="0.95039875">
System BLEU Score
Base-LARGE 0.4418
CCG-LARGE 0.4609
CCG-LARGE-GRAM 0.4688
</table>
<tableCaption confidence="0.998426">
Table 6: Comparing the effect of CCG-GRAM
</tableCaption>
<bodyText confidence="0.999811384615385">
flicting) roles in practice. Using an oracle to choose
the best output of the two systems gives a BLEU
score of 0.441, indicating that the combination pro-
vides significant room for improvement (cf. Ta-
ble 2). However, our efforts to build a system that
benefits from the combination using a simple log-
linear combination of the two models did not give
any significant performance change relative to the
baseline CCG system. Obviously, more informed
ways of combining the two could result in better per-
formance than a simple log-linear interpolation of
the components.
Figure 3 shows some example system output.
While the baseline system omits the verb giving “the
authorities that it had...”, both the LTAG and CCG
found a formulation “authorities reported that” with
a closer meaning to the reference translation “The
authorities said that”. Omitting verbs turns out to
be a problem for the baseline system when trans-
lating the notorious verbless Arabic sentences (see
Figure 4). The supertagged systems have a more
grammatically strict language model than a standard
word-level Markov model, thereby exhibiting a pref-
erence (in the CCG system especially) for the inser-
tion of a verb with a similar meaning to that con-
tained in the reference sentence.
</bodyText>
<sectionHeader confidence="0.999499" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99997975">
SMT practitioners have on the whole found it dif-
ficult to integrate syntax into their systems. In this
work, we have presented a novel model of PBSMT
which integrates supertags into the target language
model and the target side of the translation model.
Using LTAG supertags gives the best improve-
ment over a state-of-the-art PBSMT system for a
smaller data set, while CCG supertags work best on
a large 2 million-sentence pair training set. Adding
grammaticality factors based on algebraic composi-
tional operators gives the best result, namely 0.4688
BLEU, or a 6.1% relative increase over the baseline.
</bodyText>
<page confidence="0.997438">
294
</page>
<tableCaption confidence="0.701216">
Reference: The authorities said he was allowed to contact family members by phone from the armored vehicle he was in.
Baseline: the authorities that it had allowed him to communicate by phone with his family of the armored car where
LTAG: authorities reported that it had allowed him to contact by telephone with his family of armored car where
CCG: authorities reported that it had enabled him to communicate by phone his family members of the armored car where
</tableCaption>
<figureCaption confidence="0.986991">
Figure 3: Sample output from different systems
</figureCaption>
<figure confidence="0.902066333333333">
Source: wmn AlmErwf An Al$Eb AlSyny mHb llslAm . Ref: It is well known that the Chinese people are peace loving .
Baseline: It is known that the Chinese people a peace-loving .
LTAG: It is known that the Chinese people a peace loving. CCG: It is known that the Chinese people are peace loving.
</figure>
<figureCaption confidence="0.999706">
Figure 4: Verbless Arabic sentence and sample output from different systems
</figureCaption>
<bodyText confidence="0.9999564">
This result compares favourably with the best sys-
tems on the NIST 2005 Arabic–English task. We
expect more work on system integration to improve
results still further, and anticipate that similar in-
creases are to be seen for other language pairs.
</bodyText>
<sectionHeader confidence="0.9976" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999686714285714">
We would like to thank Srinivas Bangalore and
the anonymous reviewers for useful comments on
earlier versions of this paper. This work is par-
tially funded by Science Foundation Ireland Princi-
pal Investigator Award 05/IN/1732, and Netherlands
Organization for Scientific Research (NWO) VIDI
Award.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999919910714286">
S. Bangalore and A. Joshi, “Supertagging: An Ap-
proach to Almost Parsing”, Computational Linguistics
25(2):237–265, 1999.
J. Chen, S. Bangalore, and K. Vijay-Shanker, “Au-
tomated extraction of tree-adjoining grammars
from treebanks”. Natural Language Engineering,
12(3):251–299, 2006.
D. Chiang, “A Hierarchical Phrase-Based Model for Sta-
tistical Machine Translation”, in Proceedings of ACL
2005, Ann Arbor, MI., pp.263–270, 2005.
S. Clark and J. Curran, “The Importance of Supertagging
for Wide-Coverage CCG Parsing”, in Proceedings of
COLING-04, Geneva, Switzerland, pp.282–288, 2004.
J. Hockenmaier, Data and Models for Statistical Parsing
with Combinatory Categorial Grammar, PhD thesis,
University of Edinburgh, UK, 2003.
A. Joshi and Y. Schabes, “Tree Adjoining Grammars and
Lexicalized Grammars” in M. Nivat and A. Podelski
(eds.) Tree Automata and Languages, Amsterdam, The
Netherlands: North-Holland, pp.409–431, 1992.
P. Koehn, “Pharaoh: A Beam Search Decoder for phrase-
based Statistical Machine Translation Models”, in Pro-
ceedings of AMTA-04, Berlin/Heidelberg, Germany:
Springer Verlag, pp.115–124, 2004.
P. Koehn, F. Och, and D. Marcu, “Statistical Phrase-
Based Translation”, in Proceedings of HLT-NAACL
2003, Edmonton, Canada, pp.127–133, 2003.
D. Marcu, W. Wang, A. Echihabi and K. Knight, “SPMT:
Statistical Machine Translation with Syntactified Tar-
get Language Phrases”, in Proceedings of EMNLP,
Sydney, Australia, pp.44–52, 2006.
D. Marcu and W. Wong, “A Phrase-Based, Joint Probabil-
ity Model for Statistical Machine Translation”, in Pro-
ceedings of EMNLP, Philadelphia, PA., pp.133–139,
2002.
F. Och, “Minimum Error Rate Training in Statistical Ma-
chine Translation”, in Proceedings of ACL 2003, Sap-
poro, Japan, pp.160–167, 2003.
F. Och and H. Ney, “A Systematic Comparison of Var-
ious Statistical Alignment Models”, Computational
Linguistics 29:19–51, 2003.
K. Papineni, S. Roukos, T. Ward and W-J. Zhu, “BLEU:
A Method for Automatic Evaluation of Machine
Translation”, in Proceedings of ACL 2002, Philadel-
phia, PA., pp.311–318, 2002.
L. Rabiner, “A Tutorial on Hidden Markov Models and
Selected Applications in Speech Recognition”, in A.
Waibel &amp; F-K. Lee (eds.) Readings in Speech Recog-
nition, San Mateo, CA.: Morgan Kaufmann, pp.267–
296, 1990.
M. Steedman, The Syntactic Process. Cambridge, MA:
The MIT Press, 2000.
C. Tillmann and F. Xia, “A Phrase-based Unigram Model
for Statistical Machine Translation”, in Proceedings of
HLT-NAACL 2003, Edmonton, Canada. pp.106–108,
2003.
</reference>
<page confidence="0.998566">
295
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.802514">
<title confidence="0.999736">Supertagged Phrase-Based Statistical Machine Translation</title>
<author confidence="0.996886">Hany Hassan</author>
<affiliation confidence="0.9989995">School of Computing, Dublin City University,</affiliation>
<address confidence="0.994146">Dublin 9, Ireland</address>
<email confidence="0.971609">hhasan@computing.dcu.ie</email>
<author confidence="0.999035">Khalil Sima’an</author>
<affiliation confidence="0.9923615">Language and Computation, University of Amsterdam,</affiliation>
<address confidence="0.99639">Amsterdam, The Netherlands</address>
<email confidence="0.994742">simaan@science.uva.nl</email>
<author confidence="0.999892">Andy Way</author>
<affiliation confidence="0.9990005">School of Computing, Dublin City University,</affiliation>
<address confidence="0.995032">Dublin 9, Ireland</address>
<email confidence="0.984664">away@computing.dcu.ie</email>
<abstract confidence="0.995066642857143">Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate. In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Despite the differences between these two approaches, the supertaggers give similar improvements. In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A Joshi</author>
</authors>
<title>Supertagging: An Approach to Almost Parsing”,</title>
<date>1999</date>
<journal>Computational Linguistics</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2663" citStr="Bangalore &amp; Joshi, 1999" startWordPosition="382" endWordPosition="385">5) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi &amp; Schabes, 1992; Bangalore &amp; Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000). In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robus</context>
<context position="4194" citStr="Bangalore &amp; Joshi, 1999" startWordPosition="606" endWordPosition="609">fficient supertagging approach: like standard English. They employ a stochastic, top-down transtaggers, supertaggers employ probabilities based on duction process that assigns a joint probability to local context and can be implemented using finite a source sentence and each of its alternative transstate technology, e.g. Hidden Markov Models (Ban- lations when rewriting the target parse-tree into a galore &amp; Joshi, 1999). source sentence. The rewriting/transduction process There are currently two supertagging approaches is driven by “xRS rules”, each consisting of a pair available: LTAG-based (Bangalore &amp; Joshi, 1999) of a source phrase and a (possibly only partially) and CCG-based (Clark &amp; Curran, 2004). Both the lexicalized syntactified target phrase. In order to LTAG (Chen et al., 2006) and the CCG supertag extract xRS rules, the word-to-word alignment insets (Hockenmaier, 2003) were acquired from the duced from the parallel training corpus is used to WSJ section of the Penn-II Treebank using hand- guide heuristic tree ‘cutting’ criteria. built extraction rules. Here we test both the LTAG While the research of (Marcu et al., 2006) has and CCG supertaggers. We interpolate (log-linearly) much in common wi</context>
<context position="11142" citStr="Bangalore &amp; Joshi, 1999" startWordPosition="1732" endWordPosition="1735">istic theory proposes that a syntactic parser has access to an extensive lexicon of wordstructure pairs and a small, impoverished set of operations to manipulate and combine the lexical entries into parses. Examples of formal instantiations of this idea include CCG and LTAG. The lexical entries are syntactic constructs (graphs) that specify information such as POS tag, subcategorization/dependency information and other syntactic constraints at the level of agreement features. One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks (Bangalore &amp; Joshi, 1999; Clark &amp; Curran, 2004). A supertag (see Figure 1) represents a complex, linguistic word category that encodes a syntactic structure expressing a specific local behaviour of a word, in terms of the arguments it takes (e.g. subject, object) and the syntactic environment in which it appears. In fact, in LTAG a supertag is an elementary tree and in CCG it is a CCG lexical category. Both descriptions can be viewed as closely related functional descriptions. The term “supertagging” (Bangalore &amp; Joshi, 1999) refers to tagging the words of a sentence, each 290 with a supertag. When well-formed, an or</context>
<context position="12464" citStr="Bangalore &amp; Joshi, 1999" startWordPosition="1945" endWordPosition="1948">ts/parses that can be obtained by assembling the supertags together using the appropriate combinatory operators (such as substitution and adjunction in LTAG or function application and combination in CCG). Akin to POS tagging, the process of supertagging an input utterance proceeds with statistics that are based on the probability of a word-supertag pair given their Markovian or local context (Bangalore &amp; Joshi, 1999; Clark &amp; Curran, 2004). This is the main difference with full parsing: supertagging the input utterance need not result in a fully connected graph. The LTAG-based supertagger of (Bangalore &amp; Joshi, 1999) is a standard HMM tagger and consists of a (second-order) Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag (just like standard HMM-based POS taggers). The CCG supertagger (Clark &amp; Curran, 2004) is based on log-linear probabilities that condition a supertag on features representing its context. The CCG supertagger does not constitute a language model nor are the Maximum Entropy estimates directly interpretable as such. In our model we employ the CCG supertagger to obtain the best sequences of supertags for a corpus of sente</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>S. Bangalore and A. Joshi, “Supertagging: An Approach to Almost Parsing”, Computational Linguistics 25(2):237–265, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>S Bangalore</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automated extraction of tree-adjoining grammars from treebanks”.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="4369" citStr="Chen et al., 2006" startWordPosition="635" endWordPosition="638">nt probability to local context and can be implemented using finite a source sentence and each of its alternative transstate technology, e.g. Hidden Markov Models (Ban- lations when rewriting the target parse-tree into a galore &amp; Joshi, 1999). source sentence. The rewriting/transduction process There are currently two supertagging approaches is driven by “xRS rules”, each consisting of a pair available: LTAG-based (Bangalore &amp; Joshi, 1999) of a source phrase and a (possibly only partially) and CCG-based (Clark &amp; Curran, 2004). Both the lexicalized syntactified target phrase. In order to LTAG (Chen et al., 2006) and the CCG supertag extract xRS rules, the word-to-word alignment insets (Hockenmaier, 2003) were acquired from the duced from the parallel training corpus is used to WSJ section of the Penn-II Treebank using hand- guide heuristic tree ‘cutting’ criteria. built extraction rules. Here we test both the LTAG While the research of (Marcu et al., 2006) has and CCG supertaggers. We interpolate (log-linearly) much in common with the approach proposed here the supertagged components (language model and (such as the syntactified target phrases), there rephrase table) with the components of a standard</context>
<context position="13319" citStr="Chen et al., 2006" startWordPosition="2085" endWordPosition="2088">agger (Clark &amp; Curran, 2004) is based on log-linear probabilities that condition a supertag on features representing its context. The CCG supertagger does not constitute a language model nor are the Maximum Entropy estimates directly interpretable as such. In our model we employ the CCG supertagger to obtain the best sequences of supertags for a corpus of sentences from which we obtain language model statistics. Besides the difference in probabilities and statistical estimates, these two supertaggers differ in the way the supertags are extracted from the Penn Treebank, cf. (Hockenmaier, 2003; Chen et al., 2006). Both supertaggers achieve a supertagging accuracy of 90–92%. Three aspects make supertags attractive in the context of SMT. Firstly, supertags are rich syntactic constructs that exist for individual words and so they are easy to integrate into SMT models that can be based on any level of granularity, be it wordor phrase-based. Secondly, supertags specify the local syntactic constraints for a word, which resonates well with sequential (finite state) statistical (e.g. Markov) models. Finally, because supertags are rich lexical descriptions that represent underspecification in parsing, it is po</context>
</contexts>
<marker>Chen, Bangalore, Vijay-Shanker, 2006</marker>
<rawString>J. Chen, S. Bangalore, and K. Vijay-Shanker, “Automated extraction of tree-adjoining grammars from treebanks”. Natural Language Engineering, 12(3):251–299, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation”,</title>
<date>2005</date>
<booktitle>in Proceedings of ACL 2005,</booktitle>
<location>Ann Arbor, MI., pp.263–270,</location>
<contexts>
<context position="1883" citStr="Chiang, 2005" startWordPosition="266" endWordPosition="267">bility and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task. 1 Introduction Within the field of Machine Translation, by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003; Tillmann &amp; Xia, 2003). However, unlike in rule- and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ</context>
<context position="7190" citStr="Chiang, 2005" startWordPosition="1083" endWordPosition="1084">y aim at building a fully con2 Related Work nected graph. Until very recently, the experience with adding syn- A second major difference with xRS rules is that tax to PBSMT systems was negative. For example, our supertag-enriched target phrases need not be (Koehn et al., 2003) demonstrated that adding syn- generalized into (xRS or any other) rules that work tax actually harmed the quality of their SMT system. with abstract categories. Finally, like POS tagging, Among the first to demonstrate improvement when supertagging is more efficient than actual parsing or adding recursive structure was (Chiang, 2005), who tree transduction. allows for hierarchical phrase probabilities that han- 3 Baseline Phrase-Based SMT System dle a range of reordering phenomena in the correct We present the baseline PBSMT model which we fashion. Chiang’s derived grammar does not rely on extend with supertags in the next section. Our any linguistic annotations or assumptions, so that the baseline PBSMT model uses GIZA++2 to obtain ‘syntax’ induced is not linguistically motivated. word-level alignments in both language directions. Coming right up to date, (Marcu et al., 2006) The bidirectional word alignment is used to o</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang, “A Hierarchical Phrase-Based Model for Statistical Machine Translation”, in Proceedings of ACL 2005, Ann Arbor, MI., pp.263–270, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>The Importance of Supertagging for Wide-Coverage CCG Parsing”,</title>
<date>2004</date>
<booktitle>in Proceedings of COLING-04,</booktitle>
<location>Geneva, Switzerland, pp.282–288,</location>
<contexts>
<context position="4282" citStr="Clark &amp; Curran, 2004" startWordPosition="621" endWordPosition="624">ranstaggers, supertaggers employ probabilities based on duction process that assigns a joint probability to local context and can be implemented using finite a source sentence and each of its alternative transstate technology, e.g. Hidden Markov Models (Ban- lations when rewriting the target parse-tree into a galore &amp; Joshi, 1999). source sentence. The rewriting/transduction process There are currently two supertagging approaches is driven by “xRS rules”, each consisting of a pair available: LTAG-based (Bangalore &amp; Joshi, 1999) of a source phrase and a (possibly only partially) and CCG-based (Clark &amp; Curran, 2004). Both the lexicalized syntactified target phrase. In order to LTAG (Chen et al., 2006) and the CCG supertag extract xRS rules, the word-to-word alignment insets (Hockenmaier, 2003) were acquired from the duced from the parallel training corpus is used to WSJ section of the Penn-II Treebank using hand- guide heuristic tree ‘cutting’ criteria. built extraction rules. Here we test both the LTAG While the research of (Marcu et al., 2006) has and CCG supertaggers. We interpolate (log-linearly) much in common with the approach proposed here the supertagged components (language model and (such as th</context>
<context position="11165" citStr="Clark &amp; Curran, 2004" startWordPosition="1736" endWordPosition="1739">t a syntactic parser has access to an extensive lexicon of wordstructure pairs and a small, impoverished set of operations to manipulate and combine the lexical entries into parses. Examples of formal instantiations of this idea include CCG and LTAG. The lexical entries are syntactic constructs (graphs) that specify information such as POS tag, subcategorization/dependency information and other syntactic constraints at the level of agreement features. One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks (Bangalore &amp; Joshi, 1999; Clark &amp; Curran, 2004). A supertag (see Figure 1) represents a complex, linguistic word category that encodes a syntactic structure expressing a specific local behaviour of a word, in terms of the arguments it takes (e.g. subject, object) and the syntactic environment in which it appears. In fact, in LTAG a supertag is an elementary tree and in CCG it is a CCG lexical category. Both descriptions can be viewed as closely related functional descriptions. The term “supertagging” (Bangalore &amp; Joshi, 1999) refers to tagging the words of a sentence, each 290 with a supertag. When well-formed, an ordered sequence of super</context>
<context position="12729" citStr="Clark &amp; Curran, 2004" startWordPosition="1989" endWordPosition="1992">rance proceeds with statistics that are based on the probability of a word-supertag pair given their Markovian or local context (Bangalore &amp; Joshi, 1999; Clark &amp; Curran, 2004). This is the main difference with full parsing: supertagging the input utterance need not result in a fully connected graph. The LTAG-based supertagger of (Bangalore &amp; Joshi, 1999) is a standard HMM tagger and consists of a (second-order) Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag (just like standard HMM-based POS taggers). The CCG supertagger (Clark &amp; Curran, 2004) is based on log-linear probabilities that condition a supertag on features representing its context. The CCG supertagger does not constitute a language model nor are the Maximum Entropy estimates directly interpretable as such. In our model we employ the CCG supertagger to obtain the best sequences of supertags for a corpus of sentences from which we obtain language model statistics. Besides the difference in probabilities and statistical estimates, these two supertaggers differ in the way the supertags are extracted from the Penn Treebank, cf. (Hockenmaier, 2003; Chen et al., 2006). Both sup</context>
<context position="20101" citStr="Clark &amp; Curran, 2004" startWordPosition="3179" endWordPosition="3182">s from the English GigaWord Corpus using the SRILM toolkit.4 Taking 10% of the English GigaWord Corpus used for building our target language model, the supertag-based target language models were built from 25M words that were supertagged. For the LTAG supertags experiments, we used the LTAG English supertagger5 (Bangalore 4http://www.speech.sri.com/projects/srilm/ 5http://www.cis.upenn.edu/˜xtag/gramrelease.html count(s, t, st) 292 &amp; Joshi, 1999) to tag the English part of the parallel data and the supertag language model data. For the CCG supertag experiments, we used the CCG supertagger of (Clark &amp; Curran, 2004) and the Edinburgh CCG tools6 to tag the English part of the parallel corpus as well as the CCG supertag language model data. The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003). Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3. We built two baseline systems with two different-sized training sets: ‘Base-SMALL’ (5 million words) and ‘Base-LARGE’ (50 million words) as described above. Both systems use a trigram language model built using 250 million words</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>S. Clark and J. Curran, “The Importance of Supertagging for Wide-Coverage CCG Parsing”, in Proceedings of COLING-04, Geneva, Switzerland, pp.282–288, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar, PhD thesis,</title>
<date>2003</date>
<institution>University of Edinburgh,</institution>
<location>UK,</location>
<contexts>
<context position="4463" citStr="Hockenmaier, 2003" startWordPosition="651" endWordPosition="652"> of its alternative transstate technology, e.g. Hidden Markov Models (Ban- lations when rewriting the target parse-tree into a galore &amp; Joshi, 1999). source sentence. The rewriting/transduction process There are currently two supertagging approaches is driven by “xRS rules”, each consisting of a pair available: LTAG-based (Bangalore &amp; Joshi, 1999) of a source phrase and a (possibly only partially) and CCG-based (Clark &amp; Curran, 2004). Both the lexicalized syntactified target phrase. In order to LTAG (Chen et al., 2006) and the CCG supertag extract xRS rules, the word-to-word alignment insets (Hockenmaier, 2003) were acquired from the duced from the parallel training corpus is used to WSJ section of the Penn-II Treebank using hand- guide heuristic tree ‘cutting’ criteria. built extraction rules. Here we test both the LTAG While the research of (Marcu et al., 2006) has and CCG supertaggers. We interpolate (log-linearly) much in common with the approach proposed here the supertagged components (language model and (such as the syntactified target phrases), there rephrase table) with the components of a standard main a number of significant differences. Firstly, PBSMT system. Our experiments on the Arabi</context>
<context position="13299" citStr="Hockenmaier, 2003" startWordPosition="2082" endWordPosition="2084">rs). The CCG supertagger (Clark &amp; Curran, 2004) is based on log-linear probabilities that condition a supertag on features representing its context. The CCG supertagger does not constitute a language model nor are the Maximum Entropy estimates directly interpretable as such. In our model we employ the CCG supertagger to obtain the best sequences of supertags for a corpus of sentences from which we obtain language model statistics. Besides the difference in probabilities and statistical estimates, these two supertaggers differ in the way the supertags are extracted from the Penn Treebank, cf. (Hockenmaier, 2003; Chen et al., 2006). Both supertaggers achieve a supertagging accuracy of 90–92%. Three aspects make supertags attractive in the context of SMT. Firstly, supertags are rich syntactic constructs that exist for individual words and so they are easy to integrate into SMT models that can be based on any level of granularity, be it wordor phrase-based. Secondly, supertags specify the local syntactic constraints for a word, which resonates well with sequential (finite state) statistical (e.g. Markov) models. Finally, because supertags are rich lexical descriptions that represent underspecification </context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>J. Hockenmaier, Data and Models for Statistical Parsing with Combinatory Categorial Grammar, PhD thesis, University of Edinburgh, UK, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree Adjoining Grammars and Lexicalized Grammars”</title>
<date>1992</date>
<booktitle>Tree Automata and Languages,</booktitle>
<pages>409--431</pages>
<editor>in M. Nivat and A. Podelski (eds.)</editor>
<publisher>North-Holland,</publisher>
<location>Amsterdam, The Netherlands:</location>
<contexts>
<context position="2637" citStr="Joshi &amp; Schabes, 1992" startWordPosition="378" endWordPosition="381">tem. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi &amp; Schabes, 1992; Bangalore &amp; Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000). In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory ope</context>
</contexts>
<marker>Joshi, Schabes, 1992</marker>
<rawString>A. Joshi and Y. Schabes, “Tree Adjoining Grammars and Lexicalized Grammars” in M. Nivat and A. Podelski (eds.) Tree Automata and Languages, Amsterdam, The Netherlands: North-Holland, pp.409–431, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: A Beam Search Decoder for phrasebased Statistical Machine Translation Models”,</title>
<date>2004</date>
<booktitle>in Proceedings of AMTA-04,</booktitle>
<pages>115--124</pages>
<publisher>Springer Verlag,</publisher>
<location>Berlin/Heidelberg, Germany:</location>
<contexts>
<context position="18537" citStr="Koehn, 2004" startWordPosition="2930" endWordPosition="2931">atory operator violations in a sequence of supertags (cf. Figure 2). For a supertag sequence of length (L) which has (V ) operator violations (as measured by the CCG system), the language model P will be adjusted as P* = P x (1 − �i ). This is of course no longer a simple smoothed maximum-likelihood estimate nor is it a true probability. Nevertheless, this mechanism provides a simple, efficient integration of a global compositionality (grammaticality) measure into the n-gram language model over supertags. Decoder The decoder used in this work is Moses, a log-linear decoder similar to Pharaoh (Koehn, 2004), modified to accommodate supertag phrase probabilities and supertag language models. 5 Experiments In this section we present a number of experiments that demonstrate the effect of lexical syntax on translation quality. We carried out experiments on the NIST open domain news translation task from Arabic into English. We performed a number of experiments to examine the effect of supertagging approaches (CCG or LTAG) with varying data sizes. Data and Settings The experiments were conducted for Arabic to English translation and tested on the NIST 2005 evaluation set. The systems were trained on </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn, “Pharaoh: A Beam Search Decoder for phrasebased Statistical Machine Translation Models”, in Proceedings of AMTA-04, Berlin/Heidelberg, Germany: Springer Verlag, pp.115–124, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical PhraseBased Translation”,</title>
<date>2003</date>
<booktitle>in Proceedings of HLT-NAACL 2003,</booktitle>
<location>Edmonton, Canada, pp.127–133,</location>
<contexts>
<context position="1652" citStr="Koehn et al., 2003" startWordPosition="230" endWordPosition="233"> supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task. 1 Introduction Within the field of Machine Translation, by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003; Tillmann &amp; Xia, 2003). However, unlike in rule- and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending </context>
<context position="6854" citStr="Koehn et al., 2003" startWordPosition="1031" endWordPosition="1034"> detail our ap- a parse). On the other hand, because supertags are proach. Section 5 describes the experiments carried lexical entries, they facilitate robust syntactic proout, together with the results obtained. Section 6 cessing (using Markov models, for instance) which concludes, and provides avenues for further work. does not necessarily aim at building a fully con2 Related Work nected graph. Until very recently, the experience with adding syn- A second major difference with xRS rules is that tax to PBSMT systems was negative. For example, our supertag-enriched target phrases need not be (Koehn et al., 2003) demonstrated that adding syn- generalized into (xRS or any other) rules that work tax actually harmed the quality of their SMT system. with abstract categories. Finally, like POS tagging, Among the first to demonstrate improvement when supertagging is more efficient than actual parsing or adding recursive structure was (Chiang, 2005), who tree transduction. allows for hierarchical phrase probabilities that han- 3 Baseline Phrase-Based SMT System dle a range of reordering phenomena in the correct We present the baseline PBSMT model which we fashion. Chiang’s derived grammar does not rely on ex</context>
<context position="16683" citStr="Koehn et al., 2003" startWordPosition="2629" endWordPosition="2632"> 2: Example CCG operator violations: V = 2 and L = 3, and so the penalty factor is 1/3. backoff version P(s |t)P(s |ST), where we import the baseline phrase table probability and exploit the probability of a source phrase given the target supertag sequence. A model in which we omit P(s |ST) turns out to be slightly less optimal than this one. As in most state-of-the-art PBSMT systems, we use GIZA++ to obtain word-level alignments in both language directions. The bidirectional word alignment is used to obtain lexical phrase translation pairs using heuristics presented in (Och &amp; Ney, 2003) and (Koehn et al., 2003). Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency as follows: P _ count (s, t) ph(s |t) Es count(s, t) For each extracted lexical phrase pair, we extract the corresponding supertagged phrase pairs from the supertagged target sequence in the training corpus (cf. section 5). For each lexical phrase pair, there is at least one corresponding supertagged phrase pair. The probability of the supertagged phrase pair is estimated by relative frequency as follows: Pst(s|t, st) = E s count(s, t, st) 4.3 LMs with a Grammaticality Factor T</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu, “Statistical PhraseBased Translation”, in Proceedings of HLT-NAACL 2003, Edmonton, Canada, pp.127–133, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wang</author>
<author>A Echihabi</author>
<author>K Knight</author>
</authors>
<title>SPMT: Statistical Machine Translation with Syntactified Target Language Phrases”,</title>
<date>2006</date>
<booktitle>in Proceedings of EMNLP,</booktitle>
<location>Sydney, Australia, pp.44–52,</location>
<contexts>
<context position="1908" citStr="Marcu et al., 2006" startWordPosition="269" endWordPosition="272">ity of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task. 1 Introduction Within the field of Machine Translation, by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003; Tillmann &amp; Xia, 2003). However, unlike in rule- and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commo</context>
<context position="4720" citStr="Marcu et al., 2006" startWordPosition="692" endWordPosition="695">ven by “xRS rules”, each consisting of a pair available: LTAG-based (Bangalore &amp; Joshi, 1999) of a source phrase and a (possibly only partially) and CCG-based (Clark &amp; Curran, 2004). Both the lexicalized syntactified target phrase. In order to LTAG (Chen et al., 2006) and the CCG supertag extract xRS rules, the word-to-word alignment insets (Hockenmaier, 2003) were acquired from the duced from the parallel training corpus is used to WSJ section of the Penn-II Treebank using hand- guide heuristic tree ‘cutting’ criteria. built extraction rules. Here we test both the LTAG While the research of (Marcu et al., 2006) has and CCG supertaggers. We interpolate (log-linearly) much in common with the approach proposed here the supertagged components (language model and (such as the syntactified target phrases), there rephrase table) with the components of a standard main a number of significant differences. Firstly, PBSMT system. Our experiments on the Arabic– rather than induce millions of xRS rules from parEnglish NIST 2005 test suite show that each of the allel data, we extract phrase pairs in the standard supertagged systems significantly improves over the way (Och &amp; Ney, 2003) and associate with each base</context>
<context position="7744" citStr="Marcu et al., 2006" startWordPosition="1165" endWordPosition="1168">n actual parsing or adding recursive structure was (Chiang, 2005), who tree transduction. allows for hierarchical phrase probabilities that han- 3 Baseline Phrase-Based SMT System dle a range of reordering phenomena in the correct We present the baseline PBSMT model which we fashion. Chiang’s derived grammar does not rely on extend with supertags in the next section. Our any linguistic annotations or assumptions, so that the baseline PBSMT model uses GIZA++2 to obtain ‘syntax’ induced is not linguistically motivated. word-level alignments in both language directions. Coming right up to date, (Marcu et al., 2006) The bidirectional word alignment is used to obtain demonstrate that ‘syntactified’ target language phrase translation pairs using heuristics presented in phrases can improve translation quality for Chinese– 289 2http://www.fjoch.com/GIZA++.html (Och &amp; Ney, 2003) and (Koehn et al., 2003), and the Moses decoder was used for phrase extraction and decoding.3 Let t and s be the target and source language sentences respectively. Any (target or source) sentence x will consist of two parts: a bag of elements (words/phrases etc.) and an order over that bag. In other words, x = (0x� Ox), where 0x stand</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>D. Marcu, W. Wang, A. Echihabi and K. Knight, “SPMT: Statistical Machine Translation with Syntactified Target Language Phrases”, in Proceedings of EMNLP, Sydney, Australia, pp.44–52, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
<author>“A Phrase-Based</author>
</authors>
<title>Joint Probability Model for Statistical Machine Translation”,</title>
<date>2002</date>
<booktitle>in Proceedings of EMNLP,</booktitle>
<location>Philadelphia, PA., pp.133–139,</location>
<marker>Marcu, Wong, Phrase-Based, 2002</marker>
<rawString>D. Marcu and W. Wong, “A Phrase-Based, Joint Probability Model for Statistical Machine Translation”, in Proceedings of EMNLP, Philadelphia, PA., pp.133–139, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation”,</title>
<date>2003</date>
<booktitle>in Proceedings of ACL 2003,</booktitle>
<location>Sapporo, Japan, pp.160–167,</location>
<contexts>
<context position="20374" citStr="Och, 2003" startWordPosition="3228" endWordPosition="3229">he LTAG English supertagger5 (Bangalore 4http://www.speech.sri.com/projects/srilm/ 5http://www.cis.upenn.edu/˜xtag/gramrelease.html count(s, t, st) 292 &amp; Joshi, 1999) to tag the English part of the parallel data and the supertag language model data. For the CCG supertag experiments, we used the CCG supertagger of (Clark &amp; Curran, 2004) and the Edinburgh CCG tools6 to tag the English part of the parallel corpus as well as the CCG supertag language model data. The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003). Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3. We built two baseline systems with two different-sized training sets: ‘Base-SMALL’ (5 million words) and ‘Base-LARGE’ (50 million words) as described above. Both systems use a trigram language model built using 250 million words from the English GigaWord Corpus. Table 1 presents the BLEU scores (Papineni et al., 2002) of both systems on the NIST 2005 MT Evaluation test set. System BLEU Score Base-SMALL 0.4008 Base-LARGE 0.4418 Table 1: Baseline systems’ BLEU scores 5.1 Baseline vs. Supertags on S</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och, “Minimum Error Rate Training in Statistical Machine Translation”, in Proceedings of ACL 2003, Sapporo, Japan, pp.160–167, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<date>2003</date>
<journal>A Systematic Comparison of Various Statistical Alignment Models”, Computational Linguistics</journal>
<volume>29</volume>
<contexts>
<context position="5291" citStr="Och &amp; Ney, 2003" startWordPosition="782" endWordPosition="785">AG While the research of (Marcu et al., 2006) has and CCG supertaggers. We interpolate (log-linearly) much in common with the approach proposed here the supertagged components (language model and (such as the syntactified target phrases), there rephrase table) with the components of a standard main a number of significant differences. Firstly, PBSMT system. Our experiments on the Arabic– rather than induce millions of xRS rules from parEnglish NIST 2005 test suite show that each of the allel data, we extract phrase pairs in the standard supertagged systems significantly improves over the way (Och &amp; Ney, 2003) and associate with each baseline PBSMT system. Interestingly, combining phrase-pair a set of target language syntactic structhe two taggers together diminishes the benefits of tures based on supertag sequences. Relative to using supertagging seen with the individual LTAG and arbitrary parse-chunks, the power of supertags lies CCG systems. In this paper we discuss these and in the fact that they are, syntactically speaking, rich other empirical issues. lexical descriptions. A supertag can be assigned to The remainder of the paper is organised as fol- every word in a phrase. On the one hand, th</context>
<context position="8007" citStr="Och &amp; Ney, 2003" startWordPosition="1198" endWordPosition="1201"> which we fashion. Chiang’s derived grammar does not rely on extend with supertags in the next section. Our any linguistic annotations or assumptions, so that the baseline PBSMT model uses GIZA++2 to obtain ‘syntax’ induced is not linguistically motivated. word-level alignments in both language directions. Coming right up to date, (Marcu et al., 2006) The bidirectional word alignment is used to obtain demonstrate that ‘syntactified’ target language phrase translation pairs using heuristics presented in phrases can improve translation quality for Chinese– 289 2http://www.fjoch.com/GIZA++.html (Och &amp; Ney, 2003) and (Koehn et al., 2003), and the Moses decoder was used for phrase extraction and decoding.3 Let t and s be the target and source language sentences respectively. Any (target or source) sentence x will consist of two parts: a bag of elements (words/phrases etc.) and an order over that bag. In other words, x = (0x� Ox), where 0x stands for the bag of phrases that constitute x, and Ox for the order of the phrases as given in x (Ox can be implemented as a function from a bag of tokens 0x to a set with a finite number of positions). Hence, we may separate order from content: 4 Our Approach: Supe</context>
<context position="16658" citStr="Och &amp; Ney, 2003" startWordPosition="2624" endWordPosition="2627">_N 2 Violations Figure 2: Example CCG operator violations: V = 2 and L = 3, and so the penalty factor is 1/3. backoff version P(s |t)P(s |ST), where we import the baseline phrase table probability and exploit the probability of a source phrase given the target supertag sequence. A model in which we omit P(s |ST) turns out to be slightly less optimal than this one. As in most state-of-the-art PBSMT systems, we use GIZA++ to obtain word-level alignments in both language directions. The bidirectional word alignment is used to obtain lexical phrase translation pairs using heuristics presented in (Och &amp; Ney, 2003) and (Koehn et al., 2003). Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency as follows: P _ count (s, t) ph(s |t) Es count(s, t) For each extracted lexical phrase pair, we extract the corresponding supertagged phrase pairs from the supertagged target sequence in the training corpus (cf. section 5). For each lexical phrase pair, there is at least one corresponding supertagged phrase pair. The probability of the supertagged phrase pair is estimated by relative frequency as follows: Pst(s|t, st) = E s count(s, t, st) 4.3 LMs with </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney, “A Systematic Comparison of Various Statistical Alignment Models”, Computational Linguistics 29:19–51, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation”,</title>
<date>2002</date>
<booktitle>in Proceedings of ACL 2002,</booktitle>
<location>Philadelphia, PA., pp.311–318,</location>
<contexts>
<context position="20792" citStr="Papineni et al., 2002" startWordPosition="3294" endWordPosition="3297"> corpus as well as the CCG supertag language model data. The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003). Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3. We built two baseline systems with two different-sized training sets: ‘Base-SMALL’ (5 million words) and ‘Base-LARGE’ (50 million words) as described above. Both systems use a trigram language model built using 250 million words from the English GigaWord Corpus. Table 1 presents the BLEU scores (Papineni et al., 2002) of both systems on the NIST 2005 MT Evaluation test set. System BLEU Score Base-SMALL 0.4008 Base-LARGE 0.4418 Table 1: Baseline systems’ BLEU scores 5.1 Baseline vs. Supertags on Small Data Sets We compared the translation quality of the baseline systems with the LTAG and CCG supertags systems (LTAG-SMALL and CCG-SMALL). The results are System BLEU Score Base-SMALL 0.4008 LTAG-SMALL 0.4205 CCG-SMALL 0.4174 Table 2: LTAG and CCG systems on small data given in Table 2. All systems were trained on the same parallel data. The LTAG supertag-based system outperforms the baseline by 1.97 BLEU point</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward and W-J. Zhu, “BLEU: A Method for Automatic Evaluation of Machine Translation”, in Proceedings of ACL 2002, Philadelphia, PA., pp.311–318, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition”,</title>
<date>1990</date>
<booktitle>Readings in Speech Recognition,</booktitle>
<pages>267--296</pages>
<editor>in A. Waibel &amp; F-K. Lee (eds.)</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.:</location>
<marker>Rabiner, 1990</marker>
<rawString>L. Rabiner, “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition”, in A. Waibel &amp; F-K. Lee (eds.) Readings in Speech Recognition, San Mateo, CA.: Morgan Kaufmann, pp.267– 296, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="2713" citStr="Steedman, 2000" startWordPosition="390" endWordPosition="391">d, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi &amp; Schabes, 1992; Bangalore &amp; Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000). In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs (‘supertags’) that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robust 1These operators neither carry nor presuppose fu</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman, The Syntactic Process. Cambridge, MA: The MIT Press, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>F Xia</author>
</authors>
<title>A Phrase-based Unigram Model for Statistical Machine Translation”,</title>
<date>2003</date>
<booktitle>in Proceedings of HLT-NAACL 2003,</booktitle>
<location>Edmonton,</location>
<contexts>
<context position="1675" citStr="Tillmann &amp; Xia, 2003" startWordPosition="234" endWordPosition="237">so explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task. 1 Introduction Within the field of Machine Translation, by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003; Tillmann &amp; Xia, 2003). However, unlike in rule- and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table. In this paper we explore a novel approach towards extending a standard PBSMT system</context>
</contexts>
<marker>Tillmann, Xia, 2003</marker>
<rawString>C. Tillmann and F. Xia, “A Phrase-based Unigram Model for Statistical Machine Translation”, in Proceedings of HLT-NAACL 2003, Edmonton, Canada. pp.106–108, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>