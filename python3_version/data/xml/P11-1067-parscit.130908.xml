<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000526">
<title confidence="0.9993085">
Neutralizing Linguistically Problematic Annotations
in Unsupervised Dependency Parsing Evaluation
</title>
<author confidence="0.999584">
Roy Schwartz&apos; Omri Abend&apos;* Roi Reichart2 Ari Rappoport&apos;
</author>
<affiliation confidence="0.999584">
&apos;Institute of Computer Science
Hebrew University of Jerusalem
</affiliation>
<email confidence="0.972064">
{roys02|omria01|arir}@cs.huji.ac.il
</email>
<affiliation confidence="0.9819475">
2Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.99776">
roiri@csail.mit.edu
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998578647058824">
Dependency parsing is a central NLP task. In
this paper we show that the common eval-
uation for unsupervised dependency parsing
is highly sensitive to problematic annotations.
We show that for three leading unsupervised
parsers (Klein and Manning, 2004; Cohen and
Smith, 2009; Spitkovsky et al., 2010a), a small
set of parameters can be found whose mod-
ification yields a significant improvement in
standard evaluation measures. These param-
eters correspond to local cases where no lin-
guistic consensus exists as to the proper gold
annotation. Therefore, the standard evaluation
does not provide a true indication of algorithm
quality. We present a new measure, Neutral
Edge Direction (NED), and show that it greatly
reduces this undesired phenomenon.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999804461538462">
Unsupervised induction of dependency parsers is a
major NLP task that attracts a substantial amount
of research (Klein and Manning, 2004; Cohen et
al., 2008; Headden et al., 2009; Spitkovsky et al.,
2010a; Gillenwater et al., 2010; Berg-Kirkpatrick
et al., 2010; Blunsom and Cohn, 2010, inter alia).
Parser quality is usually evaluated by comparing its
output to a gold standard whose annotations are lin-
guistically motivated. However, there are cases in
which there is no linguistic consensus as to what the
correct annotation is (K¨ubler et al., 2009). Examples
include which verb is the head in a verb group struc-
ture (e.g., “can” or “eat” in “can eat”), and which
</bodyText>
<note confidence="0.815662">
* Omri Abend is grateful to the Azrieli Foundation for the
award of an Azrieli Fellowship.
</note>
<bodyText confidence="0.999869206896552">
noun is the head in a sequence of proper nouns (e.g.,
“John” or “Doe” in “John Doe”). We refer to such
annotations as (linguistically) problematic. For such
cases, evaluation measures should not punish the al-
gorithm for deviating from the gold standard.
In this paper we show that the evaluation mea-
sures reported in current works are highly sensitive
to the annotation in problematic cases, and propose
a simple new measure that greatly neutralizes the
problem.
We start from the following observation: for three
leading algorithms (Klein and Manning, 2004; Co-
hen and Smith, 2009; Spitkovsky et al., 2010a), a
small set (at most 18 out of a few thousands) of pa-
rameters can be found whose modification dramati-
cally improves the standard evaluation measures (the
attachment score measure by 9.3-15.1%, and the
undirected measure by a smaller but still significant
1.3-7.7%). The phenomenon is implementation in-
dependent, occurring with several algorithms based
on a fundamental probabilistic dependency model1.
We show that these parameter changes can be
mapped to edge direction changes in local structures
in the dependency graph, and that these correspond
to problematic annotations. Thus, the standard eval-
uation measures do not reflect the true quality of the
evaluated algorithm.
We explain why the standard undirected evalua-
tion measure is in fact sensitive to such edge direc-
</bodyText>
<footnote confidence="0.9989276">
1It is also language-independent; we have produced it in five
different languages: English, Czech, Japanese, Portuguese, and
Turkish. Due to space considerations, in this paper we focus
on English, because it is the most studied language for this task
and the most practically useful one at present.
</footnote>
<page confidence="0.978971">
663
</page>
<note confidence="0.9797">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 663–672,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999891">
tion changes, and present a new evaluation measure,
Neutral Edge Direction (NED), which greatly allevi-
ates the problem by ignoring the edge direction in lo-
cal structures. Using NED, manual modifications of
model parameters always yields small performance
differences. Moreover, NED sometimes punishes
such manual parameter tweaking by yielding worse
results. We explain this behavior using an exper-
iment revealing that NED always prefers the struc-
tures that are more consistent with the modeling as-
sumptions lying in the basis of the algorithm. When
manual parameter modification is done against this
preference, the NED results decrease.
The contributions of this paper are as follows.
First, we show the impact of a small number of an-
notation decisions on the performance of unsuper-
vised dependency parsers. Second, we observe that
often these decisions are linguistically controversial
and therefore this impact is misleading. This reveals
a problem in the common evaluation of unsuper-
vised dependency parsing. This is further demon-
strated by noting that recent papers evaluate the task
using three gold standards which differ in such deci-
sions and which yield substantially different results.
Third, we present the NED measure, which is agnos-
tic to errors arising from choosing the non-gold di-
rection in such cases.
Section 2 reviews related work. Section 3 de-
scribes the performed parameter modifications. Sec-
tion 4 discusses the linguistic controversies in anno-
tating problematic dependency structures. Section 5
presents NED. Section 6 describes experiments with
it. A discussion is given in Section 7.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999708083333333">
Grammar induction received considerable attention
over the years (see (Clark, 2001; Klein, 2005) for
reviews). For unsupervised dependency parsing, the
Dependency Model with Valence (DMV) (Klein and
Manning, 2004) was the first to beat the simple
right-branching baseline. A technical description of
DMV is given at the end of this section.
The great majority of recent works, including
those experimented with in this paper, are elabora-
tions of DMV. Smith and Eisner (2005) improved
the DMV results by generalizing the function maxi-
mized by DMV’s EM training algorithm. Smith and
Eisner (2006) used a structural locality bias, experi-
menting on five languages. Cohen et al. (2008) ex-
tended DMV by using a variational EM training al-
gorithm and adding logistic normal priors. Cohen
and Smith (2009, 2010) further extended it by us-
ing a shared logistic normal prior which provided a
new way to encode the knowledge that some POS
tags are more similar than others. A bilingual joint
learning further improved their performance.
Headden et al. (2009) obtained the best reported
results on WSJ10 by using a lexical extension of
DMV. Gillenwater et al. (2010) used posterior reg-
ularization to bias the training towards a small num-
ber of parent-child combinations. Berg-Kirkpatrick
et al. (2010) added new features to the M step of the
DMV EM procedure. Berg-Kirkpatrick and Klein
(2010) used a phylogenetic tree to model parame-
ter drift between different languages. Spitkovsky
et al. (2010a) explored several training protocols
for DMV. Spitkovsky et al. (2010c) showed the
benefits of Viterbi (“hard”) EM to DMV training.
Spitkovsky et al. (2010b) presented a novel lightly-
supervised approach that used hyper-text mark-up
annotation of web-pages to train DMV.
A few non-DMV-based works were recently pre-
sented. Daum´e III (2009) used shift-reduce tech-
niques. Blunsom and Cohn (2010) used tree sub-
stitution grammar to achieve best results on WSJ&apos;.
Druck et al. (2009) took a semi-supervised ap-
proach, using a set of rules such as “A noun is usu-
ally the parent of a determiner which is to its left”,
experimenting on several languages. Naseem et al.
(2010) further extended this idea by using a single
set of rules which globally applies to six different
languages. The latter used a model similar to DMV.
The controversial nature of some dependency
structures was discussed in (Nivre, 2006; K¨ubler
et al., 2009). Klein (2005) discussed controversial
constituency structures and the evaluation problems
stemming from them, stressing the importance of a
consistent standard of evaluation.
A few works explored the effects of annotation
conventions on parsing performance. Nilsson et
al. (2006) transformed the dependency annotations
of coordinations and verb groups in the Prague
TreeBank. They trained the supervised MaltParser
(Nivre et al., 2006) on the transformed data, parsed
the test data and re-transformed the resulting parse,
</bodyText>
<page confidence="0.990426">
664
</page>
<figure confidence="0.999388285714286">
PRP
VBP
TO
VB
ROOT
w3 w2 w1
(a)
w3 w2 w1
(b)
.
I
want
to
eat
</figure>
<figureCaption confidence="0.9869735">
Figure 1: A dependency structure on the words
w1, w2, w3 before (Figure 1(a)) and after (Figure 1(b))
an edge-lip of w2—*w1.
Figure 2: A parse of the sentence “I want to eat”, before
(straight line) and after (dashed line) an edge-lip of the
edge “to”�--“eat”.
</figureCaption>
<bodyText confidence="0.997637285714286">
thus improving performance. Klein and Manning
(2004) observed that a large portion of their errors is
caused by predicting the wrong direction of the edge
between a noun and its determiner. K¨ubler (2005)
compared two different conversion schemes in Ger-
man supervised constituency parsing and found one
to have positive influence on parsing quality.
</bodyText>
<subsectionHeader confidence="0.608479">
Dependency Model with Valence (DMV). DMV
</subsectionHeader>
<bodyText confidence="0.9990695">
(Klein and Manning, 2004) defines a probabilistic
grammar for unlabeled dependency structures. It is
defined as follows: the root of the sentence is first
generated, and then each head recursively generates
its right and left dependents. The parameters of the
model are of two types: PSTOP and PATTACH.
PSTOP(dir, h, adj) determines the probability to
stop generating arguments, and is conditioned on 3
arguments: the head h, the direction dir ((L)eft
or (R)ight) and adjacency adj (whether the head
already has dependents ((Y )es) in direction dir or
not ((N)o)). PATTACH(arg|h, dir) determines the
probability to generate arg as head h’s dependent in
direction dir.
</bodyText>
<sectionHeader confidence="0.978394" genericHeader="method">
3 Significant Effects of Edge Flipping
</sectionHeader>
<bodyText confidence="0.999955553571429">
In this section we present recurring error patterns
in some of the leading unsupervised dependency
parsers. These patterns are all local, confined to a
sequence of up to three words (but mainly of just
two consecutive words). They can often be mended
by changing the directions of a few types of edges.
The modified parameters described in this section
were handpicked to improve performance: we ex-
amined the local parser errors occurring the largest
number of times, and found the corresponding pa-
rameters. Note that this is a valid methodology,
since our goal is not to design a new algorithm but
to demonstrate that modifying a small set of param-
eters can yield a major performance boost and even-
tually discover problems with evaluation methods or
algorithms.
We start with a few definitions. Consider Fig-
ure 1(a) that shows a dependency structure on the
words w1, w2, w3. Edge flipping (henceforth, edge-
lip) the edge w2—*w1 is the following modification
of a parse tree: (1) setting w2’s parent as w1 (instead
of the other way around), and (2) setting w1’s par-
ent as w3 (instead of the edge w3—*w2). Figure 1(b)
shows the dependency structure after the edge-lip.
Note that (1) imposes setting a new parent to w2,
as otherwise it would have had no parent. Setting
this parent to be w3 is the minimal modification of
the original parse, since it does not change the at-
tachment of the structure [w2, w1] to the rest of the
sentence, but only the direction of the internal edge.
Figure 2 presents a parse of the sentence “I want
to eat”, before and after an edge-lip of the edge
“to”�--“eat”.
Since unsupervised dependency parsers are gen-
erally structure prediction models, the predictions
of the parse edges are not independent. Therefore,
there is no single parameter which completely con-
trols the edge direction, and hence there is no direct
way to perform an edge-lip by parameter modifica-
tion. However, setting extreme values for the param-
eters controlling the direction of a certain edge type
creates a strong preference towards one of the direc-
tions, and effectively determines the edge direction.
This procedure is henceforth termed parameter-lip.
We show that by performing a few parameter-
lips, a substantial improvement in the attachment
score can be obtained. Results are reported for three
algorithms.
Parameter Changes. All the works experimented
with in this paper are not lexical and use sequences
of POS tags as their input. In addition, they all use
the DMV parameter set (PSTOP and PATTACH) for
parsing. We will henceforth refer to this set, condi-
tioned on POS tags, as the model parameter set.
We show how an edge in the dependency graph
is encoded using the DMV parameters. Say the
</bodyText>
<page confidence="0.994066">
665
</page>
<bodyText confidence="0.999964857142857">
model prefers setting “to” (POS tag: TO) as a de-
pendent of the infinitive verb (POS tag: V B) to its
right (e.g., “to eat”). This is reflected by a high
value of PATTACH(TO|V B,L), a low value of
PATTACH(V B|TO, R), since “to” tends to be a left
dependent of the verb and not the other way around,
and a low value of PSTOP(V B, L, N), as the verb
usually has at least one left argument (i.e., “to”).
A parameterfflip of w1—*w2 is hence performed
by setting PATTACH(w2|w1, R) to a very low
value and PATTACH(w1|w2, L) to a very high
value. When the modifications to PATTACH
are insufficient to modify the edge direction,
PSTOP(w2, L, N) is set to a very low value and
PSTOP(w1, R, N) to a very high value2.
Table 1 describes the changes made for the three
algorithms. The ‘+’ signs in the table correspond to
edges in which the algorithm disagreed with the gold
standard, and were thus modified. Similarly, the ‘–’
signs in the table correspond to edges in which the
algorithm agreed with the gold standard, and were
thus not modified. The number of modified param-
eters does not exceed 18 (out of a few thousands).
The Freq. column in the table shows the percent-
age of the tokens in sections 2-21 of PTB WSJ that
participate in each structure. Equivalently, the per-
centage of edges in the corpus which are of either
of the types appearing in the Orig. Edge column.
As the table shows, the modified structures cover a
significant portion of the tokens. Indeed, 42.9% of
the tokens in the corpus participate in at least one of
them3.
Experimenting with Edge Flipping. We experi-
mented with three DMV-based algorithms: a repli-
cation of (Klein and Manning, 2004), as appears in
(Cohen et al., 2008) (henceforth, km04), Cohen and
Smith (2009) (henceforth, cs09), and Spitkovsky et
al. (2010a) (henceforth, saj10a). Decoding is done
using the Viterbi algorithm4. For each of these algo-
rithms we present the performance gain when com-
pared to the original parameters.
The training set is sections 2-21 of the Wall Street
</bodyText>
<footnote confidence="0.9870948">
2Note that this yields unnormalized models. Again, this is
justified since the resulting model is only used as a basis for
discussion and is not a fully fledged algorithm.
3Some tokens participate in more than one structure.
4http://www.cs.cmu.edu/—scohen/parser.html.
</footnote>
<table confidence="0.999458421052631">
Structure Freq. Orig. Edge km04 cs09 saj10a
Coordination 2.9% CC→NNP – + –
(“John &amp; Mary”)
DT→NN + + +
DT→NNP – + +
Prepositional DT→NNS – – +
IN→DT + + –
Phrase (“in 32.7%
the house”) IN←NN + + –
IN←NNP + – –
IN←NNS – + –
PRP$→NN – – +
Modal Verb 2.4% MD←V B – + –
(“can eat”)
Infinitive Verb 4.5% TO→V B – + +
(“to eat”)
Proper Name
Sequence 18.5% NNP→NNP + – –
(“John Doe”)
</table>
<tableCaption confidence="0.98236">
Table 1: Parameter changes for the three algorithms. The
</tableCaption>
<figureCaption confidence="0.865907">
Freq. column shows what percentage of the tokens in sec-
tions 2-21 of PTB WSJ participate in each structure. The
Orig. column indicates the original edge. The modified
edge is of the opposite direction. The other columns show
the different algorithms: km04: basic DMV model (repli-
cation of (Klein and Manning, 2004)); cs09; (Cohen and
Smith, 2009); saj10a: (Spitkovsky et al., 2010a).
</figureCaption>
<bodyText confidence="0.9997156875">
Journal Penn TreeBank (Marcus et al., 1993). Test-
ing is done on section 23. The constituency annota-
tion was converted to dependencies using the rules
of (Yamada and Matsumoto, 2003)5.
Following standard practice, we present the at-
tachment score (i.e., percentage of words that have a
correct head) of each algorithm, with both the origi-
nal parameters and the modified ones. We present
results both on all sentences and on sentences of
length G 10, excluding punctuation.
Table 2 shows results for all algorithms6. The
performance difference between the original and the
modified parameter set is considerable for all data
sets, where differences exceed 9.3%, and go up to
15.1%. These are enormous differences from the
perspective of current algorithm evaluation results.
</bodyText>
<sectionHeader confidence="0.997127" genericHeader="method">
4 Linguistically Problematic Annotations
</sectionHeader>
<bodyText confidence="0.995817">
In this section, we discuss the controversial nature
of the annotation in the modified structures (K¨ubler
</bodyText>
<footnote confidence="0.994183333333333">
5http://www.jaist.ac.jp/—h-yamada/
6Results are slightly worse than the ones published in the
original papers due to the different decoding algorithms (cs09
use MBR while we used Viterbi) and a different conversion pro-
cedure (saj10a used (Collins, 1999) and not (Yamada and Mat-
sumoto, 2003)) ; see Section 5.
</footnote>
<page confidence="0.977697">
666
</page>
<table confidence="0.9978538">
Algo. &lt; 10 &lt; �
Orig. Mod. A Orig. Mod. A
km04 45.8 59.8 14 34.6 43.9 9.3
cs09 60.9 72.9 12 39.9 54.6 14.7
saj10a 54.7 69.8 15.1 41.6 54.3 12.7
</table>
<tableCaption confidence="0.946124">
Table 2: Results of the original (Orig. columns), the
modified (Mod. columns) parameter sets and their dif-
ference (A columns) for the three algorithms.
</tableCaption>
<bodyText confidence="0.999863827586207">
et al., 2009). We remind the reader that structures
for which no linguistic consensus exists as to their
correct annotation are referred to as (linguistically)
problematic.
We begin by showing that all the structures mod-
ified are indeed linguistically problematic. We then
note that these controversies are reflected in the eval-
uation of this task, resulting in three, significantly
different, gold standards currently in use.
Coordination Structures are composed of two
proper nouns, separated by a conjunctor (e.g., “John
and Mary”). It is not clear which token should be the
head of this structure, if any (Nilsson et al., 2006).
Prepositional Phrases (e.g., “in the house” or “in
Rome”), where every word is a reasonable candidate
to head this structure. For example, in the annotation
scheme used by (Collins, 1999) the preposition is the
head, in the scheme used by (Johansson and Nugues,
2007) the noun is the head, while TUT annotation,
presented in (Bosco and Lombardo, 2004), takes the
determiner to be the noun’s head.
Verb Groups are composed of a verb and an aux-
iliary or a modal verb (e.g., “can eat”). Some
schemes choose the modal as the head (Collins,
1999), others choose the verb (Rambow et al., 2002).
Infinitive Verbs (e.g., “to eat”) are also in contro-
versy, as in (Yamada and Matsumoto, 2003) the verb
is the head while in (Collins, 1999; Bosco and Lom-
bardo, 2004) the “to” token is the head.
Sequences of Proper Nouns (e.g., “John Doe”)
are also subject to debate, as PTB’s scheme takes the
last proper noun as the head, and BIO’s scheme de-
fines a more complex scheme (Dredze et al., 2007).
Evaluation Inconsistency Across Papers. A fact
that may not be recognized by some readers is that
comparing the results of unsupervised dependency
parsers across different papers is not directly pos-
sible, since different papers use different gold stan-
dard annotations even when they are all derivedfrom
the Penn Treebank constituency annotation. This
happens because they use different rules for con-
verting constituency annotation to dependency an-
notation. A probable explanation for this fact is that
people have tried to correct linguistically problem-
atic annotations in different ways, which is why we
note this issue here7.
There are three different annotation schemes
in current use: (1) Collins head rules (Collins,
1999), used in e.g., (Berg-Kirkpatrick et al., 2010;
Spitkovsky et al., 2010a); (2) Conversion rules of
(Yamada and Matsumoto, 2003), used in e.g., (Co-
hen and Smith, 2009; Gillenwater et al., 2010); (3)
Conversion rules of (Johansson and Nugues, 2007)
used, e.g., in the CoNLL shared task 2007 (Nivre et
al., 2007) and in (Blunsom and Cohn, 2010).
The differences between the schemes are substan-
tial. For instance, 14.4% of section 23 is tagged dif-
ferently by (1) and (2)8.
</bodyText>
<sectionHeader confidence="0.883135" genericHeader="method">
5 The Neutral Edge Direction (NED)
Measure
</sectionHeader>
<bodyText confidence="0.999171923076923">
As shown in the previous sections, the annotation
of problematic edges can substantially affect perfor-
mance. This was briefly discussed in (Klein and
Manning, 2004), which used undirected evaluation
as a measure which is less sensitive to alternative
annotations. Undirected accuracy was commonly
used since to assess the performance of unsuper-
vised parsers (e.g., (Smith and Eisner, 2006; Head-
den et al., 2008; Spitkovsky et al., 2010a)) but also
of supervised ones (Wang et al., 2005; Wang et al.,
2006). In this section we discuss why this measure
is in fact not indifferent to edgefflips and propose a
new measure, Neutral Edge Direction (NED).
</bodyText>
<footnote confidence="0.9581745">
7Indeed, half a dozen flags in the LTH Constituent-to-
Dependency Conversion Tool (Johansson and Nugues, 2007)
are used to control the conversion in problematic cases.
8In our experiments we used the scheme of (Yamada and
Matsumoto, 2003), see Section 3. The significant effects of
edge flipping were observed with the other two schemes as well.
</footnote>
<page confidence="0.983314">
667
</page>
<figureCaption confidence="0.697596">
Figure 3: A dependency structure on the words
w1, w2, w3 before (Figure 3(a)) and after (Figure 3(b)) an
edge-lip of w2→w3, and when the direction of the edge
between w2 and w3 is switched and the new parent of w3
is set to be some other word, w4 (Figure 3(c)).
Undirected Evaluation. The measure is defined
as follows: traverse over the tokens and mark a cor-
rect attachment if the token’s induced parent is either
(1) its gold parent or (2) its gold child. The score is
the ratio of correct attachments and the number of
tokens.
</figureCaption>
<bodyText confidence="0.9994184">
We show that this measure does not ignore edge-
lips. Consider Figure 3 that shows a depen-
dency structure on the words w1, w2, w3 before (Fig-
ure 3(a)) and after (Figure 3(b)) an edge-lip of
w2→w3. Assume that 3(a) is the gold standard and
that 3(b) is the induced parse. Consider w2. Its
induced parent (w3) is its gold child, and thus undi-
rected evaluation does not consider it an error. On
the other hand, w3 is assigned w2’s gold parent, w1.
This is considered an error, since w1 is neither w3’s
gold parent (as it is w2), nor its gold child9. There-
fore, one of the two tokens involved in the edge-lip
is penalized by the measure.
Recall the example “I want to eat” and the edge-
lip of the edge “to”+—“eat” (Figure 2). As “to”’s
parent in the induced graph (“want”) is neither its
gold parent nor its gold child, the undirected evalu-
ation measure marks it as an error. This is an exam-
ple where an edge-lip in a problematic edge, which
should not be considered an error, was in fact con-
sidered an error by undirected evaluation.
Neutral Edge Direction (NED). The NED measure
is a simple extension of the undirected evaluation
measure10. Unlike undirected evaluation, NED ig-
nores all errors directly resulting from an edge-lip.
</bodyText>
<footnote confidence="0.932025">
9Otherwise, the gold parse would have contained a
W1--+W2--+W3--+W1 cycle.
10An implementation of NED is available at
http://www.cs.huji.ac.il/∼roys02/software/ned.html
</footnote>
<bodyText confidence="0.999670106382979">
NED is defined as follows: traverse over the to-
kens and mark a correct attachment if the token’s in-
duced parent is either (1) its gold parent (2) its gold
child or (3) its gold grandparent. The score is the ra-
tio of correct attachments and the number of tokens.
NED, by its definition, ignores edge-lips. Con-
sider again Figure 3, where we assume that 3(a) is
the gold standard and that 3(b) is the induced parse.
Much like undirected evaluation, NED will mark the
attachment of w2 as correct, since its induced parent
is its gold child. However, unlike undirected evalua-
tion, w3’s induced attachment will also be marked as
correct, as its induced parent is its gold grandparent.
Now consider another induced parse in which the
direction of the edge between w2 and w3 is switched
and the w3’s parent is set to be some other word,
w4 (Figure 3(c)). This should be marked as an er-
ror, even if the direction of the edge between w2 and
w3 is controversial, since the structure [w2, w31 is no
longer a dependent of w1. It is indeed a NED error.
Note that undirected evaluation gives the parses in
Figure 3(b) and Figure 3(c) the same score, while if
the structure [w2, w31 is problematic, there is a major
difference in their correctness.
Discussion. Problematic structures are ubiquitous,
with more than 40% of the tokens in PTB WSJ
appearing in at least one of them (see Section 3).
Therefore, even a substantial difference in the at-
tachment between two parsers is not necessarily in-
dicative of a true quality difference. However, an at-
tachment score difference that persists under NED is
an indication of a true quality difference, since gen-
erally problematic structures are local (i.e., obtained
by an edge-lip) and NED ignores such errors.
Reporting NED alone is insufficient, as obviously
the edge direction does matter in some cases. For
example, in adjective–noun structures (e.g., “big
house”), the correct edge direction is widely agreed
upon (“big”+—“house”) (K¨ubler et al., 2009), and
thus choosing the wrong direction should be con-
sidered an error. Therefore, we suggest evaluating
using both NED and attachment score in order to get
a full picture of the parser’s performance.
A possible criticism on NED is that it is only in-
different to alternative annotations in structures of
size 2 (e.g., “to eat”) and does not necessarily handle
larger problematic structures, such as coordinations
</bodyText>
<figure confidence="0.99529572972973">
w1
w1
w4
w3
(a)
w2
(b)
w2
(c)
w2
w3
w3
668
and Mary
(a)
ROOT
John
ROOT
John
and
Mary
(b)
ROOT
in
house
the
(c)
ROOT
in
the
house
(d)
ROOT
house
in
the
(e)
</figure>
<figureCaption confidence="0.9943655">
Figure 4: Alternative parses of “John and Mary” and “in
the house”. Figure 4(a) follows (Collins, 1999), Fig-
ure 4(b) follows (Johansson and Nugues, 2007). Fig-
ure 4(c) follows (Collins, 1999; Yamada and Matsumoto,
2003). Figure 4(d) and Figure 4(e) show induced parses
made by (km04,saj10a) and cs09, respectively.
</figureCaption>
<bodyText confidence="0.996493311111111">
(see Section 4). For example, Figure 4(a) and Fig-
ure 4(b) present two alternative annotations of the
sentence “John and Mary”. Assume the parse in Fig-
ure 4(a) is the gold parse and that in Figure 4(b) is
the induced parse. The word “Mary” is a NED error,
since its induced parent (“and”) is neither its gold
child nor its gold grandparent. Thus, NED does not
accept all possible annotations of structures of size
3. On the other hand, using a method which accepts
all possible annotations of structures of size 3 seems
too permissive. A better solution may be to modify
the gold standard annotation, so to explicitly anno-
tate problematic structures as such. We defer this
line of research to future work.
NED is therefore an evaluation measure which is
indifferent to edge-lips, and is consequently less
sensitive to alternative annotations. We now show
that NED is indifferent to the differences between the
structures originally learned by the algorithms men-
tioned in Section 3 and the gold standard annotation
in all the problematic cases we consider.
Most of the modifications made are edge-lips,
and are therefore ignored by NED. The exceptions
are coordinations and prepositional phrases which
are structures of size 3. In the former, the alter-
native annotations differ only in a single edge-lip
(i.e., CC—*NNP), and are thus not NED errors. Re-
garding prepositional phrases, Figure 4(c) presents
the gold standard of “in the house”, Figure 4(d) the
parse induced by km04 and saj10a and Figure 4(e)
the parse induced by cs09. As the reader can verify,
both induced parses receive a perfect NED score.
In order to further demonstrate NED’s insensitiv-
ity to alternative annotations, we took two of the
three common gold standard annotations (see Sec-
tion 4) and evaluated them one against the other. We
considered section 23 of WSJ following the scheme
of (Yamada and Matsumoto, 2003) as the gold stan-
dard and of (Collins, 1999) as the evaluated set. Re-
sults show that the attachment score is only 85.6%,
the undirected accuracy is improved to 90.3%, while
the NED score is 95.3%. This shows that NED is sig-
nificantly less sensitive to the differences between
the different annotation schemes, compared to the
other evaluation measures.
</bodyText>
<sectionHeader confidence="0.799004" genericHeader="method">
6 Experimenting with NED
</sectionHeader>
<bodyText confidence="0.999976942857143">
In this section we show that NED indeed reduces
the performance difference between the original and
the modified parameter sets, thus providing empiri-
cal evidence for its validity. For brevity, we present
results only for the entire WSJ corpus. Results on
WSJ10 are similar. The datasets and decoding algo-
rithms are the same as those used in Section 3.
Table 3 shows the score differences between the
parameter sets using attachment score, undirected
evaluation and NED. A substantial difference per-
sists under undirected evaluation: a gap of 7.7% in
cs09, of 3.5% in saj10a and of 1.3% in km04.
The differences are further reduced using NED.
This is consistent with our discussion in Section 5,
and shows that undirected evaluation only ignores
some of the errors inflicted by edge-lips.
For cs09, the difference is substantially reduced,
but a 4.2% performance gap remains. For km04 and
saj10a, the original parameters outperform the new
ones by 3.6% and 1% respectively.
We can see that even when ignoring edge-lips,
some difference remains, albeit not necessarily in
the favor of the modified models. This is because
we did not directly perform edge-lips, but rather
parameter-lips. The difference is thus a result of
second-order effects stemming from the parameter-
lips. In the next section, we explain why the remain-
ing difference is positive for some algorithms (cs09)
and negative for others (km04, saj10a).
For completeness, Table 4 shows a comparison of
some of the current state-of-the-art algorithms, using
attachment score, undirected evaluation and NED.
The training and test sets are those used in Section 3.
The table shows that the relative orderings of the al-
gorithms under NED is different than under the other
</bodyText>
<page confidence="0.997236">
669
</page>
<table confidence="0.9953932">
Algo. Mod. – Orig.
Attach. Undir. NED
km04 9.3 (43.9–34.6) 1.3 (54.2–52.9) –3.6 (63–66.6)
cs09 14.7 (54.6–39.9) 7.7 (56.9–49.2) 4.2 (66.8–62.6)
saj10a 12.7 (54.3–41.6) 3.5 (59.4–55.9) –1 (66.8–67.8)
</table>
<tableCaption confidence="0.7393304">
Table 3: Differences between the modified and original
parameter sets when evaluated using attachment score
(Attach.), undirected evaluation (Undir.), and NED.
measures. This is an indication that NED provides a
different perspective on algorithm quality11.
</tableCaption>
<table confidence="0.999912666666667">
Algo. Att10 Att. Un10 Un. NED10 NED.
bbdk10 66.1 49.6 70.1 56.0 75.5 61.8
bc10 67.2 53.6 73 61.7 81.6 70.2
cs09 61.5 42 66.9 50.4 81.5 62.9
gggtp10 57.1 45 62.5 53.2 80.4 65.1
km04 45.8 34.6 60.3 52.9 78.4 66.6
saj10a 54.7 41.6 66.5 55.9 78.9 67.8
saj10c 63.8 46.1 72.6 58.8 84.2 70.8
saj10b* 67.9 48.2 74.0 57.7 86.0 70.7
</table>
<tableCaption confidence="0.793896666666667">
Table 4: A comparison of recent works, using Att (at-
tachment score) Un (undirected evaluation) and NED, on
sentences of length &lt; 10 (excluding punctuation) and
</tableCaption>
<figureCaption confidence="0.7487355">
on all sentences. The gold standard is obtained using
the rules of (Yamada and Matsumoto, 2003). bbdk10:
(Berg-Kirkpatrick et al., 2010), bc10: (Blunsom and
Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10:
(Gillenwater et al., 2010), km04: A replication of (Klein
and Manning, 2004), saj10a: (Spitkovsky et al., 2010a),
saj10c: (Spitkovsky et al., 2010c), saj10b*: A lightly-
supervised algorithm (Spitkovsky et al., 2010b).
</figureCaption>
<sectionHeader confidence="0.995859" genericHeader="evaluation">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9539576875">
In this paper we explored two ways of dealing with
cases in which there is no clear theoretical justifi-
cation to prefer one dependency structure over an-
other. Our experiments suggest that it is crucial to
deal with such structures if we would like to have
a proper evaluation of unsupervised parsing algo-
rithms against a gold standard.
The first way was to modify the parameters of the
parsing algorithms so that in cases where such prob-
lematic decisions are to be made they follow the gold
standard annotation. Indeed, this modification leads
to a substantial improvement in the attachment score
of the algorithms.
11Results may be different than the ones published in the
original papers due to the different conversion procedures used
in each work. See Section 4 for discussion.
The second way was to change the evaluation.
The NED measure we proposed does not punish for
differences between gold and induced structures in
the problematic cases. Indeed, in Section 6 (Table 3)
we show that the differences between the original
and modified models are much smaller when eval-
uating with NED compared to when evaluating with
the traditional attachment score.
As Table 3 reveals, however, even when evaluat-
ing with NED, there is still some difference between
the original and the modified model, for each of the
algorithms we consider. Moreover, for two of the al-
gorithms (km04 and saj10a) NED prefers the original
model while for one (cs09) it prefers the modified
version. In this section we explain these patterns and
show that they are both consistent and predictable.
Our hypothesis, for which we provide empirical
justification, is that in cases where there is no theo-
retically preferred annotation, NED prefers the struc-
tures that are more learnable by DMV. That is, NED
gives higher scores to the annotations that better fit
the assumptions and modeling decisions of DMV,
the model that lies in the basis of the parsing algo-
rithms.
To support our hypothesis we perform an experi-
ment requiring two preparatory steps for each algo-
rithm. First, we construct a supervised version of
the algorithm. This supervised version consists of
the same statistical model as the original unsuper-
vised algorithm, but the parameters are estimated to
maximize the likelihood of a syntactically annotated
training corpus, rather than of a plain text corpus.
Second, we construct two corpora for the algo-
rithm, both consist of the same text and differ only
in their syntactic annotation. The first is annotated
with the gold standard annotation. The second is
similarly annotated except in the linguistically prob-
lematic structures. We replace these structures with
the ones that would have been created with the un-
supervised version of the algorithm (see Table 1 for
the relevant structures for each algorithm)12. Each
12In cases the structures are comprised of a single edge, the
second corpus is obtained from the gold standard by an edge-
flip. The only exceptions are the cases of the prepositional
phrases. Their gold standard and the learned structures for each
of the algorithms are shown in Figure 4. In this case, the sec-
ond corpus is obtained from the gold standard by replacing each
prepositional phrase in the gold standard with the corresponding
</bodyText>
<page confidence="0.994031">
670
</page>
<bodyText confidence="0.999385769230769">
corpus is divided into a training and a test set.
We then train the supervised version of the algo-
rithms on each of the training sets. We parse the test
data twice, once with each of the resulting models.
We evaluate both parsed corpora against the corpus
annotation from which they originated.
The training set of each corpus consists of sec-
tions 2–21 of WSJ20 (i.e., WSJ sentences of length
G20, excluding punctuation)13 and the test set is sec-
tion 23 of WSJ&apos;. Evaluation is performed using
both NED and attachment score. The patterns we
observed are very similar for both. For brevity, we
report only attachment score results.
</bodyText>
<table confidence="0.9966118">
km04 cs09 saj10a
Orig. Gold Orig. Gold Orig. Gold
NED, 66.6 63 62.6 66.8 67.8 66.8
Unsup.
Sup. 71.3 69.9 63.3 69.9 71.8 69.9
</table>
<tableCaption confidence="0.996858">
Table 5: The first line shows the NED results from
</tableCaption>
<bodyText confidence="0.968529166666667">
Section 6, when using the original parameters (Orig.
columns) and the modified parameters (Gold columns).
The second line shows the results of the supervised ver-
sions of the algorithms using the corpus which agrees
with the unsupervised model in the problematic cases
(Orig.) and the gold standard (Gold).
The results of our experiment are presented in Ta-
ble 5 along with a comparison to the NED scores
from Section 6. The table clearly demonstrates that a
set of parameters (original or modified) is preferred
by NED in the unsupervised experiments reported in
Section 6 (top line) if and only if the structures pro-
duced by this set are better learned by the supervised
version of the algorithm (bottom line).
This observation supports our hypothesis that in
cases where there is no theoretical preference for
one structure over the other, NED (unlike the other
measures) prefers the structures that are more con-
sistent with the modeling assumptions lying in the
basis of the algorithm. We consider this to be a de-
sired property of a measure since a more consistent
model should be preferred where no theoretical pref-
erence exists.
learned structure.
</bodyText>
<footnote confidence="0.975243">
13In using WSJ20, we follow (Spitkovsky et al., 2010a),
which showed that training the DMV on sentences of bounded
length yields a higher score than using the entire corpus. We
use it as we aim to use an optimal setting.
</footnote>
<sectionHeader confidence="0.984624" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9997487">
In this paper we showed that the standard evalua-
tion of unsupervised dependency parsers is highly
sensitive to problematic annotations. We modified a
small set of parameters that controls the annotation
in such problematic cases in three leading parsers.
This resulted in a major performance boost, which
is unindicative of a true difference in quality.
We presented Neutral Edge Direction (NED), a
measure that is less sensitive to the annotation of
local structures. As the problematic structures are
generally local, NED is less sensitive to their alterna-
tive annotations. In the future, we suggest reporting
NED along with the current measures.
Acknowledgements. We would like to thank Shay
Cohen for his assistance with his implementation of
the DMV parser and Taylor Berg-Kirkpatrick, Phil
Blunsom and Jennifer Gillenwater for providing us
with their data sets. We would also like to thank
Valentin I. Spitkovsky for his comments and for pro-
viding us with his data sets.
</bodyText>
<sectionHeader confidence="0.999096" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999641076923077">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero and Dan Klein, 2010. Painless unsu-
pervised learning with features. In Proc. ofNAACL.
Taylor Berg-Kirkpatrick and Dan Klein, 2010. Phyloge-
netic Grammar Induction. In Proc. ofACL.
Cristina Bosco and Vincenzo Lombardo, 2004. Depen-
dency and relational structure in treebank annotation.
In Proc. of the Workshop on Recent Advances in De-
pendency Grammar at COLING’04.
Phil Blunsom and Trevor Cohn, 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP.
Shay B. Cohen, Kevin Gimpel and Noah A. Smith, 2008.
Logistic Normal Priors for Unsupervised Probabilistic
Grammar Induction. In Proc. ofNIPS.
Shay B. Cohen and Noah A. Smith, 2009. Shared Logis-
tic Normal Distributions for Soft Parameter Tying. In
Proc. ofHLT-NAACL.
Michael J. Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia.
Alexander Clark, 2001. Unsupervised language acquisi-
tion: theory and practice. Ph.D. thesis, University of
Sussex.
Hal Daum´e III, 2009. Unsupervised search-based struc-
tured prediction. In Proc. ofICML.
</reference>
<page confidence="0.98142">
671
</page>
<reference confidence="0.99972255952381">
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Jo˜ao V. Grac¸a and Fernando Pereira,
2007. Frustratingly Hard Domain Adaptation for De-
pendency Parsing. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
Gregory Druck, Gideon Mann and Andrew McCal-
lum, 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In
Proc. ofACL.
Jennifer Gillenwater, Kuzman Ganchev, Jo˜ao V. Grac¸a,
Ben Taskar and Fernando Preira, 2010. Sparsity in
dependency grammar induction. In Proc. ofACL.
William P. Headden III, David McClosky, and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. In Proc. of
COLING.
William P. Headden III, Mark Johnson and David Mc-
Closky, 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Proc.
ofHLT-NAACL.
Richard Johansson and Pierre Nugues, 2007. Ex-
tended Constituent-to-Dependency Conversion for En-
glish. In Proc. ofNODALIDA.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. ofACL.
Sandra K¨ubler, 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proc. of RANLP.
Sandra K¨ubler, R. McDonald and Joakim Nivre, 2009.
Dependency Parsing. Morgan And Claypool Publish-
ers.
Mitchell Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz, 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics 19:313-330.
Tahira Naseem, Harr Chen, Regina Barzilay and Mark
Johnson, 2010. Using universal linguistic knowledge
to guide grammar induction. In Proc. ofEMNLP.
Joakim Nivre, 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre, Johan Hall and Jens Nilsson, 2006. Malt-
Parser: A data-driven parser-generator for depen-
dency parsing. In Proc. ofLREC-2006.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel and Deniz Yuret,
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of the CoNLL Shared Task, EMNLP-
CoNLL, 2007.
Jens Nilsson, Joakim Nivre and Johan Hall, 2006. Graph
transformations in data-driven dependency parsing.
In Proc. ofACL.
Owen Rambow, Cassandre Creswell, Rachel Szekely,
Harriet Tauber and Marilyn Walker, 2002. A depen-
dency treebankfor English. In Proc. ofLREC.
Noah A. Smith and Jason Eisner, 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In Proc. ofIJCAI.
Noah A. Smith and Jason Eisner, 2006. Annealing struc-
tural bias in multilingual weighted grammar induc-
tion. In Proc. ofACL.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010a. From Baby Steps to Leapfrog: How “Less
is More” in Unsupervised Dependency Parsing. In
Proc. ofNAACL-HLT.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010b. Profiting from Mark-Up: Hyper-Text An-
notations for Guided Parsing. In Proc. ofACL.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky
and Christopher D. Manning, 2010c. Viterbi training
improves unsupervised dependency parsing. In Proc.
of CoNLL.
Qin Iris Wang, Dale Schuurmans and Dekang Lin, 2005.
Strictly Lexical Dependency Parsing. In IWPT.
Qin Iris Wang, Colin Cherry, Dan Lizotte and Dale Schu-
urmans, 2006. Improved Large Margin Dependency
Parsing via Local Constraints and Laplacian Regular-
ization. In Proc. of CoNLL.
Hiroyasu Yamada and Yuji Matsumoto, 2003. Statistical
dependency analysis with support vector machines. In
Proc. of the International Workshop on Parsing Tech-
nologies.
</reference>
<page confidence="0.998232">
672
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968766">
<title confidence="0.9998335">Neutralizing Linguistically Problematic in Unsupervised Dependency Parsing Evaluation</title>
<author confidence="0.99999">Roi Ari</author>
<affiliation confidence="0.99749525">of Computer Hebrew University of Jerusalem Science and Artificial Intelligence Massachusetts Institute of Technology</affiliation>
<email confidence="0.999529">roiri@csail.mit.edu</email>
<abstract confidence="0.998540833333333">Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm We present a new measure, Direction and show that it greatly reduces this undesired phenomenon.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proc. ofNAACL.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero and Dan Klein, 2010. Painless unsupervised learning with features. In Proc. ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Phylogenetic Grammar Induction. In</title>
<date>2010</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="6779" citStr="Berg-Kirkpatrick and Klein (2010)" startWordPosition="1042" endWordPosition="1045">ing logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ&apos;. Druck et al. (2009) took a semi-superv</context>
</contexts>
<marker>Berg-Kirkpatrick, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick and Dan Klein, 2010. Phylogenetic Grammar Induction. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Bosco</author>
<author>Vincenzo Lombardo</author>
</authors>
<title>Dependency and relational structure in treebank annotation.</title>
<date>2004</date>
<booktitle>In Proc. of the Workshop on Recent Advances in Dependency Grammar at COLING’04.</booktitle>
<contexts>
<context position="18068" citStr="Bosco and Lombardo, 2004" startWordPosition="2931" endWordPosition="2934">in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex</context>
</contexts>
<marker>Bosco, Lombardo, 2004</marker>
<rawString>Cristina Bosco and Vincenzo Lombardo, 2004. Dependency and relational structure in treebank annotation. In Proc. of the Workshop on Recent Advances in Dependency Grammar at COLING’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1425" citStr="Blunsom and Cohn, 2010" startWordPosition="197" endWordPosition="200">These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 2004; Cohen et al., 2008; Headden et al., 2009; Spitkovsky et al., 2010a; Gillenwater et al., 2010; Berg-Kirkpatrick et al., 2010; Blunsom and Cohn, 2010, inter alia). Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated. However, there are cases in which there is no linguistic consensus as to what the correct annotation is (K¨ubler et al., 2009). Examples include which verb is the head in a verb group structure (e.g., “can” or “eat” in “can eat”), and which * Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguis</context>
<context position="7276" citStr="Blunsom and Cohn (2010)" startWordPosition="1117" endWordPosition="1120">ns. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ&apos;. Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the </context>
<context position="19767" citStr="Blunsom and Cohn, 2010" startWordPosition="3214" endWordPosition="3217">annotation. A probable explanation for this fact is that people have tried to correct linguistically problematic annotations in different ways, which is why we note this issue here7. There are three different annotation schemes in current use: (1) Collins head rules (Collins, 1999), used in e.g., (Berg-Kirkpatrick et al., 2010; Spitkovsky et al., 2010a); (2) Conversion rules of (Yamada and Matsumoto, 2003), used in e.g., (Cohen and Smith, 2009; Gillenwater et al., 2010); (3) Conversion rules of (Johansson and Nugues, 2007) used, e.g., in the CoNLL shared task 2007 (Nivre et al., 2007) and in (Blunsom and Cohn, 2010). The differences between the schemes are substantial. For instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, 2004), which used undirected evaluation as a measure which is less sensitive to alternative annotations. Undirected accuracy was commonly used since to assess the performance of unsupervised parsers (e.g., (Smith and Eisner, 2006; Headden et al., 2008; Spitkovsky et al.</context>
<context position="30833" citStr="Blunsom and Cohn, 2010" startWordPosition="5071" endWordPosition="5074">D10 NED. bbdk10 66.1 49.6 70.1 56.0 75.5 61.8 bc10 67.2 53.6 73 61.7 81.6 70.2 cs09 61.5 42 66.9 50.4 81.5 62.9 gggtp10 57.1 45 62.5 53.2 80.4 65.1 km04 45.8 34.6 60.3 52.9 78.4 66.6 saj10a 54.7 41.6 66.5 55.9 78.9 67.8 saj10c 63.8 46.1 72.6 58.8 84.2 70.8 saj10b* 67.9 48.2 74.0 57.7 86.0 70.7 Table 4: A comparison of recent works, using Att (attachment score) Un (undirected evaluation) and NED, on sentences of length &lt; 10 (excluding punctuation) and on all sentences. The gold standard is obtained using the rules of (Yamada and Matsumoto, 2003). bbdk10: (Berg-Kirkpatrick et al., 2010), bc10: (Blunsom and Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10: (Gillenwater et al., 2010), km04: A replication of (Klein and Manning, 2004), saj10a: (Spitkovsky et al., 2010a), saj10c: (Spitkovsky et al., 2010c), saj10b*: A lightlysupervised algorithm (Spitkovsky et al., 2010b). 7 Discussion In this paper we explored two ways of dealing with cases in which there is no clear theoretical justification to prefer one dependency structure over another. Our experiments suggest that it is crucial to deal with such structures if we would like to have a proper evaluation of unsupervised parsing algorithms against a gold st</context>
</contexts>
<marker>Blunsom, Cohn, 2010</marker>
<rawString>Phil Blunsom and Trevor Cohn, 2010. Unsupervised Induction of Tree Substitution Grammars for Dependency Parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction. In</title>
<date>2008</date>
<booktitle>Proc. ofNIPS.</booktitle>
<contexts>
<context position="1296" citStr="Cohen et al., 2008" startWordPosition="177" endWordPosition="180"> a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 2004; Cohen et al., 2008; Headden et al., 2009; Spitkovsky et al., 2010a; Gillenwater et al., 2010; Berg-Kirkpatrick et al., 2010; Blunsom and Cohn, 2010, inter alia). Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated. However, there are cases in which there is no linguistic consensus as to what the correct annotation is (K¨ubler et al., 2009). Examples include which verb is the head in a verb group structure (e.g., “can” or “eat” in “can eat”), and which * Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowshi</context>
<context position="6080" citStr="Cohen et al. (2008)" startWordPosition="926" endWordPosition="929">ars (see (Clark, 2001; Klein, 2005) for reviews). For unsupervised dependency parsing, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) was the first to beat the simple right-branching baseline. A technical description of DMV is given at the end of this section. The great majority of recent works, including those experimented with in this paper, are elaborations of DMV. Smith and Eisner (2005) improved the DMV results by generalizing the function maximized by DMV’s EM training algorithm. Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Cohen et al. (2008) extended DMV by using a variational EM training algorithm and adding logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al.</context>
<context position="14150" citStr="Cohen et al., 2008" startWordPosition="2284" endWordPosition="2287"> not exceed 18 (out of a few thousands). The Freq. column in the table shows the percentage of the tokens in sections 2-21 of PTB WSJ that participate in each structure. Equivalently, the percentage of edges in the corpus which are of either of the types appearing in the Orig. Edge column. As the table shows, the modified structures cover a significant portion of the tokens. Indeed, 42.9% of the tokens in the corpus participate in at least one of them3. Experimenting with Edge Flipping. We experimented with three DMV-based algorithms: a replication of (Klein and Manning, 2004), as appears in (Cohen et al., 2008) (henceforth, km04), Cohen and Smith (2009) (henceforth, cs09), and Spitkovsky et al. (2010a) (henceforth, saj10a). Decoding is done using the Viterbi algorithm4. For each of these algorithms we present the performance gain when compared to the original parameters. The training set is sections 2-21 of the Wall Street 2Note that this yields unnormalized models. Again, this is justified since the resulting model is only used as a basis for discussion and is not a fully fledged algorithm. 3Some tokens participate in more than one structure. 4http://www.cs.cmu.edu/—scohen/parser.html. Structure Fr</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>Shay B. Cohen, Kevin Gimpel and Noah A. Smith, 2008. Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction. In Proc. ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared Logistic Normal Distributions for Soft Parameter Tying. In</title>
<date>2009</date>
<booktitle>Proc. ofHLT-NAACL.</booktitle>
<contexts>
<context position="650" citStr="Cohen and Smith, 2009" startWordPosition="77" endWordPosition="80">roblematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz&apos; Omri Abend&apos;* Roi Reichart2 Ari Rappoport&apos; &apos;Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of researc</context>
<context position="2479" citStr="Cohen and Smith, 2009" startWordPosition="372" endWordPosition="376"> for the award of an Azrieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguistically) problematic. For such cases, evaluation measures should not punish the algorithm for deviating from the gold standard. In this paper we show that the evaluation measures reported in current works are highly sensitive to the annotation in problematic cases, and propose a simple new measure that greatly neutralizes the problem. We start from the following observation: for three leading algorithms (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set (at most 18 out of a few thousands) of parameters can be found whose modification dramatically improves the standard evaluation measures (the attachment score measure by 9.3-15.1%, and the undirected measure by a smaller but still significant 1.3-7.7%). The phenomenon is implementation independent, occurring with several algorithms based on a fundamental probabilistic dependency model1. We show that these parameter changes can be mapped to edge direction changes in local structures in the dependency graph, and that these correspond to problematic annota</context>
<context position="6195" citStr="Cohen and Smith (2009" startWordPosition="946" endWordPosition="949">Valence (DMV) (Klein and Manning, 2004) was the first to beat the simple right-branching baseline. A technical description of DMV is given at the end of this section. The great majority of recent works, including those experimented with in this paper, are elaborations of DMV. Smith and Eisner (2005) improved the DMV results by generalizing the function maximized by DMV’s EM training algorithm. Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Cohen et al. (2008) extended DMV by using a variational EM training algorithm and adding logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogen</context>
<context position="14193" citStr="Cohen and Smith (2009)" startWordPosition="2290" endWordPosition="2293"> The Freq. column in the table shows the percentage of the tokens in sections 2-21 of PTB WSJ that participate in each structure. Equivalently, the percentage of edges in the corpus which are of either of the types appearing in the Orig. Edge column. As the table shows, the modified structures cover a significant portion of the tokens. Indeed, 42.9% of the tokens in the corpus participate in at least one of them3. Experimenting with Edge Flipping. We experimented with three DMV-based algorithms: a replication of (Klein and Manning, 2004), as appears in (Cohen et al., 2008) (henceforth, km04), Cohen and Smith (2009) (henceforth, cs09), and Spitkovsky et al. (2010a) (henceforth, saj10a). Decoding is done using the Viterbi algorithm4. For each of these algorithms we present the performance gain when compared to the original parameters. The training set is sections 2-21 of the Wall Street 2Note that this yields unnormalized models. Again, this is justified since the resulting model is only used as a basis for discussion and is not a fully fledged algorithm. 3Some tokens participate in more than one structure. 4http://www.cs.cmu.edu/—scohen/parser.html. Structure Freq. Orig. Edge km04 cs09 saj10a Coordinatio</context>
<context position="15518" citStr="Cohen and Smith, 2009" startWordPosition="2522" endWordPosition="2525">ase (“in 32.7% the house”) IN←NN + + – IN←NNP + – – IN←NNS – + – PRP$→NN – – + Modal Verb 2.4% MD←V B – + – (“can eat”) Infinitive Verb 4.5% TO→V B – + + (“to eat”) Proper Name Sequence 18.5% NNP→NNP + – – (“John Doe”) Table 1: Parameter changes for the three algorithms. The Freq. column shows what percentage of the tokens in sections 2-21 of PTB WSJ participate in each structure. The Orig. column indicates the original edge. The modified edge is of the opposite direction. The other columns show the different algorithms: km04: basic DMV model (replication of (Klein and Manning, 2004)); cs09; (Cohen and Smith, 2009); saj10a: (Spitkovsky et al., 2010a). Journal Penn TreeBank (Marcus et al., 1993). Testing is done on section 23. The constituency annotation was converted to dependencies using the rules of (Yamada and Matsumoto, 2003)5. Following standard practice, we present the attachment score (i.e., percentage of words that have a correct head) of each algorithm, with both the original parameters and the modified ones. We present results both on all sentences and on sentences of length G 10, excluding punctuation. Table 2 shows results for all algorithms6. The performance difference between the original </context>
<context position="19591" citStr="Cohen and Smith, 2009" startWordPosition="3183" endWordPosition="3187">n when they are all derivedfrom the Penn Treebank constituency annotation. This happens because they use different rules for converting constituency annotation to dependency annotation. A probable explanation for this fact is that people have tried to correct linguistically problematic annotations in different ways, which is why we note this issue here7. There are three different annotation schemes in current use: (1) Collins head rules (Collins, 1999), used in e.g., (Berg-Kirkpatrick et al., 2010; Spitkovsky et al., 2010a); (2) Conversion rules of (Yamada and Matsumoto, 2003), used in e.g., (Cohen and Smith, 2009; Gillenwater et al., 2010); (3) Conversion rules of (Johansson and Nugues, 2007) used, e.g., in the CoNLL shared task 2007 (Nivre et al., 2007) and in (Blunsom and Cohn, 2010). The differences between the schemes are substantial. For instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, 2004), which used undirected evaluation as a measure which is less sensitive to alternative an</context>
<context position="30864" citStr="Cohen and Smith, 2009" startWordPosition="5076" endWordPosition="5079">6.0 75.5 61.8 bc10 67.2 53.6 73 61.7 81.6 70.2 cs09 61.5 42 66.9 50.4 81.5 62.9 gggtp10 57.1 45 62.5 53.2 80.4 65.1 km04 45.8 34.6 60.3 52.9 78.4 66.6 saj10a 54.7 41.6 66.5 55.9 78.9 67.8 saj10c 63.8 46.1 72.6 58.8 84.2 70.8 saj10b* 67.9 48.2 74.0 57.7 86.0 70.7 Table 4: A comparison of recent works, using Att (attachment score) Un (undirected evaluation) and NED, on sentences of length &lt; 10 (excluding punctuation) and on all sentences. The gold standard is obtained using the rules of (Yamada and Matsumoto, 2003). bbdk10: (Berg-Kirkpatrick et al., 2010), bc10: (Blunsom and Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10: (Gillenwater et al., 2010), km04: A replication of (Klein and Manning, 2004), saj10a: (Spitkovsky et al., 2010a), saj10c: (Spitkovsky et al., 2010c), saj10b*: A lightlysupervised algorithm (Spitkovsky et al., 2010b). 7 Discussion In this paper we explored two ways of dealing with cases in which there is no clear theoretical justification to prefer one dependency structure over another. Our experiments suggest that it is crucial to deal with such structures if we would like to have a proper evaluation of unsupervised parsing algorithms against a gold standard. The first way was to mo</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith, 2009. Shared Logistic Normal Distributions for Soft Parameter Tying. In Proc. ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="16729" citStr="Collins, 1999" startWordPosition="2707" endWordPosition="2708">al and the modified parameter set is considerable for all data sets, where differences exceed 9.3%, and go up to 15.1%. These are enormous differences from the perspective of current algorithm evaluation results. 4 Linguistically Problematic Annotations In this section, we discuss the controversial nature of the annotation in the modified structures (K¨ubler 5http://www.jaist.ac.jp/—h-yamada/ 6Results are slightly worse than the ones published in the original papers due to the different decoding algorithms (cs09 use MBR while we used Viterbi) and a different conversion procedure (saj10a used (Collins, 1999) and not (Yamada and Matsumoto, 2003)) ; see Section 5. 666 Algo. &lt; 10 &lt; � Orig. Mod. A Orig. Mod. A km04 45.8 59.8 14 34.6 43.9 9.3 cs09 60.9 72.9 12 39.9 54.6 14.7 saj10a 54.7 69.8 15.1 41.6 54.3 12.7 Table 2: Results of the original (Orig. columns), the modified (Mod. columns) parameter sets and their difference (A columns) for the three algorithms. et al., 2009). We remind the reader that structures for which no linguistic consensus exists as to their correct annotation are referred to as (linguistically) problematic. We begin by showing that all the structures modified are indeed linguist</context>
<context position="18258" citStr="Collins, 1999" startWordPosition="2969" endWordPosition="2970">ken should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers acro</context>
<context position="25516" citStr="Collins, 1999" startWordPosition="4205" endWordPosition="4206">ered an error. Therefore, we suggest evaluating using both NED and attachment score in order to get a full picture of the parser’s performance. A possible criticism on NED is that it is only indifferent to alternative annotations in structures of size 2 (e.g., “to eat”) and does not necessarily handle larger problematic structures, such as coordinations w1 w1 w4 w3 (a) w2 (b) w2 (c) w2 w3 w3 668 and Mary (a) ROOT John ROOT John and Mary (b) ROOT in house the (c) ROOT in the house (d) ROOT house in the (e) Figure 4: Alternative parses of “John and Mary” and “in the house”. Figure 4(a) follows (Collins, 1999), Figure 4(b) follows (Johansson and Nugues, 2007). Figure 4(c) follows (Collins, 1999; Yamada and Matsumoto, 2003). Figure 4(d) and Figure 4(e) show induced parses made by (km04,saj10a) and cs09, respectively. (see Section 4). For example, Figure 4(a) and Figure 4(b) present two alternative annotations of the sentence “John and Mary”. Assume the parse in Figure 4(a) is the gold parse and that in Figure 4(b) is the induced parse. The word “Mary” is a NED error, since its induced parent (“and”) is neither its gold child nor its gold grandparent. Thus, NED does not accept all possible annotation</context>
<context position="27657" citStr="Collins, 1999" startWordPosition="4561" endWordPosition="4562">), and are thus not NED errors. Regarding prepositional phrases, Figure 4(c) presents the gold standard of “in the house”, Figure 4(d) the parse induced by km04 and saj10a and Figure 4(e) the parse induced by cs09. As the reader can verify, both induced parses receive a perfect NED score. In order to further demonstrate NED’s insensitivity to alternative annotations, we took two of the three common gold standard annotations (see Section 4) and evaluated them one against the other. We considered section 23 of WSJ following the scheme of (Yamada and Matsumoto, 2003) as the gold standard and of (Collins, 1999) as the evaluated set. Results show that the attachment score is only 85.6%, the undirected accuracy is improved to 90.3%, while the NED score is 95.3%. This shows that NED is significantly less sensitive to the differences between the different annotation schemes, compared to the other evaluation measures. 6 Experimenting with NED In this section we show that NED indeed reduces the performance difference between the original and the modified parameter sets, thus providing empirical evidence for its validity. For brevity, we present results only for the entire WSJ corpus. Results on WSJ10 are </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael J. Collins, 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Unsupervised language acquisition: theory and practice.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sussex.</institution>
<contexts>
<context position="5482" citStr="Clark, 2001" startWordPosition="833" endWordPosition="834">three gold standards which differ in such decisions and which yield substantially different results. Third, we present the NED measure, which is agnostic to errors arising from choosing the non-gold direction in such cases. Section 2 reviews related work. Section 3 describes the performed parameter modifications. Section 4 discusses the linguistic controversies in annotating problematic dependency structures. Section 5 presents NED. Section 6 describes experiments with it. A discussion is given in Section 7. 2 Related Work Grammar induction received considerable attention over the years (see (Clark, 2001; Klein, 2005) for reviews). For unsupervised dependency parsing, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) was the first to beat the simple right-branching baseline. A technical description of DMV is given at the end of this section. The great majority of recent works, including those experimented with in this paper, are elaborations of DMV. Smith and Eisner (2005) improved the DMV results by generalizing the function maximized by DMV’s EM training algorithm. Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Cohen et al. (2008) e</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark, 2001. Unsupervised language acquisition: theory and practice. Ph.D. thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Unsupervised search-based structured prediction.</title>
<date>2009</date>
<booktitle>In Proc. ofICML.</booktitle>
<marker>Daum´e, 2009</marker>
<rawString>Hal Daum´e III, 2009. Unsupervised search-based structured prediction. In Proc. ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao V. Grac¸a and Fernando Pereira,</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL</booktitle>
<note>Shared Task. EMNLP-CoNLL.</note>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao V. Grac¸a and Fernando Pereira, 2007. Frustratingly Hard Domain Adaptation for Dependency Parsing. In Proc. of the CoNLL 2007 Shared Task. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Gideon Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Semi-supervised learning of dependency parsers using generalized expectation criteria.</title>
<date>2009</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="7360" citStr="Druck et al. (2009)" startWordPosition="1132" endWordPosition="1135">ure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ&apos;. Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent sta</context>
</contexts>
<marker>Druck, Mann, McCallum, 2009</marker>
<rawString>Gregory Druck, Gideon Mann and Andrew McCallum, 2009. Semi-supervised learning of dependency parsers using generalized expectation criteria. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Gillenwater</author>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
<author>Fernando Preira</author>
</authors>
<title>Sparsity in dependency grammar induction.</title>
<date>2010</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Gillenwater, Ganchev, Grac¸a, Taskar, Preira, 2010</marker>
<rawString>Jennifer Gillenwater, Kuzman Ganchev, Jo˜ao V. Grac¸a, Ben Taskar and Fernando Preira, 2010. Sparsity in dependency grammar induction. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Evaluating Unsupervised Part-ofSpeech Tagging for Grammar Induction.</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>McClosky, Charniak, 2008</marker>
<rawString>William P. Headden III, David McClosky, and Eugene Charniak, 2008. Evaluating Unsupervised Part-ofSpeech Tagging for Grammar Induction. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proc. ofHLT-NAACL.</booktitle>
<marker>Johnson, McClosky, 2009</marker>
<rawString>William P. Headden III, Mark Johnson and David McClosky, 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proc. ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended Constituent-to-Dependency Conversion for English. In</title>
<date>2007</date>
<booktitle>Proc. ofNODALIDA.</booktitle>
<contexts>
<context position="17984" citStr="Johansson and Nugues, 2007" startWordPosition="2917" endWordPosition="2920">note that these controversies are reflected in the evaluation of this task, resulting in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s sc</context>
<context position="19672" citStr="Johansson and Nugues, 2007" startWordPosition="3196" endWordPosition="3199">This happens because they use different rules for converting constituency annotation to dependency annotation. A probable explanation for this fact is that people have tried to correct linguistically problematic annotations in different ways, which is why we note this issue here7. There are three different annotation schemes in current use: (1) Collins head rules (Collins, 1999), used in e.g., (Berg-Kirkpatrick et al., 2010; Spitkovsky et al., 2010a); (2) Conversion rules of (Yamada and Matsumoto, 2003), used in e.g., (Cohen and Smith, 2009; Gillenwater et al., 2010); (3) Conversion rules of (Johansson and Nugues, 2007) used, e.g., in the CoNLL shared task 2007 (Nivre et al., 2007) and in (Blunsom and Cohn, 2010). The differences between the schemes are substantial. For instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, 2004), which used undirected evaluation as a measure which is less sensitive to alternative annotations. Undirected accuracy was commonly used since to assess the performance </context>
<context position="25566" citStr="Johansson and Nugues, 2007" startWordPosition="4211" endWordPosition="4214">evaluating using both NED and attachment score in order to get a full picture of the parser’s performance. A possible criticism on NED is that it is only indifferent to alternative annotations in structures of size 2 (e.g., “to eat”) and does not necessarily handle larger problematic structures, such as coordinations w1 w1 w4 w3 (a) w2 (b) w2 (c) w2 w3 w3 668 and Mary (a) ROOT John ROOT John and Mary (b) ROOT in house the (c) ROOT in the house (d) ROOT house in the (e) Figure 4: Alternative parses of “John and Mary” and “in the house”. Figure 4(a) follows (Collins, 1999), Figure 4(b) follows (Johansson and Nugues, 2007). Figure 4(c) follows (Collins, 1999; Yamada and Matsumoto, 2003). Figure 4(d) and Figure 4(e) show induced parses made by (km04,saj10a) and cs09, respectively. (see Section 4). For example, Figure 4(a) and Figure 4(b) present two alternative annotations of the sentence “John and Mary”. Assume the parse in Figure 4(a) is the gold parse and that in Figure 4(b) is the induced parse. The word “Mary” is a NED error, since its induced parent (“and”) is neither its gold child nor its gold grandparent. Thus, NED does not accept all possible annotations of structures of size 3. On the other hand, usin</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues, 2007. Extended Constituent-to-Dependency Conversion for English. In Proc. ofNODALIDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The unsupervised learning of natural language structure.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="5496" citStr="Klein, 2005" startWordPosition="835" endWordPosition="836">andards which differ in such decisions and which yield substantially different results. Third, we present the NED measure, which is agnostic to errors arising from choosing the non-gold direction in such cases. Section 2 reviews related work. Section 3 describes the performed parameter modifications. Section 4 discusses the linguistic controversies in annotating problematic dependency structures. Section 5 presents NED. Section 6 describes experiments with it. A discussion is given in Section 7. 2 Related Work Grammar induction received considerable attention over the years (see (Clark, 2001; Klein, 2005) for reviews). For unsupervised dependency parsing, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) was the first to beat the simple right-branching baseline. A technical description of DMV is given at the end of this section. The great majority of recent works, including those experimented with in this paper, are elaborations of DMV. Smith and Eisner (2005) improved the DMV results by generalizing the function maximized by DMV’s EM training algorithm. Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Cohen et al. (2008) extended DMV by</context>
<context position="7819" citStr="Klein (2005)" startWordPosition="1213" endWordPosition="1214">um´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ&apos;. Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, 664 PRP VBP TO VB ROOT w3 w2 w1 (a) w3 w2 w1 (b) . I want to eat Figure 1: A dependency str</context>
</contexts>
<marker>Klein, 2005</marker>
<rawString>Dan Klein, 2005. The unsupervised learning of natural language structure. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="627" citStr="Klein and Manning, 2004" startWordPosition="73" endWordPosition="76">ralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz&apos; Omri Abend&apos;* Roi Reichart2 Ari Rappoport&apos; &apos;Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substa</context>
<context position="2456" citStr="Klein and Manning, 2004" startWordPosition="368" endWordPosition="371">to the Azrieli Foundation for the award of an Azrieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguistically) problematic. For such cases, evaluation measures should not punish the algorithm for deviating from the gold standard. In this paper we show that the evaluation measures reported in current works are highly sensitive to the annotation in problematic cases, and propose a simple new measure that greatly neutralizes the problem. We start from the following observation: for three leading algorithms (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set (at most 18 out of a few thousands) of parameters can be found whose modification dramatically improves the standard evaluation measures (the attachment score measure by 9.3-15.1%, and the undirected measure by a smaller but still significant 1.3-7.7%). The phenomenon is implementation independent, occurring with several algorithms based on a fundamental probabilistic dependency model1. We show that these parameter changes can be mapped to edge direction changes in local structures in the dependency graph, and that these correspon</context>
<context position="5613" citStr="Klein and Manning, 2004" startWordPosition="849" endWordPosition="852"> the NED measure, which is agnostic to errors arising from choosing the non-gold direction in such cases. Section 2 reviews related work. Section 3 describes the performed parameter modifications. Section 4 discusses the linguistic controversies in annotating problematic dependency structures. Section 5 presents NED. Section 6 describes experiments with it. A discussion is given in Section 7. 2 Related Work Grammar induction received considerable attention over the years (see (Clark, 2001; Klein, 2005) for reviews). For unsupervised dependency parsing, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) was the first to beat the simple right-branching baseline. A technical description of DMV is given at the end of this section. The great majority of recent works, including those experimented with in this paper, are elaborations of DMV. Smith and Eisner (2005) improved the DMV results by generalizing the function maximized by DMV’s EM training algorithm. Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Cohen et al. (2008) extended DMV by using a variational EM training algorithm and adding logistic normal priors. Cohen and Smith (2009, 2010) further ex</context>
<context position="8706" citStr="Klein and Manning (2004)" startWordPosition="1354" endWordPosition="1357">06) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, 664 PRP VBP TO VB ROOT w3 w2 w1 (a) w3 w2 w1 (b) . I want to eat Figure 1: A dependency structure on the words w1, w2, w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-lip of w2—*w1. Figure 2: A parse of the sentence “I want to eat”, before (straight line) and after (dashed line) an edge-lip of the edge “to”�--“eat”. thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (2005) compared two different conversion schemes in German supervised constituency parsing and found one to have positive influence on parsing quality. Dependency Model with Valence (DMV). DMV (Klein and Manning, 2004) defines a probabilistic grammar for unlabeled dependency structures. It is defined as follows: the root of the sentence is first generated, and then each head recursively generates its right and left dependents. The parameters of the m</context>
<context position="14114" citStr="Klein and Manning, 2004" startWordPosition="2277" endWordPosition="2280">d. The number of modified parameters does not exceed 18 (out of a few thousands). The Freq. column in the table shows the percentage of the tokens in sections 2-21 of PTB WSJ that participate in each structure. Equivalently, the percentage of edges in the corpus which are of either of the types appearing in the Orig. Edge column. As the table shows, the modified structures cover a significant portion of the tokens. Indeed, 42.9% of the tokens in the corpus participate in at least one of them3. Experimenting with Edge Flipping. We experimented with three DMV-based algorithms: a replication of (Klein and Manning, 2004), as appears in (Cohen et al., 2008) (henceforth, km04), Cohen and Smith (2009) (henceforth, cs09), and Spitkovsky et al. (2010a) (henceforth, saj10a). Decoding is done using the Viterbi algorithm4. For each of these algorithms we present the performance gain when compared to the original parameters. The training set is sections 2-21 of the Wall Street 2Note that this yields unnormalized models. Again, this is justified since the resulting model is only used as a basis for discussion and is not a fully fledged algorithm. 3Some tokens participate in more than one structure. 4http://www.cs.cmu.e</context>
<context position="15486" citStr="Klein and Manning, 2004" startWordPosition="2517" endWordPosition="2520">ional DT→NNS – – + IN→DT + + – Phrase (“in 32.7% the house”) IN←NN + + – IN←NNP + – – IN←NNS – + – PRP$→NN – – + Modal Verb 2.4% MD←V B – + – (“can eat”) Infinitive Verb 4.5% TO→V B – + + (“to eat”) Proper Name Sequence 18.5% NNP→NNP + – – (“John Doe”) Table 1: Parameter changes for the three algorithms. The Freq. column shows what percentage of the tokens in sections 2-21 of PTB WSJ participate in each structure. The Orig. column indicates the original edge. The modified edge is of the opposite direction. The other columns show the different algorithms: km04: basic DMV model (replication of (Klein and Manning, 2004)); cs09; (Cohen and Smith, 2009); saj10a: (Spitkovsky et al., 2010a). Journal Penn TreeBank (Marcus et al., 1993). Testing is done on section 23. The constituency annotation was converted to dependencies using the rules of (Yamada and Matsumoto, 2003)5. Following standard practice, we present the attachment score (i.e., percentage of words that have a correct head) of each algorithm, with both the original parameters and the modified ones. We present results both on all sentences and on sentences of length G 10, excluding punctuation. Table 2 shows results for all algorithms6. The performance </context>
<context position="20102" citStr="Klein and Manning, 2004" startWordPosition="3268" endWordPosition="3271">kovsky et al., 2010a); (2) Conversion rules of (Yamada and Matsumoto, 2003), used in e.g., (Cohen and Smith, 2009; Gillenwater et al., 2010); (3) Conversion rules of (Johansson and Nugues, 2007) used, e.g., in the CoNLL shared task 2007 (Nivre et al., 2007) and in (Blunsom and Cohn, 2010). The differences between the schemes are substantial. For instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, 2004), which used undirected evaluation as a measure which is less sensitive to alternative annotations. Undirected accuracy was commonly used since to assess the performance of unsupervised parsers (e.g., (Smith and Eisner, 2006; Headden et al., 2008; Spitkovsky et al., 2010a)) but also of supervised ones (Wang et al., 2005; Wang et al., 2006). In this section we discuss why this measure is in fact not indifferent to edgefflips and propose a new measure, Neutral Edge Direction (NED). 7Indeed, half a dozen flags in the LTH Constituent-toDependency Conversion Tool (Johansson and Nugues, 2007) are us</context>
<context position="30951" citStr="Klein and Manning, 2004" startWordPosition="5089" endWordPosition="5092">10 57.1 45 62.5 53.2 80.4 65.1 km04 45.8 34.6 60.3 52.9 78.4 66.6 saj10a 54.7 41.6 66.5 55.9 78.9 67.8 saj10c 63.8 46.1 72.6 58.8 84.2 70.8 saj10b* 67.9 48.2 74.0 57.7 86.0 70.7 Table 4: A comparison of recent works, using Att (attachment score) Un (undirected evaluation) and NED, on sentences of length &lt; 10 (excluding punctuation) and on all sentences. The gold standard is obtained using the rules of (Yamada and Matsumoto, 2003). bbdk10: (Berg-Kirkpatrick et al., 2010), bc10: (Blunsom and Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10: (Gillenwater et al., 2010), km04: A replication of (Klein and Manning, 2004), saj10a: (Spitkovsky et al., 2010a), saj10c: (Spitkovsky et al., 2010c), saj10b*: A lightlysupervised algorithm (Spitkovsky et al., 2010b). 7 Discussion In this paper we explored two ways of dealing with cases in which there is no clear theoretical justification to prefer one dependency structure over another. Our experiments suggest that it is crucial to deal with such structures if we would like to have a proper evaluation of unsupervised parsing algorithms against a gold standard. The first way was to modify the parameters of the parsing algorithms so that in cases where such problematic d</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher Manning, 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
</authors>
<title>How Do Treebank Annotation Schemes Influence Parsing Results? Or How Not to Compare Apples And Oranges.</title>
<date>2005</date>
<booktitle>In Proc. of RANLP.</booktitle>
<marker>K¨ubler, 2005</marker>
<rawString>Sandra K¨ubler, 2005. How Do Treebank Annotation Schemes Influence Parsing Results? Or How Not to Compare Apples And Oranges. In Proc. of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>R McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing.</title>
<date>2009</date>
<publisher>Morgan And Claypool Publishers.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, R. McDonald and Joakim Nivre, 2009. Dependency Parsing. Morgan And Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<pages>19--313</pages>
<contexts>
<context position="15599" citStr="Marcus et al., 1993" startWordPosition="2534" endWordPosition="2537">l Verb 2.4% MD←V B – + – (“can eat”) Infinitive Verb 4.5% TO→V B – + + (“to eat”) Proper Name Sequence 18.5% NNP→NNP + – – (“John Doe”) Table 1: Parameter changes for the three algorithms. The Freq. column shows what percentage of the tokens in sections 2-21 of PTB WSJ participate in each structure. The Orig. column indicates the original edge. The modified edge is of the opposite direction. The other columns show the different algorithms: km04: basic DMV model (replication of (Klein and Manning, 2004)); cs09; (Cohen and Smith, 2009); saj10a: (Spitkovsky et al., 2010a). Journal Penn TreeBank (Marcus et al., 1993). Testing is done on section 23. The constituency annotation was converted to dependencies using the rules of (Yamada and Matsumoto, 2003)5. Following standard practice, we present the attachment score (i.e., percentage of words that have a correct head) of each algorithm, with both the original parameters and the modified ones. We present results both on all sentences and on sentences of length G 10, excluding punctuation. Table 2 shows results for all algorithms6. The performance difference between the original and the modified parameter set is considerable for all data sets, where differenc</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini and Mary Ann Marcinkiewicz, 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics 19:313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In Proc. ofEMNLP.</booktitle>
<contexts>
<context position="7548" citStr="Naseem et al. (2010)" startWordPosition="1167" endWordPosition="1170">. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ&apos;. Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and ve</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay and Mark Johnson, 2010. Using universal linguistic knowledge to guide grammar induction. In Proc. ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="7782" citStr="Nivre, 2006" startWordPosition="1207" endWordPosition="1208">ed works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ&apos;. Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, 664 PRP VBP TO VB ROOT w3 w2 w1 (a) w3 w2 w1 (b) . I w</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre, 2006. Inductive Dependency Parsing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>MaltParser: A data-driven parser-generator for dependency parsing. In</title>
<date>2006</date>
<booktitle>Proc. ofLREC-2006.</booktitle>
<contexts>
<context position="8241" citStr="Nivre et al., 2006" startWordPosition="1269" endWordPosition="1272">applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, 664 PRP VBP TO VB ROOT w3 w2 w1 (a) w3 w2 w1 (b) . I want to eat Figure 1: A dependency structure on the words w1, w2, w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-lip of w2—*w1. Figure 2: A parse of the sentence “I want to eat”, before (straight line) and after (dashed line) an edge-lip of the edge “to”�--“eat”. thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determine</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall and Jens Nilsson, 2006. MaltParser: A data-driven parser-generator for dependency parsing. In Proc. ofLREC-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL Shared Task, EMNLPCoNLL,</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel and Deniz Yuret, 2007. The CoNLL 2007 shared task on dependency parsing. In Proc. of the CoNLL Shared Task, EMNLPCoNLL, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
</authors>
<title>Graph transformations in data-driven dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="8085" citStr="Nilsson et al. (2006)" startWordPosition="1247" endWordPosition="1250">iner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, 664 PRP VBP TO VB ROOT w3 w2 w1 (a) w3 w2 w1 (b) . I want to eat Figure 1: A dependency structure on the words w1, w2, w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-lip of w2—*w1. Figure 2: A parse of the sentence “I want to eat”, before (straight line) and after (dashed line) an edge-lip of the edge “to”�--“eat”. thus improving performance. Kle</context>
<context position="17715" citStr="Nilsson et al., 2006" startWordPosition="2873" endWordPosition="2876">09). We remind the reader that structures for which no linguistic consensus exists as to their correct annotation are referred to as (linguistically) problematic. We begin by showing that all the structures modified are indeed linguistically problematic. We then note that these controversies are reflected in the evaluation of this task, resulting in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitiv</context>
</contexts>
<marker>Nilsson, Nivre, Hall, 2006</marker>
<rawString>Jens Nilsson, Joakim Nivre and Johan Hall, 2006. Graph transformations in data-driven dependency parsing. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Cassandre Creswell</author>
<author>Rachel Szekely</author>
<author>Harriet Tauber</author>
<author>Marilyn Walker</author>
</authors>
<title>A dependency treebankfor English. In</title>
<date>2002</date>
<booktitle>Proc. ofLREC.</booktitle>
<contexts>
<context position="18304" citStr="Rambow et al., 2002" startWordPosition="2975" endWordPosition="2978">, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possible, </context>
</contexts>
<marker>Rambow, Creswell, Szekely, Tauber, Walker, 2002</marker>
<rawString>Owen Rambow, Cassandre Creswell, Rachel Szekely, Harriet Tauber and Marilyn Walker, 2002. A dependency treebankfor English. In Proc. ofLREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation.</title>
<date>2005</date>
<booktitle>In Proc. ofIJCAI.</booktitle>
<contexts>
<context position="5874" citStr="Smith and Eisner (2005)" startWordPosition="893" endWordPosition="896">oblematic dependency structures. Section 5 presents NED. Section 6 describes experiments with it. A discussion is given in Section 7. 2 Related Work Grammar induction received considerable attention over the years (see (Clark, 2001; Klein, 2005) for reviews). For unsupervised dependency parsing, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) was the first to beat the simple right-branching baseline. A technical description of DMV is given at the end of this section. The great majority of recent works, including those experimented with in this paper, are elaborations of DMV. Smith and Eisner (2005) improved the DMV results by generalizing the function maximized by DMV’s EM training algorithm. Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Cohen et al. (2008) extended DMV by using a variational EM training algorithm and adding logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported r</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner, 2005. Guiding unsupervised grammar induction using contrastive estimation. In Proc. ofIJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="5994" citStr="Smith and Eisner (2006)" startWordPosition="912" endWordPosition="915">in Section 7. 2 Related Work Grammar induction received considerable attention over the years (see (Clark, 2001; Klein, 2005) for reviews). For unsupervised dependency parsing, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) was the first to beat the simple right-branching baseline. A technical description of DMV is given at the end of this section. The great majority of recent works, including those experimented with in this paper, are elaborations of DMV. Smith and Eisner (2005) improved the DMV results by generalizing the function maximized by DMV’s EM training algorithm. Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Cohen et al. (2008) extended DMV by using a variational EM training algorithm and adding logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the</context>
<context position="20326" citStr="Smith and Eisner, 2006" startWordPosition="3301" endWordPosition="3304"> task 2007 (Nivre et al., 2007) and in (Blunsom and Cohn, 2010). The differences between the schemes are substantial. For instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, 2004), which used undirected evaluation as a measure which is less sensitive to alternative annotations. Undirected accuracy was commonly used since to assess the performance of unsupervised parsers (e.g., (Smith and Eisner, 2006; Headden et al., 2008; Spitkovsky et al., 2010a)) but also of supervised ones (Wang et al., 2005; Wang et al., 2006). In this section we discuss why this measure is in fact not indifferent to edgefflips and propose a new measure, Neutral Edge Direction (NED). 7Indeed, half a dozen flags in the LTH Constituent-toDependency Conversion Tool (Johansson and Nugues, 2007) are used to control the conversion in problematic cases. 8In our experiments we used the scheme of (Yamada and Matsumoto, 2003), see Section 3. The significant effects of edge flipping were observed with the other two schemes as w</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>Noah A. Smith and Jason Eisner, 2006. Annealing structural bias in multilingual weighted grammar induction. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>From Baby Steps to Leapfrog: How “Less is More” in Unsupervised Dependency Parsing. In</title>
<date>2010</date>
<booktitle>Proc. ofNAACL-HLT.</booktitle>
<contexts>
<context position="675" citStr="Spitkovsky et al., 2010" startWordPosition="81" endWordPosition="84">in Unsupervised Dependency Parsing Evaluation Roy Schwartz&apos; Omri Abend&apos;* Roi Reichart2 Ari Rappoport&apos; &apos;Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200</context>
<context position="2504" citStr="Spitkovsky et al., 2010" startWordPosition="377" endWordPosition="380">rieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguistically) problematic. For such cases, evaluation measures should not punish the algorithm for deviating from the gold standard. In this paper we show that the evaluation measures reported in current works are highly sensitive to the annotation in problematic cases, and propose a simple new measure that greatly neutralizes the problem. We start from the following observation: for three leading algorithms (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set (at most 18 out of a few thousands) of parameters can be found whose modification dramatically improves the standard evaluation measures (the attachment score measure by 9.3-15.1%, and the undirected measure by a smaller but still significant 1.3-7.7%). The phenomenon is implementation independent, occurring with several algorithms based on a fundamental probabilistic dependency model1. We show that these parameter changes can be mapped to edge direction changes in local structures in the dependency graph, and that these correspond to problematic annotations. Thus, the standard</context>
<context position="6882" citStr="Spitkovsky et al. (2010" startWordPosition="1058" endWordPosition="1061">ior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ&apos;. Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to i</context>
<context position="14241" citStr="Spitkovsky et al. (2010" startWordPosition="2297" endWordPosition="2300">tage of the tokens in sections 2-21 of PTB WSJ that participate in each structure. Equivalently, the percentage of edges in the corpus which are of either of the types appearing in the Orig. Edge column. As the table shows, the modified structures cover a significant portion of the tokens. Indeed, 42.9% of the tokens in the corpus participate in at least one of them3. Experimenting with Edge Flipping. We experimented with three DMV-based algorithms: a replication of (Klein and Manning, 2004), as appears in (Cohen et al., 2008) (henceforth, km04), Cohen and Smith (2009) (henceforth, cs09), and Spitkovsky et al. (2010a) (henceforth, saj10a). Decoding is done using the Viterbi algorithm4. For each of these algorithms we present the performance gain when compared to the original parameters. The training set is sections 2-21 of the Wall Street 2Note that this yields unnormalized models. Again, this is justified since the resulting model is only used as a basis for discussion and is not a fully fledged algorithm. 3Some tokens participate in more than one structure. 4http://www.cs.cmu.edu/—scohen/parser.html. Structure Freq. Orig. Edge km04 cs09 saj10a Coordination 2.9% CC→NNP – + – (“John &amp; Mary”) DT→NN + + + </context>
<context position="15552" citStr="Spitkovsky et al., 2010" startWordPosition="2527" endWordPosition="2530">+ + – IN←NNP + – – IN←NNS – + – PRP$→NN – – + Modal Verb 2.4% MD←V B – + – (“can eat”) Infinitive Verb 4.5% TO→V B – + + (“to eat”) Proper Name Sequence 18.5% NNP→NNP + – – (“John Doe”) Table 1: Parameter changes for the three algorithms. The Freq. column shows what percentage of the tokens in sections 2-21 of PTB WSJ participate in each structure. The Orig. column indicates the original edge. The modified edge is of the opposite direction. The other columns show the different algorithms: km04: basic DMV model (replication of (Klein and Manning, 2004)); cs09; (Cohen and Smith, 2009); saj10a: (Spitkovsky et al., 2010a). Journal Penn TreeBank (Marcus et al., 1993). Testing is done on section 23. The constituency annotation was converted to dependencies using the rules of (Yamada and Matsumoto, 2003)5. Following standard practice, we present the attachment score (i.e., percentage of words that have a correct head) of each algorithm, with both the original parameters and the modified ones. We present results both on all sentences and on sentences of length G 10, excluding punctuation. Table 2 shows results for all algorithms6. The performance difference between the original and the modified parameter set is </context>
<context position="19497" citStr="Spitkovsky et al., 2010" startWordPosition="3168" endWordPosition="3171">ers is not directly possible, since different papers use different gold standard annotations even when they are all derivedfrom the Penn Treebank constituency annotation. This happens because they use different rules for converting constituency annotation to dependency annotation. A probable explanation for this fact is that people have tried to correct linguistically problematic annotations in different ways, which is why we note this issue here7. There are three different annotation schemes in current use: (1) Collins head rules (Collins, 1999), used in e.g., (Berg-Kirkpatrick et al., 2010; Spitkovsky et al., 2010a); (2) Conversion rules of (Yamada and Matsumoto, 2003), used in e.g., (Cohen and Smith, 2009; Gillenwater et al., 2010); (3) Conversion rules of (Johansson and Nugues, 2007) used, e.g., in the CoNLL shared task 2007 (Nivre et al., 2007) and in (Blunsom and Cohn, 2010). The differences between the schemes are substantial. For instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, </context>
<context position="30985" citStr="Spitkovsky et al., 2010" startWordPosition="5094" endWordPosition="5097"> 45.8 34.6 60.3 52.9 78.4 66.6 saj10a 54.7 41.6 66.5 55.9 78.9 67.8 saj10c 63.8 46.1 72.6 58.8 84.2 70.8 saj10b* 67.9 48.2 74.0 57.7 86.0 70.7 Table 4: A comparison of recent works, using Att (attachment score) Un (undirected evaluation) and NED, on sentences of length &lt; 10 (excluding punctuation) and on all sentences. The gold standard is obtained using the rules of (Yamada and Matsumoto, 2003). bbdk10: (Berg-Kirkpatrick et al., 2010), bc10: (Blunsom and Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10: (Gillenwater et al., 2010), km04: A replication of (Klein and Manning, 2004), saj10a: (Spitkovsky et al., 2010a), saj10c: (Spitkovsky et al., 2010c), saj10b*: A lightlysupervised algorithm (Spitkovsky et al., 2010b). 7 Discussion In this paper we explored two ways of dealing with cases in which there is no clear theoretical justification to prefer one dependency structure over another. Our experiments suggest that it is crucial to deal with such structures if we would like to have a proper evaluation of unsupervised parsing algorithms against a gold standard. The first way was to modify the parameters of the parsing algorithms so that in cases where such problematic decisions are to be made they follo</context>
<context position="36364" citStr="Spitkovsky et al., 2010" startWordPosition="5990" endWordPosition="5993">) if and only if the structures produced by this set are better learned by the supervised version of the algorithm (bottom line). This observation supports our hypothesis that in cases where there is no theoretical preference for one structure over the other, NED (unlike the other measures) prefers the structures that are more consistent with the modeling assumptions lying in the basis of the algorithm. We consider this to be a desired property of a measure since a more consistent model should be preferred where no theoretical preference exists. learned structure. 13In using WSJ20, we follow (Spitkovsky et al., 2010a), which showed that training the DMV on sentences of bounded length yields a higher score than using the entire corpus. We use it as we aim to use an optimal setting. 8 Conclusion In this paper we showed that the standard evaluation of unsupervised dependency parsers is highly sensitive to problematic annotations. We modified a small set of parameters that controls the annotation in such problematic cases in three leading parsers. This resulted in a major performance boost, which is unindicative of a true difference in quality. We presented Neutral Edge Direction (NED), a measure that is les</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Jurafsky, 2010a. From Baby Steps to Leapfrog: How “Less is More” in Unsupervised Dependency Parsing. In Proc. ofNAACL-HLT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Valentin I Spitkovsky</author>
</authors>
<title>Hiyan Alshawi and Daniel Jurafsky, 2010b. Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing. In</title>
<booktitle>Proc. ofACL.</booktitle>
<marker>Spitkovsky, </marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Jurafsky, 2010b. Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="675" citStr="Spitkovsky et al., 2010" startWordPosition="81" endWordPosition="84">in Unsupervised Dependency Parsing Evaluation Roy Schwartz&apos; Omri Abend&apos;* Roi Reichart2 Ari Rappoport&apos; &apos;Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200</context>
<context position="2504" citStr="Spitkovsky et al., 2010" startWordPosition="377" endWordPosition="380">rieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguistically) problematic. For such cases, evaluation measures should not punish the algorithm for deviating from the gold standard. In this paper we show that the evaluation measures reported in current works are highly sensitive to the annotation in problematic cases, and propose a simple new measure that greatly neutralizes the problem. We start from the following observation: for three leading algorithms (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set (at most 18 out of a few thousands) of parameters can be found whose modification dramatically improves the standard evaluation measures (the attachment score measure by 9.3-15.1%, and the undirected measure by a smaller but still significant 1.3-7.7%). The phenomenon is implementation independent, occurring with several algorithms based on a fundamental probabilistic dependency model1. We show that these parameter changes can be mapped to edge direction changes in local structures in the dependency graph, and that these correspond to problematic annotations. Thus, the standard</context>
<context position="6882" citStr="Spitkovsky et al. (2010" startWordPosition="1058" endWordPosition="1061">ior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ&apos;. Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to i</context>
<context position="14241" citStr="Spitkovsky et al. (2010" startWordPosition="2297" endWordPosition="2300">tage of the tokens in sections 2-21 of PTB WSJ that participate in each structure. Equivalently, the percentage of edges in the corpus which are of either of the types appearing in the Orig. Edge column. As the table shows, the modified structures cover a significant portion of the tokens. Indeed, 42.9% of the tokens in the corpus participate in at least one of them3. Experimenting with Edge Flipping. We experimented with three DMV-based algorithms: a replication of (Klein and Manning, 2004), as appears in (Cohen et al., 2008) (henceforth, km04), Cohen and Smith (2009) (henceforth, cs09), and Spitkovsky et al. (2010a) (henceforth, saj10a). Decoding is done using the Viterbi algorithm4. For each of these algorithms we present the performance gain when compared to the original parameters. The training set is sections 2-21 of the Wall Street 2Note that this yields unnormalized models. Again, this is justified since the resulting model is only used as a basis for discussion and is not a fully fledged algorithm. 3Some tokens participate in more than one structure. 4http://www.cs.cmu.edu/—scohen/parser.html. Structure Freq. Orig. Edge km04 cs09 saj10a Coordination 2.9% CC→NNP – + – (“John &amp; Mary”) DT→NN + + + </context>
<context position="15552" citStr="Spitkovsky et al., 2010" startWordPosition="2527" endWordPosition="2530">+ + – IN←NNP + – – IN←NNS – + – PRP$→NN – – + Modal Verb 2.4% MD←V B – + – (“can eat”) Infinitive Verb 4.5% TO→V B – + + (“to eat”) Proper Name Sequence 18.5% NNP→NNP + – – (“John Doe”) Table 1: Parameter changes for the three algorithms. The Freq. column shows what percentage of the tokens in sections 2-21 of PTB WSJ participate in each structure. The Orig. column indicates the original edge. The modified edge is of the opposite direction. The other columns show the different algorithms: km04: basic DMV model (replication of (Klein and Manning, 2004)); cs09; (Cohen and Smith, 2009); saj10a: (Spitkovsky et al., 2010a). Journal Penn TreeBank (Marcus et al., 1993). Testing is done on section 23. The constituency annotation was converted to dependencies using the rules of (Yamada and Matsumoto, 2003)5. Following standard practice, we present the attachment score (i.e., percentage of words that have a correct head) of each algorithm, with both the original parameters and the modified ones. We present results both on all sentences and on sentences of length G 10, excluding punctuation. Table 2 shows results for all algorithms6. The performance difference between the original and the modified parameter set is </context>
<context position="19497" citStr="Spitkovsky et al., 2010" startWordPosition="3168" endWordPosition="3171">ers is not directly possible, since different papers use different gold standard annotations even when they are all derivedfrom the Penn Treebank constituency annotation. This happens because they use different rules for converting constituency annotation to dependency annotation. A probable explanation for this fact is that people have tried to correct linguistically problematic annotations in different ways, which is why we note this issue here7. There are three different annotation schemes in current use: (1) Collins head rules (Collins, 1999), used in e.g., (Berg-Kirkpatrick et al., 2010; Spitkovsky et al., 2010a); (2) Conversion rules of (Yamada and Matsumoto, 2003), used in e.g., (Cohen and Smith, 2009; Gillenwater et al., 2010); (3) Conversion rules of (Johansson and Nugues, 2007) used, e.g., in the CoNLL shared task 2007 (Nivre et al., 2007) and in (Blunsom and Cohn, 2010). The differences between the schemes are substantial. For instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, </context>
<context position="30985" citStr="Spitkovsky et al., 2010" startWordPosition="5094" endWordPosition="5097"> 45.8 34.6 60.3 52.9 78.4 66.6 saj10a 54.7 41.6 66.5 55.9 78.9 67.8 saj10c 63.8 46.1 72.6 58.8 84.2 70.8 saj10b* 67.9 48.2 74.0 57.7 86.0 70.7 Table 4: A comparison of recent works, using Att (attachment score) Un (undirected evaluation) and NED, on sentences of length &lt; 10 (excluding punctuation) and on all sentences. The gold standard is obtained using the rules of (Yamada and Matsumoto, 2003). bbdk10: (Berg-Kirkpatrick et al., 2010), bc10: (Blunsom and Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10: (Gillenwater et al., 2010), km04: A replication of (Klein and Manning, 2004), saj10a: (Spitkovsky et al., 2010a), saj10c: (Spitkovsky et al., 2010c), saj10b*: A lightlysupervised algorithm (Spitkovsky et al., 2010b). 7 Discussion In this paper we explored two ways of dealing with cases in which there is no clear theoretical justification to prefer one dependency structure over another. Our experiments suggest that it is crucial to deal with such structures if we would like to have a proper evaluation of unsupervised parsing algorithms against a gold standard. The first way was to modify the parameters of the parsing algorithms so that in cases where such problematic decisions are to be made they follo</context>
<context position="36364" citStr="Spitkovsky et al., 2010" startWordPosition="5990" endWordPosition="5993">) if and only if the structures produced by this set are better learned by the supervised version of the algorithm (bottom line). This observation supports our hypothesis that in cases where there is no theoretical preference for one structure over the other, NED (unlike the other measures) prefers the structures that are more consistent with the modeling assumptions lying in the basis of the algorithm. We consider this to be a desired property of a measure since a more consistent model should be preferred where no theoretical preference exists. learned structure. 13In using WSJ20, we follow (Spitkovsky et al., 2010a), which showed that training the DMV on sentences of bounded length yields a higher score than using the entire corpus. We use it as we aim to use an optimal setting. 8 Conclusion In this paper we showed that the standard evaluation of unsupervised dependency parsers is highly sensitive to problematic annotations. We modified a small set of parameters that controls the annotation in such problematic cases in three leading parsers. This resulted in a major performance boost, which is unindicative of a true difference in quality. We presented Neutral Edge Direction (NED), a measure that is les</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky and Christopher D. Manning, 2010c. Viterbi training improves unsupervised dependency parsing. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Dale Schuurmans</author>
<author>Dekang Lin</author>
</authors>
<title>Strictly Lexical Dependency Parsing.</title>
<date>2005</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="20423" citStr="Wang et al., 2005" startWordPosition="3319" endWordPosition="3322">re substantial. For instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, 2004), which used undirected evaluation as a measure which is less sensitive to alternative annotations. Undirected accuracy was commonly used since to assess the performance of unsupervised parsers (e.g., (Smith and Eisner, 2006; Headden et al., 2008; Spitkovsky et al., 2010a)) but also of supervised ones (Wang et al., 2005; Wang et al., 2006). In this section we discuss why this measure is in fact not indifferent to edgefflips and propose a new measure, Neutral Edge Direction (NED). 7Indeed, half a dozen flags in the LTH Constituent-toDependency Conversion Tool (Johansson and Nugues, 2007) are used to control the conversion in problematic cases. 8In our experiments we used the scheme of (Yamada and Matsumoto, 2003), see Section 3. The significant effects of edge flipping were observed with the other two schemes as well. 667 Figure 3: A dependency structure on the words w1, w2, w3 before (Figure 3(a)) and after </context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2005</marker>
<rawString>Qin Iris Wang, Dale Schuurmans and Dekang Lin, 2005. Strictly Lexical Dependency Parsing. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Colin Cherry</author>
<author>Dan Lizotte</author>
<author>Dale Schuurmans</author>
</authors>
<title>Improved Large Margin Dependency Parsing via Local Constraints and Laplacian Regularization.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="20443" citStr="Wang et al., 2006" startWordPosition="3323" endWordPosition="3326"> instance, 14.4% of section 23 is tagged differently by (1) and (2)8. 5 The Neutral Edge Direction (NED) Measure As shown in the previous sections, the annotation of problematic edges can substantially affect performance. This was briefly discussed in (Klein and Manning, 2004), which used undirected evaluation as a measure which is less sensitive to alternative annotations. Undirected accuracy was commonly used since to assess the performance of unsupervised parsers (e.g., (Smith and Eisner, 2006; Headden et al., 2008; Spitkovsky et al., 2010a)) but also of supervised ones (Wang et al., 2005; Wang et al., 2006). In this section we discuss why this measure is in fact not indifferent to edgefflips and propose a new measure, Neutral Edge Direction (NED). 7Indeed, half a dozen flags in the LTH Constituent-toDependency Conversion Tool (Johansson and Nugues, 2007) are used to control the conversion in problematic cases. 8In our experiments we used the scheme of (Yamada and Matsumoto, 2003), see Section 3. The significant effects of edge flipping were observed with the other two schemes as well. 667 Figure 3: A dependency structure on the words w1, w2, w3 before (Figure 3(a)) and after (Figure 3(b)) an edg</context>
</contexts>
<marker>Wang, Cherry, Lizotte, Schuurmans, 2006</marker>
<rawString>Qin Iris Wang, Colin Cherry, Dan Lizotte and Dale Schuurmans, 2006. Improved Large Margin Dependency Parsing via Local Constraints and Laplacian Regularization. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of the International Workshop on Parsing Technologies.</booktitle>
<contexts>
<context position="15737" citStr="Yamada and Matsumoto, 2003" startWordPosition="2557" endWordPosition="2560">oe”) Table 1: Parameter changes for the three algorithms. The Freq. column shows what percentage of the tokens in sections 2-21 of PTB WSJ participate in each structure. The Orig. column indicates the original edge. The modified edge is of the opposite direction. The other columns show the different algorithms: km04: basic DMV model (replication of (Klein and Manning, 2004)); cs09; (Cohen and Smith, 2009); saj10a: (Spitkovsky et al., 2010a). Journal Penn TreeBank (Marcus et al., 1993). Testing is done on section 23. The constituency annotation was converted to dependencies using the rules of (Yamada and Matsumoto, 2003)5. Following standard practice, we present the attachment score (i.e., percentage of words that have a correct head) of each algorithm, with both the original parameters and the modified ones. We present results both on all sentences and on sentences of length G 10, excluding punctuation. Table 2 shows results for all algorithms6. The performance difference between the original and the modified parameter set is considerable for all data sets, where differences exceed 9.3%, and go up to 15.1%. These are enormous differences from the perspective of current algorithm evaluation results. 4 Linguis</context>
<context position="18399" citStr="Yamada and Matsumoto, 2003" startWordPosition="2991" endWordPosition="2994">”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possible, since different papers use different gold standard annotations even when they are all derivedfr</context>
<context position="20823" citStr="Yamada and Matsumoto, 2003" startWordPosition="3384" endWordPosition="3387">ions. Undirected accuracy was commonly used since to assess the performance of unsupervised parsers (e.g., (Smith and Eisner, 2006; Headden et al., 2008; Spitkovsky et al., 2010a)) but also of supervised ones (Wang et al., 2005; Wang et al., 2006). In this section we discuss why this measure is in fact not indifferent to edgefflips and propose a new measure, Neutral Edge Direction (NED). 7Indeed, half a dozen flags in the LTH Constituent-toDependency Conversion Tool (Johansson and Nugues, 2007) are used to control the conversion in problematic cases. 8In our experiments we used the scheme of (Yamada and Matsumoto, 2003), see Section 3. The significant effects of edge flipping were observed with the other two schemes as well. 667 Figure 3: A dependency structure on the words w1, w2, w3 before (Figure 3(a)) and after (Figure 3(b)) an edge-lip of w2→w3, and when the direction of the edge between w2 and w3 is switched and the new parent of w3 is set to be some other word, w4 (Figure 3(c)). Undirected Evaluation. The measure is defined as follows: traverse over the tokens and mark a correct attachment if the token’s induced parent is either (1) its gold parent or (2) its gold child. The score is the ratio of corr</context>
<context position="25631" citStr="Yamada and Matsumoto, 2003" startWordPosition="4221" endWordPosition="4224">full picture of the parser’s performance. A possible criticism on NED is that it is only indifferent to alternative annotations in structures of size 2 (e.g., “to eat”) and does not necessarily handle larger problematic structures, such as coordinations w1 w1 w4 w3 (a) w2 (b) w2 (c) w2 w3 w3 668 and Mary (a) ROOT John ROOT John and Mary (b) ROOT in house the (c) ROOT in the house (d) ROOT house in the (e) Figure 4: Alternative parses of “John and Mary” and “in the house”. Figure 4(a) follows (Collins, 1999), Figure 4(b) follows (Johansson and Nugues, 2007). Figure 4(c) follows (Collins, 1999; Yamada and Matsumoto, 2003). Figure 4(d) and Figure 4(e) show induced parses made by (km04,saj10a) and cs09, respectively. (see Section 4). For example, Figure 4(a) and Figure 4(b) present two alternative annotations of the sentence “John and Mary”. Assume the parse in Figure 4(a) is the gold parse and that in Figure 4(b) is the induced parse. The word “Mary” is a NED error, since its induced parent (“and”) is neither its gold child nor its gold grandparent. Thus, NED does not accept all possible annotations of structures of size 3. On the other hand, using a method which accepts all possible annotations of structures o</context>
<context position="27613" citStr="Yamada and Matsumoto, 2003" startWordPosition="4550" endWordPosition="4553">notations differ only in a single edge-lip (i.e., CC—*NNP), and are thus not NED errors. Regarding prepositional phrases, Figure 4(c) presents the gold standard of “in the house”, Figure 4(d) the parse induced by km04 and saj10a and Figure 4(e) the parse induced by cs09. As the reader can verify, both induced parses receive a perfect NED score. In order to further demonstrate NED’s insensitivity to alternative annotations, we took two of the three common gold standard annotations (see Section 4) and evaluated them one against the other. We considered section 23 of WSJ following the scheme of (Yamada and Matsumoto, 2003) as the gold standard and of (Collins, 1999) as the evaluated set. Results show that the attachment score is only 85.6%, the undirected accuracy is improved to 90.3%, while the NED score is 95.3%. This shows that NED is significantly less sensitive to the differences between the different annotation schemes, compared to the other evaluation measures. 6 Experimenting with NED In this section we show that NED indeed reduces the performance difference between the original and the modified parameter sets, thus providing empirical evidence for its validity. For brevity, we present results only for </context>
<context position="30760" citStr="Yamada and Matsumoto, 2003" startWordPosition="5061" endWordPosition="5064"> a different perspective on algorithm quality11. Algo. Att10 Att. Un10 Un. NED10 NED. bbdk10 66.1 49.6 70.1 56.0 75.5 61.8 bc10 67.2 53.6 73 61.7 81.6 70.2 cs09 61.5 42 66.9 50.4 81.5 62.9 gggtp10 57.1 45 62.5 53.2 80.4 65.1 km04 45.8 34.6 60.3 52.9 78.4 66.6 saj10a 54.7 41.6 66.5 55.9 78.9 67.8 saj10c 63.8 46.1 72.6 58.8 84.2 70.8 saj10b* 67.9 48.2 74.0 57.7 86.0 70.7 Table 4: A comparison of recent works, using Att (attachment score) Un (undirected evaluation) and NED, on sentences of length &lt; 10 (excluding punctuation) and on all sentences. The gold standard is obtained using the rules of (Yamada and Matsumoto, 2003). bbdk10: (Berg-Kirkpatrick et al., 2010), bc10: (Blunsom and Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10: (Gillenwater et al., 2010), km04: A replication of (Klein and Manning, 2004), saj10a: (Spitkovsky et al., 2010a), saj10c: (Spitkovsky et al., 2010c), saj10b*: A lightlysupervised algorithm (Spitkovsky et al., 2010b). 7 Discussion In this paper we explored two ways of dealing with cases in which there is no clear theoretical justification to prefer one dependency structure over another. Our experiments suggest that it is crucial to deal with such structures if we would like to have</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto, 2003. Statistical dependency analysis with support vector machines. In Proc. of the International Workshop on Parsing Technologies.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>