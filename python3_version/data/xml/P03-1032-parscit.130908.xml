<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.996651">
Extracting Key Semantic Terms from Chinese Speech Query for Web
Searches
</title>
<author confidence="0.996551">
Gang WANG
</author>
<affiliation confidence="0.7840015">
National University of
Singapore
</affiliation>
<email confidence="0.989146">
wanggang_sh@hotmail.com
</email>
<author confidence="0.966743">
Tat-Seng CHUA
</author>
<affiliation confidence="0.82116">
National University of Singa-
pore
</affiliation>
<email confidence="0.9832">
chuats@comp.nus.edu.sg
</email>
<author confidence="0.743472">
Yong-Cheng WANG
</author>
<affiliation confidence="0.741738">
Shanghai Jiao Tong Univer-
</affiliation>
<address confidence="0.547796">
sity, China, 200030
</address>
<email confidence="0.986573">
ycwang@mail.sjtu.edu.cn
</email>
<sectionHeader confidence="0.997337" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968928571429">
This paper discusses the challenges and pro-
poses a solution to performing information re-
trieval on the Web using Chinese natural language
speech query. The main contribution of this re-
search is in devising a divide-and-conquer strategy
to alleviate the speech recognition errors. It uses
the query model to facilitate the extraction of main
core semantic string (CSS) from the Chinese natu-
ral language speech query. It then breaks the CSS
into basic components corresponding to phrases,
and uses a multi-tier strategy to map the basic
components to known phrases in order to further
eliminate the errors. The resulting system has been
found to be effective.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933129032258">
We are entering an information era, where infor-
mation has become one of the major resources in
our daily activities. With its wide spread adoption,
Internet has become the largest information wealth
for all to share. Currently, most (Chinese) search
engines can only support term-based information
retrieval, where the users are required to enter the
queries directly through keyboards in front of the
computer. However, there is a large segment of
population in China and the rest of the world who
are illiterate and do not have the skills to use the
computer. They are thus unable to take advantage
of the vast amount of freely available information.
Since almost every person can speak and under-
stand spoken language, the research on “(Chinese)
natural language speech query retrieval” would
enable average persons to access information using
the current search engines without the need to learn
special computer skills or training. They can sim-
ply access the search engine using common de-
vices that they are familiar with such as the
telephone, PDA and so on.
In order to implement a speech-based informa-
tion retrieval system, one of the most important
challenges is how to obtain the correct query terms
from the spoken natural language query that con-
vey the main semantics of the query. This requires
the integration of natural language query process-
ing and speech recognition research.
Natural language query processing has been an
active area of research for many years and many
techniques have been developed (Jacobs and
Rau1993; Kupie, 1993; Strzalkowski, 1999; Yu et
al, 1999). Most of these techniques, however, focus
only on written language, with few devoted to the
study of spoken language query processing.
Speech recognition involves the conversion of
acoustic speech signals to a stream of text. Because
of the complexity of human vocal tract, the speech
signals being observed are different, even for mul-
tiple utterances of the same sequence of words by
the same person (Lee et al 1996). Furthermore, the
speech signals can be influenced by the differences
across different speakers, dialects, transmission
distortions, and speaking environments. These
have contributed to the noise and variability of
speech signals. As one of the main sources of er-
rors in Chinese speech recognition come from sub-
stitution (Wang 2002; Zhou 1997), in which a
wrong but similar sounding term is used in place of
the correct term, confusion matrix has been used to
record confused sound pairs in an attempt to elimi-
nate this error. Confusion matrix has been em-
ployed effectively in spoken document retrieval
(Singhal et al, 1999 and Srinivasan et al 2000) and
to minimize speech recognition errors (Shen et al,
1998). However, when such method is used di-
rectly to correct speech recognition errors, it tends
to bring in too many irrelevant terms (Ng 2000).
Because important terms in a long document are
often repeated several times, there is a good chance
that such terms will be correctly recognized at least
once by a speech recognition engine with a reason-
able level of word recognition rate. Many spoken
document retrieval (SDR) systems took advantage
of this fact in reducing the speech recognition and
matching errors (Meng et al 2001; Wang et al 2001;
Chen et al 2001). In contrast to SDR, very little
work has been done on Chinese spoken query
processing (SQP), which is the use of spoken que-
ries to retrieval textual documents. Moreover, spo-
ken queries in SQP tend to be very short with few
repeated terms.
In this paper, we aim to integrate the spoken
language and natural language research to process
spoken queries with speech recognition errors. The
main contribution of this research is in devising a
divide-and-conquer strategy to alleviate the speech
recognition errors. It first employs the Chinese
query model to isolate the Core Semantic String
(CSS) that conveys the semantics of the spoken
query. It then breaks the CSS into basic compo-
nents corresponding to phrases, and uses a multi-
tier strategy to map the basic components to known
phrases in a dictionary in order to further eliminate
the errors.
In the rest of this paper, an overview of the pro-
posed approach is introduced in Section 2. Section
3 describes the query model, while Section 4 out-
lines the use of multi-tier approach to eliminate
errors in CSS. Section 5 discusses the experimental
setup and results. Finally, Section 6 contains our
concluding remarks.
</bodyText>
<sectionHeader confidence="0.606166" genericHeader="method">
2 Overview of the proposed approach
</sectionHeader>
<bodyText confidence="0.989668217391304">
There are many challenges in supporting surfing of
Web by speech queries. One of the main challenges
is that the current speech recognition technology is
not very good, especially for average users that do
not have any speech trainings. For such unlimited
user group, the speech recognition engine could
achieve an accuracy of less than 50%. Because of
this, the key phrases we derived from the speech
query could be in error or missing the main seman-
tic of the query altogether. This would affect the
effectiveness of the resulting system tremendously.
Given the speech-to-text output with errors, the
key issue is on how to analyze the query in order to
grasp the Core Semantic String (CSS) as accurately
as possible. CSS is defined as the key term se-
quence in the query that conveys the main seman-
tics of the query. For example, given the query:
LG
” (Please tell
me the information on how the U.S. separates the
most-favored-nation status from human rights is-
sue in china). The CSS in the query is underlined.
We can segment the CSS into several basic com-
ponents that correspond to key concepts such as:
(U.S.), (China), (human
rights issue), (the most-favored-nation
status) and (separate).
Because of the difficulty in handling speech
recognition errors involving multiple segments of
CSSs, we limit our research to queries that contain
only one CSS string. However, we allow a CSS to
include multiple basic components as depicted in
the above example. This is reasonable as most que-
ries posed by the users on the Web tend to be short
with only a few characters (Pu 2000).
Thus the accurate extraction of CSS and its
separation into basic components is essential to
alleviate the speech recognition errors. First of all,
isolating CSS from the rest of speech enables us to
ignore errors in other parts of speech, such as the
greetings and polite remarks, which have no effects
on the outcome of the query. Second, by separating
the CSS into basic components, we can limit the
propagation of errors, and employ the set of known
phrases in the domain to help correct the errors in
these components separately.
</bodyText>
<figure confidence="0.366614">
Speech
Query
</figure>
<figureCaption confidence="0.998961">
Figure 1: Overview of the proposed approach
</figureCaption>
<bodyText confidence="0.9999704">
To achieve this, we process the query in three
main stages as illustrated in Figure 1. First, given
the user’s oral query, the system uses a speech rec-
ognition engine to convert the speech to text. Sec-
ond, we analyze the query using a query model
(QM) to extract CSS from the query with mini-
mum errors. QM defines the structures and some
of the standard phrases used in typical queries.
Third, we divide the CSS into basic components,
and employ a multi-tier approach to match the ba-
</bodyText>
<figure confidence="0.963516875">
Confusion matrix
Phrase Dictionary
QM
Multi-Tier
mapping
Basic
Components
CSS
</figure>
<bodyText confidence="0.999748714285714">
sic components to the nearest known phrases in
order to correct the speech recognition errors. The
aim here is to improve recall without excessive lost
in precision. The resulting key components are
then used as query to standard search engine.
The following sections describe the details of
our approach.
</bodyText>
<sectionHeader confidence="0.998393" genericHeader="method">
3 Query Model (QM)
</sectionHeader>
<bodyText confidence="0.999961928571429">
Query model (QM) is used to analyze the query
and extract the core semantic string (CSS) that
contains the main semantic of the query. There are
two main components for a query model. The first
is query component dictionary, which is a set of
phrases that has certain semantic functions, such as
the polite remarks, prepositions, time etc. The
other component is the query structure, which de-
fines a sequence of acceptable semantically tagged
tokens, such as “Begin, Core Semantic String,
Question Phrase, and End”. Each query structure
also includes its occurrence probability within the
query corpus. Table 2 gives some examples of
query structures.
</bodyText>
<subsectionHeader confidence="0.98869">
3.1 Query Model Generation
</subsectionHeader>
<bodyText confidence="0.999522555555555">
In order to come up with a set of generalized query
structures, we use a query log of typical queries
posed by users. The query log consists of 557 que-
ries, collected from twenty-eight human subjects at
the Shanghai Jiao Tong University (Ying 2002).
Each subject is asked to pose 20 separate queries to
retrieve general information from the Web.
After analyzing the queries, we derive a query
model comprising 51 query structures and a set of
query components. For each query structure, we
compute its probability of occurrence, which is
used to determine the more likely structure con-
taining CSS in case there are multiple CSSs found.
As part of the analysis of the query log, we classify
the query components into ten classes, as listed in
Table 1. These ten classes are called semantic tags.
They can be further divided into two main catego-
ries: the closed class and open class. Closed classes
are those that have relatively fixed word lists.
These include question phrases, quantifiers, polite
remarks, prepositions, time and commonly used
verb and subject-verb phrases. We collect all the
phrases belonging to closed classes from the query
log and store them in the query component diction-
ary. The open class is the CSS, which we do not
know in advance. CSS typically includes person’s
names, events and country’s names etc.
</bodyText>
<tableCaption confidence="0.998836">
Table 1: Definition and Examples of Semantic tags
</tableCaption>
<table confidence="0.99985405">
Sem Tag Name of tag Example
Verb-Object give
Phrase (me)
Question Phrase (is there )
Question Field
(news),
(report)
Quantifier (some)
Verb Phrase (find)
collect
Polite Remark (please help
me)
Preposition (about),
(about)
Subject-Verb
phrase
(I) (want)
Core Semantic 9.11
String (9.11 event)
Time (today)
</table>
<tableCaption confidence="0.97996">
Table 2: Examples of Query Structure
</tableCaption>
<table confidence="0.981071857142857">
1 Q1: 0, 2, 7, 9, 3, 0: 0.0025,
☎ 9.11
2 7 9 3
Is there any information on September 11?
2 Q2: 0, ★0 :0.01
�1, 7, 9, 3,
1 7 9 3
</table>
<bodyText confidence="0.948468555555555">
Give me some information about Ben laden.
Given the set of sample queries, a heuristic rule-
based approach is used to analyze the queries, and
break them into basic components with assigned
semantic tags by matching the words listed in Ta-
ble 1. Any sequences of words or phrases not
found in the closed class are tagged as CSS (with
Semantic Tag 9). We can thus derive the query
structures of the form given in Table 2.
</bodyText>
<subsectionHeader confidence="0.999985">
3.2 Modeling of Query Structure as FSA
</subsectionHeader>
<bodyText confidence="0.999580227272727">
Due to speech recognition errors, we do not expect
the query components and hence the query struc-
ture to be recognized correctly. Instead, we parse
the query structure in order to isolate and extract
CSS. To facilitate this, we employ the Finite State
Automata (FSA) to model the query structure. FSA
models the expected sequences of tokens in typical
queries and annotate the semantic tags, including
CSS. A FSA is defined for each of the 51 query
structures. An example of FSA is given in Figure 2.
Because CSS is an open set, we do not know its
content in advance. Instead, we use the following
two rules to determine the candidates for CSS: (a)
it is an unknown string not present in the Query
Component Dictionary; and (b) its length is not
less than two, as the average length of concepts in
Chinese is greater than one (Wang 1992).
At each stage of parsing the query using FSA
(Hobbs et al 1997), we need to make decision on
which state to proceed and how to handle unex-
pected tokens in the query. Thus at each stage,
FSA needs to perform three functions:
</bodyText>
<listItem confidence="0.9920095">
a) Goto function: It maps a pair consisting of a
state and an input symbol into a new state or
the fail state. We use G(N,X) =N’ to define
the goto function from State N to State N’,
given the occurrence of token X.
b) Fail function: It is consulted whenever the
goto function reports a failure when encoun-
tering an unexpected token. We use f(N) =N’
to represent the fail function.
c) Output function: In the FSA, certain states
are designated as output states, which indi-
cate that a sequence of tokens has been
found and are tagged with the appropriate
semantic tag.
</listItem>
<bodyText confidence="0.951811903225807">
To construct a goto function, we begin with a
graph consisting of one vertex which represents
State 0.We then enter each token X into the graph
by adding a directed path to the graph that begins
at the start state. New vertices and edges are added
to the graph so that there will be, starting at the
start state, a path in the graph that spells out the
token X. The token X is added to the output func-
tion of the state at which the path terminates.
For example, suppose that our Query Component
Dictionary consists of seven phrases as follows:
“ (please help me); (some);
(about); (news); (collect); (tell
me); (what do you have)”. Adding these
tokens into the graph will result in a FSA as shown
in Figure 2. The path from State 0 to State 3 spells
out the phrase “ (Please help me)”, and on
completion of this path, we associate its output
with semantic tag 6. Similarly, the output of “
(some)” is associated with State 5, and semantic
tag 4, and so on.
We now use an example to illustrate the process
of parsing the query. Suppose the user issues a
speech query: ”
” (please help me to collect some information
about Bin Laden). However, the result of speech
recognition with errors is: ” (please) (help)
(me) (receive) (send) (some)
(about) (half) (pull) (light) (of)
(news)”. Note that there are 4 mis-recognized
characters which are underlined.
</bodyText>
<figureCaption confidence="0.812946">
Note : indicates the semantic tag.
Figure 2: FSA for part of Query Component Dictionary
</figureCaption>
<bodyText confidence="0.9723775">
The FSA begins with State 0. When the system
encounters the sequence of characters (please)
(help) (me), the state changes from 0 to 1, 2
and eventually to 3. At State 3, the system recog-
nizes a polite remark phrase and output a token
with semantic tag 6.
Next, the system meets the character (receive),
it will transit to State 10, because of g(0, )=10.
When the system sees the next character (send),
which does not have a corresponding transition
rule, the goto function reports a failure. Because
the length of the string is 2 and the string is not in
the Query Component Dictionary, the semantic tag
9 is assigned to token” ” according to the defi-
nition of CSS.
By repeating the above process, we obtain the
following result:
6 9 4 7 9 3
Here the semantic tags are as defined in Table 1.
It is noted that because of speech recognition errors,
the system detected two CSSs, and both of them
contain speech recognition errors.
</bodyText>
<subsectionHeader confidence="0.95449">
3.3 CSS Extraction by Query Model
</subsectionHeader>
<bodyText confidence="0.9997928">
Given that we may find multiple CSSs, the next
stage is to analyze the CSSs found along with their
surrounding context in order to determine the most
probable CSS. The approach is based on the prem-
ise that choosing the best sense for an input vector
amounts to choosing the most probable sense given
that vector. The input vector i has three compo-
nents: left context (Li), the CSS itself (CSSi), and
right context (Ri). The probability of such a struc-
ture occurring in the Query Model is as follows:
</bodyText>
<equation confidence="0.9902395">
n
) (1)
</equation>
<bodyText confidence="0.9999799">
where Cij is set to 1 if the input vector i (Li, Ri)
matches the two corresponding left and right CSS
context of the query structure j, and 0 otherwise. pj
is the possibility of occurrence of the jth query
structure, and n is the total number of the structures
in the Query Model. Note that Equation (1) gives a
detected CSS higher weight if it matches to more
query structures with higher occurrence probabili-
ties. We simply select the best CSSi such that
according to Eqn(1).
For illustration, let’s consider the above example
with 2 detected CSSs. The two CSS vectors are: [6,
9, 4] and [7, 9, 3]. From the Query Model, we
know that the probability of occurrence, pj, of
structure [6, 9, 4] is 0, and that of structure [7, 9, 3]
is 0.03, with the latter matches to only one struc-
ture. Hence the si values for them are 0 and 0.03
respectively. Thus the most probable core semantic
structure is [7, 9, 3] and the CSS “ (half) (pull)
(light)” is extracted.
</bodyText>
<sectionHeader confidence="0.987436" genericHeader="method">
4 Query Terms Generation
</sectionHeader>
<bodyText confidence="0.9999535">
Because of speech recognition error, the CSS ob-
tained is likely to contain error, or in the worse
case, missing the main semantics of the query alto-
gether. We now discuss how we alleviate the errors
in CSS for the former case. We will first break the
CSS into one or more basic semantic parts, and
then apply the multi-tier method to map the query
components to known phrases.
</bodyText>
<subsectionHeader confidence="0.996874">
4.1 Breaking CSS into Basic Components
</subsectionHeader>
<bodyText confidence="0.999987090909091">
In many cases, the CSS obtained may be made up
of several semantic components equivalent to base
noun phrases. Here we employ a technique based
on Chinese cut marks (Wang 1992) to perform the
segmentation. The Chinese cut marks are tokens
that can separate a Chinese sentence into several
semantic parts. Zhou (1997) used such technique to
detect new Chinese words, and reported good re-
sults with precision and recall of 92% and 70%
respectively. By separating the CSS into basic key
components, we can limit the propagation of errors.
</bodyText>
<subsectionHeader confidence="0.987375">
4.2 Multi-tier query term mapping
</subsectionHeader>
<bodyText confidence="0.999968372093023">
In order to further eliminate the speech recognition
errors, we propose a multi-tier approach to map the
basic components in CSS into known phrases by
using a combination of matching techniques. To do
this, we need to build up a phrase dictionary con-
taining typical concepts used in general and spe-
cific domains. Most basic CSS components should
be mapped to one of these phrases. Thus even if a
basic component contains errors, as long as we can
find a sufficiently similar phrase in the phrase dic-
tionary, we can use this in place of the erroneous
CSS component, thus eliminating the errors.
We collected a phrase dictionary containing
about 32,842 phrases, covering mostly base noun
phrase and named entity. The phrases are derived
from two sources. We first derived a set of com-
mon phrases from the digital dictionary and the
logs in the search engine used at the Shanghai Jiao
Tong University. We also derived a set of domain
specific phrases by extracting the base noun
phrases and named entities from the on-line news
articles obtained during the period. This approach
is reasonable as in practice we can use recent web
or news articles to extract concepts to update the
phrase dictionary.
Given the phrase dictionary, the next problem
then is to map the basic CSS components to the
nearest phrases in the dictionary. As the basic
components may contain errors, we cannot match
them exactly just at the character level. We thus
propose to match each basic component with the
known phrases in the dictionary at three levels: (a)
character level; (b) syllable string level; and (c)
confusion syllable string level. The purpose of
matching at levels b and c is to overcome the
homophone problem in CSS. For example, “
(Laden)” is wrongly recognized as “ (pull
lamp)” by the speech recognition engine. Such er-
rors cannot be re-solved at the character matching
level, but it can probably be matched at the syllable
string level. The confusion matrix is used to further
reduce the effect of speech recognition errors due
to similar sounding characters.
</bodyText>
<equation confidence="0.531328">
si = ( *
Cij pj
j
0
arg max ( i
s
i
)
</equation>
<bodyText confidence="0.966163779661017">
To account for possible errors in CSS compo-
nents, we perform similarity, instead of exact,
matching at the three levels. Given the basic CSS
component qi, and a phrase cj in the dictionary, we
compute:
where LCS(qi,cj) gives the number of characters/
syllable matched between qi and ci in the order of
their appearance using the longest common subse-
quence matching (LCS) algorithm (Cormen et al
1990). Mk is introduced to accounts for the similar-
ity between the two matching units, and is depend-
ent on the level of matching. If the matching is
performed at the character or syllable string levels,
the basic matching unit is one character or one syl-
lable and the similarity between the two matching
units is 1. If the matching is done at the confusion
syllable string level, Mk is the corresponding coef-
ficients in the confusion matrix. Hence LCS (qi,cj)
gives the degree of match between qi and cj, nor-
malized by the maximum length of qi or cj; and ΣM
gives the degree of similarity between the units
being matched.
The three level of matching also ranges from be-
ing more exact at the character level, to less exact
at the confusion syllable level. Thus if we can find
a relevant phrase with sim(qi,cj)&gt; at the higher
character level, we will not perform further match-
ing at the lower levels. Otherwise, we will relax
the constraint to perform the matching at succes-
sively lower levels, probably at the expense of pre-
cision.
The detail of algorithm is listed as follows:
Input: Basic CSS Component, qi
a. Match qi with phrases in dictionary at character
level using Eqn.(2).
b. If we cannot find a match, then match qi with
phrases at the syllable level using Eqn.(2).
c. If we still cannot find a match, match qi with
phrases at the confusion syllable level using
Eqn.(2).
d. If we found a match, set q’i=cj; otherwise set
q’i=qi.
For example, given a query: “
” (please tell me some news about
Iraq). If the query is wrongly recognized as “
”. If, however, we
could correctly extract the CSS “ (Iraq)
from this mis-recognized query, then we could ig-
nore the speech recognition errors in other parts of
the above query. Even if there are errors in the
CSS extracted, such as “ (chen) (waterside)”
instead of “ (chen shui bian)”, we could ap-
ply the syllable string level matching to correct the
homophone errors. For CSS errors such as “
(corrupt) (usually)” instead of the correct CSS
“ (Taliban)”, which could not be corrected
at the syllable string matching level, we could ap-
ply the confusion syllable string matching to over-
come this error.
</bodyText>
<sectionHeader confidence="0.990764" genericHeader="evaluation">
5 Experiments and analysis
</sectionHeader>
<bodyText confidence="0.978091515151515">
As our system aims to correct the errors and ex-
tract CSS components in spoken queries, it is im-
portant to demonstrate that our system is able to
handle queries of different characteristics. To this
end, we devised two sets of test queries as follows.
a) Corpus with short queries
We devised 10 queries, each containing a CSS
with only one basic component. This is the typical
type of queries posed by the users on the web. We
asked 10 different people to “speak” the queries,
and used the IBM ViaVoice 98 to perform the
speech to text conversion. This gives rise to a col-
lection of 100 spoken queries. There is a total of
1,340 Chinese characters in the test queries with a
speech recognition error rate of 32.5%.
b) Corpus with long queries
In order to test on queries used in standard test
corpuses, we adopted the query topics (1-10) em-
ployed in TREC-5 Chinese-Language track. Here
each query contains more than one key semantic
component. We rephrased the queries into natural
language query format, and asked twelve subjects
to “read” the queries. We again used the IBM
ViaVoice 98 to perform the speech recognition on
the resulting 120 different spoken queries, giving
rise to a total of 2,354 Chinese characters with a
speech recognition error rate of 23.75%.
We devised two experiments to evaluate the per-
formance of our techniques. The first experiment
was designed to test the effectiveness of our query
model in extracting CSSs. The second was de-
signed to test the accuracy of our overall system in
extracting basic query components.
</bodyText>
<subsectionHeader confidence="0.937196">
5.1 Test 1: Accuracy of extracting CSSs
</subsectionHeader>
<figure confidence="0.904211083333333">
Sim(qi , ci ) = (q0 ci) * Mk (2)|, ||}
ci
LCS
( ,
q i c i
)
LCS
max{|
qi
0
=
k
</figure>
<bodyText confidence="0.999967909090909">
The test results show that by using our query
model, we could correctly extract 99% and 96% of
CSSs from the spoken queries for the short and
long query category respectively. The errors are
mainly due to the wrong tagging of some query
components, which caused the query model to miss
the correct query structure, or match to a wrong
structure.
For example: given the query “
” (please tell me some news about
Taliban). If it is wrongly recognized as:
</bodyText>
<equation confidence="0.693244">
9 7 9 10
</equation>
<bodyText confidence="0.999811545454545">
which is a nonsensical sentence. Since the prob-
abilities of occurrence both query structures [0,9,7]
and [7,9,10] are 0, we could not find the CSS at all.
This error is mainly due to the mis-recognition of
the last query component “ (news)” to “
(afternoon)”. It confuses the Query Model, which
could not find the correct CSS.
The overall results indicate that there are fewer
errors in short queries as such queries contain only
one CSS component. This is encouraging as in
practice most users issue only short queries.
</bodyText>
<subsectionHeader confidence="0.7372335">
5.2 Test 2: Accuracy of extracting basic query
components
</subsectionHeader>
<bodyText confidence="0.999573">
In order to test the accuracy of extracting basic
query components, we asked one subject to manu-
ally divide the CSS into basic components, and
used that as the ground truth. We compared the
following two methods of extracting CSS compo-
nents:
a) As a baseline, we simply performed the stan-
dard stop word removal and divided the query
into components with the help of a dictionary.
However, there is no attempt to correct the
speech recognition errors in these components.
Here we assume that the natural language query
is a bag of words with stop word removed (Ri-
cardo, 1999). Currently, most search engines are
based on this approach.
b) We applied our query model to extract CSS and
employed the multi-tier mapping approach to
extract and correct the errors in the basic CSS
components.
Tables 3 and 4 give the comparisons between
Methods (a) and (b), which clearly show that our
method outperforms the baseline method by over
20.2% and 20 % in F1 measure for the short and
long queries respectively.
</bodyText>
<tableCaption confidence="0.999791">
Table 3: Comparison of Methods a and b for short query
</tableCaption>
<table confidence="0.9999656">
Average Average F1
Precision Recall
Method a 31% 58.5% 40.5%
Method b 53.98% 69.4% 60.7%
+22.98% +10.9% +20.2%
</table>
<tableCaption confidence="0.999244">
Table 4: Comparison of Methods a and b for long que
</tableCaption>
<table confidence="0.9993432">
Average Average F1
Precision Recall
Method a 39.23% 85.99% 53.9%
Method b 67.75% 81.31% 73.9%
+28.52% -4.68% +20.0%
</table>
<bodyText confidence="0.999282">
The improvement is largely due to the use of our
approach to extract CSS and correct the speech
recognition errors in the CSS components. More
detailed analysis of long queries in Table 3 reveals
that our method performs worse than the baseline
method in recall. This is mainly due to errors in
extracting and breaking CSS into basic compo-
nents. Although we used the multi-tier mapping
approach to reduce the errors from speech recogni-
tion, its improvement is insufficient to offset the
lost in recall due to errors in extracting CSS. On
the other hand, for the short query cases, without
the errors in breaking CSS, our system is more ef-
fective than the baseline in recall. It is noted that in
both cases, our system performs significantly bet-
ter than the baseline in terms of precision and F1
measures.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999964576923077">
Although research on natural language query proc-
essing and speech recognition has been carried out
for many years, the combination of these two ap-
proaches to help a large population of infrequent
users to “surf the web by voice” has been relatively
recent. This paper outlines a divide-and-conquer
approach to alleviate the effect of speech recogni-
tion error, and in extracting key CSS components
for use in a standard search engine to retrieve rele-
vant documents. The main innovative steps in our
system are: (a) we use a query model to isolate
CSS in speech queries; (b) we break the CSS into
basic components; and (c) we employ a multi-tier
approach to map the basic components to known
phrases in the dictionary. The tests demonstrate
that our approach is effective.
The work is only the beginning. Further research
can be carried out as follows. First, as most of the
queries are about named entities such as the per-
sons or organizations, we need to perform named
entity analysis on the queries to better extract its
structure, and in mapping to known named entities.
Second, most speech recognition engine will return
a list of probable words for each syllable. This
could be incorporated into our framework to facili-
tate multi-tier mapping.
</bodyText>
<sectionHeader confidence="0.999171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998987268292683">
Berlin Chen, Hsin-min Wang, and Lin-Shan Lee
(2001), “Improved Spoken Document Retrieval
by Exploring Extra Acoustic and Linguistic
Cues”, Proceedings of the 7th European Confer-
ence on Speech Communication and Technology
located at http://homepage.iis.sinica.edu.tw/
Paul S. Jacobs and Lisa F. Rau (1993), Innova-
tions in Text Interpretation, Artificial Intelli-
gence, Volume 63, October 1993 (Special Issue
on Text Understanding) pp.143-191
Thomas H. Cormen, Charles E. Leiserson and
Ronald L. Rivest (1990), “Introduction to algo-
rithms”, published by McGraw-Hill.
Jerry R. Hobbs, et al,(1997) , FASTUS: A Cas-
caded Finite-State Transducer for Extracting In-
formation from Natural-Language Text, Finite-
State Language Processing, Emmanuel Roche
and Yves Schabes, pp. 383 - 406, MIT Press,
Julian Kupiec (1993), MURAX: “A robust linguis-
tic approach for question answering using an
one-line encyclopedia”, Proceedings of 16th an-
nual conference on Research and Development
in Information Retrieval (SIGIR), pp.181-190
Chin-Hui Lee et al (1996), “A Survey on Auto-
matic Speech Recognition with an Illustrative
Example On Continuous Speech Recognition of
Mandarin”, in Computational Linguistics and
Chinese Language Processing, pp. 1-36
Helen Meng and Pui Yu Hui (2001), “Spoken
Document Retrieval for the languages of Hong
Kong”, International Symposium on Intelligent
Multimedia, Video and Speech Processing, May
2001, located at www.se.cuhk.edu.hk/PEOPLE/
Kenney Ng (2000), “Information Fusion For Spo-
ken Document Retrieval”, Proceedings of
ICASSP’00, Istanbul, Turkey, Jun, located at
http://www.sls.lcs.mit.edu/sls/publications/
Hsiao Tieh Pu (2000), “Understanding Chinese
Users’ Information Behaviors through Analysis
of Web Search Term Logs”, Journal of Com-
puters, pp.75-82
Liqin, Shen, Haixin Chai, Yong Qin and Tang
Donald (1998), “Character Error Correction for
Chinese Speech Recognition System”, Proceed-
ings of International Symposium on Chinese
Spoken Language Processing Symposium Pro-
ceedings, pp.136-138
Amit Singhal and Fernando Pereira (1999),
“Document Expansion for Speech Retrieval”,
Proceedings of the 22nd Annual International
conference on Research and Development in In-
formation Retrieval (SIGIR), pp. 34~41
Tomek Strzalkowski (1999), “Natural language
information retrieval”, Boston: Kluwer Publish-
ing.
Gang Wang (2002), “Web surfing by Chinese
Speech”, Master thesis, National University of
Singapore.
Hsin-min Wang, Helen Meng, Patrick Schone, Ber-
lin Chen and Wai-Kt Lo (2001), “Multi-Scale
Audio Indexing for translingual spoken docu-
ment retrieval”, Proceedings of IEEE Interna-
tional Conference on Acoustics, Speech, Signal
processing , Salt Lake City, USA, May 2001, lo-
cated at http://www.iis.sinica.edu.tw/~whm/
Yongcheng Wang (1992), Technology and basis of
Chinese Information Processing, Shanghai Jiao
Tong University Press
Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier
(1999), “Introduction to modern information re-
trieval”, Published by London: Library Associa-
tion Publishing.
Hai-nan Ying, Yong Ji and Wei Shen, (2002), “re-
port of query log”, internal report in Shanghai
Jiao Tong University
Guodong Zhou and Kim Teng Lua (1997) Detec-
tion of Unknown Chinese Words Using a Hybrid
Approach Computer Processing of Oriental Lan-
guages, Vol 11, No 1, 1997, 63-75
Guodong Zhou (1997), “Language Modelling in
Mandarin Speech Recognition”, Ph.D. Thesis,
National University of Singapore.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327640">
<title confidence="0.9934655">Extracting Key Semantic Terms from Chinese Speech Query for Web Searches</title>
<author confidence="0.989091">Gang WANG</author>
<affiliation confidence="0.99928">National University of</affiliation>
<address confidence="0.529772">Singapore</address>
<email confidence="0.999738">wanggang_sh@hotmail.com</email>
<author confidence="0.970878">Tat-Seng CHUA</author>
<affiliation confidence="0.999835">University of Singa-</affiliation>
<address confidence="0.932172">pore</address>
<email confidence="0.976316">chuats@comp.nus.edu.sg</email>
<author confidence="0.920988">Yong-Cheng WANG Jiao Tong</author>
<address confidence="0.993815">sity, China, 200030</address>
<email confidence="0.955577">ycwang@mail.sjtu.edu.cn</email>
<abstract confidence="0.991946066666667">This paper discusses the challenges and proposes a solution to performing information retrieval on the Web using Chinese natural language speech query. The main contribution of this research is in devising a divide-and-conquer strategy to alleviate the speech recognition errors. It uses the query model to facilitate the extraction of main core semantic string (CSS) from the Chinese natural language speech query. It then breaks the CSS into basic components corresponding to phrases, and uses a multi-tier strategy to map the basic components to known phrases in order to further eliminate the errors. The resulting system has been found to be effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Berlin Chen</author>
<author>Hsin-min Wang</author>
<author>Lin-Shan Lee</author>
</authors>
<title>Improved Spoken Document Retrieval by Exploring Extra Acoustic and Linguistic Cues”,</title>
<date>2001</date>
<booktitle>Proceedings of the 7th European Conference on Speech Communication and Technology</booktitle>
<note>located at http://homepage.iis.sinica.edu.tw/</note>
<contexts>
<context position="4254" citStr="Chen et al 2001" startWordPosition="680" endWordPosition="683">to minimize speech recognition errors (Shen et al, 1998). However, when such method is used directly to correct speech recognition errors, it tends to bring in too many irrelevant terms (Ng 2000). Because important terms in a long document are often repeated several times, there is a good chance that such terms will be correctly recognized at least once by a speech recognition engine with a reasonable level of word recognition rate. Many spoken document retrieval (SDR) systems took advantage of this fact in reducing the speech recognition and matching errors (Meng et al 2001; Wang et al 2001; Chen et al 2001). In contrast to SDR, very little work has been done on Chinese spoken query processing (SQP), which is the use of spoken queries to retrieval textual documents. Moreover, spoken queries in SQP tend to be very short with few repeated terms. In this paper, we aim to integrate the spoken language and natural language research to process spoken queries with speech recognition errors. The main contribution of this research is in devising a divide-and-conquer strategy to alleviate the speech recognition errors. It first employs the Chinese query model to isolate the Core Semantic String (CSS) that </context>
</contexts>
<marker>Chen, Wang, Lee, 2001</marker>
<rawString>Berlin Chen, Hsin-min Wang, and Lin-Shan Lee (2001), “Improved Spoken Document Retrieval by Exploring Extra Acoustic and Linguistic Cues”, Proceedings of the 7th European Conference on Speech Communication and Technology located at http://homepage.iis.sinica.edu.tw/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacobs</author>
<author>Lisa F Rau</author>
</authors>
<title>Innovations in Text Interpretation,</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<volume>63</volume>
<pages>143--191</pages>
<marker>Jacobs, Rau, 1993</marker>
<rawString>Paul S. Jacobs and Lisa F. Rau (1993), Innovations in Text Interpretation, Artificial Intelligence, Volume 63, October 1993 (Special Issue on Text Understanding) pp.143-191</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
</authors>
<date>1990</date>
<note>Introduction to algorithms”, published by McGraw-Hill.</note>
<contexts>
<context position="20505" citStr="Cormen et al 1990" startWordPosition="3519" endWordPosition="3522">it can probably be matched at the syllable string level. The confusion matrix is used to further reduce the effect of speech recognition errors due to similar sounding characters. si = ( * Cij pj j 0 arg max ( i s i ) To account for possible errors in CSS components, we perform similarity, instead of exact, matching at the three levels. Given the basic CSS component qi, and a phrase cj in the dictionary, we compute: where LCS(qi,cj) gives the number of characters/ syllable matched between qi and ci in the order of their appearance using the longest common subsequence matching (LCS) algorithm (Cormen et al 1990). Mk is introduced to accounts for the similarity between the two matching units, and is dependent on the level of matching. If the matching is performed at the character or syllable string levels, the basic matching unit is one character or one syllable and the similarity between the two matching units is 1. If the matching is done at the confusion syllable string level, Mk is the corresponding coefficients in the confusion matrix. Hence LCS (qi,cj) gives the degree of match between qi and cj, normalized by the maximum length of qi or cj; and ΣM gives the degree of similarity between the unit</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson and Ronald L. Rivest (1990), “Introduction to algorithms”, published by McGraw-Hill.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>et al,(1997) , FASTUS: A Cascaded Finite-State Transducer for Extracting Information from Natural-Language Text,</title>
<booktitle>FiniteState Language Processing, Emmanuel Roche and Yves Schabes,</booktitle>
<pages>383--406</pages>
<publisher>MIT Press,</publisher>
<marker>Hobbs, </marker>
<rawString>Jerry R. Hobbs, et al,(1997) , FASTUS: A Cascaded Finite-State Transducer for Extracting Information from Natural-Language Text, FiniteState Language Processing, Emmanuel Roche and Yves Schabes, pp. 383 - 406, MIT Press,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>MURAX: “A robust linguistic approach for question answering using an one-line encyclopedia”,</title>
<date>1993</date>
<booktitle>Proceedings of 16th annual conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>181--190</pages>
<marker>Kupiec, 1993</marker>
<rawString>Julian Kupiec (1993), MURAX: “A robust linguistic approach for question answering using an one-line encyclopedia”, Proceedings of 16th annual conference on Research and Development in Information Retrieval (SIGIR), pp.181-190</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Hui Lee</author>
</authors>
<title>A Survey on Automatic Speech Recognition with an Illustrative Example On Continuous Speech Recognition of Mandarin”,</title>
<date>1996</date>
<booktitle>in Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>1--36</pages>
<marker>Lee, 1996</marker>
<rawString>Chin-Hui Lee et al (1996), “A Survey on Automatic Speech Recognition with an Illustrative Example On Continuous Speech Recognition of Mandarin”, in Computational Linguistics and Chinese Language Processing, pp. 1-36</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Meng</author>
<author>Pui Yu Hui</author>
</authors>
<title>Spoken Document Retrieval for the languages of Hong Kong”,</title>
<date>2001</date>
<booktitle>International Symposium on Intelligent Multimedia, Video and Speech Processing,</booktitle>
<note>located at www.se.cuhk.edu.hk/PEOPLE/</note>
<marker>Meng, Hui, 2001</marker>
<rawString>Helen Meng and Pui Yu Hui (2001), “Spoken Document Retrieval for the languages of Hong Kong”, International Symposium on Intelligent Multimedia, Video and Speech Processing, May 2001, located at www.se.cuhk.edu.hk/PEOPLE/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenney Ng</author>
</authors>
<title>Information Fusion For Spoken Document Retrieval”,</title>
<date>2000</date>
<booktitle>Proceedings of ICASSP’00,</booktitle>
<location>Istanbul, Turkey,</location>
<note>located at http://www.sls.lcs.mit.edu/sls/publications/</note>
<contexts>
<context position="3833" citStr="Ng 2000" startWordPosition="610" endWordPosition="611">he main sources of errors in Chinese speech recognition come from substitution (Wang 2002; Zhou 1997), in which a wrong but similar sounding term is used in place of the correct term, confusion matrix has been used to record confused sound pairs in an attempt to eliminate this error. Confusion matrix has been employed effectively in spoken document retrieval (Singhal et al, 1999 and Srinivasan et al 2000) and to minimize speech recognition errors (Shen et al, 1998). However, when such method is used directly to correct speech recognition errors, it tends to bring in too many irrelevant terms (Ng 2000). Because important terms in a long document are often repeated several times, there is a good chance that such terms will be correctly recognized at least once by a speech recognition engine with a reasonable level of word recognition rate. Many spoken document retrieval (SDR) systems took advantage of this fact in reducing the speech recognition and matching errors (Meng et al 2001; Wang et al 2001; Chen et al 2001). In contrast to SDR, very little work has been done on Chinese spoken query processing (SQP), which is the use of spoken queries to retrieval textual documents. Moreover, spoken </context>
</contexts>
<marker>Ng, 2000</marker>
<rawString>Kenney Ng (2000), “Information Fusion For Spoken Document Retrieval”, Proceedings of ICASSP’00, Istanbul, Turkey, Jun, located at http://www.sls.lcs.mit.edu/sls/publications/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsiao Tieh Pu</author>
</authors>
<title>Understanding Chinese Users’ Information Behaviors through Analysis of Web Search Term Logs”,</title>
<date>2000</date>
<journal>Journal of Computers,</journal>
<pages>75--82</pages>
<contexts>
<context position="7035" citStr="Pu 2000" startWordPosition="1153" endWordPosition="1154"> china). The CSS in the query is underlined. We can segment the CSS into several basic components that correspond to key concepts such as: (U.S.), (China), (human rights issue), (the most-favored-nation status) and (separate). Because of the difficulty in handling speech recognition errors involving multiple segments of CSSs, we limit our research to queries that contain only one CSS string. However, we allow a CSS to include multiple basic components as depicted in the above example. This is reasonable as most queries posed by the users on the Web tend to be short with only a few characters (Pu 2000). Thus the accurate extraction of CSS and its separation into basic components is essential to alleviate the speech recognition errors. First of all, isolating CSS from the rest of speech enables us to ignore errors in other parts of speech, such as the greetings and polite remarks, which have no effects on the outcome of the query. Second, by separating the CSS into basic components, we can limit the propagation of errors, and employ the set of known phrases in the domain to help correct the errors in these components separately. Speech Query Figure 1: Overview of the proposed approach To ach</context>
</contexts>
<marker>Pu, 2000</marker>
<rawString>Hsiao Tieh Pu (2000), “Understanding Chinese Users’ Information Behaviors through Analysis of Web Search Term Logs”, Journal of Computers, pp.75-82</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Liqin</author>
<author>Haixin Chai</author>
</authors>
<title>Yong Qin and Tang Donald</title>
<date>1998</date>
<booktitle>Proceedings of International Symposium on Chinese Spoken Language Processing Symposium Proceedings,</booktitle>
<pages>136--138</pages>
<marker>Liqin, Chai, 1998</marker>
<rawString>Liqin, Shen, Haixin Chai, Yong Qin and Tang Donald (1998), “Character Error Correction for Chinese Speech Recognition System”, Proceedings of International Symposium on Chinese Spoken Language Processing Symposium Proceedings, pp.136-138</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
<author>Fernando Pereira</author>
</authors>
<title>Document Expansion for Speech Retrieval”,</title>
<date>1999</date>
<booktitle>Proceedings of the 22nd Annual International conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>34--41</pages>
<marker>Singhal, Pereira, 1999</marker>
<rawString>Amit Singhal and Fernando Pereira (1999), “Document Expansion for Speech Retrieval”, Proceedings of the 22nd Annual International conference on Research and Development in Information Retrieval (SIGIR), pp. 34~41</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
</authors>
<title>Natural language information retrieval”,</title>
<date>1999</date>
<publisher>Kluwer Publishing.</publisher>
<location>Boston:</location>
<contexts>
<context position="2547" citStr="Strzalkowski, 1999" startWordPosition="398" endWordPosition="399">access the search engine using common devices that they are familiar with such as the telephone, PDA and so on. In order to implement a speech-based information retrieval system, one of the most important challenges is how to obtain the correct query terms from the spoken natural language query that convey the main semantics of the query. This requires the integration of natural language query processing and speech recognition research. Natural language query processing has been an active area of research for many years and many techniques have been developed (Jacobs and Rau1993; Kupie, 1993; Strzalkowski, 1999; Yu et al, 1999). Most of these techniques, however, focus only on written language, with few devoted to the study of spoken language query processing. Speech recognition involves the conversion of acoustic speech signals to a stream of text. Because of the complexity of human vocal tract, the speech signals being observed are different, even for multiple utterances of the same sequence of words by the same person (Lee et al 1996). Furthermore, the speech signals can be influenced by the differences across different speakers, dialects, transmission distortions, and speaking environments. Thes</context>
</contexts>
<marker>Strzalkowski, 1999</marker>
<rawString>Tomek Strzalkowski (1999), “Natural language information retrieval”, Boston: Kluwer Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gang Wang</author>
</authors>
<title>Web surfing by Chinese Speech”, Master thesis,</title>
<date>2002</date>
<institution>National University of Singapore.</institution>
<contexts>
<context position="3314" citStr="Wang 2002" startWordPosition="520" endWordPosition="521">h recognition involves the conversion of acoustic speech signals to a stream of text. Because of the complexity of human vocal tract, the speech signals being observed are different, even for multiple utterances of the same sequence of words by the same person (Lee et al 1996). Furthermore, the speech signals can be influenced by the differences across different speakers, dialects, transmission distortions, and speaking environments. These have contributed to the noise and variability of speech signals. As one of the main sources of errors in Chinese speech recognition come from substitution (Wang 2002; Zhou 1997), in which a wrong but similar sounding term is used in place of the correct term, confusion matrix has been used to record confused sound pairs in an attempt to eliminate this error. Confusion matrix has been employed effectively in spoken document retrieval (Singhal et al, 1999 and Srinivasan et al 2000) and to minimize speech recognition errors (Shen et al, 1998). However, when such method is used directly to correct speech recognition errors, it tends to bring in too many irrelevant terms (Ng 2000). Because important terms in a long document are often repeated several times, th</context>
</contexts>
<marker>Wang, 2002</marker>
<rawString>Gang Wang (2002), “Web surfing by Chinese Speech”, Master thesis, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-min Wang</author>
<author>Helen Meng</author>
<author>Patrick Schone</author>
<author>Berlin Chen</author>
<author>Wai-Kt Lo</author>
</authors>
<title>Multi-Scale Audio Indexing for translingual spoken document retrieval”,</title>
<date>2001</date>
<booktitle>Proceedings of IEEE International Conference on Acoustics, Speech, Signal processing ,</booktitle>
<location>Salt Lake City, USA,</location>
<note>located at http://www.iis.sinica.edu.tw/~whm/</note>
<contexts>
<context position="4236" citStr="Wang et al 2001" startWordPosition="676" endWordPosition="679"> et al 2000) and to minimize speech recognition errors (Shen et al, 1998). However, when such method is used directly to correct speech recognition errors, it tends to bring in too many irrelevant terms (Ng 2000). Because important terms in a long document are often repeated several times, there is a good chance that such terms will be correctly recognized at least once by a speech recognition engine with a reasonable level of word recognition rate. Many spoken document retrieval (SDR) systems took advantage of this fact in reducing the speech recognition and matching errors (Meng et al 2001; Wang et al 2001; Chen et al 2001). In contrast to SDR, very little work has been done on Chinese spoken query processing (SQP), which is the use of spoken queries to retrieval textual documents. Moreover, spoken queries in SQP tend to be very short with few repeated terms. In this paper, we aim to integrate the spoken language and natural language research to process spoken queries with speech recognition errors. The main contribution of this research is in devising a divide-and-conquer strategy to alleviate the speech recognition errors. It first employs the Chinese query model to isolate the Core Semantic </context>
</contexts>
<marker>Wang, Meng, Schone, Chen, Lo, 2001</marker>
<rawString>Hsin-min Wang, Helen Meng, Patrick Schone, Berlin Chen and Wai-Kt Lo (2001), “Multi-Scale Audio Indexing for translingual spoken document retrieval”, Proceedings of IEEE International Conference on Acoustics, Speech, Signal processing , Salt Lake City, USA, May 2001, located at http://www.iis.sinica.edu.tw/~whm/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongcheng Wang</author>
</authors>
<date>1992</date>
<booktitle>Technology and basis of Chinese Information Processing, Shanghai Jiao</booktitle>
<publisher>Tong University Press</publisher>
<contexts>
<context position="12339" citStr="Wang 1992" startWordPosition="2065" endWordPosition="2066"> employ the Finite State Automata (FSA) to model the query structure. FSA models the expected sequences of tokens in typical queries and annotate the semantic tags, including CSS. A FSA is defined for each of the 51 query structures. An example of FSA is given in Figure 2. Because CSS is an open set, we do not know its content in advance. Instead, we use the following two rules to determine the candidates for CSS: (a) it is an unknown string not present in the Query Component Dictionary; and (b) its length is not less than two, as the average length of concepts in Chinese is greater than one (Wang 1992). At each stage of parsing the query using FSA (Hobbs et al 1997), we need to make decision on which state to proceed and how to handle unexpected tokens in the query. Thus at each stage, FSA needs to perform three functions: a) Goto function: It maps a pair consisting of a state and an input symbol into a new state or the fail state. We use G(N,X) =N’ to define the goto function from State N to State N’, given the occurrence of token X. b) Fail function: It is consulted whenever the goto function reports a failure when encountering an unexpected token. We use f(N) =N’ to represent the fail fu</context>
<context position="17619" citStr="Wang 1992" startWordPosition="3027" endWordPosition="3028"> Terms Generation Because of speech recognition error, the CSS obtained is likely to contain error, or in the worse case, missing the main semantics of the query altogether. We now discuss how we alleviate the errors in CSS for the former case. We will first break the CSS into one or more basic semantic parts, and then apply the multi-tier method to map the query components to known phrases. 4.1 Breaking CSS into Basic Components In many cases, the CSS obtained may be made up of several semantic components equivalent to base noun phrases. Here we employ a technique based on Chinese cut marks (Wang 1992) to perform the segmentation. The Chinese cut marks are tokens that can separate a Chinese sentence into several semantic parts. Zhou (1997) used such technique to detect new Chinese words, and reported good results with precision and recall of 92% and 70% respectively. By separating the CSS into basic key components, we can limit the propagation of errors. 4.2 Multi-tier query term mapping In order to further eliminate the speech recognition errors, we propose a multi-tier approach to map the basic components in CSS into known phrases by using a combination of matching techniques. To do this,</context>
</contexts>
<marker>Wang, 1992</marker>
<rawString>Yongcheng Wang (1992), Technology and basis of Chinese Information Processing, Shanghai Jiao Tong University Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Introduction to modern information retrieval”, Published by London:</title>
<date>1999</date>
<publisher>Library Association Publishing.</publisher>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier (1999), “Introduction to modern information retrieval”, Published by London: Library Association Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-nan Ying</author>
<author>Yong Ji</author>
<author>Wei Shen</author>
</authors>
<title>report of query log”, internal report in</title>
<date>2002</date>
<institution>Shanghai Jiao Tong University</institution>
<marker>Ying, Ji, Shen, 2002</marker>
<rawString>Hai-nan Ying, Yong Ji and Wei Shen, (2002), “report of query log”, internal report in Shanghai Jiao Tong University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Kim Teng Lua</author>
</authors>
<title>Detection of Unknown Chinese Words Using a Hybrid Approach Computer Processing of Oriental Languages,</title>
<date>1997</date>
<volume>11</volume>
<pages>63--75</pages>
<marker>Zhou, Lua, 1997</marker>
<rawString>Guodong Zhou and Kim Teng Lua (1997) Detection of Unknown Chinese Words Using a Hybrid Approach Computer Processing of Oriental Languages, Vol 11, No 1, 1997, 63-75</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
</authors>
<title>Language Modelling in Mandarin Speech Recognition”,</title>
<date>1997</date>
<tech>Ph.D. Thesis,</tech>
<institution>National University of Singapore.</institution>
<contexts>
<context position="3326" citStr="Zhou 1997" startWordPosition="522" endWordPosition="523">on involves the conversion of acoustic speech signals to a stream of text. Because of the complexity of human vocal tract, the speech signals being observed are different, even for multiple utterances of the same sequence of words by the same person (Lee et al 1996). Furthermore, the speech signals can be influenced by the differences across different speakers, dialects, transmission distortions, and speaking environments. These have contributed to the noise and variability of speech signals. As one of the main sources of errors in Chinese speech recognition come from substitution (Wang 2002; Zhou 1997), in which a wrong but similar sounding term is used in place of the correct term, confusion matrix has been used to record confused sound pairs in an attempt to eliminate this error. Confusion matrix has been employed effectively in spoken document retrieval (Singhal et al, 1999 and Srinivasan et al 2000) and to minimize speech recognition errors (Shen et al, 1998). However, when such method is used directly to correct speech recognition errors, it tends to bring in too many irrelevant terms (Ng 2000). Because important terms in a long document are often repeated several times, there is a goo</context>
<context position="17759" citStr="Zhou (1997)" startWordPosition="3049" endWordPosition="3050">semantics of the query altogether. We now discuss how we alleviate the errors in CSS for the former case. We will first break the CSS into one or more basic semantic parts, and then apply the multi-tier method to map the query components to known phrases. 4.1 Breaking CSS into Basic Components In many cases, the CSS obtained may be made up of several semantic components equivalent to base noun phrases. Here we employ a technique based on Chinese cut marks (Wang 1992) to perform the segmentation. The Chinese cut marks are tokens that can separate a Chinese sentence into several semantic parts. Zhou (1997) used such technique to detect new Chinese words, and reported good results with precision and recall of 92% and 70% respectively. By separating the CSS into basic key components, we can limit the propagation of errors. 4.2 Multi-tier query term mapping In order to further eliminate the speech recognition errors, we propose a multi-tier approach to map the basic components in CSS into known phrases by using a combination of matching techniques. To do this, we need to build up a phrase dictionary containing typical concepts used in general and specific domains. Most basic CSS components should </context>
</contexts>
<marker>Zhou, 1997</marker>
<rawString>Guodong Zhou (1997), “Language Modelling in Mandarin Speech Recognition”, Ph.D. Thesis, National University of Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>