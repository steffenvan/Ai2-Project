<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.995252">
A Logic of Semantic Representations for Shallow Parsing
</title>
<author confidence="0.999273">
Alexander Koller Alex Lascarides
</author>
<affiliation confidence="0.999973">
Saarland University University of Edinburgh
</affiliation>
<address confidence="0.892662">
Saarbr¨ucken, Germany Edinburgh, UK
</address>
<email confidence="0.983745">
koller@mmci.uni-saarland.de alex@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993452" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998190636363636">
One way to construct semantic represen-
tations in a robust manner is to enhance
shallow language processors with seman-
tic components. Here, we provide a model
theory for a semantic formalism that is de-
signed for this, namely Robust Minimal
Recursion Semantics (RMRS). We show
that RMRS supports a notion of entailment
that allows it to form the basis for compar-
ing the semantic output of different parses
of varying depth.
</bodyText>
<sectionHeader confidence="0.998792" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965164179105">
Representing semantics as a logical form that sup-
ports automated inference and model construc-
tion is vital for deeper language engineering tasks,
such as dialogue systems. Logical forms can be
obtained from hand-crafted deep grammars (Butt
et al., 1999; Copestake and Flickinger, 2000) but
this lacks robustness: not all words and con-
structions are covered and by design ill-formed
phrases fail to parse. There has thus been a trend
recently towards robust wide-coverage semantic
construction (e.g., (Bos et al., 2004; Zettlemoyer
and Collins, 2007)). But there are certain seman-
tic phenomena that these robust approaches don’t
capture reliably, including quantifier scope, op-
tional arguments, and long-distance dependencies
(for instance, Clark et al. (2004) report that the
parser used by Bos et al. (2004) yields 63% ac-
curacy on object extraction; e.g., the man that I
met... ). Forcing a robust parser to make a de-
cision about these phenomena can therefore be
error-prone. Depending on the application, it may
be preferable to give the parser the option to leave
a semantic decision open when it’s not sufficiently
informed—i.e., to compute a partial semantic rep-
resentation and to complete it later, using informa-
tion extraneous to the parser.
In this paper, we focus on an approach to se-
mantic representation that supports this strategy:
Robust Minimal Recursion Semantics (RMRS,
Copestake (2007a)). RMRS is designed to support
underspecification of lexical information, scope,
and predicate-argument structure. It is an emerg-
ing standard for representing partial semantics,
and has been applied in several implemented sys-
tems. For instance, Copestake (2003) and Frank
(2004) use it to specify semantic components to
shallow parsers ranging in depth from POS tag-
gers to chunk parsers and intermediate parsers
such as RASP (Briscoe et al., 2006). MRS anal-
yses (Copestake et al., 2005) derived from deep
grammars, such as the English Resource Grammar
(ERG, (Copestake and Flickinger, 2000)) are spe-
cial cases of RMRS. But RMRS, unlike MRS and re-
lated formalisms like dominance constraints (Egg
et al., 2001), is able to express semantic infor-
mation in the absence of full predicate argument
structure and lexical subcategorisation.
The key contribution we make is to cast RMRS,
for the first time, as a logic with a well-defined
model theory. Previously, no such model theory
existed, and so RMRS had to be used in a some-
what ad-hoc manner that left open exactly what
any given RMRS representation actually means.
This has hindered practical progress, both in terms
of understanding the relationship of RMRS to other
frameworks such as MRS and predicate logic and
in terms of the development of efficient algo-
rithms. As one application of our formalisation,
we use entailment to propose a novel way of char-
acterising consistency of RMRS analyses across
different parsers.
Section 2 introduces RMRS informally and illus-
trates why it is necessary and useful for represent-
ing semantic information across deep and shallow
language processors. Section 3 defines the syntax
and model-theory of RMRS. We finish in Section 4
by pointing out some avenues for future research.
</bodyText>
<note confidence="0.923023">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 451–459,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997582">
451
</page>
<sectionHeader confidence="0.7399835" genericHeader="introduction">
2 Deep and shallow semantic
construction
</sectionHeader>
<bodyText confidence="0.967278222222222">
Consider the following (toy) sentence:
(1) Every fat cat chased some dog.
It exhibits several kinds of ambiguity, includ-
ing a quantifier scope ambiguity and lexical
ambiguities—e.g., the nouns “cat” and “dog” have
8 and 7 WordNet senses respectively. Simplifying
slightly by ignoring tense information, two of its
readings are shown as logical forms below; these
can be represented as trees as shown in Fig. 1.
</bodyText>
<figure confidence="0.868567833333333">
(2) every q 1(x, fat j 1(e&apos;, x) ∧ cat n 1(x),
some q 1(y, dog n 1(y),
chase v 1(e, x, y)))
(3) some q 1(y, dog n 2(y),
every q 1(x, fat j 1(e&apos;, x) ∧ cat n 2(x),
chase v 1(e, x, y)))
</figure>
<bodyText confidence="0.999714722222222">
Now imagine trying to extract semantic infor-
mation from the output of a part-of-speech (POS)
tagger by using the word lemmas as lexical pred-
icate symbols. Such a semantic representation
is highly partial. It will use predicate symbols
such as cat n, which might resolve to the pred-
icate symbols cat n 1 or cat n 2 in the com-
plete semantic representation. (Notice the dif-
ferent fonts for the ambiguous and unambiguous
predicate symbols.) But most underspecification
formalisms (e.g., MRS (Copestake et al., 2005) and
CLLS (Egg et al., 2001)) are unable to represent se-
mantic information that is as partial as what we get
from a POS tagger because they cannot underspec-
ify predicate-argument structure. RMRS (Copes-
take, 2007a) is designed to address this problem.
In RMRS, the information we get from the POS tag-
ger is as follows:
</bodyText>
<equation confidence="0.999652833333333">
(4) l1 : a1 : every q(x1),
l41 : a41 : fat j(e&apos;),
l42 : a42 : cat n(x3)
l5 : a5 : chase v(e),
l6 : a6 : some q(x6),
l9 : a9 : dog n(x7)
</equation>
<bodyText confidence="0.966441166666667">
This RMRS expresses only that certain predica-
tions are present in the semantic representation—
it doesn’t say anything about semantic scope,
about most arguments of the predicates (e.g.,
chase v(e) doesn’t say who chases whom), or
about the coindexation of variables ( every q
</bodyText>
<figureCaption confidence="0.942803">
Figure 1: Semantic representations (2) and (3) as
trees.
</figureCaption>
<bodyText confidence="0.9950425">
binds the variable x1, whereas cat n speaks about
x3), and it maintains the lexical ambiguities. Tech-
nically, it consists of six elementary predications
(EPs), one for each word lemma in the sentence;
each of them is prefixed by a label and an anchor,
which are essentially variables that refer to nodes
in the trees in Fig. 1. We can say that the two trees
satisfy this RMRS because it is possible to map the
labels and anchors in (4) into nodes in each tree
and variable names like x1 and x3 into variable
names in the tree in such a way that the predica-
tions of the nodes that labels and anchors denote
are consistent with those in the EPs of (4)—e.g., l1
and a1 can map to the root of the first tree in Fig. 1,
x1 to x, and the root label every q 1 is consistent
with the EP predicate every q.
There are of course many other trees (and thus,
fully specific semantic representations such as (2))
that are described equally well by the RMRS (4);
this is not surprising, given that the semantic out-
put from the POS tagger is so incomplete. If we
have information about subjects and objects from
a chunk parser like Cass (Abney, 1996), we can
represent it in a more detailed RMRS:
</bodyText>
<equation confidence="0.999456125">
(5) l1 : a1 : every q(x1),
l41 : a41 : fat j(e&apos;),
l42 : a42 : cat n(x3)
l5 : a5 : chase v(e),
ARG1(a5, x4), ARG2(a5, x5)
l6 : a6 : some q(x6),
l9 : a9 : dog n(x7)
x3 = x4, x5 = x7
</equation>
<bodyText confidence="0.817954">
This introduces two new types of atoms. x3 =
x4 means that x3 and x4 map to the same variable
in any fully specific logical form; e.g., both to the
variable x in Fig. 1. ARGZ(a, z) (and ARGZ(a, h))
</bodyText>
<equation confidence="0.70932205">
_some_q_1
x A
_every_q_1
_fat_j_1
e&apos; x
_cat_n_1
x
y _dog_n_1
y
_chase_v_1
e x y
_some_q_1
y _dog_n_2
_every_q_1
y x n _chase_v_1
_fat_j_1
e&apos; x
_cat_n_2
x
e x y
</equation>
<page confidence="0.993487">
452
</page>
<bodyText confidence="0.99955585">
express that the i-th child (counting from 0) of the
node to which the anchor a refers is the variable
name that z denotes (or the node that the hole h
denotes). So unlike earlier underspecification for-
malisms, RMRS can specify the predicate of an
atom separately from its arguments; this is nec-
essary for supporting parsers where information
about lexical subcategorisation is absent. If we
also allow atoms of the form ARG12,31(a, x) to ex-
press uncertainty as to whether x is the second or
third child of the anchor a, then RMRS can even
specify the arguments to a predicate while under-
specifying their position. This is useful for speci-
fying arguments to give v when a parser doesn’t
handle unbounded dependencies and is faced with
Which bone did you give the dog? vs. To which
dog did you give the bone?
Finally, the RMRS (6) is a notational variant of
the MRS derived by the ERG, a wide-coverage deep
grammar:
</bodyText>
<equation confidence="0.99961875">
(6) l1 : a1: every q 1(x1),
RSTR(a1, h2), BODY(a1, h3)
l41 : a41: fat j 1(e&apos;), ARG1(a41, x2)
l42 : a42: cat n 1(x3)
l5 : a5: chase v 1(e),
ARG1(a5, x4), ARG2(a5, x5)
l6 : a6: some q 1(x6),
RSTR(a6, h7), BODY(a6, h8)
l9 : a9: dog n 1(x7)
h2 =q l42, l41 = l42, h7 =q l9
x1 = x2, x2 = x3, x3 = x4,
x5 = x6, x5 = x7
</equation>
<bodyText confidence="0.999178470588236">
RSTR and BODY are conventional names for
the ARG1 and ARG2 of a quantifier predicate sym-
bol. Atoms like h2 =q l42 (“qeq”) specify a cer-
tain kind of “outscopes” relationship between the
hole and the label, and are used here to underspec-
ify the scope of the two quantifiers. Notice that the
labels of the EPs for “fat” and “cat” are stipulated
to be equal in (6), whereas the anchors are not. In
the tree, it is the anchors that are mapped to the
nodes with the labels fat j 1 and cat n 1; the la-
bel is mapped to the conjunction node just above
them. In other words, the role of the anchor in an
EP is to connect a predicate to its arguments, while
the role of the label is to connect the EP to the sur-
rounding formula. Representing conjunction with
label sharing stems from MRS and provides com-
pact representations.
Finally, (6) uses predicate symbols like
dog n 1 that are meant to be more specific than
symbols like dog n which the earlier RMRSs
used. This reflects the fact that the deep gram-
mar performs some lexical disambiguation that the
chunker and POS tagger don’t. The fact that the
former symbol should be more specific than the
latter can be represented using SPEC atoms like
dog n 1 C dog n. Note that even a deep gram-
mar will not fully disambiguate to semantic pred-
icate symbols, such as WordNet senses, and so
dog n 1 can still be consistent with multiple sym-
bols like dog n 1 and dog n 2 in the semantic
representation. However, unlike the output of a
POS tagger, an RMRS symbol that’s output by a
deep grammar is consistent with symbols that all
have the same arity, because a deep grammar fully
determines lexical subcategorisation.
In summary, RMRS allows us to represent in a
uniform way the (partial) semantics that can be
extracted from a wide range of NLP tools. This
is useful for hybrid systems which exploit shal-
lower analyses when deeper parsing fails, or which
try to match deeply parsed queries against shal-
low parses of large corpora; and in fact, RMRS is
gaining popularity as a practical interchange for-
mat for exactly these purposes (Copestake, 2003).
However, RMRS is still relatively ad-hoc in that its
formal semantics is not defined; we don’t know,
formally, what an RMRS means in terms of seman-
tic representations like (2) and (3), and this hin-
ders our ability to design efficient algorithms for
processing RMRS. The purpose of this paper is to
lay the groundwork for fixing this problem.
</bodyText>
<sectionHeader confidence="0.984303" genericHeader="method">
3 Robust Minimal Recursion Semantics
</sectionHeader>
<bodyText confidence="0.999982428571428">
We will now make the basic ideas from Section
2 precise. We will first define the syntax of the
RMRS language; this is a notational variant of ear-
lier definitions in the literature. We will then de-
fine a model theory for our version of RMRS, and
conclude this section by carrying over the notion
of solved forms from CLLS (Egg et al., 2001).
</bodyText>
<subsectionHeader confidence="0.989439">
3.1 RMRS Syntax
</subsectionHeader>
<bodyText confidence="0.99997525">
We define RMRS syntax in the style of CLLS (Egg
et al., 2001). We assume an infinite set of node
variables NVar = {X, Y, X1,...}, used as labels,
anchors, and holes; the distinction between these
will come from their position in the formulas. We
also assume an infinite set of base variables BVar,
consisting of individual variables {x, x1, y,...}
and event variables {e1, ...}, and a vocabulary of
</bodyText>
<page confidence="0.996532">
453
</page>
<bodyText confidence="0.994566">
predicate symbols Pred = {P, Q, P1, ...}. RMRS
formulas are defined as follows.
</bodyText>
<construct confidence="0.5150585">
Definition 1. An RMRS is a finite set cp of atoms
of one of the following forms; S C_ N is a set of
numbers that is either finite or N itself (throughout
the paper, we assume 0 E N).
</construct>
<equation confidence="0.987066142857143">
A ::= X:Y :P
 |ARGS(X, v)
 |ARGS(X,Y )
 |X !* Y
 |v1 = v2  |v1 =�v2
 |X = Y  |X =�Y
 |PEQ
</equation>
<bodyText confidence="0.999499173913043">
A node variable X is called a label iff cp con-
tains an atom of the form X:Y :P or Y !* X; it
is an anchor iff cp contains an atom of the form
Y :X:P or ARGS(X, i); and it is a hole iff cp con-
tains an atom of the form ARGS(Y, X) or X !*Y .
Def. 1 combines similarities to earlier presen-
tations of RMRS (Copestake, 2003; Copestake,
2007b) and to CLLS/dominance constraints (Egg
et al., 2001). For the most part, our syntax
generalises that of older versions of RMRS: We
use ARG{Z} (with a singleton set S) instead of
ARGZ and ARGN instead of ARG,,, and the EP
l:a:P(v) (as in Section 2) is an abbreviation of
{l:a:P, ARG{0}(a, v)}. Similarly, we don’t as-
sume that labels, anchors, and holes are syntacti-
cally different objects; they receive their function
from their positions in the formula. One major dif-
ference is that we use dominance (!*) rather than
qeq; see Section 3.4 for a discussion. Compared
to dominance constraints, the primary difference
is that we now have a mechanism for representing
lexical ambiguity, and we can specify a predicate
and its arguments separately.
</bodyText>
<subsectionHeader confidence="0.999811">
3.2 Model Theory
</subsectionHeader>
<bodyText confidence="0.965995982142857">
The model theory formalises the relationship be-
tween an RMRS and the fully specific, alternative
logical forms that it describes, expressed in the
base language. We represent such a logical form
as a tree T, such as the ones in Fig. 1, and we can
then define satisfaction of formulas in the usual
way, by taking the tree as a model structure that
interprets all predicate symbols specified above.
In this paper, we assume for simplicity that the
base language is as in MRS; essentially, T becomes
the structure tree of a formula of predicate logic.
We assume that E is a ranked signature consist-
ing of the symbols of predicate logic: a unary con-
structor ¬ and binary constructors ∧, —*, etc.; a set
of 3-place quantifier symbols such as every q 1
and some q 1 (with the children being the bound
variable, the restrictor, and the scope); and con-
structors of various arities for the predicate sym-
bols; e.g., chase v 1 is of arity 3. Other base lan-
guages may require a different signature E and/or
a different mapping between formulas and trees;
the only strict requirement we make is that the
signature contains a binary constructor ∧ to rep-
resent conjunction. We write EZ and E&gt;Z for the
set of all constructors in E with arity i and at least
i, respectively. We will follow the typographical
convention that non-logical symbols in E are writ-
ten in sans-serif, as opposed to the RMRS predicate
symbols like cat n and cat n 1.
The models of RMRS are then defined to be fi-
nite constructor trees (see also (Egg et al., 2001)):
Definition 2. A finite constructor tree T is a func-
tion T : D —* E such that D is a tree domain (i.e.,
a subset of N* which is closed under prefix and left
sibling) and the number of children of each node
u E D is equal to the arity of T(u).
We write D(T) for the tree domain of a con-
structor tree T, and further define the following re-
lations between nodes in a finite constructor tree:
Definition 3. u !* v (dominance) iff u is a prefix
of v, i.e. the node u is equal to or above the node
v in the tree. u !* � v iff u !* v, and all symbols on
the path from u to v (not including v) are ∧.
The satisfaction relation between an RMRS cp
and a finite constructor tree T is defined in terms
of several assignment functions. First, a node
variable assignment function a : NVar —* D(T)
maps the node variables in an RMRS to the nodes
of T. Second, a base language assignment func-
tion g : BVar —* E0 maps the base variables to
nullary constructors representing variables in the
base language. Finally, a function a from Pred to
the power set of E&gt;1 maps each RMRS predicate
symbol to a set of constructors from E. As we’ll
see shortly, this function allows an RMRS to under-
specify lexical ambiguities.
</bodyText>
<subsubsectionHeader confidence="0.391166">
Definition 4. Satisfaction of atoms is defined as
</subsubsectionHeader>
<page confidence="0.996655">
454
</page>
<construct confidence="0.932224230769231">
follows:
τ, α, g, σ |= X:Y :P iff
τ(α(Y )) E σ(P) and α(X) !∗ ∧ α(Y )
τ, α, g, σ |= ARGS(X, a) iff exists i E 5 s.t.
α(X) · i E D(τ) and τ(α(X) · i) = g(a)
τ, α, g, σ |= ARGS(X, Y ) iff exists i E 5 s.t.
α(X) · i E D(τ), α(X) · i = α(Y )
τ, α, g, σ |= X !∗ Y iff α(X) !∗ α(Y )
τ, α,g,σ |= X =/7� Y iff α(X) =/7� α(Y )
τ, α, g, σ |= v1 =/7� v2 iff g(v1) =/7� g(v2)
τ, α, g, σ |= P C Q iff σ(P) 9 σ(Q)
A 4-tuple τ, α, g, σ satisfies an RMRS ϕ (written
τ, α, g, σ |= ϕ) iff it satisfies all of its elements.
</construct>
<bodyText confidence="0.999935918918919">
Notice that one RMRS may be satisfied by mul-
tiple trees; we can take the RMRS to be a par-
tial description of each of these trees. In partic-
ular, RMRSs may represent semantic scope ambi-
guities and/or missing information about seman-
tic dependencies, lexical subcategorisation and
lexical senses. For j = {1, 2}, suppose that
τj, αj, gj, σ |= ϕ. Then ϕ exhibits a semantic
scope ambiguity if there are variables Y, Y ! E
NVar such that α1(Y ) !∗ α1(Y !) and α2(Y !) !∗
α2(Y ). It exhibits missing information about se-
mantic dependencies if there are base-language
variables v, v! E BVar such that g1(v) = g1(v!)
and g2(v) =� g2(v!). It exhibits missing lex-
ical subcategorisation information if there is a
Y E NVar such that τ1(α1(Y )) is a construc-
tor of a different type from τ2(α2(Y )) (i.e., the
constructors are of a different arity or they dif-
fer in whether their arguments are scopal vs. non-
scopal). And it exhibits missing lexical sense in-
formation if τ1(α1(Y )) and τ2(α2(Y )) are differ-
ent base-language constructors, but of the same
type.
Let’s look again at the RMRS (4). This is sat-
isfied by the trees in Fig. 1 (among others) to-
gether with some particular α, g, and σ. For in-
stance, consider the left-hand side tree in Fig. 1.
The RMRS (4) satisfies this tree with an assign-
ment function α that maps the variables l1 and a1
to the root node, l41 and l42 to its second child
(labeled with “∧”), a41 to the first child of that
node (i.e. the node 21, labelled with “fat”) and
a42 to the node 22, and so forth. g will map x1
and x3 to x, and x6 and x7 to y, and so on. And
σ will map each RMRS predicate symbol (which
represents a word) to the set of its fully resolved
meanings, e.g. cat n to a set containing cat n 1
</bodyText>
<figureCaption confidence="0.994945">
Figure 2: Another tree which satisfies (6).
</figureCaption>
<bodyText confidence="0.714473">
and possibly others. It is then easy to verify
that every single atom in the RMRS is satisfied—
most interestingly, the EPs l41:a41: fat j(e!) and
l42:a42: cat n(x3) are satisfied because α(l41) !∗∧
α(a41) and α(l42) !∗∧ α(a42).
Truth, validity and entailment can now be de-
fined in terms of satisfiability in the usual way:
</bodyText>
<equation confidence="0.53760425">
Definition 5. truth: τ |= ϕ iff ]α, g, σ such that
τ, α,g,σ |= ϕ
validity: |= ϕ iff Vτ, τ |= ϕ.
entailment: ϕ |= ϕ! iff Vτ, if τ |= ϕ then τ |= ϕ!.
</equation>
<subsectionHeader confidence="0.994644">
3.3 Solved Forms
</subsectionHeader>
<bodyText confidence="0.999972689655172">
One aspect in which our definition of RMRS is like
dominance constraints and unlike MRS is that any
satisfiable RMRS has an infinite number of mod-
els which only differ in the areas that the RMRS
didn’t “talk about”. Reading (6) as an MRS or as
an RMRS of the previous literature, this formula
is an instruction to build a semantic representa-
tion out of the pieces for “every fat cat”, “some
dog”, and “chased”; a semantic representation as
in Fig. 2 would not be taken as described by this
RMRS. However, under the semantics we proposed
above, this tree is a correct model of (6) because
all atoms are still satisfied; the RMRS didn’t say
anything about “sleep” or “run”, but it couldn’t en-
force that the tree shouldn’t contain those subfor-
mulas either.
In the context of robust semantic processing,
this is a desirable feature, because it means that
when we enrich an RMRS obtained from a shal-
low processor with more semantic information—
such as the relation symbols introduced by syntac-
tic constructions such as appositives, noun-noun
compounds and free adjuncts—we don’t change
the set of models; we only restrict the set of mod-
els further and further towards the semantic rep-
resentation we are trying to reconstruct. Further-
more, it has been shown in the literature that a
dominance-constraint style semantics for under-
specified representations gives us more room to
</bodyText>
<figure confidence="0.958235">
_chase_v_1
_every_q_1
_some_q_1
x A
fat_j_1
e. x
_cat_n_1
y _dog_n_1
e... y
_sleep_v_1
y
_run_v_1
e x y
x e.. x
</figure>
<page confidence="0.998033">
455
</page>
<bodyText confidence="0.99989105">
manoeuvre when developing efficient solvers than
an MRS-style semantics (Althaus et al., 2003).
However, enumerating an infinite number of
models is of course infeasible. For this reason,
we will now transfer the concept of solved forms
from dominance constraints to RMRS. An RMRS
in solved form is guaranteed to be satisfiable, and
thus each solved form represents an infinite class
of models. However, each satisfiable RMRS has
only a finite number of solved forms which parti-
tion the space of possible models into classes such
that models within a class differ only in ‘irrele-
vant’ details. A solver can then enumerate the
solved forms rather than all models.
Intuitively, an RMRS in solved form is fully
specified with respect to the predicate-argument
structure, all variable equalities and inequalities
and scope ambiguities have been resolved, and
only lexical sense ambiguities remain. This is
made precise below.
</bodyText>
<construct confidence="0.454802">
Definition 6. An RMRS ϕ is in solved form iff:
</construct>
<listItem confidence="0.999825285714286">
1. every variable in ϕ is either a hole, a label or
an anchor (but not two of these);
2. ϕ doesn’t contain equality, inequality, and
SPEC (C) atoms;
3. if ARGS(Y, i) is in ϕ, then |S |= 1;
4. for any label Y and index set S, there are no
two atoms ARGS(Y, i) and ARGS(Y, i!) in ϕ;
5. if Y is an anchor in some EP X:Y :P
and k is the maximum number such that
ARG{k}(X, i) is in ϕ for any i, then there is a
constructor p E σ(P) whose arity is at least
k;
6. no label occurs on the right-hand side of two
different a∗ atoms.
</listItem>
<bodyText confidence="0.987504142857143">
Because solved forms are so restricted, we can
‘read off’ at least one model from each solved
form:
into holes is straightforward because no label is
dominated by more than one hole; and spaces
between the labels and anchors are filled with
conjunctions.
We can now define the solved forms of an RMRS
ϕ; these finitely many RMRSs in solved form parti-
tion the space of models of ϕ into classes of mod-
els with trivial differences.
Definition 7. The syntactic dominance relation
D(ϕ) in an RMRS ϕ is the reflexive, transitive clo-
sure of the binary relation
</bodyText>
<construct confidence="0.5791205">
{(X, Y )  |ϕ contains X a∗ Y or
ARGS(X, Y ) for some S}
</construct>
<listItem confidence="0.911049363636364">
An RMRS ϕ! is a solved form of the RMRS ϕ iff
ϕ! is in solved form and there is a substitution s
that maps the node and base variables of ϕ to the
node and base variables of ϕ! such that
1. ϕ! contains the EP X!:Y !:P iff there are vari-
ables X,Y such that X:Y :P is in ϕ, X! =
s(X), and Y ! = s(Y );
2. for every atom ARGS(X, i) in ϕ, there is
exactly one atom ARGS�(X!, i!) in ϕ! with
X! = s(X), i! = s(i), and S! C S;
3. D(ϕ!) Q s(D(ϕ)).
</listItem>
<construct confidence="0.896585">
Proposition 2. For every tuple (τ, α, g, σ) that
satisfies some RMRS ϕ, there is a solved form ϕ!
of ϕ such that (τ, α, g, σ) also satisfies ϕ!.
</construct>
<bodyText confidence="0.957318961538462">
Proof. We construct the substitution s from α and
g. Then we add all dominance atoms that are satis-
fied by α and restrict the ARG atoms to those child
indices that are actually used in τ. The result is in
solved form because τ is a tree; it is a solved form
of ϕ by construction.
Proposition 3. Every RMRS ϕ has only a finite
number of solved forms, up to renaming of vari-
ables.
Proposition 1. Every RMRS in solved form is sat-
isfiable.
Proof (sketch; see also (Duchier and Niehren, 2000)).
For each EP, we choose to label the anchor with
the constructor p of sufficiently high arity whose
existence we assumed; we determine the edges
between an anchor and its children from the
uniquely determined ARG atoms; plugging labels
Proof. Up to renaming of variables, there is only a
finite number of substitutions on the node and base
variables of ϕ. Let s be such a substitution. This
fixes the set of EPs of any solved form of ϕ that is
based on s uniquely. There is only a finite set of
choices for the subsets S! in condition 2 of Def. 7,
and there is only a finite set of choices of new dom-
inance atoms that satisfy condition 3. Therefore,
the set of solved forms of ϕ is finite.
</bodyText>
<page confidence="0.998056">
456
</page>
<bodyText confidence="0.99994635">
Let’s look at an example for all these defini-
tions. All the RMRSs presented in Section 2 (re-
placing =Q by !*) are in solved form; this is least
obvious for (6), but becomes clear once we notice
that no label is on the right-hand side of two dom-
inance atoms. However, the model constructed in
the proof of Prop. 1 looks a bit like Fig. 2; both
models are problematic in several ways and in par-
ticular contain an unbound variable y even though
they also contains a quantifier that binds y. If we
restrict the class of models to those in which such
variables are bound (as Copestake et al. (2005)
do), we can enforce that the quantifiers outscope
their bound variables without changing models of
the RMRS further—i.e., we add the atoms h3 !* l5
and h8!*l5. Fig. 2 is no longer a model for the ex-
tended RMRS, which in turn is no longer in solved
form because the label l5 is on the right-hand side
of two dominance atoms. Instead, it has the fol-
lowing two solved forms:
</bodyText>
<equation confidence="0.9870052">
(7) l1:a1: every q 1(x1),
RSTR(a1, h2), BODY(a1, h3),
l41:a41: fat j 1(e&apos;), ARG1(a41, x1),
l41:a42: cat n 1(x1),
l6:a6: some q 1(x6),
RSTR(a6, h7), BODY(a6, h8),
l9:a9: dog n 1(x6),
l5:a5: chase v 1(e),
ARG1(a5, x1), ARG2(a5, x6),
h2 !* l41, h3 !* l6, h7 !* l9, h8 !* l5
(8) l1:a1: every q 1(x1),
RSTR(a1, h2), BODY(a1, h3),
l41:a41: fat j 1(e&apos;), ARG1(a41, x1),
l41:a42: cat n 1(x1),
l6:a6: some q 1(x6),
RSTR(a6, h7), BODY(a6, h8),
l9:a9: dog n 1(x6),
l5:a5: chase v 1(e),
ARG1(a5, x1), ARG2(a5, x6),
h2 !* l41, h3 !* l5, h7 !* l9, h8 !* l1
</equation>
<bodyText confidence="0.999872142857143">
Notice that we have eliminated all equalities by
unifying the variable names, and we have fixed the
relative scope of the two quantifiers. Each of these
solved forms now stands for a separate class of
models; for instance, the first model in Fig. 1 is
a model of (7), whereas the second is a model of
(8).
</bodyText>
<subsectionHeader confidence="0.747113">
3.4 Extensions
</subsectionHeader>
<bodyText confidence="0.999923923076923">
So far we have based the syntax and semantics of
RMRS on the dominance relation from Egg et al.
(2001) rather than the qeq relation from Copestake
et al. (2005). This is partly because dominance is
the weaker relation: If a dependency parser links a
determiner to a noun and this noun to a verb, then
we can use dominance but not qeq to represent that
the predicate introduced by the verb is outscoped
by the quantifier introduced by the determiner (see
earlier discussion). However, it is very straightfor-
ward to extend the syntax and semantics of the lan-
guage to include the qeq relation. This extension
adds a new atom X =Q Y to Def. 1, and T, a, g, a
will satisfy X =Q Y iff a(X) !* a(Y), each node
on the path is a quantifier, and each step in the path
goes to the rightmost child. All the above proposi-
tions about solved forms still hold if “dominance”
is replaced with “qeq”.
Furthermore, grammar developers such as those
in the DELPH-IN community typically adopt con-
ventions that restrict them to a fragment of the lan-
guage from Def. 1 (once qeq is added to it), or they
restrict attention to only a subset of the models
(e.g., ones with correctly bound variables, or ones
which don’t contain extra material like Fig. 2).
Our formalism provides a general framework into
which all these various fragments fit, and it’s a
matter of future work to explore these fragments
further.
Another feature of the existing RMRS literature
is that each term of an RMRS is equipped with a
sort. In particular, individual variables x, event
variables e and holes h are arranged together with
their subsorts (e.g., epast) and supersorts (e.g.,
sort i abstracts over x and e) into a sort hierar-
chy S. For simplicity we defined RMRS without
sorts, but it is straightforward to add them. For
this, one assumes that the signature E is sorted, i.e.
assigns a sort s1 x ... sn -* s to each constructor,
where n is the constructor’s arity (possibly zero)
and s, s1, ... , sn E S are atomic sorts. We restrict
the models of RMRS to trees that are well-sorted in
the usual sense, i.e. those in which we can infer a
sort for each subtree, and require that the variable
assignment functions likewise respect the sorts. If
we then modify Def. 6 such that the constructor p
of sufficiently high arity is also consistent with the
sorts of the known arguments—i.e., if p has sort
s1 x ... x sn -* s and the RMRS contains an atom
ARG{k}(Y,i) and i is of sort s&apos;, then s&apos; is a sub-
sort of sk—all the above propositions about solved
forms remain true.
</bodyText>
<page confidence="0.997685">
457
</page>
<sectionHeader confidence="0.999421" genericHeader="method">
4 Future work
</sectionHeader>
<bodyText confidence="0.969528462962963">
The above definitions serve an important theoret-
ical purpose: they formally underpin the use of
RMRS in practical systems. Next to the peace of
mind that comes with the use of a well-understood
formalism, we hope that the work reported here
will serve as a starting point for future research.
One direction to pursue from this paper is the
development of efficient solvers for RMRS. As a
first step, it would be interesting to define a practi-
cally useful fragment of RMRS with polynomial-
time satisfiability. Our definition is sufficiently
close to that of dominance constraints that we ex-
pect that it should be feasible to carry over the def-
inition of normal dominance constraints (Althaus
et al., 2003) to RMRS; neither the lexical ambigu-
ity of the node labels nor the separate specification
of predicates and arguments should make satisfia-
bility harder.
Furthermore, the above definition of RMRS pro-
vides new concepts which can help us phrase ques-
tions of practical grammar engineering in well-
defined formal terms. For instance, one crucial is-
sue in developing a hybrid system that combines
or compares the outputs of deep and shallow pro-
cessors is to determine whether the RMRSs pro-
duced by the two systems are compatible. In the
new formal terms, we can characterise compati-
bility of a more detailed RMRS ϕ (perhaps from a
deep grammar) and a less detailed RMRS ϕ! sim-
ply as entailment ϕ �= ϕ!. If entailment holds,
this tells us that all claims that ϕ! makes about the
semantic content of a sentence are consistent with
the claims that ϕ makes.
At this point, we cannot provide an efficient al-
gorithm for testing entailment of RMRS. However,
we propose the following novel syntactic charac-
terisation as a starting point for research along
those lines. We call an RMRS ϕ! an extension of
the RMRS ϕ if ϕ! contains all the EPs of ϕ and
D(ϕ!) Q D(ϕ).
Proposition 4. Let ϕ, ϕ! be two RMRSs. Then
ϕ �= ϕ! ifffor every solved form S of ϕ, there is a
solved form S! of ϕ! such that S is an extension of
S!.
Proof (sketch). “�” follows from Props. 1 and 2.
“�”: We construct a solved form for ϕ! by
choosing a solved form for ϕ and appropriate sub-
stitutions for mapping the variables of ϕ and ϕ!
onto each other, and removing all atoms using
variables that don’t occur in ϕ! . The hard part
is the proof that the result is a solved form of ϕ!;
this step involves proving that if ϕ �= ϕ! with the
same variable assignments, then all EPs in ϕ! also
occur in ϕ.
</bodyText>
<sectionHeader confidence="0.998188" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999970696969697">
In this paper, we motivated and defined RMRS—a
semantic framework that has been used to repre-
sent, compare, and combine semantic information
computed from deep and shallow parsers. RMRS
is designed to be maximally flexible on the type
of semantic information that can be left under-
specified, so that the semantic output of a shallow
parser needn’t over-determine or under-determine
the semantics that can be extracted from the shal-
low syntactic analysis. Our key contribution was
to lay the formal foundations for a formalism that
is emerging as a standard in robust semantic pro-
cessing.
Although we have not directly provided new
tools for modelling or processing language, we
believe that a cleanly defined model theory for
RMRS is a crucial prerequisite for the future de-
velopment of such tools; this strategy was highly
successful for dominance constraints (Althaus et
al., 2003). We hope that future research will build
upon this paper to develop efficient algorithms and
implementations for solving RMRSs, performing
inferences that enrich RMRSs from shallow analy-
ses with deeper information, and checking consis-
tency of RMRSs that were obtained from different
parsers.
Acknowledgments. We thank Ann Copestake,
Dan Flickinger, and Stefan Thater for extremely
fruitful discussions and the reviewers for their
comments. The work of Alexander Koller was
funded by a DFG Research Fellowship and the
Cluster of Excellence “Multimodal Computing
and Interaction”.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999682714285714">
S. Abney. 1996. Partial parsing via finite-state cas-
cades. In John Carroll, editor, Workshop on Robust
Parsing (ESSLLI-96), pages 8–15, Prague.
E. Althaus, D. Duchier, A. Koller, K. Mehlhorn,
J. Niehren, and S. Thiel. 2003. An efficient graph
algorithm for dominance constraints. J. Algorithms,
48:194–219.
</reference>
<page confidence="0.982925">
458
</page>
<reference confidence="0.999834018181818">
J. Bos, S. Clark, M. Steedman, J. Curran, and J. Hock-
enmaier. 2004. Wide coverage semantic representa-
tions from a CCG parser. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING 2004), Geneva, Switzerland.
E.J. Briscoe, J. Carroll, and R. Watson. 2006. The
second release of the rasp system. In Proceedings
of the COLING/ACL 2006 Interaction Presentation
Sessions, Sydney, Australia.
M. Butt, T. Holloway King, M. Ni˜no, and F. Segond.
1999. A Grammar Writer’s Cookbook. CSLI Publi-
cations.
S. Clark, M. Steedman, and J. Curran. 2004. Object
extraction and question parsing using CCG. In Pro-
ceedings from the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
111–118, Barcelona.
A. Copestake and D. Flickinger. 2000. An open-
source grammar development environment and en-
glish grammar using HPSG. In Proceedings of
the Second Conference on Language Resources and
Evaluation (LREC 2000), pages 591–600, Athens.
A. Copestake, D. Flickinger, I. Sag, and C. Pollard.
2005. Minimal recursion semantics: An introduc-
tion. Research on Language and Computation, 3(2–
3):281–332.
A. Copestake. 2003. Report on the design of RMRS.
Technical Report EU Deliverable for Project num-
ber IST-2001-37836, WP1a, Computer Laboratory,
University of Cambridge.
A. Copestake. 2007a. Applying robust semantics.
In Proceedings of the 10th Conference of the Pa-
cific Assocation for Computational Linguistics (PA-
CLING), pages 1–12, Melbourne. Invited talk.
A. Copestake. 2007b. Semantic composition with
(robust) minimal recursion semantics. In ACL-07
workshop on Deep Linguistic Processing, pages 73–
80, Prague.
D. Duchier and J. Niehren. 2000. Dominance con-
straints with set operators. In In Proceedings of the
First International Conference on Computational
Logic (CL2000), LNCS, pages 326–341. Springer.
M. Egg, A. Koller, and J. Niehren. 2001. The con-
straint language for lambda structures. Journal of
Logic, Language, and Information, 10:457–485.
A. Frank. 2004. Constraint-based RMRS construc-
tion from shallow grammars. In Proceedings of the
International Conference in Computational Linguis-
tics (COLING 2004), Geneva, Switzerland.
L. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to log-
ical form. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 678–687.
</reference>
<page confidence="0.999124">
459
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956477">
<title confidence="0.999933">A Logic of Semantic Representations for Shallow Parsing</title>
<author confidence="0.998243">Alexander Koller Alex Lascarides</author>
<affiliation confidence="0.999976">Saarland University University of Edinburgh</affiliation>
<address confidence="0.996202">Saarbr¨ucken, Germany Edinburgh, UK</address>
<email confidence="0.970382">koller@mmci.uni-saarland.dealex@inf.ed.ac.uk</email>
<abstract confidence="0.999167416666667">One way to construct semantic representations in a robust manner is to enhance shallow language processors with semantic components. Here, we provide a model theory for a semantic formalism that is designed for this, namely Robust Minimal Semantics We show a notion of entailment that allows it to form the basis for comparing the semantic output of different parses of varying depth.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Partial parsing via finite-state cascades. In</title>
<date>1996</date>
<booktitle>Workshop on Robust Parsing (ESSLLI-96),</booktitle>
<pages>8--15</pages>
<editor>John Carroll, editor,</editor>
<location>Prague.</location>
<contexts>
<context position="7079" citStr="Abney, 1996" startWordPosition="1190" endWordPosition="1191">ree in such a way that the predications of the nodes that labels and anchors denote are consistent with those in the EPs of (4)—e.g., l1 and a1 can map to the root of the first tree in Fig. 1, x1 to x, and the root label every q 1 is consistent with the EP predicate every q. There are of course many other trees (and thus, fully specific semantic representations such as (2)) that are described equally well by the RMRS (4); this is not surprising, given that the semantic output from the POS tagger is so incomplete. If we have information about subjects and objects from a chunk parser like Cass (Abney, 1996), we can represent it in a more detailed RMRS: (5) l1 : a1 : every q(x1), l41 : a41 : fat j(e&apos;), l42 : a42 : cat n(x3) l5 : a5 : chase v(e), ARG1(a5, x4), ARG2(a5, x5) l6 : a6 : some q(x6), l9 : a9 : dog n(x7) x3 = x4, x5 = x7 This introduces two new types of atoms. x3 = x4 means that x3 and x4 map to the same variable in any fully specific logical form; e.g., both to the variable x in Fig. 1. ARGZ(a, z) (and ARGZ(a, h)) _some_q_1 x A _every_q_1 _fat_j_1 e&apos; x _cat_n_1 x y _dog_n_1 y _chase_v_1 e x y _some_q_1 y _dog_n_2 _every_q_1 y x n _chase_v_1 _fat_j_1 e&apos; x _cat_n_2 x e x y 452 express tha</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>S. Abney. 1996. Partial parsing via finite-state cascades. In John Carroll, editor, Workshop on Robust Parsing (ESSLLI-96), pages 8–15, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Althaus</author>
<author>D Duchier</author>
<author>A Koller</author>
<author>K Mehlhorn</author>
<author>J Niehren</author>
<author>S Thiel</author>
</authors>
<title>An efficient graph algorithm for dominance constraints.</title>
<date>2003</date>
<journal>J. Algorithms,</journal>
<pages>48--194</pages>
<contexts>
<context position="20681" citStr="Althaus et al., 2003" startWordPosition="3788" endWordPosition="3791">symbols introduced by syntactic constructions such as appositives, noun-noun compounds and free adjuncts—we don’t change the set of models; we only restrict the set of models further and further towards the semantic representation we are trying to reconstruct. Furthermore, it has been shown in the literature that a dominance-constraint style semantics for underspecified representations gives us more room to _chase_v_1 _every_q_1 _some_q_1 x A fat_j_1 e. x _cat_n_1 y _dog_n_1 e... y _sleep_v_1 y _run_v_1 e x y x e.. x 455 manoeuvre when developing efficient solvers than an MRS-style semantics (Althaus et al., 2003). However, enumerating an infinite number of models is of course infeasible. For this reason, we will now transfer the concept of solved forms from dominance constraints to RMRS. An RMRS in solved form is guaranteed to be satisfiable, and thus each solved form represents an infinite class of models. However, each satisfiable RMRS has only a finite number of solved forms which partition the space of possible models into classes such that models within a class differ only in ‘irrelevant’ details. A solver can then enumerate the solved forms rather than all models. Intuitively, an RMRS in solved </context>
<context position="29530" citStr="Althaus et al., 2003" startWordPosition="5449" endWordPosition="5452">in the use of RMRS in practical systems. Next to the peace of mind that comes with the use of a well-understood formalism, we hope that the work reported here will serve as a starting point for future research. One direction to pursue from this paper is the development of efficient solvers for RMRS. As a first step, it would be interesting to define a practically useful fragment of RMRS with polynomialtime satisfiability. Our definition is sufficiently close to that of dominance constraints that we expect that it should be feasible to carry over the definition of normal dominance constraints (Althaus et al., 2003) to RMRS; neither the lexical ambiguity of the node labels nor the separate specification of predicates and arguments should make satisfiability harder. Furthermore, the above definition of RMRS provides new concepts which can help us phrase questions of practical grammar engineering in welldefined formal terms. For instance, one crucial issue in developing a hybrid system that combines or compares the outputs of deep and shallow processors is to determine whether the RMRSs produced by the two systems are compatible. In the new formal terms, we can characterise compatibility of a more detailed</context>
</contexts>
<marker>Althaus, Duchier, Koller, Mehlhorn, Niehren, Thiel, 2003</marker>
<rawString>E. Althaus, D. Duchier, A. Koller, K. Mehlhorn, J. Niehren, and S. Thiel. 2003. An efficient graph algorithm for dominance constraints. J. Algorithms, 48:194–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>S Clark</author>
<author>M Steedman</author>
<author>J Curran</author>
<author>J Hockenmaier</author>
</authors>
<title>Wide coverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING 2004),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1178" citStr="Bos et al., 2004" startWordPosition="173" endWordPosition="176">m the basis for comparing the semantic output of different parses of varying depth. 1 Introduction Representing semantics as a logical form that supports automated inference and model construction is vital for deeper language engineering tasks, such as dialogue systems. Logical forms can be obtained from hand-crafted deep grammars (Butt et al., 1999; Copestake and Flickinger, 2000) but this lacks robustness: not all words and constructions are covered and by design ill-formed phrases fail to parse. There has thus been a trend recently towards robust wide-coverage semantic construction (e.g., (Bos et al., 2004; Zettlemoyer and Collins, 2007)). But there are certain semantic phenomena that these robust approaches don’t capture reliably, including quantifier scope, optional arguments, and long-distance dependencies (for instance, Clark et al. (2004) report that the parser used by Bos et al. (2004) yields 63% accuracy on object extraction; e.g., the man that I met... ). Forcing a robust parser to make a decision about these phenomena can therefore be error-prone. Depending on the application, it may be preferable to give the parser the option to leave a semantic decision open when it’s not sufficientl</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>J. Bos, S. Clark, M. Steedman, J. Curran, and J. Hockenmaier. 2004. Wide coverage semantic representations from a CCG parser. In Proceedings of the International Conference on Computational Linguistics (COLING 2004), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second release of the rasp system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interaction Presentation Sessions,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2511" citStr="Briscoe et al., 2006" startWordPosition="381" endWordPosition="384">eous to the parser. In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make is to cast RMRS, for the first time, as a logic with a well-defined model theory. Previously, no such model theory existed, and so RMRS had to be used in a somewhat ad-hoc manner that l</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>E.J. Briscoe, J. Carroll, and R. Watson. 2006. The second release of the rasp system. In Proceedings of the COLING/ACL 2006 Interaction Presentation Sessions, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Butt</author>
<author>T Holloway King</author>
<author>M Ni˜no</author>
<author>F Segond</author>
</authors>
<title>A Grammar Writer’s Cookbook.</title>
<date>1999</date>
<publisher>CSLI Publications.</publisher>
<marker>Butt, King, Ni˜no, Segond, 1999</marker>
<rawString>M. Butt, T. Holloway King, M. Ni˜no, and F. Segond. 1999. A Grammar Writer’s Cookbook. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>M Steedman</author>
<author>J Curran</author>
</authors>
<title>Object extraction and question parsing using CCG.</title>
<date>2004</date>
<booktitle>In Proceedings from the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>111--118</pages>
<location>Barcelona.</location>
<contexts>
<context position="1420" citStr="Clark et al. (2004)" startWordPosition="206" endWordPosition="209">asks, such as dialogue systems. Logical forms can be obtained from hand-crafted deep grammars (Butt et al., 1999; Copestake and Flickinger, 2000) but this lacks robustness: not all words and constructions are covered and by design ill-formed phrases fail to parse. There has thus been a trend recently towards robust wide-coverage semantic construction (e.g., (Bos et al., 2004; Zettlemoyer and Collins, 2007)). But there are certain semantic phenomena that these robust approaches don’t capture reliably, including quantifier scope, optional arguments, and long-distance dependencies (for instance, Clark et al. (2004) report that the parser used by Bos et al. (2004) yields 63% accuracy on object extraction; e.g., the man that I met... ). Forcing a robust parser to make a decision about these phenomena can therefore be error-prone. Depending on the application, it may be preferable to give the parser the option to leave a semantic decision open when it’s not sufficiently informed—i.e., to compute a partial semantic representation and to complete it later, using information extraneous to the parser. In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal </context>
</contexts>
<marker>Clark, Steedman, Curran, 2004</marker>
<rawString>S. Clark, M. Steedman, and J. Curran. 2004. Object extraction and question parsing using CCG. In Proceedings from the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 111–118, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>An opensource grammar development environment and english grammar using HPSG.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>591--600</pages>
<location>Athens.</location>
<contexts>
<context position="946" citStr="Copestake and Flickinger, 2000" startWordPosition="136" endWordPosition="139">uage processors with semantic components. Here, we provide a model theory for a semantic formalism that is designed for this, namely Robust Minimal Recursion Semantics (RMRS). We show that RMRS supports a notion of entailment that allows it to form the basis for comparing the semantic output of different parses of varying depth. 1 Introduction Representing semantics as a logical form that supports automated inference and model construction is vital for deeper language engineering tasks, such as dialogue systems. Logical forms can be obtained from hand-crafted deep grammars (Butt et al., 1999; Copestake and Flickinger, 2000) but this lacks robustness: not all words and constructions are covered and by design ill-formed phrases fail to parse. There has thus been a trend recently towards robust wide-coverage semantic construction (e.g., (Bos et al., 2004; Zettlemoyer and Collins, 2007)). But there are certain semantic phenomena that these robust approaches don’t capture reliably, including quantifier scope, optional arguments, and long-distance dependencies (for instance, Clark et al. (2004) report that the parser used by Bos et al. (2004) yields 63% accuracy on object extraction; e.g., the man that I met... ). For</context>
<context position="2654" citStr="Copestake and Flickinger, 2000" startWordPosition="403" endWordPosition="406">ursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make is to cast RMRS, for the first time, as a logic with a well-defined model theory. Previously, no such model theory existed, and so RMRS had to be used in a somewhat ad-hoc manner that left open exactly what any given RMRS representation actually means. This has hindered practical progress, both in terms of understanding the re</context>
</contexts>
<marker>Copestake, Flickinger, 2000</marker>
<rawString>A. Copestake and D. Flickinger. 2000. An opensource grammar development environment and english grammar using HPSG. In Proceedings of the Second Conference on Language Resources and Evaluation (LREC 2000), pages 591–600, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>I Sag</author>
<author>C Pollard</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>3--281</pages>
<contexts>
<context position="2550" citStr="Copestake et al., 2005" startWordPosition="388" endWordPosition="391"> focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make is to cast RMRS, for the first time, as a logic with a well-defined model theory. Previously, no such model theory existed, and so RMRS had to be used in a somewhat ad-hoc manner that left open exactly what any given RMRS re</context>
<context position="5157" citStr="Copestake et al., 2005" startWordPosition="828" endWordPosition="831">chase v 1(e, x, y))) (3) some q 1(y, dog n 2(y), every q 1(x, fat j 1(e&apos;, x) ∧ cat n 2(x), chase v 1(e, x, y))) Now imagine trying to extract semantic information from the output of a part-of-speech (POS) tagger by using the word lemmas as lexical predicate symbols. Such a semantic representation is highly partial. It will use predicate symbols such as cat n, which might resolve to the predicate symbols cat n 1 or cat n 2 in the complete semantic representation. (Notice the different fonts for the ambiguous and unambiguous predicate symbols.) But most underspecification formalisms (e.g., MRS (Copestake et al., 2005) and CLLS (Egg et al., 2001)) are unable to represent semantic information that is as partial as what we get from a POS tagger because they cannot underspecify predicate-argument structure. RMRS (Copestake, 2007a) is designed to address this problem. In RMRS, the information we get from the POS tagger is as follows: (4) l1 : a1 : every q(x1), l41 : a41 : fat j(e&apos;), l42 : a42 : cat n(x3) l5 : a5 : chase v(e), l6 : a6 : some q(x6), l9 : a9 : dog n(x7) This RMRS expresses only that certain predications are present in the semantic representation— it doesn’t say anything about semantic scope, about</context>
<context position="25049" citStr="Copestake et al. (2005)" startWordPosition="4631" endWordPosition="4634">ved forms of ϕ is finite. 456 Let’s look at an example for all these definitions. All the RMRSs presented in Section 2 (replacing =Q by !*) are in solved form; this is least obvious for (6), but becomes clear once we notice that no label is on the right-hand side of two dominance atoms. However, the model constructed in the proof of Prop. 1 looks a bit like Fig. 2; both models are problematic in several ways and in particular contain an unbound variable y even though they also contains a quantifier that binds y. If we restrict the class of models to those in which such variables are bound (as Copestake et al. (2005) do), we can enforce that the quantifiers outscope their bound variables without changing models of the RMRS further—i.e., we add the atoms h3 !* l5 and h8!*l5. Fig. 2 is no longer a model for the extended RMRS, which in turn is no longer in solved form because the label l5 is on the right-hand side of two dominance atoms. Instead, it has the following two solved forms: (7) l1:a1: every q 1(x1), RSTR(a1, h2), BODY(a1, h3), l41:a41: fat j 1(e&apos;), ARG1(a41, x1), l41:a42: cat n 1(x1), l6:a6: some q 1(x6), RSTR(a6, h7), BODY(a6, h8), l9:a9: dog n 1(x6), l5:a5: chase v 1(e), ARG1(a5, x1), ARG2(a5, x</context>
<context position="26445" citStr="Copestake et al. (2005)" startWordPosition="4892" endWordPosition="4895">x6), RSTR(a6, h7), BODY(a6, h8), l9:a9: dog n 1(x6), l5:a5: chase v 1(e), ARG1(a5, x1), ARG2(a5, x6), h2 !* l41, h3 !* l5, h7 !* l9, h8 !* l1 Notice that we have eliminated all equalities by unifying the variable names, and we have fixed the relative scope of the two quantifiers. Each of these solved forms now stands for a separate class of models; for instance, the first model in Fig. 1 is a model of (7), whereas the second is a model of (8). 3.4 Extensions So far we have based the syntax and semantics of RMRS on the dominance relation from Egg et al. (2001) rather than the qeq relation from Copestake et al. (2005). This is partly because dominance is the weaker relation: If a dependency parser links a determiner to a noun and this noun to a verb, then we can use dominance but not qeq to represent that the predicate introduced by the verb is outscoped by the quantifier introduced by the determiner (see earlier discussion). However, it is very straightforward to extend the syntax and semantics of the language to include the qeq relation. This extension adds a new atom X =Q Y to Def. 1, and T, a, g, a will satisfy X =Q Y iff a(X) !* a(Y), each node on the path is a quantifier, and each step in the path go</context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 2005</marker>
<rawString>A. Copestake, D. Flickinger, I. Sag, and C. Pollard. 2005. Minimal recursion semantics: An introduction. Research on Language and Computation, 3(2– 3):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>Report on the design of RMRS.</title>
<date>2003</date>
<tech>Technical Report EU Deliverable for Project number IST-2001-37836, WP1a,</tech>
<institution>Computer Laboratory, University of Cambridge.</institution>
<contexts>
<context position="2325" citStr="Copestake (2003)" startWordPosition="351" endWordPosition="352">he option to leave a semantic decision open when it’s not sufficiently informed—i.e., to compute a partial semantic representation and to complete it later, using information extraneous to the parser. In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make</context>
<context position="10977" citStr="Copestake, 2003" startWordPosition="1939" endWordPosition="1940"> POS tagger, an RMRS symbol that’s output by a deep grammar is consistent with symbols that all have the same arity, because a deep grammar fully determines lexical subcategorisation. In summary, RMRS allows us to represent in a uniform way the (partial) semantics that can be extracted from a wide range of NLP tools. This is useful for hybrid systems which exploit shallower analyses when deeper parsing fails, or which try to match deeply parsed queries against shallow parses of large corpora; and in fact, RMRS is gaining popularity as a practical interchange format for exactly these purposes (Copestake, 2003). However, RMRS is still relatively ad-hoc in that its formal semantics is not defined; we don’t know, formally, what an RMRS means in terms of semantic representations like (2) and (3), and this hinders our ability to design efficient algorithms for processing RMRS. The purpose of this paper is to lay the groundwork for fixing this problem. 3 Robust Minimal Recursion Semantics We will now make the basic ideas from Section 2 precise. We will first define the syntax of the RMRS language; this is a notational variant of earlier definitions in the literature. We will then define a model theory fo</context>
<context position="12781" citStr="Copestake, 2003" startWordPosition="2289" endWordPosition="2290">formulas are defined as follows. Definition 1. An RMRS is a finite set cp of atoms of one of the following forms; S C_ N is a set of numbers that is either finite or N itself (throughout the paper, we assume 0 E N). A ::= X:Y :P |ARGS(X, v) |ARGS(X,Y ) |X !* Y |v1 = v2 |v1 =�v2 |X = Y |X =�Y |PEQ A node variable X is called a label iff cp contains an atom of the form X:Y :P or Y !* X; it is an anchor iff cp contains an atom of the form Y :X:P or ARGS(X, i); and it is a hole iff cp contains an atom of the form ARGS(Y, X) or X !*Y . Def. 1 combines similarities to earlier presentations of RMRS (Copestake, 2003; Copestake, 2007b) and to CLLS/dominance constraints (Egg et al., 2001). For the most part, our syntax generalises that of older versions of RMRS: We use ARG{Z} (with a singleton set S) instead of ARGZ and ARGN instead of ARG,,, and the EP l:a:P(v) (as in Section 2) is an abbreviation of {l:a:P, ARG{0}(a, v)}. Similarly, we don’t assume that labels, anchors, and holes are syntactically different objects; they receive their function from their positions in the formula. One major difference is that we use dominance (!*) rather than qeq; see Section 3.4 for a discussion. Compared to dominance co</context>
</contexts>
<marker>Copestake, 2003</marker>
<rawString>A. Copestake. 2003. Report on the design of RMRS. Technical Report EU Deliverable for Project number IST-2001-37836, WP1a, Computer Laboratory, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>Applying robust semantics.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Assocation for Computational Linguistics (PACLING),</booktitle>
<pages>1--12</pages>
<location>Melbourne.</location>
<note>Invited talk.</note>
<contexts>
<context position="2062" citStr="Copestake (2007" startWordPosition="315" endWordPosition="316">d by Bos et al. (2004) yields 63% accuracy on object extraction; e.g., the man that I met... ). Forcing a robust parser to make a decision about these phenomena can therefore be error-prone. Depending on the application, it may be preferable to give the parser the option to leave a semantic decision open when it’s not sufficiently informed—i.e., to compute a partial semantic representation and to complete it later, using information extraneous to the parser. In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are sp</context>
<context position="5368" citStr="Copestake, 2007" startWordPosition="865" endWordPosition="867">g the word lemmas as lexical predicate symbols. Such a semantic representation is highly partial. It will use predicate symbols such as cat n, which might resolve to the predicate symbols cat n 1 or cat n 2 in the complete semantic representation. (Notice the different fonts for the ambiguous and unambiguous predicate symbols.) But most underspecification formalisms (e.g., MRS (Copestake et al., 2005) and CLLS (Egg et al., 2001)) are unable to represent semantic information that is as partial as what we get from a POS tagger because they cannot underspecify predicate-argument structure. RMRS (Copestake, 2007a) is designed to address this problem. In RMRS, the information we get from the POS tagger is as follows: (4) l1 : a1 : every q(x1), l41 : a41 : fat j(e&apos;), l42 : a42 : cat n(x3) l5 : a5 : chase v(e), l6 : a6 : some q(x6), l9 : a9 : dog n(x7) This RMRS expresses only that certain predications are present in the semantic representation— it doesn’t say anything about semantic scope, about most arguments of the predicates (e.g., chase v(e) doesn’t say who chases whom), or about the coindexation of variables ( every q Figure 1: Semantic representations (2) and (3) as trees. binds the variable x1, </context>
<context position="12798" citStr="Copestake, 2007" startWordPosition="2291" endWordPosition="2292">ned as follows. Definition 1. An RMRS is a finite set cp of atoms of one of the following forms; S C_ N is a set of numbers that is either finite or N itself (throughout the paper, we assume 0 E N). A ::= X:Y :P |ARGS(X, v) |ARGS(X,Y ) |X !* Y |v1 = v2 |v1 =�v2 |X = Y |X =�Y |PEQ A node variable X is called a label iff cp contains an atom of the form X:Y :P or Y !* X; it is an anchor iff cp contains an atom of the form Y :X:P or ARGS(X, i); and it is a hole iff cp contains an atom of the form ARGS(Y, X) or X !*Y . Def. 1 combines similarities to earlier presentations of RMRS (Copestake, 2003; Copestake, 2007b) and to CLLS/dominance constraints (Egg et al., 2001). For the most part, our syntax generalises that of older versions of RMRS: We use ARG{Z} (with a singleton set S) instead of ARGZ and ARGN instead of ARG,,, and the EP l:a:P(v) (as in Section 2) is an abbreviation of {l:a:P, ARG{0}(a, v)}. Similarly, we don’t assume that labels, anchors, and holes are syntactically different objects; they receive their function from their positions in the formula. One major difference is that we use dominance (!*) rather than qeq; see Section 3.4 for a discussion. Compared to dominance constraints, the pr</context>
</contexts>
<marker>Copestake, 2007</marker>
<rawString>A. Copestake. 2007a. Applying robust semantics. In Proceedings of the 10th Conference of the Pacific Assocation for Computational Linguistics (PACLING), pages 1–12, Melbourne. Invited talk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>Semantic composition with (robust) minimal recursion semantics.</title>
<date>2007</date>
<booktitle>In ACL-07 workshop on Deep Linguistic Processing,</booktitle>
<pages>73--80</pages>
<location>Prague.</location>
<contexts>
<context position="2062" citStr="Copestake (2007" startWordPosition="315" endWordPosition="316">d by Bos et al. (2004) yields 63% accuracy on object extraction; e.g., the man that I met... ). Forcing a robust parser to make a decision about these phenomena can therefore be error-prone. Depending on the application, it may be preferable to give the parser the option to leave a semantic decision open when it’s not sufficiently informed—i.e., to compute a partial semantic representation and to complete it later, using information extraneous to the parser. In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are sp</context>
<context position="5368" citStr="Copestake, 2007" startWordPosition="865" endWordPosition="867">g the word lemmas as lexical predicate symbols. Such a semantic representation is highly partial. It will use predicate symbols such as cat n, which might resolve to the predicate symbols cat n 1 or cat n 2 in the complete semantic representation. (Notice the different fonts for the ambiguous and unambiguous predicate symbols.) But most underspecification formalisms (e.g., MRS (Copestake et al., 2005) and CLLS (Egg et al., 2001)) are unable to represent semantic information that is as partial as what we get from a POS tagger because they cannot underspecify predicate-argument structure. RMRS (Copestake, 2007a) is designed to address this problem. In RMRS, the information we get from the POS tagger is as follows: (4) l1 : a1 : every q(x1), l41 : a41 : fat j(e&apos;), l42 : a42 : cat n(x3) l5 : a5 : chase v(e), l6 : a6 : some q(x6), l9 : a9 : dog n(x7) This RMRS expresses only that certain predications are present in the semantic representation— it doesn’t say anything about semantic scope, about most arguments of the predicates (e.g., chase v(e) doesn’t say who chases whom), or about the coindexation of variables ( every q Figure 1: Semantic representations (2) and (3) as trees. binds the variable x1, </context>
<context position="12798" citStr="Copestake, 2007" startWordPosition="2291" endWordPosition="2292">ned as follows. Definition 1. An RMRS is a finite set cp of atoms of one of the following forms; S C_ N is a set of numbers that is either finite or N itself (throughout the paper, we assume 0 E N). A ::= X:Y :P |ARGS(X, v) |ARGS(X,Y ) |X !* Y |v1 = v2 |v1 =�v2 |X = Y |X =�Y |PEQ A node variable X is called a label iff cp contains an atom of the form X:Y :P or Y !* X; it is an anchor iff cp contains an atom of the form Y :X:P or ARGS(X, i); and it is a hole iff cp contains an atom of the form ARGS(Y, X) or X !*Y . Def. 1 combines similarities to earlier presentations of RMRS (Copestake, 2003; Copestake, 2007b) and to CLLS/dominance constraints (Egg et al., 2001). For the most part, our syntax generalises that of older versions of RMRS: We use ARG{Z} (with a singleton set S) instead of ARGZ and ARGN instead of ARG,,, and the EP l:a:P(v) (as in Section 2) is an abbreviation of {l:a:P, ARG{0}(a, v)}. Similarly, we don’t assume that labels, anchors, and holes are syntactically different objects; they receive their function from their positions in the formula. One major difference is that we use dominance (!*) rather than qeq; see Section 3.4 for a discussion. Compared to dominance constraints, the pr</context>
</contexts>
<marker>Copestake, 2007</marker>
<rawString>A. Copestake. 2007b. Semantic composition with (robust) minimal recursion semantics. In ACL-07 workshop on Deep Linguistic Processing, pages 73– 80, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duchier</author>
<author>J Niehren</author>
</authors>
<title>Dominance constraints with set operators.</title>
<date>2000</date>
<booktitle>In In Proceedings of the First International Conference on Computational Logic (CL2000), LNCS,</booktitle>
<pages>326--341</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="23760" citStr="Duchier and Niehren, 2000" startWordPosition="4386" endWordPosition="4389">ition 2. For every tuple (τ, α, g, σ) that satisfies some RMRS ϕ, there is a solved form ϕ! of ϕ such that (τ, α, g, σ) also satisfies ϕ!. Proof. We construct the substitution s from α and g. Then we add all dominance atoms that are satisfied by α and restrict the ARG atoms to those child indices that are actually used in τ. The result is in solved form because τ is a tree; it is a solved form of ϕ by construction. Proposition 3. Every RMRS ϕ has only a finite number of solved forms, up to renaming of variables. Proposition 1. Every RMRS in solved form is satisfiable. Proof (sketch; see also (Duchier and Niehren, 2000)). For each EP, we choose to label the anchor with the constructor p of sufficiently high arity whose existence we assumed; we determine the edges between an anchor and its children from the uniquely determined ARG atoms; plugging labels Proof. Up to renaming of variables, there is only a finite number of substitutions on the node and base variables of ϕ. Let s be such a substitution. This fixes the set of EPs of any solved form of ϕ that is based on s uniquely. There is only a finite set of choices for the subsets S! in condition 2 of Def. 7, and there is only a finite set of choices of new d</context>
</contexts>
<marker>Duchier, Niehren, 2000</marker>
<rawString>D. Duchier and J. Niehren. 2000. Dominance constraints with set operators. In In Proceedings of the First International Conference on Computational Logic (CL2000), LNCS, pages 326–341. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Egg</author>
<author>A Koller</author>
<author>J Niehren</author>
</authors>
<title>The constraint language for lambda structures.</title>
<date>2001</date>
<journal>Journal of Logic, Language, and Information,</journal>
<pages>10--457</pages>
<contexts>
<context position="2772" citStr="Egg et al., 2001" startWordPosition="424" endWordPosition="427">ate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make is to cast RMRS, for the first time, as a logic with a well-defined model theory. Previously, no such model theory existed, and so RMRS had to be used in a somewhat ad-hoc manner that left open exactly what any given RMRS representation actually means. This has hindered practical progress, both in terms of understanding the relationship of RMRS to other frameworks such as MRS and predicate logic and in terms of the development of efficient al</context>
<context position="5185" citStr="Egg et al., 2001" startWordPosition="834" endWordPosition="837">y, dog n 2(y), every q 1(x, fat j 1(e&apos;, x) ∧ cat n 2(x), chase v 1(e, x, y))) Now imagine trying to extract semantic information from the output of a part-of-speech (POS) tagger by using the word lemmas as lexical predicate symbols. Such a semantic representation is highly partial. It will use predicate symbols such as cat n, which might resolve to the predicate symbols cat n 1 or cat n 2 in the complete semantic representation. (Notice the different fonts for the ambiguous and unambiguous predicate symbols.) But most underspecification formalisms (e.g., MRS (Copestake et al., 2005) and CLLS (Egg et al., 2001)) are unable to represent semantic information that is as partial as what we get from a POS tagger because they cannot underspecify predicate-argument structure. RMRS (Copestake, 2007a) is designed to address this problem. In RMRS, the information we get from the POS tagger is as follows: (4) l1 : a1 : every q(x1), l41 : a41 : fat j(e&apos;), l42 : a42 : cat n(x3) l5 : a5 : chase v(e), l6 : a6 : some q(x6), l9 : a9 : dog n(x7) This RMRS expresses only that certain predications are present in the semantic representation— it doesn’t say anything about semantic scope, about most arguments of the predi</context>
<context position="11698" citStr="Egg et al., 2001" startWordPosition="2065" endWordPosition="2068">ormally, what an RMRS means in terms of semantic representations like (2) and (3), and this hinders our ability to design efficient algorithms for processing RMRS. The purpose of this paper is to lay the groundwork for fixing this problem. 3 Robust Minimal Recursion Semantics We will now make the basic ideas from Section 2 precise. We will first define the syntax of the RMRS language; this is a notational variant of earlier definitions in the literature. We will then define a model theory for our version of RMRS, and conclude this section by carrying over the notion of solved forms from CLLS (Egg et al., 2001). 3.1 RMRS Syntax We define RMRS syntax in the style of CLLS (Egg et al., 2001). We assume an infinite set of node variables NVar = {X, Y, X1,...}, used as labels, anchors, and holes; the distinction between these will come from their position in the formulas. We also assume an infinite set of base variables BVar, consisting of individual variables {x, x1, y,...} and event variables {e1, ...}, and a vocabulary of 453 predicate symbols Pred = {P, Q, P1, ...}. RMRS formulas are defined as follows. Definition 1. An RMRS is a finite set cp of atoms of one of the following forms; S C_ N is a set of</context>
<context position="15081" citStr="Egg et al., 2001" startWordPosition="2692" endWordPosition="2695">., chase v 1 is of arity 3. Other base languages may require a different signature E and/or a different mapping between formulas and trees; the only strict requirement we make is that the signature contains a binary constructor ∧ to represent conjunction. We write EZ and E&gt;Z for the set of all constructors in E with arity i and at least i, respectively. We will follow the typographical convention that non-logical symbols in E are written in sans-serif, as opposed to the RMRS predicate symbols like cat n and cat n 1. The models of RMRS are then defined to be finite constructor trees (see also (Egg et al., 2001)): Definition 2. A finite constructor tree T is a function T : D —* E such that D is a tree domain (i.e., a subset of N* which is closed under prefix and left sibling) and the number of children of each node u E D is equal to the arity of T(u). We write D(T) for the tree domain of a constructor tree T, and further define the following relations between nodes in a finite constructor tree: Definition 3. u !* v (dominance) iff u is a prefix of v, i.e. the node u is equal to or above the node v in the tree. u !* � v iff u !* v, and all symbols on the path from u to v (not including v) are ∧. The s</context>
<context position="26387" citStr="Egg et al. (2001)" startWordPosition="4882" endWordPosition="4885">RG1(a41, x1), l41:a42: cat n 1(x1), l6:a6: some q 1(x6), RSTR(a6, h7), BODY(a6, h8), l9:a9: dog n 1(x6), l5:a5: chase v 1(e), ARG1(a5, x1), ARG2(a5, x6), h2 !* l41, h3 !* l5, h7 !* l9, h8 !* l1 Notice that we have eliminated all equalities by unifying the variable names, and we have fixed the relative scope of the two quantifiers. Each of these solved forms now stands for a separate class of models; for instance, the first model in Fig. 1 is a model of (7), whereas the second is a model of (8). 3.4 Extensions So far we have based the syntax and semantics of RMRS on the dominance relation from Egg et al. (2001) rather than the qeq relation from Copestake et al. (2005). This is partly because dominance is the weaker relation: If a dependency parser links a determiner to a noun and this noun to a verb, then we can use dominance but not qeq to represent that the predicate introduced by the verb is outscoped by the quantifier introduced by the determiner (see earlier discussion). However, it is very straightforward to extend the syntax and semantics of the language to include the qeq relation. This extension adds a new atom X =Q Y to Def. 1, and T, a, g, a will satisfy X =Q Y iff a(X) !* a(Y), each node</context>
</contexts>
<marker>Egg, Koller, Niehren, 2001</marker>
<rawString>M. Egg, A. Koller, and J. Niehren. 2001. The constraint language for lambda structures. Journal of Logic, Language, and Information, 10:457–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Frank</author>
</authors>
<title>Constraint-based RMRS construction from shallow grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference in Computational Linguistics (COLING 2004),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2342" citStr="Frank (2004)" startWordPosition="354" endWordPosition="355">semantic decision open when it’s not sufficiently informed—i.e., to compute a partial semantic representation and to complete it later, using information extraneous to the parser. In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics (RMRS, Copestake (2007a)). RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure. It is an emerging standard for representing partial semantics, and has been applied in several implemented systems. For instance, Copestake (2003) and Frank (2004) use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP (Briscoe et al., 2006). MRS analyses (Copestake et al., 2005) derived from deep grammars, such as the English Resource Grammar (ERG, (Copestake and Flickinger, 2000)) are special cases of RMRS. But RMRS, unlike MRS and related formalisms like dominance constraints (Egg et al., 2001), is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation. The key contribution we make is to cast RMRS,</context>
</contexts>
<marker>Frank, 2004</marker>
<rawString>A. Frank. 2004. Constraint-based RMRS construction from shallow grammars. In Proceedings of the International Conference in Computational Linguistics (COLING 2004), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>678--687</pages>
<contexts>
<context position="1210" citStr="Zettlemoyer and Collins, 2007" startWordPosition="177" endWordPosition="180">mparing the semantic output of different parses of varying depth. 1 Introduction Representing semantics as a logical form that supports automated inference and model construction is vital for deeper language engineering tasks, such as dialogue systems. Logical forms can be obtained from hand-crafted deep grammars (Butt et al., 1999; Copestake and Flickinger, 2000) but this lacks robustness: not all words and constructions are covered and by design ill-formed phrases fail to parse. There has thus been a trend recently towards robust wide-coverage semantic construction (e.g., (Bos et al., 2004; Zettlemoyer and Collins, 2007)). But there are certain semantic phenomena that these robust approaches don’t capture reliably, including quantifier scope, optional arguments, and long-distance dependencies (for instance, Clark et al. (2004) report that the parser used by Bos et al. (2004) yields 63% accuracy on object extraction; e.g., the man that I met... ). Forcing a robust parser to make a decision about these phenomena can therefore be error-prone. Depending on the application, it may be preferable to give the parser the option to leave a semantic decision open when it’s not sufficiently informed—i.e., to compute a pa</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>L. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>