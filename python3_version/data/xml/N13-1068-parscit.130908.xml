<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001233">
<title confidence="0.849428">
Estimating effect size across datasets
</title>
<author confidence="0.987872">
Anders Søgaard
</author>
<affiliation confidence="0.998082">
Center for Language Technology
University of Copenhagen
</affiliation>
<email confidence="0.99184">
soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.996572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.991458619047619">
Most NLP tools are applied to text that is dif-
ferent from the kind of text they were eval-
uated on. Common evaluation practice pre-
scribes significance testing across data points
in available test data, but typically we only
have a single test sample. This short paper
argues that in order to assess the robustness
of NLP tools we need to evaluate them on
diverse samples, and we consider the prob-
lem of finding the most appropriate way to es-
timate the true effect size across datasets of
our systems over their baselines. We apply
meta-analysis and show experimentally – by
comparing estimated error reduction over ob-
served error reduction on held-out datasets –
that this method is significantly more predic-
tive of success than the usual practice of using
macro- or micro-averages. Finally, we present
a new parametric meta-analysis based on non-
standard assumptions that seems superior to
standard parametric meta-analysis.
</bodyText>
<sectionHeader confidence="0.99892" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999683111111111">
NLP tools and online services such as the Stanford
Parser or Google Translate are used for a wide va-
riety of purposes and therefore also on very differ-
ent kinds of data. Some use the Stanford Parser
to parse literature (van Cranenburgh, 2012), while
others use it for processing social media content
(Brown, 2011). The parser, however, was not neces-
sarily evaluated on literature or social media content
during development. Still, users typically expect
reasonable performance on any natural language in-
put. This paper asks what we as developers can do
to estimate the effect of a change to our system – not
on the labeled test data that happens to be available
to us, but on future, still unseen datasets provided by
our end users.
The usual practice in NLP is to evaluate a sys-
tem on a small sample of held-out labeled data.
The observed effect size on this sample is then val-
idated by significance testing across data points,
testing whether the observed difference in perfor-
mance means is likely to be due to mere chance.
The preferred significance test is probably the non-
parametric paired bootstrap (Efron and Tibshirani,
1993; Berg-Kirkpatrick et al., 2012), but many re-
searchers also resort to Student’s t-test for depen-
dent means relying on the assumption that their met-
ric scores are normally distributed.
Such significance tests tell us nothing about how
likely our change to our system is to lead to improve-
ments on new datasets. The significance tests all rely
on the assumption that our datapoints are sampled
i.i.d. at random. The significance tests only tell us
how likely it is that the observed difference in per-
formance means would change if we sampled a big-
ger test sample the same way we sampled the one
we have available to us right now.
In standard machine learning papers a similar sit-
uation arises. If we are developing a new percep-
tron learning algorithm, for example, we are inter-
ested in how likely the new learning algorithm is to
perform better than other perceptron learning algo-
rithms across datasets, and we may for that reason
evaluate it on a large set of repository datasets.
Demsar (2006) presents motivation for using non-
parametric methods such as the Wilcoxon signed
</bodyText>
<page confidence="0.973683">
607
</page>
<subsectionHeader confidence="0.294521">
Proceedings of NAACL-HLT 2013, pages 607–611,
</subsectionHeader>
<bodyText confidence="0.990498263157895">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
rank test to estimate significance across datasets.
The t-test is based on means, and typically results
across datasets are not commensurable. The t-
test is also extremely sentitive to outliers. Notice
also that typically we do not have enough datasets
to do paired bootstrapping (van den Noortgate and
Onghena, 2005).
In this paper we will assume that the Wilcoxon
signed rank test provides a reasonable estimate of
the significance of an observed difference in perfor-
mance means across datasets, or of the significance
of observed error reductions, but note that this still
depends on the assumption that datasets are sampled
i.i.d. at random. More importantly, a non-parametric
test across data sets does not provide an actual esti-
mate of the effect size. Estimating effect size is im-
portant, e.g. when there is a trade-off between per-
formance gains and computational efficiency.
In evaluations across datasets in NLP we typically
use the macro-average as an estimate of effect size,
but in other fields such as psychology or medicine it
is more common to use a weighted mean obtained
using what is known as the fixed effects model or
the random effects model for meta-analysis.
The experiments reported on in this paper fo-
cus on estimating error reduction and show that
meta-analysis is generally superior to macro- and
micro-average in terms of predicting future error re-
ductions. Parametric meta-analysis, however, over-
parameterizes the distribution of error reductions,
leading to some instability. While meta-analysis is
generally superior to macro-average, it is sometimes
off by a large margin. We therefore introduce a new
parametric meta-analysis that seems better suited to
predicting error reductions. In our experiments test
set sizes are balanced, so micro-averages will be
near-identical to macro-averages.
</bodyText>
<sectionHeader confidence="0.736732" genericHeader="introduction">
2 Meta-analysis
</sectionHeader>
<bodyText confidence="0.999945785714286">
Meta-analysis is the statistical analysis of the ef-
fect sizes of several studies and is very popular
in fields such as psychology or medicine. Meta-
analysis has not been applied very often to NLP.
In NLP most people work on applying new meth-
ods to old datasets, and meta-analysis is designed
to analyze series of studies applying old methods to
new datasets, e.g. running the same experiments on
new subjects. However, meta-analysis is applicable
to experiments with multiple datasets.
In psychology or medicine you often see stud-
ies running similar experiments on different sam-
ples with very different results. Meta-analysis stems
from the observation that if we want to estimate an
effect from a large set of studies, the average ef-
fect across all the studies will put too much weight
on results obtained on small datasets in which you
typically see more variance. The most popular ap-
proaches to meta-analysis are the fixed effects and
the random effects model. The fixed effects model is
applicable when you assume a true effect size (esti-
mated by the individual studies). If you cannot make
that assumption because the studies may differ in
various aspects, leading the within-study estimates
to be estimates of slightly different effect sizes, you
need to use the random effects model. Both ap-
proaches to meta-analysis are parametric and rely on
the effect sizes to be normally distributed.
</bodyText>
<subsectionHeader confidence="0.933047">
2.1 Fixed effects model
</subsectionHeader>
<bodyText confidence="0.99790525">
In the fixed effects model we weight the effect sizes
T1, ... , TM – or error reductions, in our case – by
the inverse of the variance vi in the study, i.e. wi =
1 . The combined effect size T is then:
</bodyText>
<equation confidence="0.942743555555555">
vi
EMi�1wiTi
�EM
i&gt;1 wi
The variance of the combined effect is now:
1
v =
EM
i&gt;1 wi
</equation>
<bodyText confidence="0.9821435">
and the 95% confidence interval is then T� f
1.96√v.
</bodyText>
<subsectionHeader confidence="0.989094">
2.2 Random effects model
</subsectionHeader>
<bodyText confidence="0.961929">
In the random effects model we replace the variance
vi with the variance plus between-studies variance
,r2:
</bodyText>
<equation confidence="0.990906222222222">
k i�12 (Ek
wiTi)2
wiT −
i&gt; 1 i Eki&gt;1 wi
2
k Ei&gt;1 wi
i&gt;
E1 wi −
Ek
</equation>
<bodyText confidence="0.8743615">
with df = N − 1, except all negative values are
replaced by 0.
</bodyText>
<equation confidence="0.8274326">
T�=
72 =
df
(1)
i&gt;1 wi
</equation>
<page confidence="0.850862">
608
</page>
<figure confidence="0.9897188">
⊤
comp rec sci talk other
sys others sport vehicles
politics religion
(a) (b) (c) (d) (e) (f) (g) (h)
</figure>
<figureCaption confidence="0.968412">
Figure 1: Hierarchical structure of 20 Newsgroups. (a) IBM, MAC, (b) GRAPHICS, MS-WINDOWS, X-WINDOWS,
(c) BASEBALL, HOCKEY, (d) AUTOS, MOTORCYCLES, (e) CRYPTOGRAPHY, ELECTRONICS, MEDICINE, SPACE,
(f) GUNS, MIDEAST, MISCELLANEOUS, (g) ATHEISM, CHRISTIANITY, MISCELLANEOUS, (h) FORSALE
</figureCaption>
<table confidence="0.9780621">
macro-av fixed random gumbel
k = 5
err. -0.1656 -0.0350 -0.0428 -0.0400
p-value - &lt; 0.001 &lt; 0.001 &lt; 0.001
k = 10
err. -0.1402 -0.0329 -0.0413 -0.0359
p-value - &lt; 0.001 &lt; 0.001 &lt; 0.001
k = 15
err. -0.0809 -0.0799 -0.0804 -0.0704
p-value - &lt; 0.001 &lt; 0.001 &lt; 0.001
</table>
<figureCaption confidence="0.995783">
Figure 2: Using macro-average and meta-analysis to pre-
dict error reductions on document classification datasets
based on k observations. The scores are averages across
20 experiments. The p-values were computed using
Wilcoxon signed rank tests.
</figureCaption>
<bodyText confidence="0.999970588235294">
The random effects model is obviously more con-
servative in its confidence intervals, and often we
will not be able to obtain significance across datasets
using a random effects model. If, for example, we
apply a fixed effects model to test whether Bernoulli
naive Bayes (NB) fairs better than a perceptron (P)
model on 25 randomly extracted cross-domain doc-
ument classification problem instances from the 20
Newsgroups dataset (see Sect. 4), the 95% confi-
dence interval is [3.9%,5.2%]. The weighted mean
is 4.6% (macro-average 3.9%). Using a random
effects model on the same 25 datasets, the 95%
confidence interval becomes [−6.5%,6.6%]. The
weighted mean estimate is also slighly different
from that of a fixed effects model. The first question
we ask is which of these models provides the best es-
timate of effect size as observed on future datasets?
</bodyText>
<subsectionHeader confidence="0.992666">
2.3 The error reductions distribution
</subsectionHeader>
<bodyText confidence="0.999819068965517">
Both the fixed effects and the random effects model
assume that effect sizes are normally distributed. We
can apply Darling-Anderson tests to test whether er-
ror reductions in 20 Newsgroups are in fact normally
distributed. Even a small sample of ten 20 News-
groups datasets provides enough evidence to reject
the hypothesis that error reductions (of NB over
P) are normally distributed. The Darling-Anderson
tests consistently tell us that the chance that our sam-
ple distribtutions of error reductions are normally
distributed is below 1%. The over-paramaterization
means that the estimates we get are unstable. While
both models are superior to macro-average esti-
mates, they may provide ’far-off’ estimates.
Using Darling-Anderson tests we could also re-
ject the hypothesis that error reductions were lo-
gistically distributed, but we did not find evidence
for rejecting the hypothesis that error reductions are
Gumbel-distributed.1 Gumbel distributions are used
to model error distributions in the latent variable for-
mulation of multinomial logit regression. A para-
metric meta-analysis model based on the assumption
that error reductions are Gumbel distributed is an in-
teresting alternative to non-parametric meta-analysis
(Hedges and Olkin, 1984; van den Noortgate and
Onghena, 2005), since there seems to be little con-
sensus in the literature about the best way to ap-
proach non-parametric meta-analysis.
Gumbel distributions take the following form:
</bodyText>
<equation confidence="0.506397">
1 ez−e
Q
</equation>
<bodyText confidence="0.86191475">
where z = &amp;quot; with α the location, and Q the
13
scale. We fit a Gumbel distribution to our weighted
error reductions (wiTi) and compute the combined
</bodyText>
<footnote confidence="0.872653">
1Abidin et al. (2012) has shown that Darling-Anderson is
superior to other goodness-of-fit tests for Gumbel distributions.
</footnote>
<page confidence="0.8321555">
−z
609
</page>
<table confidence="0.9995717">
macro-av fixed random gumbel
k = 5
err. 0.0531 0.0525 0.0526 0.0489
p-value - — 0.98 — 0.98 — 0.79
k = 7
err. 0.0928 0.0852 0.0852 0.0858
p-value - &lt; 0.001 &lt; 0.001 &lt; 0.001
k = 9
err. 0.0587 0.05743 0.05743 0.0532
p-value - — 0.68 — 0.68 — 0.13
</table>
<figureCaption confidence="0.876590666666667">
Figure 3: Using macro-average and meta-analysis to pre-
dict error reductions in cross-lingual dependency parsing.
See text for details.
</figureCaption>
<equation confidence="0.946417">
effect
_ α + 0.577210
T ��
1 z�1 wz
�
</equation>
<bodyText confidence="0.99953">
where 0.57721 is the Euler-Mascheroni constant,
and the variance of the combined effect v = 6 02.
</bodyText>
<sectionHeader confidence="0.992441" genericHeader="method">
3 Experiments in document classification
</sectionHeader>
<subsectionHeader confidence="0.696463">
and dependency parsing
</subsectionHeader>
<bodyText confidence="0.999989416666667">
Our first experiment makes use of the 20 News-
groups document classification dataset.2 The top-
ics in 20 Newsgroups are hierarchically structured,
which enables us to extract a large set of binary
classification problems with considerable bias be-
tween source and target data (Chen et al., 2009;
Sun et al., 2011). See the hierarchy in Figure 1.
We extract 20 high-level binary classification prob-
lems by considering all pairs of top-level cate-
gories, e.g. COMPUTERS-RECREATIVE (comp-rec).
For each of these 20 problems, we have differ-
ent possible datasets, e.g. IBM-BASEBALL, MAC-
MOTORCYCLES, etc. A problem instance takes
training and test data from two different datasets be-
long to the same high-level problem. For exam-
ple a problem instance could be learning to dis-
tinguish articles about Macintosh and motorcycles
MAC-MOTORCYCLES (evaluated on the 20 News-
groups test section) using labeled data from IBM-
BASEBALL (the training section). In total we have
288 available problem instances in the 20 News-
groups dataset.
In our first experiment we are interested in pre-
dicting the error reductions of a naive Bayes learner
</bodyText>
<footnote confidence="0.827929">
2http://people.csail.mit.edu/jrennie/20Newsgroups/
</footnote>
<bodyText confidence="0.999882375">
over a perceptron model. We use publicly available
implementations with default parameters.3 In each
experiment we randomly select k datasets and es-
timate the true effect size using macro-average, a
fixed effects model, a random effects model, and
a corrected random effects model. In order to es-
timate the within-study variance we take 50 paired
bootstrap samples of the system outputs. We evalu-
ate our estimates against the observed average effect
across 5 new randomly extracted datasets. For each
k we repeat the experiment 20 times and report aver-
age error. We vary k to see how many observations
are needed for our estimates to be reliable.
The results are presented in Figure 2. We note that
meta-analysis provides much better estimates than
macro-averages across the board. Our parametric
meta-analysis based on the assumption that error re-
ductions are Gumbel distributed performs best with
more observations.
Our second experiment repeats the same proce-
dure using available data from cross-lingual depen-
dency parsing. We use the submitted results by par-
ticipants in the CoNLL-X shared task (Buchholz and
Marsi, 2006) and try to predict the error reduction of
one system over another given k many observations.
Given that we only have 12 submissions per system
we use k E 15, 7, 9} randomly extracted datasets
for observations and test on another five randomly
extracted datasets. While results (Figure 3) are only
statistically significant with k = 7, we see that meta-
analysis estimates effect size across data sets better
than macro-average in all cases.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999932375">
We have argued that evaluation across datasets is
important for developing robust NLP tools, and
that meta-analysis can provide better estimates
of effect size across datasets than macro-average.
We also noted that parametric meta-analysis over-
parameterizes error reduction distributions and sug-
gested a new parametric method for estimating ef-
fect size across datasets.
</bodyText>
<sectionHeader confidence="0.999647" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.9098665">
Anders Søgaard is funded by the ERC Starting Grant
LOWLANDS No. 313695.
</reference>
<footnote confidence="0.791979">
3http://scikit-learn.org/stable/
</footnote>
<page confidence="0.996483">
610
</page>
<sectionHeader confidence="0.993707" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999017342857143">
Nahdiya Abidin, Mohd Adam, and Habshah Midi. 2012.
The goodness-of-fit test for Gumbel distribution: a
comparative study. MATEMATIKA, 28(1):35–48.
Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.
2012. An empirical investigation of statistical signifi-
cance in nlp. In EMNLP.
Gregory Brown. 2011. An error analysis of relation ex-
traction in social media documents. In ACL.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
CoNLL.
Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.
2009. Extracting discriminative concepts for domain
adaptation in text mining. In KDD.
Janez Demsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1–30.
Bradley Efron and Robert Tibshirani. 1993. An introduc-
tion to the bootstrap. Chapman &amp; Hall, Boca Raton,
FL.
Larry Hedges and Ingram Olkin. 1984. Nonparametric
estimators of effect size in meta-analysis. Psychologi-
cal Bulletin, 96:573–580.
Qian Sun, Rita Chattopadhyay, Sethuraman Pan-
chanathan, and Jieping Ye. 2011. Two-stage weight-
ing framework for multi-source domain adaptation. In
NIPS.
Andreas van Cranenburgh. 2012. Literary author-
ship attribution with phrase-structure fragments. In
Workshop on Computational Linguistics for Litera-
ture, NAACL.
Wim van den Noortgate and Patrick Onghena. 2005.
Parametric and nonparametric bootstrap methods for
meta-analysis. Behavior Research Methods, 37:11–
22.
</reference>
<page confidence="0.997976">
611
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.870173">
<title confidence="0.996759">Estimating effect size across datasets</title>
<author confidence="0.947539">Anders</author>
<affiliation confidence="0.9983035">Center for Language University of</affiliation>
<email confidence="0.93995">soegaard@hum.ku.dk</email>
<abstract confidence="0.998991090909091">Most NLP tools are applied to text that is different from the kind of text they were evaluated on. Common evaluation practice prescribes significance testing across data points in available test data, but typically we only have a single test sample. This short paper argues that in order to assess the robustness of NLP tools we need to evaluate them on diverse samples, and we consider the problem of finding the most appropriate way to estimate the true effect size across datasets of our systems over their baselines. We apply show experimentally – by comparing estimated error reduction over observed error reduction on held-out datasets – that this method is significantly more predictive of success than the usual practice of using macroor micro-averages. Finally, we present a new parametric meta-analysis based on nonstandard assumptions that seems superior to standard parametric meta-analysis.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Anders Søgaard is funded by the ERC Starting Grant LOWLANDS</title>
<tech>No. 313695.</tech>
<marker></marker>
<rawString>Anders Søgaard is funded by the ERC Starting Grant LOWLANDS No. 313695.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nahdiya Abidin</author>
<author>Mohd Adam</author>
<author>Habshah Midi</author>
</authors>
<title>The goodness-of-fit test for Gumbel distribution: a comparative study.</title>
<date>2012</date>
<journal>MATEMATIKA,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="10713" citStr="Abidin et al. (2012)" startWordPosition="1756" endWordPosition="1759">able formulation of multinomial logit regression. A parametric meta-analysis model based on the assumption that error reductions are Gumbel distributed is an interesting alternative to non-parametric meta-analysis (Hedges and Olkin, 1984; van den Noortgate and Onghena, 2005), since there seems to be little consensus in the literature about the best way to approach non-parametric meta-analysis. Gumbel distributions take the following form: 1 ez−e Q where z = &amp;quot; with α the location, and Q the 13 scale. We fit a Gumbel distribution to our weighted error reductions (wiTi) and compute the combined 1Abidin et al. (2012) has shown that Darling-Anderson is superior to other goodness-of-fit tests for Gumbel distributions. −z 609 macro-av fixed random gumbel k = 5 err. 0.0531 0.0525 0.0526 0.0489 p-value - — 0.98 — 0.98 — 0.79 k = 7 err. 0.0928 0.0852 0.0852 0.0858 p-value - &lt; 0.001 &lt; 0.001 &lt; 0.001 k = 9 err. 0.0587 0.05743 0.05743 0.0532 p-value - — 0.68 — 0.68 — 0.13 Figure 3: Using macro-average and meta-analysis to predict error reductions in cross-lingual dependency parsing. See text for details. effect _ α + 0.577210 T �� 1 z�1 wz � where 0.57721 is the Euler-Mascheroni constant, and the variance of the co</context>
</contexts>
<marker>Abidin, Adam, Midi, 2012</marker>
<rawString>Nahdiya Abidin, Mohd Adam, and Habshah Midi. 2012. The goodness-of-fit test for Gumbel distribution: a comparative study. MATEMATIKA, 28(1):35–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>An empirical investigation of statistical significance in nlp.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2241" citStr="Berg-Kirkpatrick et al., 2012" startWordPosition="364" endWordPosition="367"> developers can do to estimate the effect of a change to our system – not on the labeled test data that happens to be available to us, but on future, still unseen datasets provided by our end users. The usual practice in NLP is to evaluate a system on a small sample of held-out labeled data. The observed effect size on this sample is then validated by significance testing across data points, testing whether the observed difference in performance means is likely to be due to mere chance. The preferred significance test is probably the nonparametric paired bootstrap (Efron and Tibshirani, 1993; Berg-Kirkpatrick et al., 2012), but many researchers also resort to Student’s t-test for dependent means relying on the assumption that their metric scores are normally distributed. Such significance tests tell us nothing about how likely our change to our system is to lead to improvements on new datasets. The significance tests all rely on the assumption that our datapoints are sampled i.i.d. at random. The significance tests only tell us how likely it is that the observed difference in performance means would change if we sampled a bigger test sample the same way we sampled the one we have available to us right now. In s</context>
</contexts>
<marker>Berg-Kirkpatrick, Burkett, Klein, 2012</marker>
<rawString>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical significance in nlp. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Brown</author>
</authors>
<title>An error analysis of relation extraction in social media documents.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1390" citStr="Brown, 2011" startWordPosition="222" endWordPosition="223">n held-out datasets – that this method is significantly more predictive of success than the usual practice of using macro- or micro-averages. Finally, we present a new parametric meta-analysis based on nonstandard assumptions that seems superior to standard parametric meta-analysis. 1 Introduction NLP tools and online services such as the Stanford Parser or Google Translate are used for a wide variety of purposes and therefore also on very different kinds of data. Some use the Stanford Parser to parse literature (van Cranenburgh, 2012), while others use it for processing social media content (Brown, 2011). The parser, however, was not necessarily evaluated on literature or social media content during development. Still, users typically expect reasonable performance on any natural language input. This paper asks what we as developers can do to estimate the effect of a change to our system – not on the labeled test data that happens to be available to us, but on future, still unseen datasets provided by our end users. The usual practice in NLP is to evaluate a system on a small sample of held-out labeled data. The observed effect size on this sample is then validated by significance testing acro</context>
</contexts>
<marker>Brown, 2011</marker>
<rawString>Gregory Brown. 2011. An error analysis of relation extraction in social media documents. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X Shared Task on Multilingual Dependency Parsing. In CoNLL.</title>
<date>2006</date>
<contexts>
<context position="13695" citStr="Buchholz and Marsi, 2006" startWordPosition="2237" endWordPosition="2240"> k we repeat the experiment 20 times and report average error. We vary k to see how many observations are needed for our estimates to be reliable. The results are presented in Figure 2. We note that meta-analysis provides much better estimates than macro-averages across the board. Our parametric meta-analysis based on the assumption that error reductions are Gumbel distributed performs best with more observations. Our second experiment repeats the same procedure using available data from cross-lingual dependency parsing. We use the submitted results by participants in the CoNLL-X shared task (Buchholz and Marsi, 2006) and try to predict the error reduction of one system over another given k many observations. Given that we only have 12 submissions per system we use k E 15, 7, 9} randomly extracted datasets for observations and test on another five randomly extracted datasets. While results (Figure 3) are only statistically significant with k = 7, we see that metaanalysis estimates effect size across data sets better than macro-average in all cases. 4 Conclusions We have argued that evaluation across datasets is important for developing robust NLP tools, and that meta-analysis can provide better estimates o</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Chen</author>
<author>Wai Lam</author>
<author>Ivor Tsang</author>
<author>Tak-Lam Wong</author>
</authors>
<title>Extracting discriminative concepts for domain adaptation in text mining.</title>
<date>2009</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="11692" citStr="Chen et al., 2009" startWordPosition="1925" endWordPosition="1928"> Using macro-average and meta-analysis to predict error reductions in cross-lingual dependency parsing. See text for details. effect _ α + 0.577210 T �� 1 z�1 wz � where 0.57721 is the Euler-Mascheroni constant, and the variance of the combined effect v = 6 02. 3 Experiments in document classification and dependency parsing Our first experiment makes use of the 20 Newsgroups document classification dataset.2 The topics in 20 Newsgroups are hierarchically structured, which enables us to extract a large set of binary classification problems with considerable bias between source and target data (Chen et al., 2009; Sun et al., 2011). See the hierarchy in Figure 1. We extract 20 high-level binary classification problems by considering all pairs of top-level categories, e.g. COMPUTERS-RECREATIVE (comp-rec). For each of these 20 problems, we have different possible datasets, e.g. IBM-BASEBALL, MACMOTORCYCLES, etc. A problem instance takes training and test data from two different datasets belong to the same high-level problem. For example a problem instance could be learning to distinguish articles about Macintosh and motorcycles MAC-MOTORCYCLES (evaluated on the 20 Newsgroups test section) using labeled </context>
</contexts>
<marker>Chen, Lam, Tsang, Wong, 2009</marker>
<rawString>Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong. 2009. Extracting discriminative concepts for domain adaptation in text mining. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janez Demsar</author>
</authors>
<title>Statistical comparisons of classifiers over multiple data sets.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--1</pages>
<contexts>
<context position="3202" citStr="Demsar (2006)" startWordPosition="534" endWordPosition="535">are sampled i.i.d. at random. The significance tests only tell us how likely it is that the observed difference in performance means would change if we sampled a bigger test sample the same way we sampled the one we have available to us right now. In standard machine learning papers a similar situation arises. If we are developing a new perceptron learning algorithm, for example, we are interested in how likely the new learning algorithm is to perform better than other perceptron learning algorithms across datasets, and we may for that reason evaluate it on a large set of repository datasets. Demsar (2006) presents motivation for using nonparametric methods such as the Wilcoxon signed 607 Proceedings of NAACL-HLT 2013, pages 607–611, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics rank test to estimate significance across datasets. The t-test is based on means, and typically results across datasets are not commensurable. The ttest is also extremely sentitive to outliers. Notice also that typically we do not have enough datasets to do paired bootstrapping (van den Noortgate and Onghena, 2005). In this paper we will assume that the Wilcoxon signed rank test prov</context>
</contexts>
<marker>Demsar, 2006</marker>
<rawString>Janez Demsar. 2006. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert Tibshirani</author>
</authors>
<title>An introduction to the bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall,</publisher>
<location>Boca Raton, FL.</location>
<contexts>
<context position="2209" citStr="Efron and Tibshirani, 1993" startWordPosition="360" endWordPosition="363">. This paper asks what we as developers can do to estimate the effect of a change to our system – not on the labeled test data that happens to be available to us, but on future, still unseen datasets provided by our end users. The usual practice in NLP is to evaluate a system on a small sample of held-out labeled data. The observed effect size on this sample is then validated by significance testing across data points, testing whether the observed difference in performance means is likely to be due to mere chance. The preferred significance test is probably the nonparametric paired bootstrap (Efron and Tibshirani, 1993; Berg-Kirkpatrick et al., 2012), but many researchers also resort to Student’s t-test for dependent means relying on the assumption that their metric scores are normally distributed. Such significance tests tell us nothing about how likely our change to our system is to lead to improvements on new datasets. The significance tests all rely on the assumption that our datapoints are sampled i.i.d. at random. The significance tests only tell us how likely it is that the observed difference in performance means would change if we sampled a bigger test sample the same way we sampled the one we have</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert Tibshirani. 1993. An introduction to the bootstrap. Chapman &amp; Hall, Boca Raton, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Hedges</author>
<author>Ingram Olkin</author>
</authors>
<title>Nonparametric estimators of effect size in meta-analysis.</title>
<date>1984</date>
<journal>Psychological Bulletin,</journal>
<pages>96--573</pages>
<contexts>
<context position="10330" citStr="Hedges and Olkin, 1984" startWordPosition="1689" endWordPosition="1692">odels are superior to macro-average estimates, they may provide ’far-off’ estimates. Using Darling-Anderson tests we could also reject the hypothesis that error reductions were logistically distributed, but we did not find evidence for rejecting the hypothesis that error reductions are Gumbel-distributed.1 Gumbel distributions are used to model error distributions in the latent variable formulation of multinomial logit regression. A parametric meta-analysis model based on the assumption that error reductions are Gumbel distributed is an interesting alternative to non-parametric meta-analysis (Hedges and Olkin, 1984; van den Noortgate and Onghena, 2005), since there seems to be little consensus in the literature about the best way to approach non-parametric meta-analysis. Gumbel distributions take the following form: 1 ez−e Q where z = &amp;quot; with α the location, and Q the 13 scale. We fit a Gumbel distribution to our weighted error reductions (wiTi) and compute the combined 1Abidin et al. (2012) has shown that Darling-Anderson is superior to other goodness-of-fit tests for Gumbel distributions. −z 609 macro-av fixed random gumbel k = 5 err. 0.0531 0.0525 0.0526 0.0489 p-value - — 0.98 — 0.98 — 0.79 k = 7 err</context>
</contexts>
<marker>Hedges, Olkin, 1984</marker>
<rawString>Larry Hedges and Ingram Olkin. 1984. Nonparametric estimators of effect size in meta-analysis. Psychological Bulletin, 96:573–580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qian Sun</author>
<author>Rita Chattopadhyay</author>
<author>Sethuraman Panchanathan</author>
<author>Jieping Ye</author>
</authors>
<title>Two-stage weighting framework for multi-source domain adaptation.</title>
<date>2011</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="11711" citStr="Sun et al., 2011" startWordPosition="1929" endWordPosition="1932">e and meta-analysis to predict error reductions in cross-lingual dependency parsing. See text for details. effect _ α + 0.577210 T �� 1 z�1 wz � where 0.57721 is the Euler-Mascheroni constant, and the variance of the combined effect v = 6 02. 3 Experiments in document classification and dependency parsing Our first experiment makes use of the 20 Newsgroups document classification dataset.2 The topics in 20 Newsgroups are hierarchically structured, which enables us to extract a large set of binary classification problems with considerable bias between source and target data (Chen et al., 2009; Sun et al., 2011). See the hierarchy in Figure 1. We extract 20 high-level binary classification problems by considering all pairs of top-level categories, e.g. COMPUTERS-RECREATIVE (comp-rec). For each of these 20 problems, we have different possible datasets, e.g. IBM-BASEBALL, MACMOTORCYCLES, etc. A problem instance takes training and test data from two different datasets belong to the same high-level problem. For example a problem instance could be learning to distinguish articles about Macintosh and motorcycles MAC-MOTORCYCLES (evaluated on the 20 Newsgroups test section) using labeled data from IBMBASEBA</context>
</contexts>
<marker>Sun, Chattopadhyay, Panchanathan, Ye, 2011</marker>
<rawString>Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, and Jieping Ye. 2011. Two-stage weighting framework for multi-source domain adaptation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas van Cranenburgh</author>
</authors>
<title>Literary authorship attribution with phrase-structure fragments.</title>
<date>2012</date>
<booktitle>In Workshop on Computational Linguistics for Literature, NAACL.</booktitle>
<marker>van Cranenburgh, 2012</marker>
<rawString>Andreas van Cranenburgh. 2012. Literary authorship attribution with phrase-structure fragments. In Workshop on Computational Linguistics for Literature, NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wim van den Noortgate</author>
<author>Patrick Onghena</author>
</authors>
<title>Parametric and nonparametric bootstrap methods for meta-analysis. Behavior Research Methods,</title>
<date>2005</date>
<pages>37--11</pages>
<marker>van den Noortgate, Onghena, 2005</marker>
<rawString>Wim van den Noortgate and Patrick Onghena. 2005. Parametric and nonparametric bootstrap methods for meta-analysis. Behavior Research Methods, 37:11– 22.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>