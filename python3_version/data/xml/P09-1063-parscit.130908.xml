<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.894331">
Improving Tree-to-Tree Translation with Packed Forests
</title>
<author confidence="0.9588">
Yang Liu and Yajuan L¨u and Qun Liu
</author>
<affiliation confidence="0.970200666666667">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.875971">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.999008">
{yliu,lvyajuan,liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.994796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999792444444445">
Current tree-to-tree models suffer from
parsing errors as they usually use only 1-
best parses for rule extraction and decod-
ing. We instead propose a forest-based
tree-to-tree model that uses packed forests.
The model is based on a probabilis-
tic synchronous tree substitution gram-
mar (STSG), which can be learned from
aligned forest pairs automatically. The de-
coder finds ways of decomposing trees in
the source forest into elementary trees us-
ing the source projection of STSG while
building target forest in parallel. Compa-
rable to the state-of-the-art phrase-based
system Moses, using packed forests in
tree-to-tree translation results in a signif-
icant absolute improvement of 3.6 BLEU
points over using 1-best trees.
</bodyText>
<sectionHeader confidence="0.998676" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999427927272728">
Approaches to syntax-based statistical machine
translation make use of parallel data with syntactic
annotations, either in the form of phrase structure
trees or dependency trees. They can be roughly
divided into three categories: string-to-tree mod-
els (e.g., (Galley et al., 2006; Marcu et al., 2006;
Shen et al., 2008)), tree-to-string models (e.g.,
(Liu et al., 2006; Huang et al., 2006)), and tree-to-
tree models (e.g., (Eisner, 2003; Ding and Palmer,
2005; Cowan et al., 2006; Zhang et al., 2008)).
By modeling the syntax of both source and tar-
get languages, tree-to-tree approaches have the po-
tential benefit of providing rules linguistically bet-
ter motivated. However, while string-to-tree and
tree-to-string models demonstrate promising re-
sults in empirical evaluations, tree-to-tree models
have still been underachieving.
We believe that tree-to-tree models face two
major challenges. First, tree-to-tree models are
more vulnerable to parsing errors. Obtaining
syntactic annotations in quantity usually entails
running automatic parsers on a parallel corpus.
As the amount and domain of the data used to
train parsers are relatively limited, parsers will
inevitably output ill-formed trees when handling
real-world text. Guided by such noisy syntactic in-
formation, syntax-based models that rely on 1-best
parses are prone to learn noisy translation rules
in training phase and produce degenerate trans-
lations in decoding phase (Quirk and Corston-
Oliver, 2006). This situation aggravates for tree-
to-tree models that use syntax on both sides.
Second, tree-to-tree rules provide poorer rule
coverage. As a tree-to-tree rule requires that there
must be trees on both sides, tree-to-tree mod-
els lose a larger amount of linguistically unmoti-
vated mappings. Studies reveal that the absence of
such non-syntactic mappings will impair transla-
tion quality dramatically (Marcu et al., 2006; Liu
et al., 2007; DeNeefe et al., 2007; Zhang et al.,
2008).
Compactly encoding exponentially many
parses, packed forests prove to be an excellent
fit for alleviating the above two problems (Mi et
al., 2008; Mi and Huang, 2008). In this paper,
we propose a forest-based tree-to-tree model. To
learn STSG rules from aligned forest pairs, we in-
troduce a series of notions for identifying minimal
tree-to-tree rules. Our decoder first converts the
source forest to a translation forest and then finds
the best derivation that has the source yield of one
source tree in the forest. Comparable to Moses,
our forest-based tree-to-tree model achieves an
absolute improvement of 3.6 BLEU points over
conventional tree-based model.
</bodyText>
<page confidence="0.954541">
558
</page>
<note confidence="0.622475285714286">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
IP1
NP2 VP3
PP4 VP-B5
NP-B6 NP-B7 NP-B8
NR9 CC10P11 NR12 VV13 AS14 NN15
</note>
<figure confidence="0.675869125">
bushi yu shalong juxing le huitan
Bush held a talk with Sharon
NNP16 VBD17 DT18 NN19 IN 20 NNP21
NP22 NP23 NP24
NP25 PP26
NP27
VP28
S 29
</figure>
<figureCaption confidence="0.7694056">
Figure 1: An aligned packed forest pair. Each
node is assigned a unique identity for reference.
The solid lines denote hyperedges and the dashed
lines denote word alignments. Shaded nodes are
frontier nodes.
</figureCaption>
<sectionHeader confidence="0.98333" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999779818181818">
Figure 1 shows an aligned forest pair for a Chinese
sentence and an English sentence. The solid lines
denote hyperedges and the dashed lines denote
word alignments between the two forests. Each
node is assigned a unique identity for reference.
Each hyperedge is associated with a probability,
which we omit in Figure 1 for clarity. In a forest,
a node usually has multiple incoming hyperedges.
We use IN(v) to denote the set of incoming hy-
peredges of node v. For example, the source node
“IP1” has following two incoming hyperedges: 1
</bodyText>
<equation confidence="0.999201">
e1 = ((NP-B6, VP3), IP1)
e2 = ((NP2, VP-B5), IP1)
</equation>
<footnote confidence="0.903615333333333">
1As there are both source and target forests, it might be
confusing by just using a span to refer to a node. In addition,
some nodes will often have the same labels and spans. There-
fore, it is more convenient to use an identity for referring to a
node. The notation “IP&apos;” denotes the node that has a label of
“IP” and has an identity of “1”.
</footnote>
<bodyText confidence="0.9866322">
Formally, a packed parse forest is a compact
representation of all the derivations (i.e., parse
trees) for a given sentence under a context-free
grammar. Huang and Chiang (2005) define a for-
est as a tuple (V, E, v, R), where V is a finite set
of nodes, E is a finite set of hyperedges, v� E V is
a distinguished node that denotes the goal item in
parsing, and R is the set of weights. For a given
sentence w1:l = w1 ... wl, each node v E V is in
the form of Xij, which denotes the recognition of
non-terminal X spanning the substring from posi-
tions i through j (that is, wi+1 ... wj). Each hy-
peredge e E E is a triple e = (T(e), h(e), f(e)),
where h(e) E V is its head, T(e) E V * is a vector
of tail nodes, and f(e) is a weight function from
R|T(e) |to R.
Our forest-based tree-to-tree model is based on
a probabilistic STSG (Eisner, 2003). Formally,
an STSG can be defined as a quintuple G =
(Fs, Ft, Ss, St, P), where
</bodyText>
<listItem confidence="0.997713333333333">
• Fs and Ft are the source and target alphabets,
respectively,
• Ss and St are the source and target start sym-
bols, and
• P is a set of production rules. A rule r is a
triple (ts, tt, —) that describes the correspon-
</listItem>
<bodyText confidence="0.858487333333333">
dence — between a source tree ts and a target
tree tt.
To integrate packed forests into tree-to-tree
translation, we model the process of synchronous
generation of a source forest Fs and a target forest
Ft using a probabilistic STSG grammar:
</bodyText>
<equation confidence="0.951882428571429">
X
Pr(Fs, Ft) =
T.EF.
X=
T.EF.
X=
T.EF.
</equation>
<bodyText confidence="0.822139625">
where Ts is a source tree, Tt is a target tree, D is
the set of all possible derivations that transform Ts
into Tt, d is one such derivation, and r is a tree-to-
tree rule.
Table 1 shows a derivation of the forest pair in
Figure 1. A derivation is a sequence of tree-to-tree
rules. Note that we use x to represent a nontermi-
nal.
</bodyText>
<equation confidence="0.980891481481481">
X
TtEFt
X
dED
Pr(Ts, Tt)
Pr(d)
X
TtEFt
Y
rEd
p(r) (1)
X
dED
X
TtEFt
559
(1) IP(x1:NP-B, x2:VP) → S(x1:NP, x2:VP)
(2) NP-B(x1:NR) → NP(x1:NNP)
(3) NR(bushi) → NNP(Bush)
(4) VP(x1:PP, VP-B(x2:VV, AS(le), x3:NP-B)) → VP(x2:VBD, NP(DT(a), x3:NP), x1:PP)
(5) PP(x1:P, x2:NP-B) → PP(x1:IN, x2:NP)
(6) P(yu) → IN(with)
(7) NP-B(x1:NR) → NP(x1:NP)
(8) NR(shalong) → NNP(Sharon)
(9) VV(juxing) → VBD(held)
(10) NP-B(x1:NN) → NP(x1:NN)
(11) NN(huitan) → NN(talk)
</equation>
<tableCaption confidence="0.99918">
Table 1: A minimal derivation of the forest pair in Figure 1.
</tableCaption>
<table confidence="0.999092033333333">
id span cspan complement consistent frontier counterparts
1 1-6 1-2, 4-6 1 1 29
2 1-3 1, 5-6 2, 4 0 0
3 2-6 2, 4-6 1 1 1 28
4 2-3 5-6 1-2, 4 1 1 25, 26
5 4-6 2, 4 1, 5-6 1 0
6 1-1 1 2, 4-6 1 1 16, 22
7 3-3 6 1-2, 4-5 1 1 21, 24
8 6-6 4 1-2, 5-6 1 1 19, 23
9 1-1 1 2, 4-6 1 1 16, 22
10 2-2 5 1-2, 4, 6 1 1 20
11 2-2 5 1-2, 4, 6 1 1 20
12 3-3 6 1-2, 4-5 1 1 21, 24
13 4-4 2 1, 4-6 1 1 17
14 5-5 1-2, 4-6 1 0
15 6-6 4 1-2, 5-6 1 1 19, 23
16 1-1 1 2-4, 6 1 1 6, 9
17 2-2 4 1-3, 6 1 1 13
18 3-3 1-4, 6 1 0
19 4-4 6 1-4 1 1 8, 15
20 5-5 2 1, 3-4, 6 1 1 10, 11
21 6-6 3 1-2, 4, 6 1 1 7, 12
22 1-1 1 2-4, 6 1 1 6, 9
23 3-4 6 1-4 1 1 8, 15
24 6-6 3 1-2, 4, 6 1 1 7, 12
25 5-6 2-3 1, 4, 6 1 1 4
26 5-6 2-3 1, 4, 6 1 1 4
27 3-6 2-3, 6 1, 4 0 0
28 2-6 2-4, 6 1 1 1 3
29 1-6 1-4, 6 1 1 1
</table>
<tableCaption confidence="0.997975">
Table 2: Node attributes of the example forest pair.
</tableCaption>
<sectionHeader confidence="0.946995" genericHeader="method">
3 Rule Extraction
</sectionHeader>
<bodyText confidence="0.999972043478261">
Given an aligned forest pair as shown in Figure
1, how to extract all valid tree-to-tree rules that
explain its synchronous generation process? By
constructing a theory that gives formal seman-
tics to word alignments, Galley et al. (2004)
give principled answers to these questions for ex-
tracting tree-to-string rules. Their GHKM proce-
dure draws connections among word alignments,
derivations, and rules. They first identify the
tree nodes that subsume tree-string pairs consis-
tent with word alignments and then extract rules
from these nodes. By this means, GHKM proves
to be able to extract all valid tree-to-string rules
from training instances. Although originally de-
veloped for the tree-to-string case, it is possible to
extend GHKM to extract all valid tree-to-tree rules
from aligned packed forests.
In this section, we introduce our tree-to-tree rule
extraction method adapted from GHKM, which
involves four steps: (1) identifying the correspon-
dence between the nodes in forest pairs, (2) iden-
tifying minimum rules, (3) inferring composed
rules, and (4) estimating rule probabilities.
</bodyText>
<subsectionHeader confidence="0.9970935">
3.1 Identifying Correspondence Between
Nodes
</subsectionHeader>
<bodyText confidence="0.997863636363637">
To learn tree-to-tree rules, we need to find aligned
tree pairs in the forest pairs. To do this, the start-
ing point is to identify the correspondence be-
tween nodes. We propose a number of attributes
for nodes, most of which derive from GHKM, to
facilitate the identification.
Definition 1 Given a node v, its span Q(v) is an
index set of the words it covers.
For example, the span of the source node
“VP-B5” is {4, 5, 6} as it covers three source
words: “juxing”, “le”, and “huitan”. For conve-
nience, we use {4-6} to denotes a contiguous span
{4, 5, 6}.
Definition 2 Given a node v, its corresponding
span -y(v) is the index set of aligned words on an-
other side.
For example, the corresponding span of the
source node “VP-B5” is {2, 4}, corresponding to
the target words “held” and “talk”.
Definition 3 Given a node v, its complement span
j(v) is the union of corresponding spans of nodes
that are neither antecedents nor descendants of v.
For example, the complement span of the source
node “VP-B5” is {1, 5-6}, corresponding to target
words “Bush”, “with”, and “Sharon”.
Definition 4 A node v is said to be consistent with
alignment if and only if closure(-y(v))∩j(v) = ∅.
For example, the closure of the corresponding
span of the source node “VP-B5” is {2-4} and
its complement span is {1, 5-6}. As the intersec-
tion of the closure and the complement span is an
empty set, the source node “VP-B5” is consistent
with the alignment.
</bodyText>
<page confidence="0.964205">
560
</page>
<figure confidence="0.984778714285714">
7
P 11 NR12
PP4
NP-B
IN 20
NNP21
NP24
PP26
PP4
NP-B7
IN 20 NP24
PP26
P11
7
P 11 NR12
PP4
NP-B
P11
PP 4
NP-B7
(a) (b) (c) (d)
</figure>
<figureCaption confidence="0.901344">
Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier
tree pair. All trees are taken from the example forest pair in Figure 1. Shaded nodes are frontier nodes.
Each node is assigned an identity for reference.
</figureCaption>
<construct confidence="0.8460845">
Definition 5 A node v is said to be a frontier node
if and only if:
</construct>
<listItem confidence="0.9977018">
1. v is consistent;
2. There exists at least one consistent node v′ on
another side satisfying:
• closure(γ(v′)) C σ(v);
• closure(γ(v)) C σ(v′).
</listItem>
<bodyText confidence="0.999330909090909">
v′ is said to be a counterpart of v. We use τ(v) to
denote the set of counterparts of v.
A frontier node often has multiple counter-
parts on another side due to the usage of unary
rules in parsers. For example, the source node
“NP-B6” has two counterparts on the target side:
“NNP16” and “NP22”. Conversely, the target node
“NNP16” also has two counterparts counterparts
on the source side: “NR�” and “NP-B6”.
The node attributes of the example forest pair
are listed in Table 2. We use identities to refer to
nodes. “cspan” denotes corresponding span and
“complement” denotes complement span. In Fig-
ure 1, there are 12 frontier nodes (highlighted by
shading) on the source side and 12 frontier nodes
on the target side. Note that while a consistent
node is equal to a frontier node in GHKM, this is
not the case in our method because we have a tree
on the target side. Frontier nodes play a critical
role in forest-based rule extraction because they
indicate where to cut the forest pairs to obtain tree-
to-tree rules.
</bodyText>
<subsectionHeader confidence="0.998501">
3.2 Identifying Minimum Rules
</subsectionHeader>
<bodyText confidence="0.938578727272727">
Given the frontier nodes, the next step is to iden-
tify aligned tree pairs, from which tree-to-tree
rules derive. Following Galley et al. (2006), we
distinguish between minimal and composed rules.
As a composed rule can be decomposed as a se-
quence of minimal rules, we are particularly inter-
ested in how to extract minimal rules. Also, we in-
troduce a number of notions to help identify mini-
mal rules.
Definition 6 A frontier tree is a subtree in a forest
satisfying:
</bodyText>
<listItem confidence="0.964780833333333">
1. Its root is a frontier node;
2. If the tree contains only one node, it must be
a lexicalized frontier node;
3. If the tree contains more than one nodes,
its leaves are either non-lexicalized frontier
nodes or lexicalized non-frontier nodes.
</listItem>
<bodyText confidence="0.6704">
For example, Figure 2(a) shows a frontier tree
in which all nodes are frontier nodes.
</bodyText>
<figureCaption confidence="0.734597142857143">
Definition 7 A minimal frontier tree is a frontier
tree such that all nodes other than the root and
leaves are non-frontier nodes.
For example, Figure 2(b) shows a minimal fron-
tier tree.
Definition 8 A frontier tree pair is a triple
(ts, tt, —) satisfying:
</figureCaption>
<footnote confidence="0.493491">
1. ts is a source frontier tree;
</footnote>
<page confidence="0.95135">
561
</page>
<listItem confidence="0.982207">
2. tt is a target frontier tree;
3. The root of ts is a counterpart of that of tt;
4. There is a one-to-one correspondence — be-
tween the frontier leaves of ts and tt.
</listItem>
<bodyText confidence="0.5566346">
For example, Figure 2(c) shows a frontier tree
pair.
Definition 9 A frontier tree pair (ts, tt, —) is said
to be a subgraph of another frontier tree pair
(ts′, tt′, —′) if and only if:
</bodyText>
<equation confidence="0.579331">
1. root(ts) = root(ts′);
2. root(tt) = root(tt′);
3. ts is a subgraph of ts′;
4. tt is a subgraph of tt′.
</equation>
<bodyText confidence="0.999516166666667">
For example, the frontier tree pair shown in Fig-
ure 2(d) is a subgraph of that in Figure 2(c).
Definition 10 A frontier tree pair is said to be
minimal if and only if it is not a subgraph of any
other frontier tree pair that shares with the same
root.
For example, Figure 2(d) shows a minimal fron-
tier tree pair.
Our goal is to find the minimal frontier tree
pairs, which correspond to minimal tree-to-tree
rules. For example, the tree pair shown in Figure
2(d) denotes a minimal rule as follows:
</bodyText>
<equation confidence="0.98991">
PP(x1:P,x2:NP-B) —* PP(x1:IN, x2:NP)
</equation>
<bodyText confidence="0.959678125">
Figure 3 shows the algorithm for identifying
minimal frontier tree pairs. The input is a source
forest Fs, a target forest Ft, and a source frontier
node v (line 1). We use a set P to store collected
minimal frontier tree pairs (line 2). We first call
the procedure FINDTREES(Fs, v) to identify a set
of frontier trees rooted at v in Fs (line 3). For ex-
ample, for the source frontier node “PP4” in Figure
1, we obtain two frontier trees:
(PP4(P11)(NP-B7))
(PP4(P11)(NP-B7(NR12)))
Then, we try to find the set of corresponding
target frontier trees (i.e., T). For each counter-
part v′ of v (line 5), we call the procedure FIND-
TREES(Ft, v′) to identify a set of frontier trees
rooted at v′ in Ft (line 6). For example, the source
</bodyText>
<listItem confidence="0.932596315789474">
1: procedure FINDTREEPAIRS(Fs, Ft, v)
2: P = 0
3: T FINDTREES(Fs, v)
4: , 0
5: for v′ E T(v) do
6: T , TU FINDTREES(Ft, v′)
7: end for
8: for (ts, tt) E T x T do
9: if ts — tt then
10: P , P U {(ts, tt, —)I
11: end if
12: end for
13: for (ts, tt, —) E P do
14: if I(ts′, tt′, —′) E P : (ts′, tt′, —′) �
(ts, tt, —) then
15: P , P — {(ts, tt, —)I
16: end if
17: end for
18: end procedure
</listItem>
<figureCaption confidence="0.998567">
Figure 3: Algorithm for identifying minimal fron-
tier tree pairs.
</figureCaption>
<bodyText confidence="0.999386333333333">
frontier node “PP4” has two counterparts on the
target side: “NP25” and “PP26”. There are four
target frontier trees rooted at the two nodes:
</bodyText>
<equation confidence="0.9693825">
(NP25(IN20)(NP24))
(NP25(IN20)(NP24(NNP21)))
(PP26(IN20)(NP24))
(PP26(IN20)(NP24(NNP21)))
</equation>
<bodyText confidence="0.999982">
Therefore, there are 2 x 4 = 8 pairs of trees.
We examine each tree pair (ts, tt) (line 8) to see
whether it is a frontier tree pair (line 9) and then
update P (line 10). In the above example, all the
eight tree pairs are frontier tree pairs.
Finally, we keep only minimal frontier tree pairs
in P (lines 13-15). As a result, we obtain the
following two minimal frontier tree pairs for the
source frontier node “PP4”:
</bodyText>
<equation confidence="0.9840315">
(PP4(P11)(NP-B7)) H (NP25(IN20)(NP24))
(PP4(P11)(NP-B7)) H (PP26(IN20)(NP24))
</equation>
<bodyText confidence="0.998962166666667">
To maintain a reasonable rule table size, we re-
strict that the number of nodes in a tree of an STSG
rule is no greater than n, which we refer to as max-
imal node count.
It seems more efficient to let the procedure
FINDTREES(F, v) to search for minimal frontier
</bodyText>
<page confidence="0.993845">
562
</page>
<bodyText confidence="0.978472888888889">
trees rather than frontier trees. However, a min-
imal frontier tree pair is not necessarily a pair of
minimal frontier trees. On our Chinese-English
corpus, we find that 38% of minimal frontier tree
pairs are not pairs of minimal frontier trees. As a
result, we have to first collect all frontier tree pairs
and then decide on the minimal ones.
Table 1 shows some minimal rules extracted
from the forest pair shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.998528">
3.3 Inferring Composed Rules
</subsectionHeader>
<bodyText confidence="0.9991216">
After minimal rules are learned, composed rules
can be obtained by composing two or more min-
imal rules. For example, the composition of the
second rule and the third rule in Table 1 produces
a new rule:
</bodyText>
<equation confidence="0.976108">
NP-B(NR(shalong)) —* NP(NNP(Sharon))
</equation>
<bodyText confidence="0.998588333333333">
While minimal rules derive from minimal fron-
tier tree pairs, composed rules correspond to non-
minimal frontier tree pairs.
</bodyText>
<subsectionHeader confidence="0.994945">
3.4 Estimating Rule Probabilities
</subsectionHeader>
<bodyText confidence="0.999982785714286">
We follow Mi and Huang (2008) to estimate the
fractional count of a rule extracted from an aligned
forest pair. Intuitively, the relative frequency of a
subtree that occurs in a forest is the sum of all the
trees that traverse the subtree divided by the sum
of all trees in the forest. Instead of enumerating
all trees explicitly and computing the sum of tree
probabilities, we resort to inside and outside prob-
abilities for efficient calculation:
where c(r) is the fractional count of a rule, ts is the
source tree in r, tt is the target tree in r, root(·) a
function that gets tree root, leaves(·) is a function
that gets tree leaves, and α(v) and β(v) are outside
and inside probabilities, respectively.
</bodyText>
<sectionHeader confidence="0.997131" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.9998">
Given a source packed forest Fs, our decoder finds
the target yield of the single best derivation d that
has source yield of Ts(d) E Fs:
</bodyText>
<equation confidence="0.955215">
e� = e argmax p(d)) (2)
</equation>
<bodyText confidence="0.980542">
d s.t. T3(d)∈F3
We extend the model in Eq. 1 to a log-linear
model (Och and Ney, 2002) that uses the follow-
ing eight features: relative frequencies in two di-
rections, lexical weights in two directions, num-
ber of rules used, language model score, number
of target words produced, and the probability of
matched source tree (Mi et al., 2008).
Given a source parse forest and an STSG gram-
mar G, we first apply the conversion algorithm
proposed by Mi et al. (2008) to produce a trans-
lation forest. The translation forest has a simi-
lar hypergraph structure. While the nodes are the
same as those of the parse forest, each hyperedge
is associated with an STSG rule. Then, the de-
coder runs on the translation forest. We use the
cube pruning method (Chiang, 2007) to approxi-
mately intersect the translation forest with the lan-
guage model. Traversing the translation forest in
a bottom-up order, the decoder tries to build tar-
get parses at each node. After the first pass, we
use lazy Algorithm 3 (Huang and Chiang, 2005)
to generate k-best translations for minimum error
rate training.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997423">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.99986132">
We evaluated our model on Chinese-to-English
translation. The training corpus contains 840K
Chinese words and 950K English words. A tri-
gram language model was trained on the English
sentences of the training corpus. We used the 2002
NIST MT Evaluation test set as our development
set, and used the 2005 NIST MT Evaluation test
set as our test set. We evaluated the translation
quality using the BLEU metric, as calculated by
mteval-v11b.pl with its default setting except that
we used case-insensitive matching of n-grams.
To obtain packed forests, we used the Chinese
parser (Xiong et al., 2005) modified by Haitao
Mi and the English parser (Charniak and Johnson,
2005) modified by Liang Huang to produce en-
tire parse forests. Then, we ran the Python scripts
(Huang, 2008) provided by Liang Huang to out-
put packed forests. To prune the packed forests,
Huang (2008) uses inside and outside probabili-
ties to compute the distance of the best derivation
that traverses a hyperedge away from the glob-
ally best derivation. A hyperedge will be pruned
away if the difference is greater than a threshold
p. Nodes with all incoming hyperedges pruned
are also pruned. The greater the threshold p is,
</bodyText>
<equation confidence="0.999285">
c(r) = β(�vs)
p(tt) X α(root(tt)) X Hv∈leaves(tt) β(v)
β(�vt)
p(ts) X α(root(ts)) X Hv∈leaves(t3) β(v)
X
</equation>
<page confidence="0.995823">
563
</page>
<table confidence="0.999490333333333">
p avg trees # of rules BLEU
0 1 73,614 0.2021 f 0.0089
2 238.94 105,214 0.2165 f 0.0081
5 5.78 x 106 347,526 0.2336 f 0.0078
8 6.59 x 107 573,738 0.2373 f 0.0082
10 1.05 x 108 743,211 0.2385 f 0.0084
</table>
<tableCaption confidence="0.9957845">
Table 3: Comparison of BLEU scores for tree-
based and forest-based tree-to-tree models.
</tableCaption>
<figure confidence="0.8314425">
0 1 2 3 4 5 6 7 8 9 10 11
maximal node count
</figure>
<figureCaption confidence="0.9985445">
Figure 4: Coverage of lexicalized STSG rules on
bilingual phrases.
</figureCaption>
<bodyText confidence="0.9651178">
the more parses are encoded in a packed forest.
We obtained word alignments of the training
data by first running GIZA++ (Och and Ney, 2003)
and then applying the refinement rule “grow-diag-
final-and” (Koehn et al., 2003).
</bodyText>
<subsectionHeader confidence="0.99512">
5.2 Forests Vs. 1-best Trees
</subsectionHeader>
<bodyText confidence="0.998769473684211">
Table 3 shows the BLEU scores of tree-based and
forest-based tree-to-tree models achieved on the
test set over different pruning thresholds. p is the
threshold for pruning packed forests, “avg trees”
is the average number of trees encoded in one for-
est on the test set, and “# of rules” is the number
of STSG rules used on the test set. We restrict that
both source and target trees in a tree-to-tree rule
can contain at most 10 nodes (i.e., the maximal
node count n = 10). The 95% confidence inter-
vals were computed using Zhang ’s significance
tester (Zhang et al., 2004).
We chose five different pruning thresholds in
our experiments: p = 0, 2, 5, 8, 10. The forests
pruned by p = 0 contained only 1-best tree per
sentence. With the increase of p, the average num-
ber of trees encoded in one forest rose dramati-
cally. When p was set to 10, there were over 100M
parses encoded in one forest on average.
</bodyText>
<table confidence="0.999584">
p extraction decoding
0 1.26 6.76
2 2.35 8.52
5 6.34 14.87
8 8.51 19.78
10 10.21 25.81
</table>
<tableCaption confidence="0.820956333333333">
Table 4: Comparison of rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence)
</tableCaption>
<bodyText confidence="0.999924260869565">
Moreover, the more trees are encoded in packed
forests, the more rules are made available to
forest-based models. The number of rules when
p = 10 was almost 10 times of p = 0. With the
increase of the number of rules used, the BLEU
score increased accordingly. This suggests that
packed forests enable tree-to-tree model to learn
more useful rules on the training data. However,
when a pack forest encodes over 1M parses per
sentence, the improvements are less significant,
which echoes the results in (Mi et al., 2008).
The forest-based tree-to-tree model outper-
forms the original model that uses 1-best trees
dramatically. The absolute improvement of 3.6
BLEU points (from 0.2021 to 0.2385) is statis-
tically significant at p &lt; 0.01 using the sign-
test as described by Collins et al. (2005), with
700(+1), 360(-1), and 15(0). We also ran Moses
(Koehn et al., 2007) with its default setting us-
ing the same data and obtained a BLEU score of
0.2366, slightly lower than our best result (i.e.,
0.2385). But this difference is not statistically sig-
nificant.
</bodyText>
<subsectionHeader confidence="0.999508">
5.3 Effect on Rule Coverage
</subsectionHeader>
<bodyText confidence="0.999788733333333">
Figure 4 demonstrates the effect of pruning thresh-
old and maximal node count on rule coverage.
We extracted phrase pairs from the training data
to investigate how many phrase pairs can be cap-
tured by lexicalized tree-to-tree rules that con-
tain only terminals. We set the maximal length
of phrase pairs to 10. For tree-based tree-to-tree
model, the coverage was below 8% even the max-
imal node count was set to 10. This suggests that
conventional tree-to-tree models lose over 92%
linguistically unmotivated mappings due to hard
syntactic constraints. The absence of such non-
syntactic mappings prevents tree-based tree-to-
tree models from achieving comparable results to
phrase-based models. With more parses included
</bodyText>
<figure confidence="0.956314892857143">
coverage
0.10
0.09
0.08
0.07
0.06
0.05
0.04
p=10
p=0
p=2
p=5
p=8
564
BLEU 0.20 the new training corpus contained about 260K sen-
0.19 tence pairs with 7.39M Chinese words and 9.41M
0.18 English words. We set the forest pruning threshold
0.17 p = 5. Moses obtained a BLEU score of 0.3043
0.16 and our forest-based tree-to-tree system achieved
0.15 a BLEU score of 0.3059. The difference is still not
0.14 significant statistically.
0.13 6 Related Work
0.12
0.11
0.10
0.09
0 1 2 3 4 5 6 7 8 9 10 11
maximal node count
</figure>
<figureCaption confidence="0.981207">
Figure 5: Effect of maximal node count on BLEU
scores.
</figureCaption>
<bodyText confidence="0.999501166666667">
in packed forests, the rule coverage increased ac-
cordingly. When p = 10 and n = 10, the cov-
erage was 9.7%, higher than that of p = 0. As
a result, packed forests enable tree-to-tree models
to capture more useful source-target mappings and
therefore improve translation quality. 2
</bodyText>
<subsectionHeader confidence="0.978163">
5.4 Training and Decoding Time
</subsectionHeader>
<bodyText confidence="0.999984125">
Table 4 gives the rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence) with varying pruning thresholds.
We found that the extraction time grew faster than
decoding time with the increase of p. One possi-
ble reason is that the number of frontier tree pairs
(see Figure 3) rose dramatically when more parses
were included in packed forests.
</bodyText>
<subsectionHeader confidence="0.996941">
5.5 Effect of Maximal Node Count
</subsectionHeader>
<bodyText confidence="0.999910333333333">
Figure 5 shows the effect of maximal node count
on BLEU scores. With the increase of maximal
node count, the BLEU score increased dramati-
cally. This implies that allowing tree-to-tree rules
to capture larger contexts will strengthen the ex-
pressive power of tree-to-tree model.
</bodyText>
<subsectionHeader confidence="0.767418">
5.6 Results on Larger Data
</subsectionHeader>
<bodyText confidence="0.944343571428571">
We also conducted an experiment on larger data
to further examine the effectiveness of our ap-
proach. We concatenated the small corpus we
used above and the FBIS corpus. After remov-
ing the sentences that we failed to obtain forests,
2Note that even we used packed forests, the rule coverage
was still very low. One reason is that we set the maximal
phrase length to 10 words, while an STSG rule with 10 nodes
in each tree usually cannot subsume 10 words.
In machine translation, the concept of packed for-
est is first used by Huang and Chiang (2007) to
characterize the search space of decoding with lan-
guage models. The first direct use of packed for-
est is proposed by Mi et al. (2008). They replace
1-best trees with packed forests both in training
and decoding and show superior translation qual-
ity over the state-of-the-art hierarchical phrase-
based system. We follow the same direction and
apply packed forests to tree-to-tree translation.
Zhang et al. (2008) present a tree-to-tree model
that uses STSG. To capture non-syntactic phrases,
they apply tree-sequence rules (Liu et al., 2007)
to tree-to-tree models. Their extraction algorithm
first identifies initial rules and then obtains abstract
rules. While this method works for 1-best tree
pairs, it cannot be applied to packed forest pairs
because it is impractical to enumerate all tree pairs
over a phrase pair.
While Galley (2004) describes extracting tree-
to-string rules from 1-best trees, Mi and Huang et
al. (2008) go further by proposing a method for
extracting tree-to-string rules from aligned forest-
string pairs. We follow their work and focus on
identifying tree-tree pairs in a forest pair, which is
more difficult than the tree-to-string case.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999775">
We have shown how to improve tree-to-tree trans-
lation with packed forests, which compactly en-
code exponentially many parses. To learn STSG
rules from aligned forest pairs, we first identify
minimal rules and then get composed rules. The
decoder finds the best derivation that have the
source yield of one source tree in the forest. Ex-
periments show that using packed forests in tree-
to-tree translation results in dramatic improve-
ments over using 1-best trees. Our system also
achieves comparable performance with the state-
of-the-art phrase-based system Moses.
</bodyText>
<page confidence="0.997493">
565
</page>
<sectionHeader confidence="0.986967" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99992375">
The authors were supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done
while Yang Liu was visiting the SMT group led
by Stephan Vogel at CMU. We thank the anony-
mous reviewers for their insightful comments.
Many thanks go to Liang Huang, Haitao Mi, and
Hao Xiong for their invaluable help in producing
packed forests. We are also grateful to Andreas
Zollmann, Vamshi Ambati, and Kevin Gimpel for
their helpful feedback.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999919564705882">
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. ofACL 2005.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Brooke Cowan, Ivona Ku˘cerov´a, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. ofEMNLP 2006.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. ofEMNLP 2007.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proc. ofACL 2005.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003 (Companion Volume).
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. ofNAACL/HLT 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of COLING/ACL 2006.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. ofIWPT 2005.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. ofACL 2007.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. ofAMTA 2006.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proc. of
ACL/HLT 2008.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. ofACL 2007 (demonstration session).
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of COLING/ACL 2006.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. ofACL 2007.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP 2006.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. ofEMNLP 2008.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. ofACL/HLT 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ofACL 2002.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Chris Quirk and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proc. of EMNLP
2006.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ofACL/HLT 2008.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proc. ofIJCNLP 2005.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores how much improve-
ment do we need to have a better system? In Proc.
ofLREC 2004.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. ofACL/HLT 2008.
</reference>
<page confidence="0.998508">
566
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.699889">
<title confidence="0.999773">Improving Tree-to-Tree Translation with Packed Forests</title>
<author confidence="0.993205">Liu L¨u Liu</author>
<affiliation confidence="0.948917">Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.994625">P.O. Box 2704, Beijing 100190, China</address>
<abstract confidence="0.989719789473684">Current tree-to-tree models suffer from parsing errors as they usually use only 1best parses for rule extraction and decoding. We instead propose a forest-based tree-to-tree model that uses packed forests. The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically. The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="20237" citStr="Charniak and Johnson, 2005" startWordPosition="3641" endWordPosition="3644">h translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams. To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The greater the threshold p is, c(r) = β(�vs) p(tt) X α(root(tt)) X Hv∈leaves(tt) β(v) β(�vt) p(ts) X α(root(ts)</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proc. ofACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="19209" citStr="Chiang, 2007" startWordPosition="3476" endWordPosition="3477">ncies in two directions, lexical weights in two directions, number of rules used, language model score, number of target words produced, and the probability of matched source tree (Mi et al., 2008). Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al. (2008) to produce a translation forest. The translation forest has a similar hypergraph structure. While the nodes are the same as those of the parse forest, each hyperedge is associated with an STSG rule. Then, the decoder runs on the translation forest. We use the cube pruning method (Chiang, 2007) to approximately intersect the translation forest with the language model. Traversing the translation forest in a bottom-up order, the decoder tries to build target parses at each node. After the first pass, we use lazy Algorithm 3 (Huang and Chiang, 2005) to generate k-best translations for minimum error rate training. 5 Experiments 5.1 Data Preparation We evaluated our model on Chinese-to-English translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT E</context>
<context position="26548" citStr="Chiang (2007)" startWordPosition="4746" endWordPosition="4747">xpressive power of tree-to-tree model. 5.6 Results on Larger Data We also conducted an experiment on larger data to further examine the effectiveness of our approach. We concatenated the small corpus we used above and the FBIS corpus. After removing the sentences that we failed to obtain forests, 2Note that even we used packed forests, the rule coverage was still very low. One reason is that we set the maximal phrase length to 10 words, while an STSG rule with 10 nodes in each tree usually cannot subsume 10 words. In machine translation, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models. The first direct use of packed forest is proposed by Mi et al. (2008). They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrasebased system. We follow the same direction and apply packed forests to tree-to-tree translation. Zhang et al. (2008) present a tree-to-tree model that uses STSG. To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models. Their extraction algorithm first</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Ivona Ku˘cerov´a</author>
<author>Michael Collins</author>
</authors>
<title>A discriminative model for tree-to-tree translation.</title>
<date>2006</date>
<booktitle>In Proc. ofEMNLP</booktitle>
<marker>Cowan, Ku˘cerov´a, Collins, 2006</marker>
<rawString>Brooke Cowan, Ivona Ku˘cerov´a, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proc. ofEMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In Proc. ofEMNLP</booktitle>
<contexts>
<context position="2944" citStr="DeNeefe et al., 2007" startWordPosition="435" endWordPosition="438"> 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model ach</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proc. ofEMNLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="1478" citStr="Ding and Palmer, 2005" startWordPosition="217" endWordPosition="220">m Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. We believe that tree-to-tree models face two major challenges. First, tree-to-tree models are more vulnerable to parsing errors. Obtaining syntactic annotations in quantity usually entails running automatic parsers on a parallel </context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proc. ofACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<location>(Companion Volume).</location>
<contexts>
<context position="1455" citStr="Eisner, 2003" startWordPosition="215" endWordPosition="216">se-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. We believe that tree-to-tree models face two major challenges. First, tree-to-tree models are more vulnerable to parsing errors. Obtaining syntactic annotations in quantity usually entails running automatic</context>
<context position="5980" citStr="Eisner, 2003" startWordPosition="987" endWordPosition="988">is a finite set of nodes, E is a finite set of hyperedges, v� E V is a distinguished node that denotes the goal item in parsing, and R is the set of weights. For a given sentence w1:l = w1 ... wl, each node v E V is in the form of Xij, which denotes the recognition of non-terminal X spanning the substring from positions i through j (that is, wi+1 ... wj). Each hyperedge e E E is a triple e = (T(e), h(e), f(e)), where h(e) E V is its head, T(e) E V * is a vector of tail nodes, and f(e) is a weight function from R|T(e) |to R. Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). Formally, an STSG can be defined as a quintuple G = (Fs, Ft, Ss, St, P), where • Fs and Ft are the source and target alphabets, respectively, • Ss and St are the source and target start symbols, and • P is a set of production rules. A rule r is a triple (ts, tt, —) that describes the correspondence — between a source tree ts and a target tree tt. To integrate packed forests into tree-to-tree translation, we model the process of synchronous generation of a source forest Fs and a target forest Ft using a probabilistic STSG grammar: X Pr(Fs, Ft) = T.EF. X= T.EF. X= T.EF. where Ts is a source tr</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proc. of ACL 2003 (Companion Volume).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. ofNAACL/HLT</booktitle>
<contexts>
<context position="8482" citStr="Galley et al. (2004)" startWordPosition="1550" endWordPosition="1553">2-2 4 1-3, 6 1 1 13 18 3-3 1-4, 6 1 0 19 4-4 6 1-4 1 1 8, 15 20 5-5 2 1, 3-4, 6 1 1 10, 11 21 6-6 3 1-2, 4, 6 1 1 7, 12 22 1-1 1 2-4, 6 1 1 6, 9 23 3-4 6 1-4 1 1 8, 15 24 6-6 3 1-2, 4, 6 1 1 7, 12 25 5-6 2-3 1, 4, 6 1 1 4 26 5-6 2-3 1, 4, 6 1 1 4 27 3-6 2-3, 6 1, 4 0 0 28 2-6 2-4, 6 1 1 1 3 29 1-6 1-4, 6 1 1 1 Table 2: Node attributes of the example forest pair. 3 Rule Extraction Given an aligned forest pair as shown in Figure 1, how to extract all valid tree-to-tree rules that explain its synchronous generation process? By constructing a theory that gives formal semantics to word alignments, Galley et al. (2004) give principled answers to these questions for extracting tree-to-string rules. Their GHKM procedure draws connections among word alignments, derivations, and rules. They first identify the tree nodes that subsume tree-string pairs consistent with word alignments and then extract rules from these nodes. By this means, GHKM proves to be able to extract all valid tree-to-string rules from training instances. Although originally developed for the tree-to-string case, it is possible to extend GHKM to extract all valid tree-to-tree rules from aligned packed forests. In this section, we introduce o</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. ofNAACL/HLT 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<contexts>
<context position="1299" citStr="Galley et al., 2006" startWordPosition="187" endWordPosition="190">es in the source forest into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. We believe that tree-to-tree models face two major</context>
<context position="12613" citStr="Galley et al. (2006)" startWordPosition="2270" endWordPosition="2273">otes complement span. In Figure 1, there are 12 frontier nodes (highlighted by shading) on the source side and 12 frontier nodes on the target side. Note that while a consistent node is equal to a frontier node in GHKM, this is not the case in our method because we have a tree on the target side. Frontier nodes play a critical role in forest-based rule extraction because they indicate where to cut the forest pairs to obtain treeto-tree rules. 3.2 Identifying Minimum Rules Given the frontier nodes, the next step is to identify aligned tree pairs, from which tree-to-tree rules derive. Following Galley et al. (2006), we distinguish between minimal and composed rules. As a composed rule can be decomposed as a sequence of minimal rules, we are particularly interested in how to extract minimal rules. Also, we introduce a number of notions to help identify minimal rules. Definition 6 A frontier tree is a subtree in a forest satisfying: 1. Its root is a frontier node; 2. If the tree contains only one node, it must be a lexicalized frontier node; 3. If the tree contains more than one nodes, its leaves are either non-lexicalized frontier nodes or lexicalized non-frontier nodes. For example, Figure 2(a) shows a </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. of COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. ofIWPT</booktitle>
<contexts>
<context position="5317" citStr="Huang and Chiang (2005)" startWordPosition="844" endWordPosition="847">e node “IP1” has following two incoming hyperedges: 1 e1 = ((NP-B6, VP3), IP1) e2 = ((NP2, VP-B5), IP1) 1As there are both source and target forests, it might be confusing by just using a span to refer to a node. In addition, some nodes will often have the same labels and spans. Therefore, it is more convenient to use an identity for referring to a node. The notation “IP&apos;” denotes the node that has a label of “IP” and has an identity of “1”. Formally, a packed parse forest is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar. Huang and Chiang (2005) define a forest as a tuple (V, E, v, R), where V is a finite set of nodes, E is a finite set of hyperedges, v� E V is a distinguished node that denotes the goal item in parsing, and R is the set of weights. For a given sentence w1:l = w1 ... wl, each node v E V is in the form of Xij, which denotes the recognition of non-terminal X spanning the substring from positions i through j (that is, wi+1 ... wj). Each hyperedge e E E is a triple e = (T(e), h(e), f(e)), where h(e) E V is its head, T(e) E V * is a vector of tail nodes, and f(e) is a weight function from R|T(e) |to R. Our forest-based tre</context>
<context position="19466" citStr="Huang and Chiang, 2005" startWordPosition="3518" endWordPosition="3521">G, we first apply the conversion algorithm proposed by Mi et al. (2008) to produce a translation forest. The translation forest has a similar hypergraph structure. While the nodes are the same as those of the parse forest, each hyperedge is associated with an STSG rule. Then, the decoder runs on the translation forest. We use the cube pruning method (Chiang, 2007) to approximately intersect the translation forest with the language model. Traversing the translation forest in a bottom-up order, the decoder tries to build target parses at each node. After the first pass, we use lazy Algorithm 3 (Huang and Chiang, 2005) to generate k-best translations for minimum error rate training. 5 Experiments 5.1 Data Preparation We evaluated our model on Chinese-to-English translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensiti</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. ofIWPT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="26548" citStr="Huang and Chiang (2007)" startWordPosition="4744" endWordPosition="4747">then the expressive power of tree-to-tree model. 5.6 Results on Larger Data We also conducted an experiment on larger data to further examine the effectiveness of our approach. We concatenated the small corpus we used above and the FBIS corpus. After removing the sentences that we failed to obtain forests, 2Note that even we used packed forests, the rule coverage was still very low. One reason is that we set the maximal phrase length to 10 words, while an STSG rule with 10 nodes in each tree usually cannot subsume 10 words. In machine translation, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models. The first direct use of packed forest is proposed by Mi et al. (2008). They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrasebased system. We follow the same direction and apply packed forests to tree-to-tree translation. Zhang et al. (2008) present a tree-to-tree model that uses STSG. To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models. Their extraction algorithm first</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. ofACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. ofAMTA</booktitle>
<contexts>
<context position="1409" citStr="Huang et al., 2006" startWordPosition="206" endWordPosition="209"> in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. We believe that tree-to-tree models face two major challenges. First, tree-to-tree models are more vulnerable to parsing errors. Obtaining syntactic annotations</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. ofAMTA 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. of ACL/HLT</booktitle>
<contexts>
<context position="3133" citStr="Huang, 2008" startWordPosition="468" endWordPosition="469">to-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, Su</context>
<context position="17604" citStr="Huang (2008)" startWordPosition="3188" endWordPosition="3189">first collect all frontier tree pairs and then decide on the minimal ones. Table 1 shows some minimal rules extracted from the forest pair shown in Figure 1. 3.3 Inferring Composed Rules After minimal rules are learned, composed rules can be obtained by composing two or more minimal rules. For example, the composition of the second rule and the third rule in Table 1 produces a new rule: NP-B(NR(shalong)) —* NP(NNP(Sharon)) While minimal rules derive from minimal frontier tree pairs, composed rules correspond to nonminimal frontier tree pairs. 3.4 Estimating Rule Probabilities We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. Intuitively, the relative frequency of a subtree that occurs in a forest is the sum of all the trees that traverse the subtree divided by the sum of all trees in the forest. Instead of enumerating all trees explicitly and computing the sum of tree probabilities, we resort to inside and outside probabilities for efficient calculation: where c(r) is the fractional count of a rule, ts is the source tree in r, tt is the target tree in r, root(·) a function that gets tree root, leaves(·) is a function that gets tree </context>
<context position="20340" citStr="Huang, 2008" startWordPosition="3661" endWordPosition="3662">ained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams. To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The greater the threshold p is, c(r) = β(�vs) p(tt) X α(root(tt)) X Hv∈leaves(tt) β(v) β(�vt) p(ts) X α(root(ts)) X Hv∈leaves(t3) β(v) X 563 p avg trees # of rules BLEU 0 1 73,614 0.2021 f 0.0089 2 238.94 105,214 0.</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. of ACL/HLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="21485" citStr="Koehn et al., 2003" startWordPosition="3866" endWordPosition="3869">p avg trees # of rules BLEU 0 1 73,614 0.2021 f 0.0089 2 238.94 105,214 0.2165 f 0.0081 5 5.78 x 106 347,526 0.2336 f 0.0078 8 6.59 x 107 573,738 0.2373 f 0.0082 10 1.05 x 108 743,211 0.2385 f 0.0084 Table 3: Comparison of BLEU scores for treebased and forest-based tree-to-tree models. 0 1 2 3 4 5 6 7 8 9 10 11 maximal node count Figure 4: Coverage of lexicalized STSG rules on bilingual phrases. the more parses are encoded in a packed forest. We obtained word alignments of the training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diagfinal-and” (Koehn et al., 2003). 5.2 Forests Vs. 1-best Trees Table 3 shows the BLEU scores of tree-based and forest-based tree-to-tree models achieved on the test set over different pruning thresholds. p is the threshold for pruning packed forests, “avg trees” is the average number of trees encoded in one forest on the test set, and “# of rules” is the number of STSG rules used on the test set. We restrict that both source and target trees in a tree-to-tree rule can contain at most 10 nodes (i.e., the maximal node count n = 10). The 95% confidence intervals were computed using Zhang ’s significance tester (Zhang et al., 20</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ofACL</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="23480" citStr="Koehn et al., 2007" startWordPosition="4219" endWordPosition="4222">ed accordingly. This suggests that packed forests enable tree-to-tree model to learn more useful rules on the training data. However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al., 2008). The forest-based tree-to-tree model outperforms the original model that uses 1-best trees dramatically. The absolute improvement of 3.6 BLEU points (from 0.2021 to 0.2385) is statistically significant at p &lt; 0.01 using the signtest as described by Collins et al. (2005), with 700(+1), 360(-1), and 15(0). We also ran Moses (Koehn et al., 2007) with its default setting using the same data and obtained a BLEU score of 0.2366, slightly lower than our best result (i.e., 0.2385). But this difference is not statistically significant. 5.3 Effect on Rule Coverage Figure 4 demonstrates the effect of pruning threshold and maximal node count on rule coverage. We extracted phrase pairs from the training data to investigate how many phrase pairs can be captured by lexicalized tree-to-tree rules that contain only terminals. We set the maximal length of phrase pairs to 10. For tree-based tree-to-tree model, the coverage was below 8% even the maxi</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ofACL 2007 (demonstration session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<contexts>
<context position="1388" citStr="Liu et al., 2006" startWordPosition="202" endWordPosition="205">ding target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. We believe that tree-to-tree models face two major challenges. First, tree-to-tree models are more vulnerable to parsing errors. Obtaining </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proc. of COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="2922" citStr="Liu et al., 2007" startWordPosition="431" endWordPosition="434">odels that rely on 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based </context>
<context position="27091" citStr="Liu et al., 2007" startWordPosition="4830" endWordPosition="4833">ion, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models. The first direct use of packed forest is proposed by Mi et al. (2008). They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrasebased system. We follow the same direction and apply packed forests to tree-to-tree translation. Zhang et al. (2008) present a tree-to-tree model that uses STSG. To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models. Their extraction algorithm first identifies initial rules and then obtains abstract rules. While this method works for 1-best tree pairs, it cannot be applied to packed forest pairs because it is impractical to enumerate all tree pairs over a phrase pair. While Galley (2004) describes extracting treeto-string rules from 1-best trees, Mi and Huang et al. (2008) go further by proposing a method for extracting tree-to-string rules from aligned foreststring pairs. We follow their work and focus on identifying tree-tree pairs in a forest pair, which is more difficult than t</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proc. ofACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>Spmt: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="1319" citStr="Marcu et al., 2006" startWordPosition="191" endWordPosition="194">st into elementary trees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. We believe that tree-to-tree models face two major challenges. First, </context>
<context position="2904" citStr="Marcu et al., 2006" startWordPosition="427" endWordPosition="430">tion, syntax-based models that rely on 1-best parses are prone to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses,</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. Spmt: Statistical machine translation with syntactified target language phrases. In Proc. of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proc. ofEMNLP</booktitle>
<contexts>
<context position="3133" citStr="Mi and Huang, 2008" startWordPosition="466" endWordPosition="469">or treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558–566, Su</context>
<context position="17604" citStr="Mi and Huang (2008)" startWordPosition="3186" endWordPosition="3189">ave to first collect all frontier tree pairs and then decide on the minimal ones. Table 1 shows some minimal rules extracted from the forest pair shown in Figure 1. 3.3 Inferring Composed Rules After minimal rules are learned, composed rules can be obtained by composing two or more minimal rules. For example, the composition of the second rule and the third rule in Table 1 produces a new rule: NP-B(NR(shalong)) —* NP(NNP(Sharon)) While minimal rules derive from minimal frontier tree pairs, composed rules correspond to nonminimal frontier tree pairs. 3.4 Estimating Rule Probabilities We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. Intuitively, the relative frequency of a subtree that occurs in a forest is the sum of all the trees that traverse the subtree divided by the sum of all trees in the forest. Instead of enumerating all trees explicitly and computing the sum of tree probabilities, we resort to inside and outside probabilities for efficient calculation: where c(r) is the fractional count of a rule, ts is the source tree in r, tt is the target tree in r, root(·) a function that gets tree root, leaves(·) is a function that gets tree </context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proc. ofEMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forestbased translation.</title>
<date>2008</date>
<booktitle>In Proc. ofACL/HLT</booktitle>
<contexts>
<context position="3112" citStr="Mi et al., 2008" startWordPosition="462" endWordPosition="465">tion aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute improvement of 3.6 BLEU points over conventional tree-based model. 558 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFN</context>
<context position="18793" citStr="Mi et al., 2008" startWordPosition="3399" endWordPosition="3402"> a function that gets tree leaves, and α(v) and β(v) are outside and inside probabilities, respectively. 4 Decoding Given a source packed forest Fs, our decoder finds the target yield of the single best derivation d that has source yield of Ts(d) E Fs: e� = e argmax p(d)) (2) d s.t. T3(d)∈F3 We extend the model in Eq. 1 to a log-linear model (Och and Ney, 2002) that uses the following eight features: relative frequencies in two directions, lexical weights in two directions, number of rules used, language model score, number of target words produced, and the probability of matched source tree (Mi et al., 2008). Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al. (2008) to produce a translation forest. The translation forest has a similar hypergraph structure. While the nodes are the same as those of the parse forest, each hyperedge is associated with an STSG rule. Then, the decoder runs on the translation forest. We use the cube pruning method (Chiang, 2007) to approximately intersect the translation forest with the language model. Traversing the translation forest in a bottom-up order, the decoder tries to build target parses at each nod</context>
<context position="23135" citStr="Mi et al., 2008" startWordPosition="4162" endWordPosition="4165">arison of rule extraction time (seconds/1000 sentence pairs) and decoding time (second/sentence) Moreover, the more trees are encoded in packed forests, the more rules are made available to forest-based models. The number of rules when p = 10 was almost 10 times of p = 0. With the increase of the number of rules used, the BLEU score increased accordingly. This suggests that packed forests enable tree-to-tree model to learn more useful rules on the training data. However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al., 2008). The forest-based tree-to-tree model outperforms the original model that uses 1-best trees dramatically. The absolute improvement of 3.6 BLEU points (from 0.2021 to 0.2385) is statistically significant at p &lt; 0.01 using the signtest as described by Collins et al. (2005), with 700(+1), 360(-1), and 15(0). We also ran Moses (Koehn et al., 2007) with its default setting using the same data and obtained a BLEU score of 0.2366, slightly lower than our best result (i.e., 0.2385). But this difference is not statistically significant. 5.3 Effect on Rule Coverage Figure 4 demonstrates the effect of pr</context>
<context position="26685" citStr="Mi et al. (2008)" startWordPosition="4770" endWordPosition="4773">e effectiveness of our approach. We concatenated the small corpus we used above and the FBIS corpus. After removing the sentences that we failed to obtain forests, 2Note that even we used packed forests, the rule coverage was still very low. One reason is that we set the maximal phrase length to 10 words, while an STSG rule with 10 nodes in each tree usually cannot subsume 10 words. In machine translation, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models. The first direct use of packed forest is proposed by Mi et al. (2008). They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrasebased system. We follow the same direction and apply packed forests to tree-to-tree translation. Zhang et al. (2008) present a tree-to-tree model that uses STSG. To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models. Their extraction algorithm first identifies initial rules and then obtains abstract rules. While this method works for 1-best tree pairs, it cannot be applied to packed </context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forestbased translation. In Proc. ofACL/HLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL</booktitle>
<contexts>
<context position="18540" citStr="Och and Ney, 2002" startWordPosition="3357" endWordPosition="3360"> sum of tree probabilities, we resort to inside and outside probabilities for efficient calculation: where c(r) is the fractional count of a rule, ts is the source tree in r, tt is the target tree in r, root(·) a function that gets tree root, leaves(·) is a function that gets tree leaves, and α(v) and β(v) are outside and inside probabilities, respectively. 4 Decoding Given a source packed forest Fs, our decoder finds the target yield of the single best derivation d that has source yield of Ts(d) E Fs: e� = e argmax p(d)) (2) d s.t. T3(d)∈F3 We extend the model in Eq. 1 to a log-linear model (Och and Ney, 2002) that uses the following eight features: relative frequencies in two directions, lexical weights in two directions, number of rules used, language model score, number of target words produced, and the probability of matched source tree (Mi et al., 2008). Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al. (2008) to produce a translation forest. The translation forest has a similar hypergraph structure. While the nodes are the same as those of the parse forest, each hyperedge is associated with an STSG rule. Then, the decoder runs on </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ofACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="21405" citStr="Och and Ney, 2003" startWordPosition="3854" endWordPosition="3857">t)) X Hv∈leaves(tt) β(v) β(�vt) p(ts) X α(root(ts)) X Hv∈leaves(t3) β(v) X 563 p avg trees # of rules BLEU 0 1 73,614 0.2021 f 0.0089 2 238.94 105,214 0.2165 f 0.0081 5 5.78 x 106 347,526 0.2336 f 0.0078 8 6.59 x 107 573,738 0.2373 f 0.0082 10 1.05 x 108 743,211 0.2385 f 0.0084 Table 3: Comparison of BLEU scores for treebased and forest-based tree-to-tree models. 0 1 2 3 4 5 6 7 8 9 10 11 maximal node count Figure 4: Coverage of lexicalized STSG rules on bilingual phrases. the more parses are encoded in a packed forest. We obtained word alignments of the training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule “grow-diagfinal-and” (Koehn et al., 2003). 5.2 Forests Vs. 1-best Trees Table 3 shows the BLEU scores of tree-based and forest-based tree-to-tree models achieved on the test set over different pruning thresholds. p is the threshold for pruning packed forests, “avg trees” is the average number of trees encoded in one forest on the test set, and “# of rules” is the number of STSG rules used on the test set. We restrict that both source and target trees in a tree-to-tree rule can contain at most 10 nodes (i.e., the maximal node count n = 10). The 95% confide</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Simon Corston-Oliver</author>
</authors>
<title>The impact of parsing quality on syntactically-informed statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP</booktitle>
<marker>Quirk, Corston-Oliver, 2006</marker>
<rawString>Chris Quirk and Simon Corston-Oliver. 2006. The impact of parsing quality on syntactically-informed statistical machine translation. In Proc. of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proc. ofACL/HLT</booktitle>
<contexts>
<context position="1339" citStr="Shen et al., 2008" startWordPosition="195" endWordPosition="198">rees using the source projection of STSG while building target forest in parallel. Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. We believe that tree-to-tree models face two major challenges. First, tree-to-tree models </context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proc. ofACL/HLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Parsing the penn chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proc. ofIJCNLP</booktitle>
<contexts>
<context position="20163" citStr="Xiong et al., 2005" startWordPosition="3629" endWordPosition="3632">s 5.1 Data Preparation We evaluated our model on Chinese-to-English translation. The training corpus contains 840K Chinese words and 950K English words. A trigram language model was trained on the English sentences of the training corpus. We used the 2002 NIST MT Evaluation test set as our development set, and used the 2005 NIST MT Evaluation test set as our test set. We evaluated the translation quality using the BLEU metric, as calculated by mteval-v11b.pl with its default setting except that we used case-insensitive matching of n-grams. To obtain packed forests, we used the Chinese parser (Xiong et al., 2005) modified by Haitao Mi and the English parser (Charniak and Johnson, 2005) modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned away if the difference is greater than a threshold p. Nodes with all incoming hyperedges pruned are also pruned. The greater the threshold p is, c(r) =</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin. 2005. Parsing the penn chinese treebank with semantic knowledge. In Proc. ofIJCNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting bleu/nist scores how much improvement do we need to have a better system? In</title>
<date>2004</date>
<booktitle>Proc. ofLREC</booktitle>
<contexts>
<context position="22088" citStr="Zhang et al., 2004" startWordPosition="3974" endWordPosition="3977">hn et al., 2003). 5.2 Forests Vs. 1-best Trees Table 3 shows the BLEU scores of tree-based and forest-based tree-to-tree models achieved on the test set over different pruning thresholds. p is the threshold for pruning packed forests, “avg trees” is the average number of trees encoded in one forest on the test set, and “# of rules” is the number of STSG rules used on the test set. We restrict that both source and target trees in a tree-to-tree rule can contain at most 10 nodes (i.e., the maximal node count n = 10). The 95% confidence intervals were computed using Zhang ’s significance tester (Zhang et al., 2004). We chose five different pruning thresholds in our experiments: p = 0, 2, 5, 8, 10. The forests pruned by p = 0 contained only 1-best tree per sentence. With the increase of p, the average number of trees encoded in one forest rose dramatically. When p was set to 10, there were over 100M parses encoded in one forest on average. p extraction decoding 0 1.26 6.76 2 2.35 8.52 5 6.34 14.87 8 8.51 19.78 10 10.21 25.81 Table 4: Comparison of rule extraction time (seconds/1000 sentence pairs) and decoding time (second/sentence) Moreover, the more trees are encoded in packed forests, the more rules a</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting bleu/nist scores how much improvement do we need to have a better system? In Proc. ofLREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A tree sequence alignment-based tree-to-tree translation model.</title>
<date>2008</date>
<booktitle>In Proc. ofACL/HLT</booktitle>
<contexts>
<context position="1519" citStr="Zhang et al., 2008" startWordPosition="225" endWordPosition="228">ee translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees. 1 Introduction Approaches to syntax-based statistical machine translation make use of parallel data with syntactic annotations, either in the form of phrase structure trees or dependency trees. They can be roughly divided into three categories: string-to-tree models (e.g., (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)), tree-to-string models (e.g., (Liu et al., 2006; Huang et al., 2006)), and tree-totree models (e.g., (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008)). By modeling the syntax of both source and target languages, tree-to-tree approaches have the potential benefit of providing rules linguistically better motivated. However, while string-to-tree and tree-to-string models demonstrate promising results in empirical evaluations, tree-to-tree models have still been underachieving. We believe that tree-to-tree models face two major challenges. First, tree-to-tree models are more vulnerable to parsing errors. Obtaining syntactic annotations in quantity usually entails running automatic parsers on a parallel corpus. As the amount and domain of the d</context>
<context position="2965" citStr="Zhang et al., 2008" startWordPosition="439" endWordPosition="442">ne to learn noisy translation rules in training phase and produce degenerate translations in decoding phase (Quirk and CorstonOliver, 2006). This situation aggravates for treeto-tree models that use syntax on both sides. Second, tree-to-tree rules provide poorer rule coverage. As a tree-to-tree rule requires that there must be trees on both sides, tree-to-tree models lose a larger amount of linguistically unmotivated mappings. Studies reveal that the absence of such non-syntactic mappings will impair translation quality dramatically (Marcu et al., 2006; Liu et al., 2007; DeNeefe et al., 2007; Zhang et al., 2008). Compactly encoding exponentially many parses, packed forests prove to be an excellent fit for alleviating the above two problems (Mi et al., 2008; Mi and Huang, 2008). In this paper, we propose a forest-based tree-to-tree model. To learn STSG rules from aligned forest pairs, we introduce a series of notions for identifying minimal tree-to-tree rules. Our decoder first converts the source forest to a translation forest and then finds the best derivation that has the source yield of one source tree in the forest. Comparable to Moses, our forest-based tree-to-tree model achieves an absolute imp</context>
<context position="26962" citStr="Zhang et al. (2008)" startWordPosition="4811" endWordPosition="4814">ximal phrase length to 10 words, while an STSG rule with 10 nodes in each tree usually cannot subsume 10 words. In machine translation, the concept of packed forest is first used by Huang and Chiang (2007) to characterize the search space of decoding with language models. The first direct use of packed forest is proposed by Mi et al. (2008). They replace 1-best trees with packed forests both in training and decoding and show superior translation quality over the state-of-the-art hierarchical phrasebased system. We follow the same direction and apply packed forests to tree-to-tree translation. Zhang et al. (2008) present a tree-to-tree model that uses STSG. To capture non-syntactic phrases, they apply tree-sequence rules (Liu et al., 2007) to tree-to-tree models. Their extraction algorithm first identifies initial rules and then obtains abstract rules. While this method works for 1-best tree pairs, it cannot be applied to packed forest pairs because it is impractical to enumerate all tree pairs over a phrase pair. While Galley (2004) describes extracting treeto-string rules from 1-best trees, Mi and Huang et al. (2008) go further by proposing a method for extracting tree-to-string rules from aligned f</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence alignment-based tree-to-tree translation model. In Proc. ofACL/HLT 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>