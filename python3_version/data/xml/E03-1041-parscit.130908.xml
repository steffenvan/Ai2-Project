<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.221651">
<title confidence="0.994995">
Lexicon acquisition with a large-coverage unification-based grammar
</title>
<author confidence="0.962214">
Frederik Fouvry
</author>
<affiliation confidence="0.97047">
Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.967344">
PO Box 15 11 50
D-66041 Saarbrticken, Germany
</address>
<email confidence="0.997094">
fouvry@coli.uni-sb.de
</email>
<sectionHeader confidence="0.996627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999775545454546">
We describe how unknown lexical en-
tries are processed in a unification-based
framework with large-coverage gram-
mars and how from their usage lexi-
cal entries are extracted. To keep the
time and space usage during parsing
within bounds, information from exter-
nal sources like Part of Speech (PoS)
taggers and morphological analysers is
taken into account when information is
constructed for unknown words.
</bodyText>
<sectionHeader confidence="0.99877" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895363636364">
For Natural Language Processing (NLP) in gen-
eral, and processing with linguistically rich frame-
works more specifically, unknown words are a
problem. The following gives an idea of the extent
of the problem. In an evaluation of a large-scale
grammar for unrestricted text on a newspaper cor-
pus, we found that the number of failed parses due
to unknown words accounted for around 89% of
the total number of unsuccessful analyses. Even
though this figure does not say anything about the
grammar (these failures may be hiding many oth-
ers), it shows the importance of the problem.
For unification-based implementations, which
often refer to linguistic theories and are therefore
rich in information, one approach to deal with un-
known words has been proposed several times: to
exploit the syntactic context of completed analyses
to collect information about a new word. A few
implementations have been developed to demon-
strate the feasibility of the technique, but to our
knowledge it has not been applied yet to large-
coverage grammars. In this note we discuss how
we are applying it to such a grammar for unre-
stricted text. Starting from this &amp;quot;standard&amp;quot; tech-
nique, we extend it and integrate PoS and mor-
phological information, originating from external
resources.
We will first describe the method of learning in-
formation from the syntactic context. Then we dis-
cuss the current results of our implementation, and
how the external resources are put to use. Finally
an evaluation scheme is presented and some issues
we intend to investigate next.
</bodyText>
<sectionHeader confidence="0.653145" genericHeader="method">
2 Acquiring new information
</sectionHeader>
<bodyText confidence="0.996076916666667">
In a unification-based framework, information is
percolated throughout the parse tree via the re-
entrancies. Information that is underspecified in
a lexical entry very often becomes more specific
when it is used in a parse tree. Take the follow-
ing example. The lexical entry for the French
verb form etaient (&amp;quot;were&amp;quot;) specifies that its sub-
ject should be a plural noun phrase (and in the
third person). When it is combined with the femi-
nine plural noun phrase les conditions (&amp;quot;the condi-
tions&amp;quot;) via U in (1), the information about the sub-
ject of etaient will also include the gender value.
</bodyText>
<page confidence="0.997612">
87
</page>
<figure confidence="0.610385571428571">
(1) HEAD n [verb] [PERSON [third]]
SUBJECT [HEAD [noun] NUMBER [plural] I
AGR
HEAD [noun]
PERSON [third]
AGR NUMBER [ph/rail
_GENDER [feminine]
</figure>
<bodyText confidence="0.999603527777778">
Normally this increase in information is not used
for anything outside the current analysis. With un-
known words however, this property can be used
to find out how they can be used. When a word
is encountered that cannot be found in the lexi-
con, a generic underspecified lexical entry is used,
and for the rest parsing proceeds as usual. The re-
sult is one or more analyses where the informa-
tion of the unknown entry will have been filled
in as described above by the surrounding words.
If instead of les conditions an unknown NP had
been used, we would know from the specifica-
tions on etaient that it should be a plural noun.
The feature structures thus specified are candi-
date lexical entries for the unknown words. This
technique is described by e.g. Erbach (1990) and
Walther and Barg (1998).
As pointed out by all of these authors, these
feature structures will be partly too general and
partly too specific. For instance, case informa-
tion for nouns or gender information for verb com-
plements are in most cases unwanted. On the
other hand, only very little semantic information,
if any, will be found in this way, and it will need
to be supplied by other means. Furthermore, not
all features have the same status. Some are lex-
ical, others are syntactic, semantic and still oth-
ers are bookkeeping features. What should hap-
pen with the acquired information depends on the
status of the features. Barg and Walther (1998)
talk about generalisable and revisable information.
The former are values that are too specific (e.g.
case), while the latter are values that should be
changeable. They work with a formalism that al-
lows value overwriting, and specify in the gram-
mar what values belong to which class.
</bodyText>
<sectionHeader confidence="0.996329" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999813212765957">
The system we used for our implementation is
the Linguistic Knowledge Base (LKB) (Copes-
take, 2002). It processes unification grammars
efficiently (Oepen and Carroll, 2000), and there
are large scale HPsG-style grammars available for
it (e.g. LinG0 (2001)1). We implemented the
method for acquisition of new entries that is de-
scribed in the previous section. The generic entry
for unknown words should satisfy some minimal
requirements: it should prohibit the application
of lexical rules (see below), it should restrict the
number of complements, and it should help main-
tain the presence of information (e.g. semantics),
such that it is not lost only because an unknown
word occurred in the sentence.
In the framework, lexical rules behave like
unary phrase structure rules. If the input to such
a rule is underspecified, the output might not have
a sufficient amount of information filled in to pre-
vent another application of the same rule, and so
on. Therefore, lexical rules should not be applied
to the unknown words at parse time. At this stage
we want to collect the syntactic information of a
string as it is used in the given sentence. After-
wards, the lexical rules can be applied (inversely)
to the structures that were found, so as to generate
the appropriate lexical entries.
Although preliminary tests showed encouraging
results, obtaining analyses became quickly harder
when the sentences got longer, due to the number
of rule applications that was spawned on the un-
derspecified entries. In Head-driven Phrase Struc-
ture Grammar (HPsG), the notion of the head plays
an important role. The constituent which is the
head selects for one or more dependents. If the
unknown word is a head, then the selection of the
dependents is underspecified, which leads to an in-
creased number of solutions. In the current setup,
multiple unknown words in a sentence can almost
never be treated due to the compounded ambigu-
ity.
A second observation was that the amount of in-
formation that is added to the underspecified entry
is surprisingly high. We obtained those results in
the following way. After the feature structure for
the unknown words are extracted from the chart,
we unfill them. This consists of removing the fea-
</bodyText>
<footnote confidence="0.982225333333333">
1Where we refer to the grammar or quote figures relating
to it, we assume the current version of the grammar (October
2002).
</footnote>
<page confidence="0.998487">
88
</page>
<bodyText confidence="0.999881033898305">
tures from the feature structures whose value can
be inferred from the type hierarchy and the con-
straints (Gotz, 1994). About 30% of the feature
structure nodes can be removed (this figure can
vary greatly among feature structures and gram-
mars, but this is a typical figure in our experi-
ments). These feature structures are not totally
well-typed, but can be made so. This is the in-
formation that has been unified in by the context.
The value co-occurrence in these fea-
ture structures is in principle unlimited.
Horiguchi et al. (1995) specify certain fea-
ture co-occurrences in lexical templates to limit
the underspecification, with the goal of making
the search space smaller, and the lexical entries
more specific. The co-occurrence constraints
cannot be acquired with the methods here de-
scribed. A way out is the following. The English
LinG0 grammar defines for the lexicon a set of
special types. These types contain all information
for a class of words. A lexical entry consists of
nothing but the definition of the string and the
semantic relation for the word in addition to the
appropriate lexical type. The lexicon thus relies
on a highly structured hierarchy of relations. A
strategy to increase the specificity of the lexical
entries is to collect these types, and use them as
input for the unknown word entry. The obvious
advantage is that it makes the search space much
more restricted than would be the case with one
underspecified entry.
There is however also a disadvantage: the num-
ber of these types is quite high (463). The amount
of ambiguity here is not caused by the rule appli-
cations, but by the initial number of lexical entries.
To work around that problem we decided to inte-
grate knowledge from external sources. We chose
for a statistical PoS tagger, i.c. Trigrams&apos;n&apos;Tags
(TnT) (Brants, 2000). These taggers return a num-
ber of alternatives each associated with a proba-
bility, so that the parser can decide what will be
used in the analysis. Even when the range of al-
ternatives is left wide open (currently in our ex-
periments the least likely tags that are allowed are
10,000 times less likely than the most likely one),
the number of alternatives remains far below the
number of lexical types.
The information that can be derived from the
tagger output varies with the tag set, but it usu-
ally also contains some morphological informa-
tion. Even though the lexical types are already
highly specified, still more value can be filled
when it is known that certain morphological rules
applied to them. For instance the Penn Treebank
tag NNS (Santorini, 1990) indicates a plural noun.
While the fact that it is a noun is present in the
lexical entry — and can therefore be realised by
a type — the fact that it is a plural will restrict it
further.
</bodyText>
<sectionHeader confidence="0.995511" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999994904761905">
There are two aspects that are relevant to be mea-
sured: the quality of the newly acquired lexical
entries, and the efficiency with which parsing with
unknown words takes place.
We have already discussed where the ambiguity
arises with unknown words. One of the goals that
we will pursue further is to reduce this ambiguity.
Obviously, long sentences, with several unknown
words should be processable. We have not been
able yet to fully assess the impact of the PoS tag-
ger because the mapping from tags to types does
not limit the initial number of entries for the un-
known word sufficiently yet.
The quality of the acquired lexical entries can
be measured as follows. A known entry is re-
moved from the lexicon, and parse trees are con-
structed for sentences containing the word. The
resulting entries are compared to the hand-written
entry. The minimum requirement is that a feature
structure compatible with the hand-written entry
should be found among the results.
</bodyText>
<sectionHeader confidence="0.996871" genericHeader="evaluation">
5 Outlook
</sectionHeader>
<bodyText confidence="0.999978909090909">
There are a number of issues that we should con-
sider before the newly acquired lexical entries
can be used. Among these are the problem of
homonyms and the question how long and how
many feature structures should be collected for a
string.
This approach does not seem to be able to
deal with homonyms. The criterion to distinguish
known words from unknown ones is whether the
string occurs in the lexicon. If of two words that
are homonyms one occurs in the lexicon, then that
</bodyText>
<page confidence="0.999505">
89
</page>
<bodyText confidence="0.999993833333333">
one will always be chosen to provide the feature
structures for the corresponding string. A naive
solution would be to reprocess the input consid-
ering one of the words as an unknown word, but
that is not feasible: how that word can be chosen,
without having to analyse the sentence as many
times as it contains words? Here as well, external
resources like a PoS tagger might provide useful
information: the probabilities will be higher if it
knows about the homonym.
We also intend to look at how long entries
should be collected. Currently new entries are
stored in a temporary lexicon. It is a question how
long they should stay there and how many feature
structures should be collected for a given string.
Some words, for instance spelling errors, should
(probably) not be stored in the final lexicon. When
should they be removed? We expect that these val-
ues will have to be determined experimentally.
It seems that it will also be important to have a
way to deal with conflicting information. This can
be beneficial to deal with information from differ-
ent sources, for instance from a PoS tagger and
from a morphological analyser, or from two fea-
ture structures for the same string. Even if we limit
ourselves to a tagger, there is still the problem of
the high number of solutions that is found when a
sentence contains an unknown word. We should
be able to generalise over the entries to reduce the
number of resulting entries.
</bodyText>
<sectionHeader confidence="0.998807" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.9999946">
We have discussed how new words can be ac-
quired in a large-scale grammar. The basic method
has been proposed before, but not with a grammar
of a similar coverage. We have described a way
how the information concerning unknown words
can be restricted in a grammatically sound way,
by the definition of lexical types and the use of
external knowledge sources. We have discussed
evaluation techniques and mentioned a number of
issues we will have to deal with.
</bodyText>
<sectionHeader confidence="0.998044" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9526352">
The material presented in this note has bene-
fited from discussions with Ulrich Callmeier, Ann
Copestake, Dan Flickinger, Bernd Kiefer, Stephan
Oepen and Melanie Siegel. We also thank three
anonymous reviewers for their comments. The
research was funded by the German Research
Fund DFG in the Collaborative Research Centre
SFB 378 Resource-Adaptive Cognitive Processes,
subproject Performance Modelling for Declarative
Grammar Models (MI I PERFORM).
</bodyText>
<sectionHeader confidence="0.994096" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999748725">
Petra Barg and Markus Walther. 1998. Process-
ing unknown words in HPSG. In Proceedings of
the 17th International Conference on Computational
Linguistics, volume I, pages 91-95, Universite de
Montreal, Montreal, Quebec, Canada, August 10-
14. Also as cs.CL/9809106 from http: //xxx.
lanl.gov/.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of ANLP-2000, Seat-
tle, WA, 29 April-3 May. Association for Computa-
tional Linguistics.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. Stanford, CA.
Gregor Erbach. 1990. Syntactic processing of un-
known words. In P. Jorrand and V. Sgurev, editors,
Artificial Intelligence IV: Methodology, Systems, Ap-
plications, pages 371-381. Amsterdam.
Thilo Gritz. 1994. A normal form for typed feature
structures. Master&apos;s thesis, Seminar ftir Sprachwis-
senschaft, Eberhard-Karls-Universitat, Tubingen,
April.
Keiko Horiguchi, Kentaro Torisawa, and Jun ichi Tsu-
jii. 1995. Automatic acquisition of content words
using an HPSG-based parser. In Proceedings of the
Natural Language Processing Pacific Rim Sympo-
sium, pages 320-325, Seoul, Korea, December.
LinGO. 2001. English Resource Grammar. Avail-
able on-line. http: //lingo. stanford.
edu/ f tp/erg.tgz (26 July 2002).
Stephan Oepen and John Carroll. 2000. Parser en-
gineering and performance profiling Natural Lan-
guage Engineering, 6(1):81-97, March.
Beatrice Santorini, 1990. Part-of-speech tagging
guidelines for the Penn Treebank project, June.
Third revision, second printing (February 1995).
Markus Walther and Petra Barg. 1998. Towards in-
cremental lexical acquisition in HPSG. In Gosse
Bouma, Geert-Jan Kruijff, and Richard Oehrle, ed-
itors, Proceedings of FHCG&apos;98, pages 289-297,
Saarbriicken, August.
</reference>
<page confidence="0.998639">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797431">
<title confidence="0.997721">Lexicon acquisition with a large-coverage unification-based grammar</title>
<author confidence="0.996487">Frederik Fouvry</author>
<affiliation confidence="0.996357">Computational Linguistics Saarland University</affiliation>
<address confidence="0.895412">PO Box 15 11 50 D-66041 Saarbrticken, Germany</address>
<abstract confidence="0.998833833333333">We describe how unknown lexical entries are processed in a unification-based framework with large-coverage grammars and how from their usage lexical entries are extracted. To keep the time and space usage during parsing within bounds, information from external sources like Part of Speech (PoS) taggers and morphological analysers is taken into account when information is constructed for unknown words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Petra Barg</author>
<author>Markus Walther</author>
</authors>
<title>Processing unknown words in HPSG.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics, volume I,</booktitle>
<pages>91--95</pages>
<institution>Universite de Montreal,</institution>
<location>Montreal, Quebec, Canada,</location>
<note>Also as cs.CL/9809106 from http: //xxx. lanl.gov/.</note>
<contexts>
<context position="4348" citStr="Barg and Walther (1998)" startWordPosition="715" endWordPosition="718"> pointed out by all of these authors, these feature structures will be partly too general and partly too specific. For instance, case information for nouns or gender information for verb complements are in most cases unwanted. On the other hand, only very little semantic information, if any, will be found in this way, and it will need to be supplied by other means. Furthermore, not all features have the same status. Some are lexical, others are syntactic, semantic and still others are bookkeeping features. What should happen with the acquired information depends on the status of the features. Barg and Walther (1998) talk about generalisable and revisable information. The former are values that are too specific (e.g. case), while the latter are values that should be changeable. They work with a formalism that allows value overwriting, and specify in the grammar what values belong to which class. 3 Implementation The system we used for our implementation is the Linguistic Knowledge Base (LKB) (Copestake, 2002). It processes unification grammars efficiently (Oepen and Carroll, 2000), and there are large scale HPsG-style grammars available for it (e.g. LinG0 (2001)1). We implemented the method for acquisitio</context>
</contexts>
<marker>Barg, Walther, 1998</marker>
<rawString>Petra Barg and Markus Walther. 1998. Processing unknown words in HPSG. In Proceedings of the 17th International Conference on Computational Linguistics, volume I, pages 91-95, Universite de Montreal, Montreal, Quebec, Canada, August 10-14. Also as cs.CL/9809106 from http: //xxx. lanl.gov/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT: A statistical part-ofspeech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP-2000,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, WA, 29 April-3</location>
<contexts>
<context position="8817" citStr="Brants, 2000" startWordPosition="1462" endWordPosition="1463"> the specificity of the lexical entries is to collect these types, and use them as input for the unknown word entry. The obvious advantage is that it makes the search space much more restricted than would be the case with one underspecified entry. There is however also a disadvantage: the number of these types is quite high (463). The amount of ambiguity here is not caused by the rule applications, but by the initial number of lexical entries. To work around that problem we decided to integrate knowledge from external sources. We chose for a statistical PoS tagger, i.c. Trigrams&apos;n&apos;Tags (TnT) (Brants, 2000). These taggers return a number of alternatives each associated with a probability, so that the parser can decide what will be used in the analysis. Even when the range of alternatives is left wide open (currently in our experiments the least likely tags that are allowed are 10,000 times less likely than the most likely one), the number of alternatives remains far below the number of lexical types. The information that can be derived from the tagger output varies with the tag set, but it usually also contains some morphological information. Even though the lexical types are already highly spec</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT: A statistical part-ofspeech tagger. In Proceedings of ANLP-2000, Seattle, WA, 29 April-3 May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Implementing Typed Feature Structure Grammars.</title>
<date>2002</date>
<location>Stanford, CA.</location>
<contexts>
<context position="4748" citStr="Copestake, 2002" startWordPosition="781" endWordPosition="783">same status. Some are lexical, others are syntactic, semantic and still others are bookkeeping features. What should happen with the acquired information depends on the status of the features. Barg and Walther (1998) talk about generalisable and revisable information. The former are values that are too specific (e.g. case), while the latter are values that should be changeable. They work with a formalism that allows value overwriting, and specify in the grammar what values belong to which class. 3 Implementation The system we used for our implementation is the Linguistic Knowledge Base (LKB) (Copestake, 2002). It processes unification grammars efficiently (Oepen and Carroll, 2000), and there are large scale HPsG-style grammars available for it (e.g. LinG0 (2001)1). We implemented the method for acquisition of new entries that is described in the previous section. The generic entry for unknown words should satisfy some minimal requirements: it should prohibit the application of lexical rules (see below), it should restrict the number of complements, and it should help maintain the presence of information (e.g. semantics), such that it is not lost only because an unknown word occurred in the sentenc</context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>Ann Copestake. 2002. Implementing Typed Feature Structure Grammars. Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Erbach</author>
</authors>
<title>Syntactic processing of unknown words.</title>
<date>1990</date>
<booktitle>Artificial Intelligence IV: Methodology, Systems, Applications,</booktitle>
<pages>371--381</pages>
<editor>In P. Jorrand and V. Sgurev, editors,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="3693" citStr="Erbach (1990)" startWordPosition="604" endWordPosition="605">nd out how they can be used. When a word is encountered that cannot be found in the lexicon, a generic underspecified lexical entry is used, and for the rest parsing proceeds as usual. The result is one or more analyses where the information of the unknown entry will have been filled in as described above by the surrounding words. If instead of les conditions an unknown NP had been used, we would know from the specifications on etaient that it should be a plural noun. The feature structures thus specified are candidate lexical entries for the unknown words. This technique is described by e.g. Erbach (1990) and Walther and Barg (1998). As pointed out by all of these authors, these feature structures will be partly too general and partly too specific. For instance, case information for nouns or gender information for verb complements are in most cases unwanted. On the other hand, only very little semantic information, if any, will be found in this way, and it will need to be supplied by other means. Furthermore, not all features have the same status. Some are lexical, others are syntactic, semantic and still others are bookkeeping features. What should happen with the acquired information depends</context>
</contexts>
<marker>Erbach, 1990</marker>
<rawString>Gregor Erbach. 1990. Syntactic processing of unknown words. In P. Jorrand and V. Sgurev, editors, Artificial Intelligence IV: Methodology, Systems, Applications, pages 371-381. Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thilo Gritz</author>
</authors>
<title>A normal form for typed feature structures.</title>
<date>1994</date>
<booktitle>Master&apos;s thesis, Seminar ftir Sprachwissenschaft,</booktitle>
<location>Eberhard-Karls-Universitat, Tubingen,</location>
<marker>Gritz, 1994</marker>
<rawString>Thilo Gritz. 1994. A normal form for typed feature structures. Master&apos;s thesis, Seminar ftir Sprachwissenschaft, Eberhard-Karls-Universitat, Tubingen, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiko Horiguchi</author>
<author>Kentaro Torisawa</author>
<author>Jun ichi Tsujii</author>
</authors>
<title>Automatic acquisition of content words using an HPSG-based parser.</title>
<date>1995</date>
<booktitle>In Proceedings of the Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>320--325</pages>
<location>Seoul, Korea,</location>
<contexts>
<context position="7533" citStr="Horiguchi et al. (1995)" startWordPosition="1245" endWordPosition="1248">figures relating to it, we assume the current version of the grammar (October 2002). 88 tures from the feature structures whose value can be inferred from the type hierarchy and the constraints (Gotz, 1994). About 30% of the feature structure nodes can be removed (this figure can vary greatly among feature structures and grammars, but this is a typical figure in our experiments). These feature structures are not totally well-typed, but can be made so. This is the information that has been unified in by the context. The value co-occurrence in these feature structures is in principle unlimited. Horiguchi et al. (1995) specify certain feature co-occurrences in lexical templates to limit the underspecification, with the goal of making the search space smaller, and the lexical entries more specific. The co-occurrence constraints cannot be acquired with the methods here described. A way out is the following. The English LinG0 grammar defines for the lexicon a set of special types. These types contain all information for a class of words. A lexical entry consists of nothing but the definition of the string and the semantic relation for the word in addition to the appropriate lexical type. The lexicon thus relie</context>
</contexts>
<marker>Horiguchi, Torisawa, Tsujii, 1995</marker>
<rawString>Keiko Horiguchi, Kentaro Torisawa, and Jun ichi Tsujii. 1995. Automatic acquisition of content words using an HPSG-based parser. In Proceedings of the Natural Language Processing Pacific Rim Symposium, pages 320-325, Seoul, Korea, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LinGO</author>
</authors>
<title>English Resource Grammar. Available on-line.</title>
<date>2001</date>
<note>http: //lingo. stanford. edu/ f tp/erg.tgz (26</note>
<marker>LinGO, 2001</marker>
<rawString>LinGO. 2001. English Resource Grammar. Available on-line. http: //lingo. stanford. edu/ f tp/erg.tgz (26 July 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>John Carroll</author>
</authors>
<title>Parser engineering and performance profiling Natural Language Engineering,</title>
<date>2000</date>
<pages>6--1</pages>
<contexts>
<context position="4821" citStr="Oepen and Carroll, 2000" startWordPosition="789" endWordPosition="792"> still others are bookkeeping features. What should happen with the acquired information depends on the status of the features. Barg and Walther (1998) talk about generalisable and revisable information. The former are values that are too specific (e.g. case), while the latter are values that should be changeable. They work with a formalism that allows value overwriting, and specify in the grammar what values belong to which class. 3 Implementation The system we used for our implementation is the Linguistic Knowledge Base (LKB) (Copestake, 2002). It processes unification grammars efficiently (Oepen and Carroll, 2000), and there are large scale HPsG-style grammars available for it (e.g. LinG0 (2001)1). We implemented the method for acquisition of new entries that is described in the previous section. The generic entry for unknown words should satisfy some minimal requirements: it should prohibit the application of lexical rules (see below), it should restrict the number of complements, and it should help maintain the presence of information (e.g. semantics), such that it is not lost only because an unknown word occurred in the sentence. In the framework, lexical rules behave like unary phrase structure rul</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>Stephan Oepen and John Carroll. 2000. Parser engineering and performance profiling Natural Language Engineering, 6(1):81-97, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank project,</title>
<date>1990</date>
<note>Third revision, second printing</note>
<contexts>
<context position="9578" citStr="Santorini, 1990" startWordPosition="1594" endWordPosition="1595">is. Even when the range of alternatives is left wide open (currently in our experiments the least likely tags that are allowed are 10,000 times less likely than the most likely one), the number of alternatives remains far below the number of lexical types. The information that can be derived from the tagger output varies with the tag set, but it usually also contains some morphological information. Even though the lexical types are already highly specified, still more value can be filled when it is known that certain morphological rules applied to them. For instance the Penn Treebank tag NNS (Santorini, 1990) indicates a plural noun. While the fact that it is a noun is present in the lexical entry — and can therefore be realised by a type — the fact that it is a plural will restrict it further. 4 Evaluation There are two aspects that are relevant to be measured: the quality of the newly acquired lexical entries, and the efficiency with which parsing with unknown words takes place. We have already discussed where the ambiguity arises with unknown words. One of the goals that we will pursue further is to reduce this ambiguity. Obviously, long sentences, with several unknown words should be processab</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Beatrice Santorini, 1990. Part-of-speech tagging guidelines for the Penn Treebank project, June. Third revision, second printing (February 1995).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Walther</author>
<author>Petra Barg</author>
</authors>
<title>Towards incremental lexical acquisition</title>
<date>1998</date>
<booktitle>Proceedings of FHCG&apos;98,</booktitle>
<pages>289--297</pages>
<editor>in HPSG. In Gosse Bouma, Geert-Jan Kruijff, and Richard Oehrle, editors,</editor>
<location>Saarbriicken,</location>
<contexts>
<context position="3721" citStr="Walther and Barg (1998)" startWordPosition="607" endWordPosition="610">n be used. When a word is encountered that cannot be found in the lexicon, a generic underspecified lexical entry is used, and for the rest parsing proceeds as usual. The result is one or more analyses where the information of the unknown entry will have been filled in as described above by the surrounding words. If instead of les conditions an unknown NP had been used, we would know from the specifications on etaient that it should be a plural noun. The feature structures thus specified are candidate lexical entries for the unknown words. This technique is described by e.g. Erbach (1990) and Walther and Barg (1998). As pointed out by all of these authors, these feature structures will be partly too general and partly too specific. For instance, case information for nouns or gender information for verb complements are in most cases unwanted. On the other hand, only very little semantic information, if any, will be found in this way, and it will need to be supplied by other means. Furthermore, not all features have the same status. Some are lexical, others are syntactic, semantic and still others are bookkeeping features. What should happen with the acquired information depends on the status of the featur</context>
</contexts>
<marker>Walther, Barg, 1998</marker>
<rawString>Markus Walther and Petra Barg. 1998. Towards incremental lexical acquisition in HPSG. In Gosse Bouma, Geert-Jan Kruijff, and Richard Oehrle, editors, Proceedings of FHCG&apos;98, pages 289-297, Saarbriicken, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>