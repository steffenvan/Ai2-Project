<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001612">
<title confidence="0.985351">
Exploiting Sophisticated Representations for Document
Retrieval
</title>
<author confidence="0.999457">
Steven Finch
</author>
<affiliation confidence="0.9976785">
Language Technology Group, HCRC
University of Edinburgh
</affiliation>
<email confidence="0.858762">
S.FinchOed.ac.uk
</email>
<sectionHeader confidence="0.986929" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947645833334">
The use of NLP techniques for docu-
ment classification has not produced signif-
icant improvements in performance within
the standard term weighting statistical as-
signment paradigm (Fagan 1987; Lewis,
1992bc; Buckley, 1993). This perplexing
fact needs both an explanation and a so-
lution if the power of recently developed
NLP techniques are to be successfully ap-
plied in IR. A novel method for adding lin-
guistic annotation to corpora is presented
which involves using a statistical POS tag-
ger in conjunction with unsupervised struc-
ture finding methods to derive notions of
&amp;quot;noun group&amp;quot;, &amp;quot;verb group&amp;quot;, and so on
which is inherently extensible to more so-
phisticated annotation, and does not re-
quire a pre-tagged corpus to fit. One of the
distinguishing features of a more linguisti-
cally sophisticated representation of docu-
ments over a word set based representation
of them is that linguistically sophisticated
units are more frequently individually good
predictors of document descriptors (key-
words) than single words are. This leads
us to consider the assignment of descriptors
from individual phrases rather than from
the weighted sum of a word set representa-
tion. We investigate how sets of individu-
ally high-precision rules can result in a low
precision when used together, and develop
some theory about these probably-correct
rules. We then proceed to repeat results
which show that standard statistical mod-
els are not particularly suitable for exploit-
ing linguistically sophisticated representa-
tions, and show that a statistically fitted
rule-based model provides significantly im-
proved performance for sophisticated rep-
resentations. It therefore shows that statis-
tical systems can exploit sophisticated rep-
resentations of documents, and lends some
support to the use of more linguistically
sophisticated representations for document
classification. This paper reports on work
done for the LRE project SISTA, which is
creating a PC based tool to be used in the
technical abstracting industry.
</bodyText>
<sectionHeader confidence="0.448792" genericHeader="categories and subject descriptors">
1 Models and Representations
</sectionHeader>
<bodyText confidence="0.999903458333333">
First, I discuss the general paradigm for document
classification, along with the conventions for nota-
tion used throughout this document. We have a
set of documents {xi}, and set of descriptors, {di}.
Each document is represented in one or more ways
in some domain, usually as a set. The elements of
this set will be called diagnostic units or predicates,
{wi} or {0i}. These diagnostic units might be the
words comprising the document, or more linguisti-
cally sophisticated annotations of parts of the doc-
ument. They may, in general, be predicates over
documents. The representation of the document by
diagnostic units will be called the DU-representation
of the document, and for a document x, will be de-
noted R.(x). From the DU representation of the doc-
uments, one or more descriptors are assigned to each
of them by some automatic system. This paradigm
of description is applicable to much of the work on
text classification (and other fields in information
retrieval).
This paper assesses the utility of using linguisti-
cally sophisticated diagnostic units together with a
slightly non-standard statistical assignment model
in order to assign descriptors to a document.
</bodyText>
<sectionHeader confidence="0.909382" genericHeader="method">
2 The Corpus
</sectionHeader>
<bodyText confidence="0.989568777777778">
This paper reports work undertaken for the LRE
project SISTA (Semi-automatic Indexing System for
Technical Abstracts). This section briefly describes
one of the corpora used by this project.
The RAPRA corpus comprises some 212,000 tech-
nical abstracts pertaining to research and commer-
cial exploitation in the rubber and plastics industry.
To each abstract, an average of 15 descriptors se-
lected from a thesaurus of some 10,000 descriptors
</bodyText>
<page confidence="0.999277">
65
</page>
<bodyText confidence="0.968476485714286">
is assigned to each article. The frequency of assign-
ment of descriptors varies roughly in the same way
as the frequency of word use varies (the frequencies
of descriptor tokens (very) approximately satisfies
the Zipf-Mandelbrot law). Descriptors are assigned
by expert indexers from the entire article and expert
domain knowledge, not just from the abstract, so it
is unlikely that any automatic system which analy-
ses only the abstracts can assign all the descriptors
which are manually assigned to the abstract.
We show a fairly typical example below. It is clear
that many of these descriptors must have been as-
signed from the main text of the article, and not
from the abstract alone. Moreover, this is common
practice in the technical abstract indexing industry,
so it seems unlikely that the situation will be better
for other corpora. Nevertheless, we can hope to fol-
low a strategy of assigning descriptors when there is
enough information to do so.
Macromolecular Deformation Model to Estimate
Viscoelastic Flow Effects in Polymer Melts
The elastic deformation of polymer macromolecules in a
shear field is used as the basis for quantitative predic-
tions of viscoelastic flow effects in a polymer melt. Non-
Newtonian viscosity, capillary end correction factor, maxi-
mum die swell, and die swell profile of a polymer melt are
predicted by the model. All these effects can be reduced
to generic master curves, which are independent of polymer
type. Macromolecular deformation also influences the brit-
tle failure strength of a processed polymer glass. The model
gives simple and accurate estimates of practically important
processing effects, and uses fitting parameters with the clear
physical identity of viscoelastic constants, which follow well
established trends with respect to changes in polymer com-
position or processing conditions. 12 refs.
</bodyText>
<construct confidence="0.686879714285714">
Original assignment: BRITTLE FAILURE; COMPANY;
DATA; DIE SWELL; ELASTIC DEFORMATION; EQUATION;
GRAPH; MACROMOLECULE; MELT FLOW; MODEL; NON-
NEWTONIAN; PLASTIC; POLYMERIC GLASS; PROCESSING;
RHEOLOGICAL PROPERTIES; RHEOLOGY; TECHNICAL; THE-
ORY; THERMOPLASTIC; VISCOELASTIC PROPERTIES; VIS-
COELASTICITY; VISCOSITY
</construct>
<sectionHeader confidence="0.985521" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.9918269">
Two classes of models for assessing descriptor appro-
priateness were used. One class comprises variants
of Salton&apos;s term-weighting models, and one is more
allied to fuzzy or default logic in so much as it assigns
descriptors due to the presence of certain diagnos-
tic units. What is interesting for us is that term
weighting models do not seem able to easily exploit
the additional information provided by a more so-
phisticated representation of a document, while an
alternative statistical single term model can.
</bodyText>
<subsectionHeader confidence="0.996705">
3.1 Term weighting models
</subsectionHeader>
<bodyText confidence="0.9995476">
The standard term weighting model is defined by
chosing a set of parameters erij} (one for each word-
descriptor pair) and {A} (one for each descriptor)
so that a likelihood or appropriateness function,
can be defined by
</bodyText>
<equation confidence="0.9758265">
,c(diw). E awd 4-13d (1)
evEW
</equation>
<bodyText confidence="0.99986996969697">
This has been widely used, and is provably equiv-
alent to a large class of probabilistic models (e.g.
Van Risjbergen, 1979) which make various assump-
tions about the independence between descriptors
and diagnostic units (Fuhr &amp; Buckley, 1993). Vari-
ous strategies for estimating the parameters for this
model have been proposed (e.g. Salton &amp; Yang,
1973, Buckley 1993, Fuhr &amp; Buckley, 1993). Some
of these concentrate on the need for re-estimating
weights according to relevance feedback information,
while some make use of various functions of term
frequency, document frequency, maximum within-
document frequency, and various other measure-
ments of corpora. Nevertheless, the problem of esti-
mating the huge number of parameters needed for
such a model is statistically problematic, and as
Buckley (1993) points out, the choice of weights has
a large influence on the effectiveness of any model
for classification or for retrieval.
There are so many variations on the theme of term
weighting models that it is impossible to try them
all in one experiment, so this paper uses a variation
of a model used by Lewis (1992c) in which he re-
ports the results of some experiments using phrases
in a term weighting model (which has a probabilistic
interpretation). Several term weighting models have
been tried, but they all evaluate within 5 points of
each other on both precision and recall (when suit-
ably tweaked).
The model eventually chosen for the tests reported
here was a smoothed logistic model which gave the
best results of all the probabilistically inspired term
weighting models considered.
</bodyText>
<sectionHeader confidence="0.596474" genericHeader="method">
3 2 Single term model
</sectionHeader>
<bodyText confidence="0.999786923076923">
In contrast to making assumptions of independence
about the relationship between diagnostic units and
words, the next model utilises only those diagnostic
units which strongly predict descriptors (i.e. have
frequently been associated with descriptors) with-
out making assumptions about the independence of
diagnostic units given descriptors.
We shall investigate this class of models using
probability theory. The main problem with using
probability theory for problems in document classi-
fication is that while it might be relatively easy to
estimate probabilities such as P(d1w) for some diag-
nostic unit w and some descriptor d, it is not possible
</bodyText>
<page confidence="0.946461">
66
</page>
<bodyText confidence="0.9997946">
to infer much about P(dital), where ill is some ad-
ditional information (e.g. the other DUs which rep-
resent the document), since these probabilities have
not been estimated, and would take a far larger cor-
pus to reliably estimate in any case. The situation
gets exponentially worse as the information we have
about the document increases. The exception to this
rule is when P(dlw) is close to 1, in which case it is
very unlikely that additional information changes its
value much. This fact is further investigated now.
The strategy explored here is to concentrate on
finding &amp;quot;sure-fire&amp;quot; indicators of descriptors, in a
somewhat similar manner to how Carnegie&apos;s TCS
works, by exploiting the fact that with a pre-
classified training corpus we can identify sure-fire
indicators empirically and &amp;quot;trawl&amp;quot; in a large set of
informative diagnostic units for those which identify
descriptors with high precision. The basis of the
model is the following:
We consider a likelihood function, E defined by:
</bodyText>
<equation confidence="0.8853555">
Nato
C(diw)= N.
</equation>
<bodyText confidence="0.999436666666667">
That is, the number of articles in the training cor-
pus that d was observed to occur with w divided by
the number of articles in which w occurred in the
training corpus. This is an empirical estimate of the
conditional probability, P(d1w). We shall assume
(for simplicity&apos;s sake) that we have a large enough
corpus do reliably estimate these probabilities.
The strategy for descriptor assignment we are in-
vestigating is to assign a descriptor d if and only
if one of a set of predicates over representations of
documents is true. We define the rule 0(x) d
to be Probably Correct do degree if and only if
P(dI0) &gt; 1—e. We wish to keep the precision result-
ing from using this strategy high while increasing the
number of rules to improve recall. The predicates 0
we shall consider for this paper will be very simple
(they will typically be true if w E 7(x) for some
diagnostic unit w), but in principle, they could be
arbitrarily complex (as they are in Carnegie&apos;s TCS).
The primary question of concern is whether the en-
semble of rules {0i —+ d} retains precision or not.
Unfortunately, the answer to this question is that
this is not necessarily the case unless we put some
constraints on the predicates.
</bodyText>
<construct confidence="0.968002666666667">
Proposition 1 Let (1) be a set of predicates with the
property that for some fixed descriptor d, E
P(d1)&gt; 1— E. That is each of the rules 0i --+ d is
probably correct to degree E.
The expected precision of the rule (V 00 d is
at least 1
</construct>
<note confidence="0.347483">
Proof:
</note>
<subsubsectionHeader confidence="0.780668">
[Straight-forward and omitted]
</subsubsectionHeader>
<bodyText confidence="0.982936059701493">
This proposition asserts that one cannot be guar-
anteed to be able to keep adding diagnostic units to
improve recall without hurting precision, unless the
quality of those diagnostic units is also improved (i.e.
E is decreased in proportion to the number of DUs
which are considered). This is unfortunate, but nev-
ertheless the question of how much adding diagnostic
units to help recall will hurt precision is an entirely
empirical matter dependent on the true nature of P;
this proposition is a worst case, and gives us reason
to be careful. Performance will be expected to be
poorest if there are many rules which correspond to
the same true positives, but different sets of false
positives. If the predicates are disjoint, for example,
then the precision of a disjunction is at least as great
as the precision of applying any single rule.
So if we design our predicates so that they are dis-
joint, then we retain precision while increasing recall.
In practice, this is infeasible, but it is feasible to look
more carefully at frequently co-occurring predicates,
since these will be most likely to reduce precision.&apos;
The main moral we can draw from the above two
propositions is that we must be careful about the
case where diagnostic units are highly correlated.
One situation which is relatively frequent as the
sophistication of representation increases is that
some diagnostic units always co-occur with others.
For example, if the document were represented by
sequences of words, then the sequence &amp;quot;olefin poly-
merisation&amp;quot; always occurs whenever the sequence
&amp;quot;high temperature olefin polymerisation&amp;quot; occurs. In
this case, it might be thought to pay to look only
at the most specific diagnostic units since we have
if w1 w2, then P(Xlwiw2C)
= P(XlwiC) for
any distribution P whatsoever (here, C represents
any other contextual information we have, for exam-
ple the other diagnostic units representing the doc-
ument). However, if w1 is significantly less frequent
than w2 estimation errors of P(dlwi) will be larger
for P(d1w2) for any descriptor d, so there may not be
a significant advantage. However, it does give us a
&apos;One classic example is the case of the &amp;quot;New Hamp-
shire Yankee Power Plant&amp;quot;. In a collection of New York
Times articles studied by Jacobs &amp; Rau (1990), the word
&amp;quot;Yankee&amp;quot; was found to predict NUCLEAR POWER because
of the frequent occurrence of articles about this plant.
However, &amp;quot;Yankee&amp;quot; on its own without the other words in
this phrase is a good predictor of articles about the New
York Yankees, a baseball team. If highly mutually infor-
mative words are combined into conjunctive predicates
(e.g. &amp;quot;Yankee&amp;quot; E x &amp; &amp;quot;Plant&amp;quot; E x), and a document
is represented by its most specific predicates only, then
when &amp;quot;Yankee&amp;quot; appears alone, it will be a good predic-
tor of the descriptor SPORT. This example can also show
that the bound described above is tight. Imagine (sus-
pending belief) that each of the five words in the phrase
have the same number of occurrences, i, in the document
collection without NUCLEAR POWER where they never oc-
cur together pairwise, and always occur all together in
j true positives of the descriptor. Then the precision of
assigning NUCLEAR POWER if any one of them appears
in a document is —7— and since e in this case is = the
3+5., t+.7
bound follows (for the case n = 5) with a little algebra.
e where n is the cardinality, 01.
n
</bodyText>
<page confidence="0.984261">
67
</page>
<bodyText confidence="0.99828875">
theoretical reason to believe that representing a doc-
ument by its set of most specific predicates is worth
investigating, and this shall be investigated below.
If one considers a calculus similar to the one de-
scribed here, but allows c to limit to 0, then a
weak default logic ensues which has been studied
by Adams (1975), and further investigated by Pearl
(1988).
</bodyText>
<sectionHeader confidence="0.956132" genericHeader="method">
4 Adding linguistic description
</sectionHeader>
<bodyText confidence="0.999963692307692">
The simplest way of representing a document is as
a set or multi set of words. Many people (eg. Lewis
1992bc; Jacobs &amp; Rau 1990) have suggested that a
more linguistically sophisticated representation of a
document might be more effective for the purposes of
statistical keyword assignment. Unfortunately, at-
tempts to do this have not been found to reliably
improve performance as measured by recall and pre-
cision for the task of document classification. I shall
present evidence that a more sophisticated repre-
sentation makes better predictions from the Single
Term model defined above than it does from stan-
dard term weighting models.
</bodyText>
<subsectionHeader confidence="0.986989">
4.1 Linguistic description
</subsectionHeader>
<bodyText confidence="0.999418651162791">
The simplest form of linguistic description of the
content of a machine-readable document is in the
form of a sequence (or a set) of words. More so-
phisticated linguistic information comes in several
forms, all of which may need to be represented if
performance in an automatic categorisation exper-
iment is to be improved. Typical examples of lin-
guistically sophisticated annotation include tagging
words with their syntactic category (although this
has not been found to be effective for IR), lemma of
the word (e.g. &amp;quot;corpus&amp;quot; for &amp;quot;corpora&amp;quot;), phrasal in-
formation (e.g. identifying noun groups and phrases
(Lewis 1992c, Church 1988)), and subject-predicate
identification (e.g. Hindle 1990). For the RAPRA
corpus, we currently identify noun groups and ad-
jective groups.
This is achieved in a manner similar to Church&apos;s
(1988) PARTS algorithm used by Lewis (1992bc),
in the sense that its main properties are robustness
and corpus sensitivity. All that is important for this
paper is that the technique identifies various group-
ings of words (for example, noun-groups, adjective
groups, and so on) with a high level of accuracy.
Major parts of the technique are described in detail
in Finch, 1993. As an example, this is some of the
linguistic markup which represents the title of the
sample document shown earlier.
macromolecular deformation (NG); macromolecular defor-
mation model (NG); deformation (NG); deformation model
(NG); model (NG); viscoelastic flow (NG); viscoelastic flow
effects (Nos); flow (No); flow effects (Nos); effects (Nos);
polymer (NG); polymer melts (Nos); melts (Nos)
It is clear that the markup is far from sophisti-
cated, and is very much a small variation on a sim-
ple sequence-based representation. Nevertheless, it
is fairly accurate in so much as well over 90% of
what are claimed to be noun groups can be inter-
preted as such. One very useful by-product of using
a linguistically based representation is that IR can
help in linguistic tasks such as terminological col-
lection. I shall present some examples of diagnostic
units which are highly associated with descriptors
later.
</bodyText>
<sectionHeader confidence="0.8782015" genericHeader="method">
5 Predicting from sophisticated
representations
</sectionHeader>
<bodyText confidence="0.998713608695652">
In what follows, we shall compare the relative per-
formance of a term weighting model with the single
term model as we vary the sophistication of repre-
sentation.
Proportional assignment (Lewis 1992b) is used to
assign the descriptors from statistical measurements
of their appropriateness. This method ensures that
roughly the same number of assignments of particu-
lar descriptors are made as are actually made in the
test corpus. The strategy is simply to assign descrip-
tor d to the N documents which score highest for
this descriptor, where N is chosen in proportion to
the occurrence of d in the training corpus. For term
weighting models, the score is simply the combined
weight of the document; for the single term model,
the score is sup. e/z() P(djw). The Rule Based as-
signment strategy applies only to the single term
model and the rule w --+ d is included just in case
P(d1w) &gt; 1 — E.
Figure 1 shows a few of the rules. All of these
entries share the property that P(d1w) &gt; 0.8. They
were selected at random from the 85,500 associations
which were found.
</bodyText>
<subsectionHeader confidence="0.979259">
5.1 Representations and models
</subsectionHeader>
<bodyText confidence="0.999971416666667">
Five paradigms of representation of documents will
be compared, and two term appropriateness models
will be compared. This gives us ten combinations.
The first representation paradigm is a baseline one:
represent documents as the set of the words con-
tained in them. The second paradigm is to repre-
sent documents according to word sequences, and
the third is to apply a noun-group and adjective-
group recogniser. The fourth and fifth representa-
tion modes consider representing documents by only
their most specific diagnostic units. For example, if
the sequence &amp;quot;thermoplastic elastomer compounds&amp;quot;
</bodyText>
<page confidence="0.998941">
68
</page>
<figureCaption confidence="0.992528">
Figure 1: This figure shows some probably correct rules for the RAPRA corpus. In all, there are over 85,000
such rules.
</figureCaption>
<figure confidence="0.9987719375">
polymer materials Research/NG;
EEC legislation/NGS;
venture partners/NGS;
Bergen op/NP
sheet fines/NGS
railroad/NG
injection moulding facility/NG
PHENOLPHTHALEIN/NP
unsaturated polyester composites/NGS
thermoplastic elastomer compounds/NGS
properties features/NGS
fiber Glass/NG
comparative performance/NG
automotive hose/NGS
Bitruder/NP
worldwide tyre/NG
Victrex polyethersulphone/NP
PS melts/NGS
viscoelastic characteristics/NGS
plastics waste/NG
lattice relaxation/NG
fatigue crack propagation/NG
unidirectional composites/NGS
Flory Huggins interaction/NG
DATA
LEGISLATION
-4 JOINT VENTURE
-4. PLASTIC
COMPANY
COMPANY
PLASTIC
--r DATA
THERMOSET
--■ RUBBER
PLASTIC
GLASS FIBRE REINFORCED PLASTIC
DATA
RUBBER
EXTRUDER
COMPANIES
COMPANIES
PLASTIC
VISCOELASTIC PROPERTIES
RECYCLING
--r NUCLEAR MAGNETIC RESONANCE
MECHANICAL PROPERTIES
REINFORCED PLASTIC
TECHNICAL
</figure>
<bodyText confidence="0.999460125">
appeared in the abstract, then ordinarily this would
include the sequence &amp;quot;elastomer compounds&amp;quot;, which
would be included in the representation. The results
of section 3.2 might encourage us to believe that rep-
resenting a document by only its most specific diag-
nostic units will improve performance (or, at least,
precision). Consequently, a sequence of words is de-
fined to be most specific if (a) it is a diagnostic unit
and (b) it is not properly contained in a token of any
other diagnostic unit present in the document.&apos;
The noun-groups are found by performing a sim-
ple parse of the documents as described above, and
identifying likely noun groups of length 3 or less.
The contingency table of diagnostic units verses
manually assigned descriptors on a training corpus of
200,000 documents was collected, and this was used
as the basis for two term appropriateness models.
Probabilities were estimated by adding a constant
(usually 0.02 was found fairly optimal) to each cell,
and directly estimating from these slightly adjusted
counts.
The 50,000 most frequent diagnostic unit types
were chosen, and terms which appeared in more than
10% of documents were discarded.
</bodyText>
<footnote confidence="0.63048075">
2I1 &amp;quot;elastomer compounds&amp;quot; appeared separately in
the document from &amp;quot;thermoplastic elastomer com-
pounds&amp;quot;, then both of these sequences would be rep-
resented in the experiments reported here.
</footnote>
<sectionHeader confidence="0.999852" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.889340419354839">
The results of the experiments on the RAPRA cor-
pus are presented below.3
Despite the peculiarities of the corpus, the mes-
sage is clear. The result that the standard model
fares no better on word sequence sets than on word
sets is repeated, and it is clear that the Single Term
model fares much better than the Logit model on
this data set. However, what is most interesting is
that the Single Term models fares significantly bet-
ter on the more sophisticated sequence based repre-
sentations of the document than on the simpler word
based representation. There is, however, no signifi-
cant advantage identified by parsing the corpus into
noun-groups over simply considering all word se-
quences. The recall scores for the rule-based tagging
strategy show that the improved performance of the
sequence based representations can be explained by
3A11 recall and precision scores are microaveraged
(Lewis 1992c); they are the expected probability of as-
signing or recalling correctly per tagging decision. The
training set was a set of 200,000 abstracts, and the sep-
arate test set had 10,000 abstracts. The experiments
looked at only the 520 most common descriptors. In
the table, TW means that a term-weighting model was
used, while ST means that the single term model was
used. &apos;Word&apos; means the representation was a wordset,
`Seq&apos;, the set of all sequences, and &apos;NG&apos; the set of groups
derived from the grammar. For the sequence represen-
tations, either all the possible sequences or groups were
used (denoted by &apos;all&apos;), or just the most specific ones
were used (denoted by `spec&apos;).
</bodyText>
<page confidence="0.998039">
69
</page>
<bodyText confidence="0.9455805">
the presence of many more &amp;quot;good&amp;quot; descriptor indi-
cators.
</bodyText>
<table confidence="0.9993788">
Assignment Model Repn Prec Rec
Prop. TW Word 33% 32%
Prop. TW Seq all 32% 34%
Prop. TW Seq spec 33% 34%
Prop. TW NG all 31% 36%
Prop. TW NG spec 32% 32%
Prop. ST Word 54% 48%
Prop. ST Seq all 57% 55%
Prop. ST Seq spec 55% 55%
Prop. ST NG 56% 60%
Rule c = .2 ST Word 83% 7%
Rule e = .2 ST Seq all 77% 42%
Rule E = .2 ST Seq spec 80% 40%
Rule E = .2 ST NG all 82% 42%
Rule E = .2 ST NG spec 84% 37%
</table>
<sectionHeader confidence="0.991634" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999981578947368">
The significant theoretical result is that as the so-
phistication of the representation of abstracts is in-
creased, the performance of the single term model
improves, while the performance of the term weight-
ing models does not improve significantly. This
has been a fairly universal experience among re-
searchers working within the term weighting clas-
sification paradigm.
Although there is a very marginally significant
improvement from using linguistically sophisticated
representations over simple sequence representations
if all of the sequences are represented, this largely
(though not entirely) disappears when only most
specific sequences are considered, so it might be a
result of the effects discussed in section 3.2.
The rule based assignment strategy exploits the
Single Term model&apos;s estimates, and also performs
much better on word sequence representations than
on word set representations. This assignment strat-
egy is promising because it can exploit more sophis-
ticated representations well, has a sound theory be-
hind it, and will assign descriptors only where it has
enough information to do so. Some of the descrip-
tors in the RAPRA corpus, for example, are only
ever assigned from the entire article from which the
abstract is taken, so no assignment strategy will ever
do well on these. On the other hand this model also
shows promise that IR techniques might be applied
to help infer linguistic resources such as term banks
from large classified corpora.
The next stage is to add more sophisticated lin-
guistic annotation to corpora, and to trawl for rules
in boolean combinations of descriptors, thus ad-
dressing the results of section 3.2. In this way this
work can be considered similar in spirit to that un-
dertaken by Apte et al (1994), but differs in the
forms of representation which are being considered
for documents.
</bodyText>
<sectionHeader confidence="0.991911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999364">
Adams, E. (1975) The Logic of Conditionals: an appli-
cation of probability to deductive logic Reidel.
Apte, C, F. Demerau &amp; S. Weiss (1994) Towards Lan-
guage Independent Automated Learning of Text Cate-
gorization Methods. the proceeding of the Seventeenth
ACM-SIGIR Conference on Information Retrieval. 23-
30, DCU, Dublin.
Buckley, C. (1993) The Importance of Proper Weighting
Methods. A RPA Workshop on Human Language Tech-
nology.
Church, K. (1988) A stochastic parts program and noun
phrase parser for unrestricted text. In Second conference
on applied NLP, pp 136-43.
Church, K., W. Gale, P. flanks &amp; D. Hindle (1989) Pars-
ing, Word Associations and Typical Predicate-Argument
Relations. In International Parsing Technologies Work-
shop. CMU, Pittsburgh.
Fagan, J. (1987) Experiments in Automatic Phrase In-
dexing for Document Retrieval: Comparison of Syntactic
and Non-Syntactic Methods. PhD Thesis. Cornell Uni-
versity, Dept. of Computer Science.
Finch, S. P. &amp; N. Chater (1991) A Hybrid Approach to
the Automatic Learning of Linguistic Categories. Artifi-
cial Intelligence and Simulated Behaviour Quarterly. 78
16-24.
Finch, S. (1993) Finding Structure in Language. Ph.D.
thesis, Centre for Cognitive Science, University of Edin-
burgh, Edinburgh.
Fuhr, N. (1989) Models for retrieval with probabilistic in-
dexing. Information processing and management. 25(1):
55-72.
Fuhr, N. &amp; Buckley, C (1993) Optimizing Document In-
dexing and Search Term Weighting Based on Probabilis-
tic Models First TREC Conference.
Hindle, D. (1990) Noun Classification from Predicate-
Argument Structures. In Proceedings of the 22nd meet-
ing of the Association of Computational Linguistics.
268-75.
Jacobs, P. &amp; Rau, L. (1990) SCISOR: Extracting Infor-
mation from On-line News Correspondence of the ACM
33 11 88-97
Kupiec, J. (1992) Robust part-of-speech tagging using a
hidden Markov model. Computer Speech and Language,
6:3 225-42.
Lewis, D. (1991) Evaluating text categorisation. In
Speech and natural language workshop. pp 136-143.
Lewis, D. (1992a) Representation and learning in infor-
mation retrieval. Ph.D. thesis, Computer Science Dept.,
Univ. Mass., Amherst, Ma.
Lewis, D. (1992b) An Evaluation of Phrasal and Clus-
tered Representations on a Text categorization problem.
Proceedings of SIGIR 92.
Lewis, D. (1992c) Feature selection and feature extrac-
tion for text categorization. In Speech and Natural Lan-
guage: Proceedings of a Workshop held at Harrimn, NY.
pp 212-217.
Lewis, D. &amp; K. Sparck-Jones (1993) Natural language
processing for information retrieval University of Cam-
bridge Technical report 307, Cambridge.
</reference>
<page confidence="0.966998">
70
</page>
<reference confidence="0.9985928">
Pearl, J. (1988) Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference Morgan Kauf-
mann, San Mateo, Ca.
van Rijsbergen, C. J. (1979) Information retrieval. But-
terworths, London.
Sacks-Davis, R. (1990) Using Syntactic Analysis in a
Document Retrieval System that Uses Signature Files.
ACM SIGIR-90.
Salton, G. &amp; McGill, M. J. (1983) Introduction to mod-
ern information retrieval. McGraw-Hill, NY.
Salton, G. &amp; C. Buckley (1988) Term Weighting Ap-
proaches in Automatic Text Retrieval Information Pro-
cessing and Management 24 5 513-23
Zadeh, L. (1965) Fuzzy Sets Information and control, bf
8 338-53.
</reference>
<page confidence="0.999123">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996053">Exploiting Sophisticated Representations for Document Retrieval</title>
<author confidence="0.999825">Steven Finch</author>
<affiliation confidence="0.993647">Language Technology Group, HCRC University of Edinburgh</affiliation>
<email confidence="0.996992">S.FinchOed.ac.uk</email>
<abstract confidence="0.981185115384616">The use of NLP techniques for document classification has not produced significant improvements in performance within the standard term weighting statistical assignment paradigm (Fagan 1987; Lewis, 1992bc; Buckley, 1993). This perplexing fact needs both an explanation and a solution if the power of recently developed NLP techniques are to be successfully apin novel method for adding linguistic annotation to corpora is presented which involves using a statistical POS tagger in conjunction with unsupervised structure finding methods to derive notions of &amp;quot;noun group&amp;quot;, &amp;quot;verb group&amp;quot;, and so on which is inherently extensible to more sophisticated annotation, and does not require a pre-tagged corpus to fit. One of the distinguishing features of a more linguistically sophisticated representation of documents over a word set based representation of them is that linguistically sophisticated units are more frequently individually good predictors of document descriptors (keywords) than single words are. This leads us to consider the assignment of descriptors from individual phrases rather than from the weighted sum of a word set representation. We investigate how sets of individually high-precision rules can result in a low precision when used together, and develop some theory about these probably-correct rules. We then proceed to repeat results which show that standard statistical models are not particularly suitable for exploiting linguistically sophisticated representations, and show that a statistically fitted rule-based model provides significantly improved performance for sophisticated representations. It therefore shows that statistical systems can exploit sophisticated representations of documents, and lends some support to the use of more linguistically sophisticated representations for document classification. This paper reports on work done for the LRE project SISTA, which is creating a PC based tool to be used in the technical abstracting industry. 1 Models and Representations First, I discuss the general paradigm for document classification, along with the conventions for notation used throughout this document. We have a of documents {xi}, and set of Each document is represented in one or more ways in some domain, usually as a set. The elements of set will be called units {wi} or {0i}. These diagnostic units might be the words comprising the document, or more linguistically sophisticated annotations of parts of the document. They may, in general, be predicates over documents. The representation of the document by units be called the the document, and for a document be denoted R.(x). From the DU representation of the documents, one or more descriptors are assigned to each of them by some automatic system. This paradigm of description is applicable to much of the work on text classification (and other fields in information retrieval). This paper assesses the utility of using linguistically sophisticated diagnostic units together with a slightly non-standard statistical assignment model in order to assign descriptors to a document. 2 The Corpus This paper reports work undertaken for the LRE project SISTA (Semi-automatic Indexing System for Technical Abstracts). This section briefly describes one of the corpora used by this project. The RAPRA corpus comprises some 212,000 technical abstracts pertaining to research and commercial exploitation in the rubber and plastics industry. To each abstract, an average of 15 descriptors selected from a thesaurus of some 10,000 descriptors 65 is assigned to each article. The frequency of assignment of descriptors varies roughly in the same way as the frequency of word use varies (the frequencies of descriptor tokens (very) approximately satisfies the Zipf-Mandelbrot law). Descriptors are assigned by expert indexers from the entire article and expert domain knowledge, not just from the abstract, so it is unlikely that any automatic system which analyses only the abstracts can assign all the descriptors which are manually assigned to the abstract. We show a fairly typical example below. It is clear that many of these descriptors must have been assigned from the main text of the article, and not from the abstract alone. Moreover, this is common practice in the technical abstract indexing industry, it seems unlikely that the situation better for other corpora. Nevertheless, we can hope to follow a strategy of assigning descriptors when there is enough information to do so. Macromolecular Deformation Model to Estimate Viscoelastic Flow Effects in Polymer Melts The elastic deformation of polymer macromolecules in a shear field is used as the basis for quantitative predictions of viscoelastic flow effects in a polymer melt. Non- Newtonian viscosity, capillary end correction factor, maximum die swell, and die swell profile of a polymer melt are predicted by the model. All these effects can be reduced to generic master curves, which are independent of polymer type. Macromolecular deformation also influences the brittle failure strength of a processed polymer glass. The model gives simple and accurate estimates of practically important processing effects, and uses fitting parameters with the clear physical identity of viscoelastic constants, which follow well established trends with respect to changes in polymer composition or processing conditions. 12 refs. assignment: FAILURE; COMPANY; DATA; DIE SWELL; ELASTIC DEFORMATION; EQUATION; GRAPH; MACROMOLECULE; MELT FLOW; MODEL; NON- NEWTONIAN; PLASTIC; POLYMERIC GLASS; PROCESSING; RHEOLOGICAL PROPERTIES; RHEOLOGY; TECHNICAL; THE- ORY; THERMOPLASTIC; VISCOELASTIC PROPERTIES; VIS- COELASTICITY; VISCOSITY 3 Models Two classes of models for assessing descriptor appropriateness were used. One class comprises variants Salton&apos;s and one is more allied to fuzzy or default logic in so much as it assigns descriptors due to the presence of certain diagnostic units. What is interesting for us is that term weighting models do not seem able to easily exploit the additional information provided by a more sophisticated representation of a document, while an alternative statistical single term model can. Term models The standard term weighting model is defined by a set of parameters (one for each worddescriptor pair) and {A} (one for each descriptor) so that a likelihood or appropriateness function, can be defined by evEW This has been widely used, and is provably equivalent to a large class of probabilistic models (e.g. Van Risjbergen, 1979) which make various assumptions about the independence between descriptors diagnostic units (Fuhr 1993). Various strategies for estimating the parameters for this model have been proposed (e.g. Salton &amp; Yang, Buckley 1993, Fuhr 1993). Some of these concentrate on the need for re-estimating weights according to relevance feedback information, while some make use of various functions of term frequency, document frequency, maximum withindocument frequency, and various other measurements of corpora. Nevertheless, the problem of estimating the huge number of parameters needed for such a model is statistically problematic, and as Buckley (1993) points out, the choice of weights has a large influence on the effectiveness of any model for classification or for retrieval. There are so many variations on the theme of term weighting models that it is impossible to try them one experiment, so this paper uses a variation of a model used by Lewis (1992c) in which he reports the results of some experiments using phrases in a term weighting model (which has a probabilistic interpretation). Several term weighting models have been tried, but they all evaluate within 5 points of each other on both precision and recall (when suitably tweaked). The model eventually chosen for the tests reported here was a smoothed logistic model which gave the best results of all the probabilistically inspired term weighting models considered. 2 Single model In contrast to making assumptions of independence about the relationship between diagnostic units and words, the next model utilises only those diagnostic units which strongly predict descriptors (i.e. have frequently been associated with descriptors) without making assumptions about the independence of diagnostic units given descriptors. We shall investigate this class of models using probability theory. The main problem with using probability theory for problems in document classification is that while it might be relatively easy to estimate probabilities such as P(d1w) for some diagunit w and some descriptor is 66 to infer much about P(dital), where ill is some additional information (e.g. the other DUs which represent the document), since these probabilities have not been estimated, and would take a far larger corpus to reliably estimate in any case. The situation gets exponentially worse as the information we have about the document increases. The exception to this is when close to which case it is very unlikely that additional information changes its value much. This fact is further investigated now. The strategy explored here is to concentrate on finding &amp;quot;sure-fire&amp;quot; indicators of descriptors, in a somewhat similar manner to how Carnegie&apos;s TCS works, by exploiting the fact that with a preclassified training corpus we can identify sure-fire indicators empirically and &amp;quot;trawl&amp;quot; in a large set of informative diagnostic units for those which identify descriptors with high precision. The basis of the model is the following: consider a likelihood function, by: C(diw)= N. That is, the number of articles in the training corthat observed to occur with w divided by the number of articles in which w occurred in the training corpus. This is an empirical estimate of the probability, shall assume (for simplicity&apos;s sake) that we have a large enough corpus do reliably estimate these probabilities. The strategy for descriptor assignment we are inis to assign a descriptor and only if one of a set of predicates over representations of is true. We define the rule d be Correct do degree and only if 1—e. We wish to keep the precision resulting from using this strategy high while increasing the number of rules to improve recall. The predicates 0 we shall consider for this paper will be very simple will typically be true if w E some diagnostic unit w), but in principle, they could be arbitrarily complex (as they are in Carnegie&apos;s TCS). The primary question of concern is whether the ensemble of rules {0i —+ d} retains precision or not. Unfortunately, the answer to this question is that this is not necessarily the case unless we put some constraints on the predicates. 1 (1) be a set of predicates with the that for some fixed descriptor d, is each of the rules 0i --+ d is correct degree The expected precision of the rule (V 00 d is at least 1 Proof: [Straight-forward and omitted] This proposition asserts that one cannot be guaranteed to be able to keep adding diagnostic units to improve recall without hurting precision, unless the quality of those diagnostic units is also improved (i.e. decreased in proportion to the number of DUs which are considered). This is unfortunate, but nevertheless the question of how much adding diagnostic units to help recall will hurt precision is an entirely matter dependent on the true nature of this proposition is a worst case, and gives us reason to be careful. Performance will be expected to be poorest if there are many rules which correspond to the same true positives, but different sets of false positives. If the predicates are disjoint, for example, then the precision of a disjunction is at least as great as the precision of applying any single rule. So if we design our predicates so that they are disjoint, then we retain precision while increasing recall. In practice, this is infeasible, but it is feasible to look more carefully at frequently co-occurring predicates, since these will be most likely to reduce precision.&apos; The main moral we can draw from the above two propositions is that we must be careful about the case where diagnostic units are highly correlated. One situation which is relatively frequent as the sophistication of representation increases is that some diagnostic units always co-occur with others. For example, if the document were represented by sequences of words, then the sequence &amp;quot;olefin polymerisation&amp;quot; always occurs whenever the sequence &amp;quot;high temperature olefin polymerisation&amp;quot; occurs. In this case, it might be thought to pay to look only at the most specific diagnostic units since we have then P(Xlwiw2C) distribution (here, any other contextual information we have, for example the other diagnostic units representing the doc- However, if is significantly less frequent w2 estimation errors of will be larger any descriptor there may not be a significant advantage. However, it does give us a classic example the case of the &amp;quot;New Hampshire Yankee Power Plant&amp;quot;. In a collection of New York articles studied by Jacobs (1990), the word was found to predict POWER of the frequent occurrence of articles about this plant. However, &amp;quot;Yankee&amp;quot; on its own without the other words in this phrase is a good predictor of articles about the New York Yankees, a baseball team. If highly mutually informative words are combined into conjunctive predicates &amp;quot;Yankee&amp;quot; E &amp;quot;Plant&amp;quot; E a document is represented by its most specific predicates only, then when &amp;quot;Yankee&amp;quot; appears alone, it will be a good predicof the descriptor example can also show that the bound described above is tight. Imagine (suspending belief) that each of the five words in the phrase the same number of occurrences, the document without POWER they never occur together pairwise, and always occur all together in positives of the descriptor. Then the precision of POWER any one of them appears a document is and since e in this case is = the bound follows (for the case n = 5) with a little algebra. where n is the cardinality, n 67 theoretical reason to believe that representing a document by its set of most specific predicates is worth investigating, and this shall be investigated below. If one considers a calculus similar to the one dehere, but allows limit to 0, then a weak default logic ensues which has been studied by Adams (1975), and further investigated by Pearl (1988). 4 Adding linguistic description The simplest way of representing a document is as a set or multi set of words. Many people (eg. Lewis 1992bc; Jacobs &amp; Rau 1990) have suggested that a more linguistically sophisticated representation of a document might be more effective for the purposes of statistical keyword assignment. Unfortunately, attempts to do this have not been found to reliably improve performance as measured by recall and precision for the task of document classification. I shall present evidence that a more sophisticated representation makes better predictions from the Single Term model defined above than it does from standard term weighting models. 4.1 Linguistic description The simplest form of linguistic description of the content of a machine-readable document is in the form of a sequence (or a set) of words. More sophisticated linguistic information comes in several forms, all of which may need to be represented if performance in an automatic categorisation experiment is to be improved. Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category (although this not been found to be effective for of the word (e.g. &amp;quot;corpus&amp;quot; for &amp;quot;corpora&amp;quot;), phrasal information (e.g. identifying noun groups and phrases (Lewis 1992c, Church 1988)), and subject-predicate identification (e.g. Hindle 1990). For the RAPRA corpus, we currently identify noun groups and adjective groups. This is achieved in a manner similar to Church&apos;s (1988) PARTS algorithm used by Lewis (1992bc), in the sense that its main properties are robustness and corpus sensitivity. All that is important for this paper is that the technique identifies various groupings of words (for example, noun-groups, adjective groups, and so on) with a high level of accuracy. Major parts of the technique are described in detail in Finch, 1993. As an example, this is some of the linguistic markup which represents the title of the sample document shown earlier. macromolecular deformation (NG); macromolecular deformation model (NG); deformation (NG); deformation model (NG); model (NG); viscoelastic flow (NG); viscoelastic flow effects (Nos); flow (No); flow effects (Nos); effects (Nos); polymer (NG); polymer melts (Nos); melts (Nos) It is clear that the markup is far from sophisticated, and is very much a small variation on a simple sequence-based representation. Nevertheless, it is fairly accurate in so much as well over 90% of what are claimed to be noun groups can be interpreted as such. One very useful by-product of using a linguistically based representation is that IR can help in linguistic tasks such as terminological collection. I shall present some examples of diagnostic units which are highly associated with descriptors later. 5 Predicting from sophisticated representations In what follows, we shall compare the relative performance of a term weighting model with the single term model as we vary the sophistication of representation. assignment 1992b) is used to assign the descriptors from statistical measurements of their appropriateness. This method ensures that roughly the same number of assignments of particular descriptors are made as are actually made in the test corpus. The strategy is simply to assign descripthe which score highest for descriptor, where chosen in proportion occurrence of the training corpus. For term weighting models, the score is simply the combined weight of the document; for the single term model, score is sup. e/z()P(djw). Based assignment strategy applies only to the single term and the rule w --+ included just in case 1 — Figure 1 shows a few of the rules. All of these share the property that 0.8. They were selected at random from the 85,500 associations which were found. 5.1 Representations and models Five paradigms of representation of documents will be compared, and two term appropriateness models will be compared. This gives us ten combinations. The first representation paradigm is a baseline one: represent documents as the set of the words contained in them. The second paradigm is to represent documents according to word sequences, and the third is to apply a noun-group and adjectivegroup recogniser. The fourth and fifth representation modes consider representing documents by only their most specific diagnostic units. For example, if the sequence &amp;quot;thermoplastic elastomer compounds&amp;quot; 68 Figure 1: This figure shows some probably correct rules for the RAPRA corpus. In all, there are over 85,000 such rules. polymer materials Research/NG; EEC legislation/NGS; venture partners/NGS; Bergen op/NP sheet fines/NGS railroad/NG injection moulding facility/NG PHENOLPHTHALEIN/NP unsaturated polyester composites/NGS thermoplastic elastomer compounds/NGS properties features/NGS fiber Glass/NG comparative performance/NG automotive hose/NGS Bitruder/NP worldwide tyre/NG Victrex polyethersulphone/NP PS melts/NGS viscoelastic characteristics/NGS plastics waste/NG lattice relaxation/NG fatigue crack propagation/NG unidirectional composites/NGS</abstract>
<author confidence="0.821344">Flory Huggins interactionNG</author>
<affiliation confidence="0.7089565">DATA LEGISLATION</affiliation>
<address confidence="0.4329115">4 JOINT VENTURE -4. PLASTIC</address>
<title confidence="0.913690736842105">COMPANY COMPANY PLASTIC --r DATA THERMOSET --■ RUBBER PLASTIC GLASS FIBRE REINFORCED PLASTIC DATA RUBBER EXTRUDER COMPANIES COMPANIES PLASTIC VISCOELASTIC PROPERTIES RECYCLING --r NUCLEAR MAGNETIC RESONANCE MECHANICAL PROPERTIES REINFORCED PLASTIC</title>
<abstract confidence="0.98568971875">TECHNICAL appeared in the abstract, then ordinarily this would include the sequence &amp;quot;elastomer compounds&amp;quot;, which would be included in the representation. The results of section 3.2 might encourage us to believe that representing a document by only its most specific diagnostic units will improve performance (or, at least, precision). Consequently, a sequence of words is deto be specific (a) it is a diagnostic unit and (b) it is not properly contained in a token of any other diagnostic unit present in the document.&apos; The noun-groups are found by performing a simple parse of the documents as described above, and identifying likely noun groups of length 3 or less. The contingency table of diagnostic units verses manually assigned descriptors on a training corpus of 200,000 documents was collected, and this was used as the basis for two term appropriateness models. Probabilities were estimated by adding a constant (usually 0.02 was found fairly optimal) to each cell, and directly estimating from these slightly adjusted counts. The 50,000 most frequent diagnostic unit types were chosen, and terms which appeared in more than 10% of documents were discarded. &amp;quot;elastomer compounds&amp;quot; appeared separately in the document from &amp;quot;thermoplastic elastomer compounds&amp;quot;, then both of these sequences would be represented in the experiments reported here. 6 Results results of the experiments the RAPRA corare presented Despite the peculiarities of the corpus, the message is clear. The result that the standard model fares no better on word sequence sets than on word sets is repeated, and it is clear that the Single Term model fares much better than the Logit model on this data set. However, what is most interesting is that the Single Term models fares significantly better on the more sophisticated sequence based representations of the document than on the simpler word based representation. There is, however, no significant advantage identified by parsing the corpus into noun-groups over simply considering all word sequences. The recall scores for the rule-based tagging strategy show that the improved performance of the sequence based representations can be explained by recall precision scores are (Lewis 1992c); they are the expected probability of assigning or recalling correctly per tagging decision. The training set was a set of 200,000 abstracts, and the separate test set had 10,000 abstracts. The experiments looked at only the 520 most common descriptors. In the table, TW means that a term-weighting model was used, while ST means that the single term model was used. &apos;Word&apos; means the representation was a wordset, `Seq&apos;, the set of all sequences, and &apos;NG&apos; the set of groups derived from the grammar. For the sequence representations, either all the possible sequences or groups were used (denoted by &apos;all&apos;), or just the most specific ones were used (denoted by `spec&apos;). 69 the presence of many more &amp;quot;good&amp;quot; descriptor indicators.</abstract>
<note confidence="0.8965695">Assignment Model Repn Prec Rec Prop. TW Word 33% 32% Prop. TW Seq all 32% 34% Prop. TW Seq spec 33% 34% Prop. TW NG all 31% 36% Prop. TW NG spec 32% 32% Prop. ST Word 54% 48% Prop. ST Seq all 57% 55% Prop. ST Seq spec 55% 55% Prop. ST NG 56% 60% .2 ST Word 83% 7% ST Seq all 77% 42%</note>
<abstract confidence="0.90096793877551">ST Seq spec 80% 40% = ST NG all 82% 42% = ST NG spec 84% 37% 7 Conclusion The significant theoretical result is that as the sophistication of the representation of abstracts is increased, the performance of the single term model improves, while the performance of the term weighting models does not improve significantly. This has been a fairly universal experience among researchers working within the term weighting classification paradigm. Although there is a very marginally significant improvement from using linguistically sophisticated representations over simple sequence representations if all of the sequences are represented, this largely (though not entirely) disappears when only most specific sequences are considered, so it might be a result of the effects discussed in section 3.2. The rule based assignment strategy exploits the Single Term model&apos;s estimates, and also performs much better on word sequence representations than on word set representations. This assignment strategy is promising because it can exploit more sophisticated representations well, has a sound theory behind it, and will assign descriptors only where it has enough information to do so. Some of the descriptors in the RAPRA corpus, for example, are only ever assigned from the entire article from which the abstract is taken, so no assignment strategy will ever do well on these. On the other hand this model also shows promise that IR techniques might be applied to help infer linguistic resources such as term banks from large classified corpora. The next stage is to add more sophisticated linguistic annotation to corpora, and to trawl for rules in boolean combinations of descriptors, thus addressing the results of section 3.2. In this way this work can be considered similar in spirit to that undertaken by Apte et al (1994), but differs in the forms of representation which are being considered for documents. References E. (1975) Logic of Conditionals: an appliof probability to deductive logic Apte, C, F. Demerau &amp; S. Weiss (1994) Towards Language Independent Automated Learning of Text Cate- Methods. proceeding of the Seventeenth Conference on Information Retrieval. 23-</abstract>
<address confidence="0.744048333333333">30, DCU, Dublin. Buckley, C. (1993) The Importance of Proper Weighting RPA Workshop on Human Language Tech-</address>
<email confidence="0.354247">nology.</email>
<note confidence="0.766823841269841">Church, K. (1988) A stochastic parts program and noun parser for unrestricted text. In conference applied NLP, Church, K., W. Gale, P. flanks &amp; D. Hindle (1989) Parsing, Word Associations and Typical Predicate-Argument In Parsing Technologies Work- Pittsburgh. J. (1987) in Automatic Phrase Indexing for Document Retrieval: Comparison of Syntactic Methods. Thesis. Cornell University, Dept. of Computer Science. Finch, S. P. &amp; N. Chater (1991) A Hybrid Approach to Automatic Learning of Linguistic Categories. Artifi- Intelligence Behaviour S. (1993) Structure in Language. thesis, Centre for Cognitive Science, University of Edinburgh, Edinburgh. Fuhr, N. (1989) Models for retrieval with probabilistic inprocessing and management. 25(1): 55-72. Fuhr, N. &amp; Buckley, C (1993) Optimizing Document Indexing and Search Term Weighting Based on Probabilistic Models First TREC Conference. Hindle, D. (1990) Noun Classification from Predicate- Structures. In of the 22nd meeting of the Association of Computational Linguistics. 268-75. P. L. (1990) SCISOR: Extracting Inforfrom News of the ACM 88-97 Kupiec, J. (1992) Robust part-of-speech tagging using a Markov model. Speech Language, 6:3 225-42. Lewis, D. (1991) Evaluating text categorisation. In natural language 136-143. D. (1992a) learning in information retrieval. Ph.D. thesis, Computer Science Dept., Univ. Mass., Amherst, Ma. Lewis, D. (1992b) An Evaluation of Phrasal and Clustered Representations on a Text categorization problem. Proceedings of SIGIR 92. Lewis, D. (1992c) Feature selection and feature extracfor text categorization. In Natural Lanof held Harrimn, NY. pp 212-217. Lewis, D. &amp; K. Sparck-Jones (1993) Natural language for information retrieval Cam- Technical 70 J. (1988) Reasoning in Intelligent Networks of Plausible Inference Kaufmann, San Mateo, Ca. Rijsbergen, C. J. (1979) retrieval. Butterworths, London. Sacks-Davis, R. (1990) Using Syntactic Analysis in a Document Retrieval System that Uses Signature Files. G. &amp; McGill, M. J. (1983) to modinformation retrieval. NY. Salton, G. &amp; C. Buckley (1988) Term Weighting Apin Automatic Text Retrieval Proand Management L. (1965) Fuzzy Sets bf 71</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Adams</author>
</authors>
<title>The Logic of Conditionals: an application of probability to deductive logic Reidel.</title>
<date>1975</date>
<contexts>
<context position="15216" citStr="Adams (1975)" startWordPosition="2471" endWordPosition="2472">r all together in j true positives of the descriptor. Then the precision of assigning NUCLEAR POWER if any one of them appears in a document is —7— and since e in this case is = the 3+5., t+.7 bound follows (for the case n = 5) with a little algebra. e where n is the cardinality, 01. n 67 theoretical reason to believe that representing a document by its set of most specific predicates is worth investigating, and this shall be investigated below. If one considers a calculus similar to the one described here, but allows c to limit to 0, then a weak default logic ensues which has been studied by Adams (1975), and further investigated by Pearl (1988). 4 Adding linguistic description The simplest way of representing a document is as a set or multi set of words. Many people (eg. Lewis 1992bc; Jacobs &amp; Rau 1990) have suggested that a more linguistically sophisticated representation of a document might be more effective for the purposes of statistical keyword assignment. Unfortunately, attempts to do this have not been found to reliably improve performance as measured by recall and precision for the task of document classification. I shall present evidence that a more sophisticated representation make</context>
</contexts>
<marker>Adams, 1975</marker>
<rawString>Adams, E. (1975) The Logic of Conditionals: an application of probability to deductive logic Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Apte</author>
<author>F Demerau</author>
<author>S Weiss</author>
</authors>
<title>Towards Language Independent Automated Learning of Text Categorization Methods.</title>
<date>1994</date>
<booktitle>the proceeding of the Seventeenth ACM-SIGIR Conference on Information Retrieval. 23-30, DCU,</booktitle>
<location>Dublin.</location>
<marker>Apte, Demerau, Weiss, 1994</marker>
<rawString>Apte, C, F. Demerau &amp; S. Weiss (1994) Towards Language Independent Automated Learning of Text Categorization Methods. the proceeding of the Seventeenth ACM-SIGIR Conference on Information Retrieval. 23-30, DCU, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
</authors>
<title>The Importance of Proper Weighting Methods. A RPA Workshop on Human Language Technology.</title>
<date>1993</date>
<contexts>
<context position="7004" citStr="Buckley, 1993" startWordPosition="1081" endWordPosition="1082">ophisticated representation of a document, while an alternative statistical single term model can. 3.1 Term weighting models The standard term weighting model is defined by chosing a set of parameters erij} (one for each worddescriptor pair) and {A} (one for each descriptor) so that a likelihood or appropriateness function, can be defined by ,c(diw). E awd 4-13d (1) evEW This has been widely used, and is provably equivalent to a large class of probabilistic models (e.g. Van Risjbergen, 1979) which make various assumptions about the independence between descriptors and diagnostic units (Fuhr &amp; Buckley, 1993). Various strategies for estimating the parameters for this model have been proposed (e.g. Salton &amp; Yang, 1973, Buckley 1993, Fuhr &amp; Buckley, 1993). Some of these concentrate on the need for re-estimating weights according to relevance feedback information, while some make use of various functions of term frequency, document frequency, maximum withindocument frequency, and various other measurements of corpora. Nevertheless, the problem of estimating the huge number of parameters needed for such a model is statistically problematic, and as Buckley (1993) points out, the choice of weights has a</context>
</contexts>
<marker>Buckley, 1993</marker>
<rawString>Buckley, C. (1993) The Importance of Proper Weighting Methods. A RPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Second conference on applied NLP,</booktitle>
<pages>136--43</pages>
<contexts>
<context position="16585" citStr="Church 1988" startWordPosition="2687" endWordPosition="2688">of linguistic description of the content of a machine-readable document is in the form of a sequence (or a set) of words. More sophisticated linguistic information comes in several forms, all of which may need to be represented if performance in an automatic categorisation experiment is to be improved. Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category (although this has not been found to be effective for IR), lemma of the word (e.g. &amp;quot;corpus&amp;quot; for &amp;quot;corpora&amp;quot;), phrasal information (e.g. identifying noun groups and phrases (Lewis 1992c, Church 1988)), and subject-predicate identification (e.g. Hindle 1990). For the RAPRA corpus, we currently identify noun groups and adjective groups. This is achieved in a manner similar to Church&apos;s (1988) PARTS algorithm used by Lewis (1992bc), in the sense that its main properties are robustness and corpus sensitivity. All that is important for this paper is that the technique identifies various groupings of words (for example, noun-groups, adjective groups, and so on) with a high level of accuracy. Major parts of the technique are described in detail in Finch, 1993. As an example, this is some of the l</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. (1988) A stochastic parts program and noun phrase parser for unrestricted text. In Second conference on applied NLP, pp 136-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
<author>P flanks</author>
<author>D Hindle</author>
</authors>
<title>Parsing, Word Associations and Typical Predicate-Argument Relations.</title>
<date>1989</date>
<booktitle>In International Parsing Technologies Workshop. CMU,</booktitle>
<location>Pittsburgh.</location>
<marker>Church, Gale, flanks, Hindle, 1989</marker>
<rawString>Church, K., W. Gale, P. flanks &amp; D. Hindle (1989) Parsing, Word Associations and Typical Predicate-Argument Relations. In International Parsing Technologies Workshop. CMU, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fagan</author>
</authors>
<title>Experiments in Automatic Phrase Indexing for Document Retrieval: Comparison of Syntactic and Non-Syntactic Methods. PhD Thesis.</title>
<date>1987</date>
<institution>Cornell University, Dept. of Computer Science.</institution>
<marker>Fagan, 1987</marker>
<rawString>Fagan, J. (1987) Experiments in Automatic Phrase Indexing for Document Retrieval: Comparison of Syntactic and Non-Syntactic Methods. PhD Thesis. Cornell University, Dept. of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Finch</author>
<author>N Chater</author>
</authors>
<title>A Hybrid Approach to the Automatic Learning of Linguistic Categories.</title>
<date>1991</date>
<journal>Artificial Intelligence and Simulated Behaviour Quarterly.</journal>
<volume>78</volume>
<pages>16--24</pages>
<marker>Finch, Chater, 1991</marker>
<rawString>Finch, S. P. &amp; N. Chater (1991) A Hybrid Approach to the Automatic Learning of Linguistic Categories. Artificial Intelligence and Simulated Behaviour Quarterly. 78 16-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Finch</author>
</authors>
<title>Finding Structure in Language.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Centre for Cognitive Science, University of Edinburgh,</institution>
<location>Edinburgh.</location>
<contexts>
<context position="17147" citStr="Finch, 1993" startWordPosition="2777" endWordPosition="2778">oun groups and phrases (Lewis 1992c, Church 1988)), and subject-predicate identification (e.g. Hindle 1990). For the RAPRA corpus, we currently identify noun groups and adjective groups. This is achieved in a manner similar to Church&apos;s (1988) PARTS algorithm used by Lewis (1992bc), in the sense that its main properties are robustness and corpus sensitivity. All that is important for this paper is that the technique identifies various groupings of words (for example, noun-groups, adjective groups, and so on) with a high level of accuracy. Major parts of the technique are described in detail in Finch, 1993. As an example, this is some of the linguistic markup which represents the title of the sample document shown earlier. macromolecular deformation (NG); macromolecular deformation model (NG); deformation (NG); deformation model (NG); model (NG); viscoelastic flow (NG); viscoelastic flow effects (Nos); flow (No); flow effects (Nos); effects (Nos); polymer (NG); polymer melts (Nos); melts (Nos) It is clear that the markup is far from sophisticated, and is very much a small variation on a simple sequence-based representation. Nevertheless, it is fairly accurate in so much as well over 90% of what</context>
</contexts>
<marker>Finch, 1993</marker>
<rawString>Finch, S. (1993) Finding Structure in Language. Ph.D. thesis, Centre for Cognitive Science, University of Edinburgh, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
</authors>
<title>Models for retrieval with probabilistic indexing. Information processing and management.</title>
<date>1989</date>
<volume>25</volume>
<issue>1</issue>
<pages>55--72</pages>
<marker>Fuhr, 1989</marker>
<rawString>Fuhr, N. (1989) Models for retrieval with probabilistic indexing. Information processing and management. 25(1): 55-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
<author>C Buckley</author>
</authors>
<title>Optimizing Document Indexing and Search Term Weighting Based on Probabilistic Models First TREC Conference.</title>
<date>1993</date>
<contexts>
<context position="7004" citStr="Fuhr &amp; Buckley, 1993" startWordPosition="1079" endWordPosition="1082"> more sophisticated representation of a document, while an alternative statistical single term model can. 3.1 Term weighting models The standard term weighting model is defined by chosing a set of parameters erij} (one for each worddescriptor pair) and {A} (one for each descriptor) so that a likelihood or appropriateness function, can be defined by ,c(diw). E awd 4-13d (1) evEW This has been widely used, and is provably equivalent to a large class of probabilistic models (e.g. Van Risjbergen, 1979) which make various assumptions about the independence between descriptors and diagnostic units (Fuhr &amp; Buckley, 1993). Various strategies for estimating the parameters for this model have been proposed (e.g. Salton &amp; Yang, 1973, Buckley 1993, Fuhr &amp; Buckley, 1993). Some of these concentrate on the need for re-estimating weights according to relevance feedback information, while some make use of various functions of term frequency, document frequency, maximum withindocument frequency, and various other measurements of corpora. Nevertheless, the problem of estimating the huge number of parameters needed for such a model is statistically problematic, and as Buckley (1993) points out, the choice of weights has a</context>
</contexts>
<marker>Fuhr, Buckley, 1993</marker>
<rawString>Fuhr, N. &amp; Buckley, C (1993) Optimizing Document Indexing and Search Term Weighting Based on Probabilistic Models First TREC Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun Classification from PredicateArgument Structures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 22nd meeting of the Association of Computational Linguistics.</booktitle>
<pages>268--75</pages>
<contexts>
<context position="16643" citStr="Hindle 1990" startWordPosition="2693" endWordPosition="2694">able document is in the form of a sequence (or a set) of words. More sophisticated linguistic information comes in several forms, all of which may need to be represented if performance in an automatic categorisation experiment is to be improved. Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category (although this has not been found to be effective for IR), lemma of the word (e.g. &amp;quot;corpus&amp;quot; for &amp;quot;corpora&amp;quot;), phrasal information (e.g. identifying noun groups and phrases (Lewis 1992c, Church 1988)), and subject-predicate identification (e.g. Hindle 1990). For the RAPRA corpus, we currently identify noun groups and adjective groups. This is achieved in a manner similar to Church&apos;s (1988) PARTS algorithm used by Lewis (1992bc), in the sense that its main properties are robustness and corpus sensitivity. All that is important for this paper is that the technique identifies various groupings of words (for example, noun-groups, adjective groups, and so on) with a high level of accuracy. Major parts of the technique are described in detail in Finch, 1993. As an example, this is some of the linguistic markup which represents the title of the sample </context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>Hindle, D. (1990) Noun Classification from PredicateArgument Structures. In Proceedings of the 22nd meeting of the Association of Computational Linguistics. 268-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
<author>L Rau</author>
</authors>
<title>SCISOR: Extracting Information from</title>
<date>1990</date>
<journal>On-line News Correspondence of the ACM</journal>
<volume>33</volume>
<pages>88--97</pages>
<contexts>
<context position="13787" citStr="Jacobs &amp; Rau (1990)" startWordPosition="2213" endWordPosition="2216"> to look only at the most specific diagnostic units since we have if w1 w2, then P(Xlwiw2C) = P(XlwiC) for any distribution P whatsoever (here, C represents any other contextual information we have, for example the other diagnostic units representing the document). However, if w1 is significantly less frequent than w2 estimation errors of P(dlwi) will be larger for P(d1w2) for any descriptor d, so there may not be a significant advantage. However, it does give us a &apos;One classic example is the case of the &amp;quot;New Hampshire Yankee Power Plant&amp;quot;. In a collection of New York Times articles studied by Jacobs &amp; Rau (1990), the word &amp;quot;Yankee&amp;quot; was found to predict NUCLEAR POWER because of the frequent occurrence of articles about this plant. However, &amp;quot;Yankee&amp;quot; on its own without the other words in this phrase is a good predictor of articles about the New York Yankees, a baseball team. If highly mutually informative words are combined into conjunctive predicates (e.g. &amp;quot;Yankee&amp;quot; E x &amp; &amp;quot;Plant&amp;quot; E x), and a document is represented by its most specific predicates only, then when &amp;quot;Yankee&amp;quot; appears alone, it will be a good predictor of the descriptor SPORT. This example can also show that the bound described above is tight.</context>
<context position="15420" citStr="Jacobs &amp; Rau 1990" startWordPosition="2504" endWordPosition="2507"> follows (for the case n = 5) with a little algebra. e where n is the cardinality, 01. n 67 theoretical reason to believe that representing a document by its set of most specific predicates is worth investigating, and this shall be investigated below. If one considers a calculus similar to the one described here, but allows c to limit to 0, then a weak default logic ensues which has been studied by Adams (1975), and further investigated by Pearl (1988). 4 Adding linguistic description The simplest way of representing a document is as a set or multi set of words. Many people (eg. Lewis 1992bc; Jacobs &amp; Rau 1990) have suggested that a more linguistically sophisticated representation of a document might be more effective for the purposes of statistical keyword assignment. Unfortunately, attempts to do this have not been found to reliably improve performance as measured by recall and precision for the task of document classification. I shall present evidence that a more sophisticated representation makes better predictions from the Single Term model defined above than it does from standard term weighting models. 4.1 Linguistic description The simplest form of linguistic description of the content of a m</context>
</contexts>
<marker>Jacobs, Rau, 1990</marker>
<rawString>Jacobs, P. &amp; Rau, L. (1990) SCISOR: Extracting Information from On-line News Correspondence of the ACM 33 11 88-97</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<pages>225--42</pages>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec, J. (1992) Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6:3 225-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Evaluating text categorisation. In Speech and natural language workshop.</title>
<date>1991</date>
<pages>136--143</pages>
<marker>Lewis, 1991</marker>
<rawString>Lewis, D. (1991) Evaluating text categorisation. In Speech and natural language workshop. pp 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Representation and learning in information retrieval.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Dept., Univ.</institution>
<location>Mass., Amherst, Ma.</location>
<contexts>
<context position="7877" citStr="Lewis (1992" startWordPosition="1223" endWordPosition="1224">e some make use of various functions of term frequency, document frequency, maximum withindocument frequency, and various other measurements of corpora. Nevertheless, the problem of estimating the huge number of parameters needed for such a model is statistically problematic, and as Buckley (1993) points out, the choice of weights has a large influence on the effectiveness of any model for classification or for retrieval. There are so many variations on the theme of term weighting models that it is impossible to try them all in one experiment, so this paper uses a variation of a model used by Lewis (1992c) in which he reports the results of some experiments using phrases in a term weighting model (which has a probabilistic interpretation). Several term weighting models have been tried, but they all evaluate within 5 points of each other on both precision and recall (when suitably tweaked). The model eventually chosen for the tests reported here was a smoothed logistic model which gave the best results of all the probabilistically inspired term weighting models considered. 3 2 Single term model In contrast to making assumptions of independence about the relationship between diagnostic units an</context>
<context position="15398" citStr="Lewis 1992" startWordPosition="2502" endWordPosition="2503">5., t+.7 bound follows (for the case n = 5) with a little algebra. e where n is the cardinality, 01. n 67 theoretical reason to believe that representing a document by its set of most specific predicates is worth investigating, and this shall be investigated below. If one considers a calculus similar to the one described here, but allows c to limit to 0, then a weak default logic ensues which has been studied by Adams (1975), and further investigated by Pearl (1988). 4 Adding linguistic description The simplest way of representing a document is as a set or multi set of words. Many people (eg. Lewis 1992bc; Jacobs &amp; Rau 1990) have suggested that a more linguistically sophisticated representation of a document might be more effective for the purposes of statistical keyword assignment. Unfortunately, attempts to do this have not been found to reliably improve performance as measured by recall and precision for the task of document classification. I shall present evidence that a more sophisticated representation makes better predictions from the Single Term model defined above than it does from standard term weighting models. 4.1 Linguistic description The simplest form of linguistic description</context>
<context position="16814" citStr="Lewis (1992" startWordPosition="2722" endWordPosition="2723"> performance in an automatic categorisation experiment is to be improved. Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category (although this has not been found to be effective for IR), lemma of the word (e.g. &amp;quot;corpus&amp;quot; for &amp;quot;corpora&amp;quot;), phrasal information (e.g. identifying noun groups and phrases (Lewis 1992c, Church 1988)), and subject-predicate identification (e.g. Hindle 1990). For the RAPRA corpus, we currently identify noun groups and adjective groups. This is achieved in a manner similar to Church&apos;s (1988) PARTS algorithm used by Lewis (1992bc), in the sense that its main properties are robustness and corpus sensitivity. All that is important for this paper is that the technique identifies various groupings of words (for example, noun-groups, adjective groups, and so on) with a high level of accuracy. Major parts of the technique are described in detail in Finch, 1993. As an example, this is some of the linguistic markup which represents the title of the sample document shown earlier. macromolecular deformation (NG); macromolecular deformation model (NG); deformation (NG); deformation model (NG); model (NG); viscoelastic flow (NG</context>
<context position="18301" citStr="Lewis 1992" startWordPosition="2958" endWordPosition="2959">t is fairly accurate in so much as well over 90% of what are claimed to be noun groups can be interpreted as such. One very useful by-product of using a linguistically based representation is that IR can help in linguistic tasks such as terminological collection. I shall present some examples of diagnostic units which are highly associated with descriptors later. 5 Predicting from sophisticated representations In what follows, we shall compare the relative performance of a term weighting model with the single term model as we vary the sophistication of representation. Proportional assignment (Lewis 1992b) is used to assign the descriptors from statistical measurements of their appropriateness. This method ensures that roughly the same number of assignments of particular descriptors are made as are actually made in the test corpus. The strategy is simply to assign descriptor d to the N documents which score highest for this descriptor, where N is chosen in proportion to the occurrence of d in the training corpus. For term weighting models, the score is simply the combined weight of the document; for the single term model, the score is sup. e/z() P(djw). The Rule Based assignment strategy appl</context>
<context position="23049" citStr="Lewis 1992" startWordPosition="3686" endWordPosition="3687">much better than the Logit model on this data set. However, what is most interesting is that the Single Term models fares significantly better on the more sophisticated sequence based representations of the document than on the simpler word based representation. There is, however, no significant advantage identified by parsing the corpus into noun-groups over simply considering all word sequences. The recall scores for the rule-based tagging strategy show that the improved performance of the sequence based representations can be explained by 3A11 recall and precision scores are microaveraged (Lewis 1992c); they are the expected probability of assigning or recalling correctly per tagging decision. The training set was a set of 200,000 abstracts, and the separate test set had 10,000 abstracts. The experiments looked at only the 520 most common descriptors. In the table, TW means that a term-weighting model was used, while ST means that the single term model was used. &apos;Word&apos; means the representation was a wordset, `Seq&apos;, the set of all sequences, and &apos;NG&apos; the set of groups derived from the grammar. For the sequence representations, either all the possible sequences or groups were used (denoted </context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>Lewis, D. (1992a) Representation and learning in information retrieval. Ph.D. thesis, Computer Science Dept., Univ. Mass., Amherst, Ma.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>An Evaluation of Phrasal and Clustered Representations on a Text categorization problem.</title>
<date>1992</date>
<booktitle>Proceedings of SIGIR 92.</booktitle>
<contexts>
<context position="7877" citStr="Lewis (1992" startWordPosition="1223" endWordPosition="1224">e some make use of various functions of term frequency, document frequency, maximum withindocument frequency, and various other measurements of corpora. Nevertheless, the problem of estimating the huge number of parameters needed for such a model is statistically problematic, and as Buckley (1993) points out, the choice of weights has a large influence on the effectiveness of any model for classification or for retrieval. There are so many variations on the theme of term weighting models that it is impossible to try them all in one experiment, so this paper uses a variation of a model used by Lewis (1992c) in which he reports the results of some experiments using phrases in a term weighting model (which has a probabilistic interpretation). Several term weighting models have been tried, but they all evaluate within 5 points of each other on both precision and recall (when suitably tweaked). The model eventually chosen for the tests reported here was a smoothed logistic model which gave the best results of all the probabilistically inspired term weighting models considered. 3 2 Single term model In contrast to making assumptions of independence about the relationship between diagnostic units an</context>
<context position="15398" citStr="Lewis 1992" startWordPosition="2502" endWordPosition="2503">5., t+.7 bound follows (for the case n = 5) with a little algebra. e where n is the cardinality, 01. n 67 theoretical reason to believe that representing a document by its set of most specific predicates is worth investigating, and this shall be investigated below. If one considers a calculus similar to the one described here, but allows c to limit to 0, then a weak default logic ensues which has been studied by Adams (1975), and further investigated by Pearl (1988). 4 Adding linguistic description The simplest way of representing a document is as a set or multi set of words. Many people (eg. Lewis 1992bc; Jacobs &amp; Rau 1990) have suggested that a more linguistically sophisticated representation of a document might be more effective for the purposes of statistical keyword assignment. Unfortunately, attempts to do this have not been found to reliably improve performance as measured by recall and precision for the task of document classification. I shall present evidence that a more sophisticated representation makes better predictions from the Single Term model defined above than it does from standard term weighting models. 4.1 Linguistic description The simplest form of linguistic description</context>
<context position="16814" citStr="Lewis (1992" startWordPosition="2722" endWordPosition="2723"> performance in an automatic categorisation experiment is to be improved. Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category (although this has not been found to be effective for IR), lemma of the word (e.g. &amp;quot;corpus&amp;quot; for &amp;quot;corpora&amp;quot;), phrasal information (e.g. identifying noun groups and phrases (Lewis 1992c, Church 1988)), and subject-predicate identification (e.g. Hindle 1990). For the RAPRA corpus, we currently identify noun groups and adjective groups. This is achieved in a manner similar to Church&apos;s (1988) PARTS algorithm used by Lewis (1992bc), in the sense that its main properties are robustness and corpus sensitivity. All that is important for this paper is that the technique identifies various groupings of words (for example, noun-groups, adjective groups, and so on) with a high level of accuracy. Major parts of the technique are described in detail in Finch, 1993. As an example, this is some of the linguistic markup which represents the title of the sample document shown earlier. macromolecular deformation (NG); macromolecular deformation model (NG); deformation (NG); deformation model (NG); model (NG); viscoelastic flow (NG</context>
<context position="18301" citStr="Lewis 1992" startWordPosition="2958" endWordPosition="2959">t is fairly accurate in so much as well over 90% of what are claimed to be noun groups can be interpreted as such. One very useful by-product of using a linguistically based representation is that IR can help in linguistic tasks such as terminological collection. I shall present some examples of diagnostic units which are highly associated with descriptors later. 5 Predicting from sophisticated representations In what follows, we shall compare the relative performance of a term weighting model with the single term model as we vary the sophistication of representation. Proportional assignment (Lewis 1992b) is used to assign the descriptors from statistical measurements of their appropriateness. This method ensures that roughly the same number of assignments of particular descriptors are made as are actually made in the test corpus. The strategy is simply to assign descriptor d to the N documents which score highest for this descriptor, where N is chosen in proportion to the occurrence of d in the training corpus. For term weighting models, the score is simply the combined weight of the document; for the single term model, the score is sup. e/z() P(djw). The Rule Based assignment strategy appl</context>
<context position="23049" citStr="Lewis 1992" startWordPosition="3686" endWordPosition="3687">much better than the Logit model on this data set. However, what is most interesting is that the Single Term models fares significantly better on the more sophisticated sequence based representations of the document than on the simpler word based representation. There is, however, no significant advantage identified by parsing the corpus into noun-groups over simply considering all word sequences. The recall scores for the rule-based tagging strategy show that the improved performance of the sequence based representations can be explained by 3A11 recall and precision scores are microaveraged (Lewis 1992c); they are the expected probability of assigning or recalling correctly per tagging decision. The training set was a set of 200,000 abstracts, and the separate test set had 10,000 abstracts. The experiments looked at only the 520 most common descriptors. In the table, TW means that a term-weighting model was used, while ST means that the single term model was used. &apos;Word&apos; means the representation was a wordset, `Seq&apos;, the set of all sequences, and &apos;NG&apos; the set of groups derived from the grammar. For the sequence representations, either all the possible sequences or groups were used (denoted </context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>Lewis, D. (1992b) An Evaluation of Phrasal and Clustered Representations on a Text categorization problem. Proceedings of SIGIR 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Feature selection and feature extraction for text categorization.</title>
<date>1992</date>
<booktitle>In Speech and Natural Language: Proceedings of a Workshop held at Harrimn, NY.</booktitle>
<pages>212--217</pages>
<contexts>
<context position="7877" citStr="Lewis (1992" startWordPosition="1223" endWordPosition="1224">e some make use of various functions of term frequency, document frequency, maximum withindocument frequency, and various other measurements of corpora. Nevertheless, the problem of estimating the huge number of parameters needed for such a model is statistically problematic, and as Buckley (1993) points out, the choice of weights has a large influence on the effectiveness of any model for classification or for retrieval. There are so many variations on the theme of term weighting models that it is impossible to try them all in one experiment, so this paper uses a variation of a model used by Lewis (1992c) in which he reports the results of some experiments using phrases in a term weighting model (which has a probabilistic interpretation). Several term weighting models have been tried, but they all evaluate within 5 points of each other on both precision and recall (when suitably tweaked). The model eventually chosen for the tests reported here was a smoothed logistic model which gave the best results of all the probabilistically inspired term weighting models considered. 3 2 Single term model In contrast to making assumptions of independence about the relationship between diagnostic units an</context>
<context position="15398" citStr="Lewis 1992" startWordPosition="2502" endWordPosition="2503">5., t+.7 bound follows (for the case n = 5) with a little algebra. e where n is the cardinality, 01. n 67 theoretical reason to believe that representing a document by its set of most specific predicates is worth investigating, and this shall be investigated below. If one considers a calculus similar to the one described here, but allows c to limit to 0, then a weak default logic ensues which has been studied by Adams (1975), and further investigated by Pearl (1988). 4 Adding linguistic description The simplest way of representing a document is as a set or multi set of words. Many people (eg. Lewis 1992bc; Jacobs &amp; Rau 1990) have suggested that a more linguistically sophisticated representation of a document might be more effective for the purposes of statistical keyword assignment. Unfortunately, attempts to do this have not been found to reliably improve performance as measured by recall and precision for the task of document classification. I shall present evidence that a more sophisticated representation makes better predictions from the Single Term model defined above than it does from standard term weighting models. 4.1 Linguistic description The simplest form of linguistic description</context>
<context position="16814" citStr="Lewis (1992" startWordPosition="2722" endWordPosition="2723"> performance in an automatic categorisation experiment is to be improved. Typical examples of linguistically sophisticated annotation include tagging words with their syntactic category (although this has not been found to be effective for IR), lemma of the word (e.g. &amp;quot;corpus&amp;quot; for &amp;quot;corpora&amp;quot;), phrasal information (e.g. identifying noun groups and phrases (Lewis 1992c, Church 1988)), and subject-predicate identification (e.g. Hindle 1990). For the RAPRA corpus, we currently identify noun groups and adjective groups. This is achieved in a manner similar to Church&apos;s (1988) PARTS algorithm used by Lewis (1992bc), in the sense that its main properties are robustness and corpus sensitivity. All that is important for this paper is that the technique identifies various groupings of words (for example, noun-groups, adjective groups, and so on) with a high level of accuracy. Major parts of the technique are described in detail in Finch, 1993. As an example, this is some of the linguistic markup which represents the title of the sample document shown earlier. macromolecular deformation (NG); macromolecular deformation model (NG); deformation (NG); deformation model (NG); model (NG); viscoelastic flow (NG</context>
<context position="18301" citStr="Lewis 1992" startWordPosition="2958" endWordPosition="2959">t is fairly accurate in so much as well over 90% of what are claimed to be noun groups can be interpreted as such. One very useful by-product of using a linguistically based representation is that IR can help in linguistic tasks such as terminological collection. I shall present some examples of diagnostic units which are highly associated with descriptors later. 5 Predicting from sophisticated representations In what follows, we shall compare the relative performance of a term weighting model with the single term model as we vary the sophistication of representation. Proportional assignment (Lewis 1992b) is used to assign the descriptors from statistical measurements of their appropriateness. This method ensures that roughly the same number of assignments of particular descriptors are made as are actually made in the test corpus. The strategy is simply to assign descriptor d to the N documents which score highest for this descriptor, where N is chosen in proportion to the occurrence of d in the training corpus. For term weighting models, the score is simply the combined weight of the document; for the single term model, the score is sup. e/z() P(djw). The Rule Based assignment strategy appl</context>
<context position="23049" citStr="Lewis 1992" startWordPosition="3686" endWordPosition="3687">much better than the Logit model on this data set. However, what is most interesting is that the Single Term models fares significantly better on the more sophisticated sequence based representations of the document than on the simpler word based representation. There is, however, no significant advantage identified by parsing the corpus into noun-groups over simply considering all word sequences. The recall scores for the rule-based tagging strategy show that the improved performance of the sequence based representations can be explained by 3A11 recall and precision scores are microaveraged (Lewis 1992c); they are the expected probability of assigning or recalling correctly per tagging decision. The training set was a set of 200,000 abstracts, and the separate test set had 10,000 abstracts. The experiments looked at only the 520 most common descriptors. In the table, TW means that a term-weighting model was used, while ST means that the single term model was used. &apos;Word&apos; means the representation was a wordset, `Seq&apos;, the set of all sequences, and &apos;NG&apos; the set of groups derived from the grammar. For the sequence representations, either all the possible sequences or groups were used (denoted </context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>Lewis, D. (1992c) Feature selection and feature extraction for text categorization. In Speech and Natural Language: Proceedings of a Workshop held at Harrimn, NY. pp 212-217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>K Sparck-Jones</author>
</authors>
<title>Natural language processing for information retrieval</title>
<date>1993</date>
<tech>Technical report 307,</tech>
<institution>University of Cambridge</institution>
<location>Cambridge.</location>
<marker>Lewis, Sparck-Jones, 1993</marker>
<rawString>Lewis, D. &amp; K. Sparck-Jones (1993) Natural language processing for information retrieval University of Cambridge Technical report 307, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems:</title>
<date>1988</date>
<booktitle>Networks of Plausible Inference</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, Ca.</location>
<contexts>
<context position="15258" citStr="Pearl (1988)" startWordPosition="2477" endWordPosition="2478">descriptor. Then the precision of assigning NUCLEAR POWER if any one of them appears in a document is —7— and since e in this case is = the 3+5., t+.7 bound follows (for the case n = 5) with a little algebra. e where n is the cardinality, 01. n 67 theoretical reason to believe that representing a document by its set of most specific predicates is worth investigating, and this shall be investigated below. If one considers a calculus similar to the one described here, but allows c to limit to 0, then a weak default logic ensues which has been studied by Adams (1975), and further investigated by Pearl (1988). 4 Adding linguistic description The simplest way of representing a document is as a set or multi set of words. Many people (eg. Lewis 1992bc; Jacobs &amp; Rau 1990) have suggested that a more linguistically sophisticated representation of a document might be more effective for the purposes of statistical keyword assignment. Unfortunately, attempts to do this have not been found to reliably improve performance as measured by recall and precision for the task of document classification. I shall present evidence that a more sophisticated representation makes better predictions from the Single Term </context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference Morgan Kaufmann, San Mateo, Ca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>van Rijsbergen, C. J. (1979) Information retrieval. Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sacks-Davis</author>
</authors>
<title>Using Syntactic Analysis in a Document Retrieval System that Uses Signature Files.</title>
<date>1990</date>
<booktitle>ACM SIGIR-90.</booktitle>
<marker>Sacks-Davis, 1990</marker>
<rawString>Sacks-Davis, R. (1990) Using Syntactic Analysis in a Document Retrieval System that Uses Signature Files. ACM SIGIR-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<location>McGraw-Hill, NY.</location>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. &amp; McGill, M. J. (1983) Introduction to modern information retrieval. McGraw-Hill, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term Weighting Approaches</title>
<date>1988</date>
<booktitle>in Automatic Text Retrieval Information Processing and Management</booktitle>
<volume>24</volume>
<pages>513--23</pages>
<marker>Salton, Buckley, 1988</marker>
<rawString>Salton, G. &amp; C. Buckley (1988) Term Weighting Approaches in Automatic Text Retrieval Information Processing and Management 24 5 513-23</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zadeh</author>
</authors>
<title>Fuzzy Sets Information and control,</title>
<date>1965</date>
<journal>bf</journal>
<volume>8</volume>
<pages>338--53</pages>
<marker>Zadeh, 1965</marker>
<rawString>Zadeh, L. (1965) Fuzzy Sets Information and control, bf 8 338-53.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>