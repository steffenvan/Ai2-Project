<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.82175625">
Spectral Learning for Non-Deterministic Dependency Parsing
Franco M. Luque Ariadna Quattoni and Borja Balle and Xavier Carreras
Universidad Nacional de C´ordoba Universitat Polit`ecnica de Catalunya
and CONICET Barcelona E-08034
</note>
<address confidence="0.664294">
C´ordoba X5000HUA, Argentina laquattoni,bballe,carrerasl@lsi.upc.edu
</address>
<email confidence="0.993927">
francolq@famaf.unc.edu.ar
</email>
<sectionHeader confidence="0.99497" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995278666666667">
In this paper we study spectral learning
methods for non-deterministic split head-
automata grammars, a powerful hidden-
state formalism for dependency parsing.
We present a learning algorithm that, like
other spectral methods, is efficient and non-
susceptible to local minima. We show
how this algorithm can be formulated as
a technique for inducing hidden structure
from distributions computed by forward-
backward recursions. Furthermore, we
also present an inside-outside algorithm
for the parsing model that runs in cubic
time, hence maintaining the standard pars-
ing costs for context-free grammars.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999817035714286">
Dependency structures of natural language sen-
tences exhibit a significant amount of non-local
phenomena. Historically, there have been two
main approaches to model non-locality: (1) in-
creasing the order of the factors of a dependency
model (e.g. with sibling and grandparent relations
(Eisner, 2000; McDonald and Pereira, 2006; Car-
reras, 2007; Martins et al., 2009; Koo and Collins,
2010)), and (2) using hidden states to pass in-
formation across factors (Matsuzaki et al., 2005;
Petrov et al., 2006; Musillo and Merlo, 2008).
Higher-order models have the advantage that
they are relatively easy to train, because estimat-
ing the parameters of the model can be expressed
as a convex optimization. However, they have
two main drawbacks. (1) The number of param-
eters grows significantly with the size of the fac-
tors, leading to potential data-sparsity problems.
A solution to address the data-sparsity problem
is to explicitly tell the model what properties of
higher-order factors need to be remembered. This
can be achieved by means of feature engineering,
but compressing such information into a state of
bounded size will typically be labor intensive, and
will not generalize across languages. (2) Increas-
ing the size of the factors generally results in poly-
nomial increases in the parsing cost.
In principle, hidden variable models could
solve some of the problems of feature engineering
in higher-order factorizations, since they could
automatically induce the information in a deriva-
tion history that should be passed across factors.
Potentially, they would require less feature engi-
neering since they can learn from an annotated
corpus an optimal way to compress derivations
into hidden states. For example, one line of work
has added hidden annotations to the non-terminals
of a phrase-structure grammar (Matsuzaki et al.,
2005; Petrov et al., 2006; Musillo and Merlo,
2008), resulting in compact grammars that ob-
tain parsing accuracies comparable to lexicalized
grammars. A second line of work has modeled
hidden sequential structure, like in our case, but
using PDFA (Infante-Lopez and de Rijke, 2004).
Finally, a third line of work has induced hidden
structure from the history of actions of a parser
(Titov and Henderson, 2007).
However, the main drawback of the hidden
variable approach to parsing is that, to the best
of our knowledge, there has not been any convex
formulation of the learning problem. As a result,
training a hidden-variable model is both expen-
sive and prone to local minima issues.
In this paper we present a learning algorithm
for hidden-state split head-automata grammars
(SHAG) (Eisner and Satta, 1999). In this for-
</bodyText>
<page confidence="0.987375">
409
</page>
<note confidence="0.976561">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.905176458333333">
malism, head-modifier sequences are generated
by a collection of finite-state automata. In our
case, the underlying machines are probabilistic
non-deterministic finite state automata (PNFA),
which we parameterize using the operator model
representation. This representation allows the use
of simple spectral algorithms for estimating the
model parameters from data (Hsu et al., 2009;
Bailly, 2011; Balle et al., 2012). In all previous
work, the algorithms used to induce hidden struc-
ture require running repeated inference on train-
ing data—e.g. Expectation-Maximization (Demp-
ster et al., 1977), or split-merge algorithms. In
contrast, spectral methods are simple and very ef-
ficient —parameter estimation is reduced to com-
puting some data statistics, performing SVD, and
inverting matrices.
The main contributions of this paper are:
• We present a spectral learning algorithm for
inducing PNFA with applications to head-
automata dependency grammars. Our for-
mulation is based on thinking about the dis-
tribution generated by a PNFA in terms of
the forward-backward recursions.
</bodyText>
<listItem confidence="0.998214833333333">
• Spectral learning algorithms in previous
work only use statistics of prefixes of se-
quences. In contrast, our algorithm is able
to learn from substring statistics.
• We derive an inside-outside algorithm for
non-deterministic SHAG that runs in cubic
time, keeping the costs of CFG parsing.
• In experiments we show that adding non-
determinism improves the accuracy of sev-
eral baselines. When we compare our algo-
rithm to EM we observe a reduction of two
orders of magnitude in training time.
</listItem>
<bodyText confidence="0.999891428571429">
The paper is organized as follows. Next section
describes the necessary background on SHAG
and operator models. Section 3 introduces Op-
erator SHAG for parsing, and presents a spectral
learning algorithm. Section 4 presents a parsing
algorithm. Section 5 presents experiments and
analysis of results, and section 6 concludes.
</bodyText>
<sectionHeader confidence="0.998636" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.998884">
2.1 Head-Automata Dependency Grammars
</subsectionHeader>
<bodyText confidence="0.969010285714286">
In this work we use split head-automata gram-
mars (SHAG) (Eisner and Satta, 1999; Eis-
ner, 2000), a context-free grammatical formal-
ism whose derivations are projective dependency
trees. We will use xi:j = xixi+1 · · · xj to de-
note a sequence of symbols xt with i &lt; t &lt; j.
A SHAG generates sentences s0:N, where sym-
bols st E X with 1 &lt; t &lt; N are regular words
and s0 = * E� X is a special root symbol. Let
X= X U {*}. A derivation y, i.e. a depen-
dency tree, is a collection of head-modifier se-
quences (h, d, x1:T), where h E X is a word,
d E {LEFT, RIGHT} is a direction, and x1:T is
a sequence of T words, where each xt E X is
a modifier of h in direction d. We say that h is
the head of each xt. Modifier sequences x1:T are
ordered head-outwards, i.e. among x1:T, x1 is the
word closest to h in the derived sentence, and xT
is the furthest. A derivation y of a sentence s0:N
consists of a LEFT and a RIGHT head-modifier se-
quence for each st. As special cases, the LEFT se-
quence of the root symbol is always empty, while
the RIGHT one consists of a single word corre-
sponding to the head of the sentence. We denote
by Y the set of all valid derivations.
Assume a derivation y contains (h, LEFT, x1:T)
and (h, RIGHT, xi:T,). Let L(y, h) be the derived
sentence headed by h, which can be expressed as
L(y, xT) ··· L(y, x1) h L(y, xi) ··· L(y, x�T,).1
The language generated by a SHAG are the
strings L(y, *) for any y E Y.
In this paper we use probabilistic versions of
SHAG where probabilities of head-modifier se-
quences in a derivation are independent of each
other:
</bodyText>
<equation confidence="0.9952925">
P(y) = � P(x1:T |h, d) (1)
(h,d,x1:T)Ey
</equation>
<bodyText confidence="0.8640185">
In the literature, standard arc-factored models fur-
ther assume that
</bodyText>
<equation confidence="0.998130333333333">
T+ 1
P(x1:T |h, d) = H P(xt|h, d, at) , (2)
t=1
</equation>
<bodyText confidence="0.998241">
where xT+1 is always a special STOP word, and at
is the state of a deterministic automaton generat-
ing x1:T+1. For example, setting a1 = FIRST and
at&gt;1 = REST corresponds to first-order models,
while setting a1 = NULL and at&gt;1 = xt−1 corre-
sponds to sibling models (Eisner, 2000; McDon-
ald et al., 2005; McDonald and Pereira, 2006).
</bodyText>
<footnote confidence="0.999855666666667">
1Throughout the paper we assume we can distinguish the
words in a derivation, irrespective of whether two words at
different positions correspond to the same symbol.
</footnote>
<page confidence="0.988887">
410
</page>
<subsectionHeader confidence="0.978121">
2.2 Operator Models
</subsectionHeader>
<bodyText confidence="0.99973525">
An operator model A with n states is a tuple
(α1, α�,,,,, {Aa}aEX), where Aa E Rnxn is an op-
erator matrix and α1, α,,,, E Rn are vectors. A
computes a function f : X* —* R as follows:
</bodyText>
<equation confidence="0.998243">
f(x1:T) = α� AxT ... Ax1 α1 . (3)
</equation>
<bodyText confidence="0.998770428571429">
One intuitive way of understanding operator
models is to consider the case where f computes
a probability distribution over strings. Such a dis-
tribution can be described in two equivalent ways:
by making some independence assumptions and
providing the corresponding parameters, or by ex-
plaining the process used to compute f. This is
akin to describing the distribution defined by an
HMM in terms of a factorization and its corre-
sponding transition and emission parameters, or
using the inductive equations of the forward al-
gorithm. The operator model representation takes
the latter approach.
Operator models have had numerous applica-
tions. For example, they can be used as an alter-
native parameterization of the function computed
by an HMM (Hsu et al., 2009). Consider an HMM
with n hidden states and initial-state probabilities
7r E Rn, transition probabilities T E Rnxn, and
observation probabilities Oa E Rnxn for each
a E X, with the following meaning:
</bodyText>
<listItem confidence="0.9862945">
• 7r(i) is the probability of starting at state i,
• T(i, j) is the probability of transitioning
from state j to state i,
• Oa is a diagonal matrix, such that Oa(i, i) is
the probability of generating symbol a from
state i.
</listItem>
<bodyText confidence="0.999862722222222">
Given an HMM, an equivalent operator model
can be defined by setting α1 = 7r, Aa = TOa
and α,,,, = 1. To see this, let us show that the for-
ward algorithm computes the expression in equa-
tion (3). Let at denote the state of the HMM
at time t. Consider a state-distribution vector
αt E Rn, where αt(i) = P(x1:t−1, at= i). Ini-
tially α1 = 7r. At each step in the chain of prod-
ucts (3), αt+1 = Axt αt updates the state dis-
tribution from positions t to t + 1 by applying
the appropriate operator, i.e. by emitting symbol
xt and transitioning to the new state distribution.
The probability of x1:T is given by Ei αT+1(i).
Hence, Aa(i, j) is the probability of generating
symbol a and moving to state i given that we are
at state j.
HMM are only one example of distributions
that can be parameterized by operator models.
In general, operator models can parameterize any
PNFA, where the parameters of the model corre-
spond to probabilities of emitting a symbol from
a state and moving to the next state.
The advantage of working with operator mod-
els is that, under certain mild assumptions on the
operator parameters, there exist algorithms that
can estimate the operators from observable statis-
tics of the input sequences. These algorithms are
extremely efficient and are not susceptible to local
minima issues. See (Hsu et al., 2009) for theoret-
ical proofs of the learnability of HMM under the
operator model representation.
In the following, we write x = xi:j E X* to
denote sequences of symbols, and use Axi,j as a
shorthand for Axj • • • Axi. Also, for convenience
we assume X = {1, ... , l}, so that we can index
vectors and matrices by symbols in X.
</bodyText>
<sectionHeader confidence="0.915249" genericHeader="method">
3 Learning Operator SHAG
</sectionHeader>
<bodyText confidence="0.999940166666667">
We will define a SHAG using a collection of op-
erator models to compute probabilities. Assume
that for each possible head h in the vocabulary X
and each direction d E {LEFT, RIGHT} we have
an operator model that computes probabilities of
modifier sequences as follows:
</bodyText>
<equation confidence="0.969531">
P(x1:T |h, d) = (αh,d T h,d . h,d h,d
</equation>
<bodyText confidence="0.996648428571429">
,,,, ) AxT Ax1 α1 .
Then, this collection of operator models defines
an operator SHAG that assigns a probability to
each y E Y according to (1). To learn the model
parameters, namely �αh,d
1 , αh,d,,,, , {Ah,d
a }aEX� for
h E X and d E {LEFT, RIGHT}, we use spec-
tral learning methods based on the works of Hsu
et al. (2009), Bailly (2011) and Balle et al. (2012).
The main challenge of learning an operator
model is to infer a hidden-state space from ob-
servable quantities, i.e. quantities that can be com-
puted from the distribution of sequences that we
observe. As it turns out, we cannot recover the
actual hidden-state space used by the operators
we wish to learn. The key insight of the spectral
learning method is that we can recover a hidden-
state space that corresponds to a projection of the
original hidden space. Such projected space is
equivalent to the original one in the sense that we
</bodyText>
<page confidence="0.99344">
411
</page>
<bodyText confidence="0.998968777777778">
can find operators in the projected space that pa-
rameterize the same probability distribution over
sequences.
In the rest of this section we describe an algo-
rithm for learning an operator model. We will as-
sume a fixed head word and direction, and drop h
and d from all terms. Hence, our goal is to learn
the following distribution, parameterized by oper-
ators α1, {Aa}aEX, and α00:
</bodyText>
<equation confidence="0.999329">
P(x1:T) = α 00 AxT ··· Ax α1 . (4)
</equation>
<bodyText confidence="0.999987">
Our algorithm shares many features with the
previous spectral algorithms of Hsu et al. (2009)
and Bailly (2011), though the derivation given
here is based upon the general formulation of
Balle et al. (2012). The main difference is that
our algorithm is able to learn operator models
from substring statistics, while algorithms in pre-
vious works were restricted to statistics on pre-
fixes. In principle, our algorithm should extract
much more information from a sample.
</bodyText>
<subsectionHeader confidence="0.997729">
3.1 Preliminary Definitions
</subsectionHeader>
<bodyText confidence="0.9979508">
The spectral learning algorithm will use statistics
estimated from samples of the target distribution.
More specifically, consider the function that com-
putes the expected number of occurrences of a
substring x in a random string x&apos; drawn from P:
</bodyText>
<equation confidence="0.929672">
f(x) = E(x E] x&apos;)
</equation>
<bodyText confidence="0.9961435">
Furthermore, for each b E X let Pb E Rlxl denote
the matrix whose entries are given by
</bodyText>
<equation confidence="0.846314">
Pb(c, a) = E(abc E] x) , (7)
</equation>
<bodyText confidence="0.996525888888889">
the expected number of occurrences of trigrams.
Finally, we define vectors p1 E Rl and p00 E Rl
as follows: p1(a) = PsEX* P(as), the probabil-
ity that a string begins with a particular symbol;
and p00(a) = PpEX* P(pa), the probability that
a string ends with a particular symbol.
Now we show a particularly useful way to ex-
press the quantities defined above in terms of the
operators (α1, α , {Aa}aEX) of P. First, note
</bodyText>
<equation confidence="0.952346066666667">
00
that each entry of P can be written in this form:
P(b, a) = X P(pabs) (8)
p,sEX*
X= α 00 As Ab Aa Ap α1
p,sEX*
As) Ab Aa( X Ap α1) .
pEX*
It is not hard to see that, since P is a probability
P
distribution over X*, actually α sEX* As00
=
1 . Furthermore, since PpEX* Ap =
Pk&gt;0(PaEX Aa)k = (I − PaEX Aa)−1,
we write ˜α1 = (I − PaEX Aa)−1α1. From (8) it
</equation>
<bodyText confidence="0.9974175">
is natural to define a forward matrix F E Rnxl
whose ath column contains the sum of all hidden-
state vectors obtained after generating all prefixes
ended in a:
</bodyText>
<equation confidence="0.990547375">
X
=(α
00
sEX*
X= (x E] x&apos;)P(x&apos;) X Ap α1 = Aa ˜α1 . (9)
xIEX* F(:, a) = Aa
X= P(pxs) , (5)
p,sEX*
</equation>
<bodyText confidence="0.999994181818182">
where x C] x&apos; denotes the number of times x ap-
pears in x&apos;. Here we assume that the true values
of f(x) for bigrams are known, though in practice
the algorithm will work with empirical estimates
of these.
The information about f known by the algo-
rithm is organized in matrix form as follows. Let
P E Rlxl be a matrix containing the value of f(x)
for all strings of length two, i.e. bigrams.2. That
is, each entry in P E Rlxl contains the expected
number of occurrences of a given bigram:
</bodyText>
<equation confidence="0.83117">
P(b, a) = E(ab E] x) . (6)
</equation>
<bodyText confidence="0.740000857142857">
2In fact, while we restrict ourselves to strings of length
two, an analogous algorithm can be derived that considers
longer strings to define P. See (Balle et al., 2012) for details.
pEX*
Conversely, we also define a backward matrix
B E Rlxn whose ath row contains the probability
of generating a from any possible state:
</bodyText>
<equation confidence="0.982898">
B(a, :) = α 00 X AsAa = FAa . (10)
sEX*
</equation>
<bodyText confidence="0.941286538461538">
By plugging the forward and backward matri-
ces into (8) one obtains the factorization P =
BF. With similar arguments it is easy to see
that one also has Pb = BAbF, p1 = B α1, and
p 00 = α F. Hence, if B and F were known, one
00
could in principle invert these expressions in order
to recover the operators of the model from em-
pirical estimations computed from a sample. In
the next section we show that in fact one does not
need to know B and F to learn an operator model
for P, but rather that having a “good” factorization
of P is enough.
</bodyText>
<page confidence="0.991857">
412
</page>
<subsectionHeader confidence="0.999479">
3.2 Inducing a Hidden-State Space
</subsectionHeader>
<bodyText confidence="0.999973518518519">
We have shown that an operator model A com-
puting P induces a factorization of the matrix P,
namely P = BF. More generally, it turns out that
when the rank of P equals the minimal number of
states of an operator model that computes P, then
one can prove a duality relation between opera-
tors and factorizations of P. In particular, one can
show that, for any rank factorization P = QR, the
operators given by a1 = Q+p1, aT� = pT R+,
and Aa = Q+PaR+, yield an operator model for
P. A key fact in proving this result is that the func-
tion P is invariant to the basis chosen to represent
operator matrices. See (Balle et al., 2012) for fur-
ther details.
Thus, we can recover an operator model for P
from any rank factorization of P, provided a rank
assumption on P holds (which hereafter we as-
sume to be the case). Since we only have access
to an approximation of P, it seems reasonable to
choose a factorization which is robust to estima-
tion errors. A natural such choice is the thin SVD
decomposition of P (i.e. using top n singular vec-
tors), given by: P = U(EVT) = U(UTP).
Intuitively, we can think of U and UTP as pro-
jected backward and forward matrices. Now that
we have a factorization of P we can construct an
operator model for P as follows: 3
</bodyText>
<equation confidence="0.999783333333333">
a1 = UTp1 , (11)
�αT� = pT�(UTP)+ , (12)
Aa = UTPa(UTP)+ . (13)
</equation>
<bodyText confidence="0.908646285714286">
Algorithm 1 presents pseudo-code for an algo-
rithm learning operators of a SHAG from train-
ing head-modifier sequences using this spectral
method. Note that each operator model in the
3To see that equations (11-13) define a model for P, one
must first see that the matrix M = F(ΣV &gt;)+ is invertible
with inverse M−1 = U&gt;B. Using this and recalling that
</bodyText>
<equation confidence="0.974134111111111">
P1 = Bα1, Pa = BAaF, P&gt;∞ = α&gt;∞F, one obtains that:
¯α1 = U&gt;Bα1 = M−1α1 �
α∞ = α
¯&gt; &gt;∞F(U&gt;BF)+ = α&gt;∞M �
¯Aa = U&gt;BAaF(U&gt;BF)+ = M−1AaM .
Finally:
P(x1:T) = α&gt;∞ AxT ··· Ax1 α1
&gt;∞MM−1AxT M ··· M−1Ax1MM−1α1
Algorithm 1 Learn Operator SHAG
</equation>
<bodyText confidence="0.441685">
inputs:
</bodyText>
<listItem confidence="0.789116785714286">
• An alphabet X
• A training set TRAIN = {(hi, di, xi1.T)}Mi�1
• The number of hidden states n
1: for each h E X and d E {LEFT, RIGHT} do
2: Compute an empirical estimate from TRAIN of
bPa}a∈X
3: Compute the SVD of Pb and let Ub be the matrix
of top n left singular vectors of Pb
4: Compute the observable operators for h and d:
bαh,d
5: 1 = bU&gt;bp1 6: (bαh,d ∞ )&gt; = bp&gt; ∞( bU&gt; Pb )+
= bU&gt; bPa(bU&gt; Pb)+ for each a E X
8: end for
9: return Operators (bαh,d
</listItem>
<equation confidence="0.408039333333333">
1 , bαh,d∞ , bAh,d
a )
for each h E X, d E {LEFT, RIGHT}, a E X
</equation>
<bodyText confidence="0.993328555555556">
SHAG is learned separately. The running time
of the algorithm is dominated by two computa-
tions. First, a pass over the training sequences to
compute statistics over unigrams, bigrams and tri-
grams. Second, SVD and matrix operations for
computing the operators, which run in time cubic
in the number of symbols l. However, note that
when dealing with sparse matrices many of these
operations can be performed more efficiently.
</bodyText>
<sectionHeader confidence="0.987372" genericHeader="method">
4 Parsing Algorithms
</sectionHeader>
<bodyText confidence="0.993350125">
Given a sentence s0.N we would like
to find its most likely derivation, y� =
argmaxyEY(30:N) P(y). This problem, known as
MAP inference, is known to be intractable for
hidden-state structure prediction models, as it
involves finding the most likely tree structure
while summing out over hidden states. We use
a common approximation to MAP based on first
computing posterior marginals of tree edges (i.e.
dependencies) and then maximizing over the
tree structure (see (Park and Darwiche, 2004)
for complexity of general MAP inference and
approximations). For parsing, this strategy is
sometimes known as MBR decoding; previous
work has shown that empirically it gives good
performance (Goodman, 1996; Clark and Cur-
ran, 2004; Titov and Henderson, 2006; Petrov
and Klein, 2007). In our case, we use the
non-deterministic SHAG to compute posterior
marginals of dependencies. We first explain the
general strategy of MBR decoding, and then
present an algorithm to compute marginals.
= α
= ¯α
</bodyText>
<figure confidence="0.880027666666667">
&gt;
∞
¯Ax1 ¯α1
AxT ···
statistics matrices bp1, bp∞,
Pb, and {
7:
bAh,d
a
</figure>
<page confidence="0.998065">
413
</page>
<bodyText confidence="0.999713">
Let (si, sj) denote a dependency between head
word i and modifier word j. The posterior
or marginal probability of a dependency (si, sj)
given a sentence s0:N is defined as
</bodyText>
<equation confidence="0.999563">
µi,j = P((si, sj)  |s0:N) = X P(y) .
yEY(s0:N) : (si,sj)Ey
</equation>
<bodyText confidence="0.999923">
To compute marginals, the sum over derivations
can be decomposed into a product of inside and
outside quantities (Baker, 1979). Below we de-
scribe an inside-outside algorithm for our gram-
mars. Given a sentence s0:N and marginal scores
µi,j, we compute the parse tree for s0:N as
</bodyText>
<equation confidence="0.998678">
X
y� = argmax log µi,j (14)
yEY(s0:N) (si,sj)Ey
</equation>
<bodyText confidence="0.9999095">
using the standard projective parsing algorithm
for arc-factored models (Eisner, 2000). Overall
we use a two-pass parsing process, first to com-
pute marginals and then to compute the best tree.
</bodyText>
<subsectionHeader confidence="0.999298">
4.1 An Inside-Outside Algorithm
</subsectionHeader>
<bodyText confidence="0.999903672727273">
In this section we sketch an algorithm to com-
pute marginal probabilities of dependencies. Our
algorithm is an adaptation of the parsing algo-
rithm for SHAG by Eisner and Satta (1999) to
the case of non-deterministic head-automata, and
has a runtime cost of O(n2N3), where n is the
number of states of the model, and N is the
length of the input sentence. Hence the algorithm
maintains the standard cubic cost on the sentence
length, while the quadratic cost on n is inher-
ent to the computations defined by our model in
Eq. (3). The main insight behind our extension
is that, because the computations of our model in-
volve state-distribution vectors, we need to extend
the standard inside/outside quantities to be in the
form of such state-distribution quantities.4
Throughout this section we assume a fixed sen-
tence s0:N. Let Y(xi:j) be the set of derivations
that yield a subsequence xi:j. For a derivation y,
we use root(y) to indicate the root word of it,
and use (xi, xj) E y to refer a dependency in y
from head xi to modifier xj. Following Eisner
4Technically, when working with the projected operators
the state-distribution vectors will not be distributions in the
formal sense. However, they correspond to a projection of a
state distribution, for some projection that we do not recover
from data (namely M−1 in footnote 3). This projection has
no effect on the computations because it cancels out.
and Satta (1999), we use decoding structures re-
lated to complete half-constituents (or “triangles”,
denoted C) and incomplete half-constituents (or
“trapezoids”, denoted I), each decorated with a di-
rection (denoted L and R). We assume familiarity
with their algorithm.
We define θI,R
i,j E Rn as the inside score-vector
of a right trapezoid dominated by dependency
(si, sj),
The term P(y&apos;) is the probability of head-modifier
sequences in the range si:j that do not involve
si. The term αsi,R(x1:t) is a forward state-
distribution vector —the qth coordinate of the
vector is the probability that si generates right
modifiers x1:t and remains at state q. Similarly,
we define φI,R
i,j E Rn as the outside score-vector
of a right trapezoid, as
where βsi,R(xt+1:T) E Rn is a backward state-
distribution vector —the qth coordinate is the
probability of being at state q of the right au-
tomaton of si and generating xt+1:T. Analogous
inside-outside expressions can be defined for the
rest of structures (left/right triangles and trape-
zoids). With these quantities, we can compute
marginals as
</bodyText>
<equation confidence="0.988743">
( (φI,R
i,j)T θI,R
i,j Z−1 if i &lt; j ,
j
µi = (φI,L
i,j)TθI,L
i,j Z−1 if j &lt; i , (17)
where Z =PyEY(s0:N)P(y) = (α?,R� )T θC,R
0,N.
</equation>
<bodyText confidence="0.9996455">
Finally, we sketch the equations for computing
inside scores in O(N3) time. The outside equa-
tions can be derived analogously (see (Paskin,
2001)). For 0 &lt; i &lt; j &lt; N:
</bodyText>
<equation confidence="0.996765304347826">
θC,R
i,i = αsi,R (18) 1
� �
θI,R αsk,R T θC,R (19)
i,k ( oo ) k,j
si,R CR sj,L T C,L
Asj Bi,k ((α0° ) θk+1,j (20)
X P(y�)αsi,R(x1:t) . (15)
θI,R
i,j =
yEY(si:j) : (si,sj)Ey ,
y={(si,R,x1:t)} U yi , xt=sj
X P(y�)βsi,R(xt+1:T) , (16)
I,R =
φi,j
yEY(s0:isj:n) : root(y)=s0,
y={�si,R,xt:T U y , xt=sj
C,R = X j
θi,j k=i+1
X j
k=i
I,R
θi,j =
</equation>
<page confidence="0.998447">
414
</page>
<sectionHeader confidence="0.998745" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999915">
The goal of our experiments is to show that in-
corporating hidden states in a SHAG using oper-
ator models can consistently improve parsing ac-
curacy. A second goal is to compare the spec-
tral learning algorithm to EM, a standard learning
method that also induces hidden states.
The first set of experiments involve fully unlex-
icalized models, i.e. parsing part-of-speech tag se-
quences. While this setting falls behind the state-
of-the-art, it is nonetheless valid to analyze empir-
ically the effect of incorporating hidden states via
operator models, which results in large improve-
ments. In a second set of experiments, we com-
bine the unlexicalized hidden-state models with
simple lexicalized models. Finally, we present
some analysis of the automaton learned by the
spectral algorithm to see the information that is
captured in the hidden state space.
</bodyText>
<subsectionHeader confidence="0.981647">
5.1 Fully Unlexicalized Grammars
</subsectionHeader>
<bodyText confidence="0.999975833333333">
We trained fully unlexicalized dependency gram-
mars from dependency treebanks, that is, X are
PoS tags and we parse PoS tag sequences. In
all cases, our modifier sequences include special
START and STOP symbols at the boundaries. 5 6
We compare the following SHAG models:
</bodyText>
<listItem confidence="0.996977">
• DET: a baseline deterministic grammar with
a single state.
• DET+F: a deterministic grammar with two
states, one emitting the first modifier of a
sequence, and another emitting the rest (see
(Eisner and Smith, 2010) for a similar deter-
ministic baseline).
• SPECTRAL: a non-deterministic grammar
with n hidden states trained with the spectral
algorithm. n is a parameter of the model.
• EM: a non-deterministic grammar with n
states trained with EM. Here, we estimate
</listItem>
<equation confidence="0.5605245">
bA�,d
� � using forward-
</equation>
<bodyText confidence="0.788386125">
backward for the E step. To initialize, we
mimicked an HMM initialization: (1) we set
b�1 and b�,,,, randomly; (2) we created a ran-
dom transition matrix T E Rnxn; (3) we
5Even though the operators α1 and α∞ of a PNFA ac-
count for start and stop probabilities, in preliminary experi-
ments we found that having explicit START and STOP sym-
bols results in more accurate models.
</bodyText>
<footnote confidence="0.872283666666667">
6Note that, for parsing, the operators for the START and
STOP symbols can be packed into α1 and α∞ respectively.
One just defines α01 = ASTART α1 and α0&gt;∞ = α&gt;∞ ASTOP.
</footnote>
<figureCaption confidence="0.996939">
Figure 1: Accuracy curve on English development set
for fully unlexicalized models.
</figureCaption>
<bodyText confidence="0.962560666666667">
created a diagonal matrix O�,d
� E Rnxn,
where O�,d
� (i, i) is the probability of gener-
ating symbol a from hb and d (estimated from
training); (4) we set Aa,d = T O�,d
� .
We trained SHAG models using the standard
WSJ sections of the English Penn Treebank (Mar-
cus et al., 1994). Figure 1 shows the Unlabeled
Attachment Score (UAS) curve on the develop-
ment set, in terms of the number of hidden states
for the spectral and EM models. We can see
that DET+F largely outperforms DET7, while the
hidden-state models obtain much larger improve-
ments. For the EM model, we show the accuracy
curve after 5, 10, 25 and 100 iterations.8
In terms of peak accuracies, EM gives a slightly
better result than the spectral method (80.51% for
EM with 15 states versus 79.75% for the spectral
method with 9 states). However, the spectral al-
gorithm is much faster to train. With our Matlab
implementation, it took about 30 seconds, while
each iteration of EM took from 2 to 3 minutes,
depending on the number of states. To give a con-
crete example, to reach an accuracy close to 80%,
there is a factor of 150 between the training times
of the spectral method and EM (where we com-
pare the peak performance of the spectral method
versus EM at 25 iterations with 13 states).
</bodyText>
<footnote confidence="0.997303333333333">
7For parsing with deterministic SHAG we employ MBR
inference, even though Viterbi inference can be performed
exactly. In experiments on development data DET improved
from 62.65% using Viterbi to 68.52% using MBR, and
DET+F improved from 72.72% to 74.80%.
8We ran EM 10 times under different initial conditions
and selected the run that gave the best absolute accuracy after
100 iterations. We did not observe significant differences
between the runs.
</footnote>
<figure confidence="0.88484085">
2 4 6 8 10 12 14
number of states
unlabeled attachment score
82
80
78
76
74
72
70
68
Det
Det+F
pectral
S
EM (5)
EM (10)
EM (25)
EM (100)
operators (b�1, b�.,
</figure>
<page confidence="0.994111">
415
</page>
<table confidence="0.906338">
DET DET+F SPECTRAL EM
WSJ 69.45% 75.91% 80.44% 81.68%
</table>
<tableCaption confidence="0.9950005">
Table 1: Unlabeled Attachment Score of fully unlexi-
calized models on the WSJ test set.
</tableCaption>
<bodyText confidence="0.993212083333333">
Table 1 shows results on WSJ test data, se-
lecting the models that obtain peak performances
in development. We observe the same behavior:
hidden-states largely improve over deterministic
baselines, and EM obtains a slight improvement
over the spectral algorithm. Comparing to previ-
ous work on parsing WSJ PoS sequences, Eisner
and Smith (2010) obtained an accuracy of 75.6%
using a deterministic SHAG that uses informa-
tion about dependency lengths. However, they
used Viterbi inference, which we found to per-
form worse than MBR inference (see footnote 7).
</bodyText>
<subsectionHeader confidence="0.991116">
5.2 Experiments with Lexicalized
Grammars
</subsectionHeader>
<bodyText confidence="0.999393">
We now turn to combining lexicalized determinis-
tic grammars with the unlexicalized grammars ob-
tained in the previous experiment using the spec-
tral algorithm. The goal behind this experiment
is to show that the information captured in hidden
states is complimentary to head-modifier lexical
preferences.
In this case X consists of lexical items, and we
assume access to the PoS tag of each lexical item.
We will denote as ta and wa the PoS tag and word
of a symbol a ∈ X. We will estimate condi-
tional distributions P(a  |h, d, Q), where a ∈ X
is a modifier, h ∈ X is a head, d is a direction,
and Q is a deterministic state. Following Collins
(1999), we use three configurations of determin-
istic states:
</bodyText>
<listItem confidence="0.853863714285714">
• LEX: a single state.
• LEX+F: two distinct states for first modifier
and rest of modifiers.
• LEX+FCP: four distinct states, encoding:
first modifier, previous modifier was a coor-
dination, previous modifier was punctuation,
and previous modifier was some other word.
</listItem>
<equation confidence="0.7595465">
To estimate P we use a back-off strategy:
P(a|h, d, Q) = PA(ta|h, d, Q)PB(wa|ta, h, d, 6)
</equation>
<bodyText confidence="0.89717125">
To estimate PA we use two back-off levels,
the fine level conditions on {wh, d, Q} and the
2 3 4 5 6 7 8 9 10
number of states
</bodyText>
<figureCaption confidence="0.996477">
Figure 2: Accuracy curve on English development set
for lexicalized models.
</figureCaption>
<bodyText confidence="0.9993829">
coarse level conditions on {th, d, Q}. For PB we
use three levels, which from fine to coarse are
{ta, wh, d, Q}, {ta, th, d, Q} and {ta}. We follow
Collins (1999) to estimate PA and PB from a tree-
bank using a back-off strategy.
We use a simple approach to combine lexical
models with the unlexical hidden-state models we
obtained in the previous experiment. Namely, we
use a log-linear model that computes scores for
head-modifier sequences as
</bodyText>
<equation confidence="0.9070065">
s(hh, d, x1:Ti) = log P p(x1:T|h, d) (21)
+ log Pdet(x1:T |h, d) ,
</equation>
<bodyText confidence="0.999970636363636">
where P,p and Pdet are respectively spectral and
deterministic probabilistic models. We tested
combinations of each deterministic model with
the spectral unlexicalized model using different
number of states. Figure 2 shows the accuracies of
single deterministic models, together with combi-
nations using different number of states. In all
cases, the combinations largely improve over the
purely deterministic lexical counterparts, suggest-
ing that the information encoded in hidden states
is complementary to lexical preferences.
</bodyText>
<subsectionHeader confidence="0.977377">
5.3 Results Analysis
</subsectionHeader>
<bodyText confidence="0.999944375">
We conclude the experiments by analyzing the
state space learned by the spectral algorithm.
Consider the space ][R&apos; where the forward-state
vectors lie. Generating a modifier sequence corre-
sponds to a path through the n-dimensional state
space. We clustered sets of forward-state vectors
in order to create a DFA that we can use to visu-
alize the phenomena captured by the state space.
</bodyText>
<figure confidence="0.995258285714286">
Lex
Lex+F
Lex+FCP
Lex + Spectral
Lex+F + Spectral
Lex+FCP + Spectral
unlabeled attachment score 86
84
82
80
78
76
74
72
</figure>
<page confidence="0.940516">
416
</page>
<figureCaption confidence="0.9959525">
Figure 3: DFA approximation for the generation of NN
left modifier sequences.
</figureCaption>
<bodyText confidence="0.999987229166667">
To build a DFA, we computed the forward vec-
tors corresponding to frequent prefixes of modi-
fier sequences of the development set. Then, we
clustered these vectors using a Group Average
Agglomerative algorithm using the cosine simi-
larity measure (Manning et al., 2008). This simi-
larity measure is appropriate because it compares
the angle between vectors, and is not affected by
their magnitude (the magnitude of forward vec-
tors decreases with the number of modifiers gen-
erated). Each cluster i defines a state in the DFA,
and we say that a sequence x1:t is in state i if its
corresponding forward vector at time t is in clus-
ter i. Then, transitions in the DFA are defined us-
ing a procedure that looks at how sequences tra-
verse the states. If a sequence x1:t is at state i at
time t − 1, and goes to state j at time t, then we
define a transition from state i to state j with la-
bel xt. This procedure may require merging states
to give a consistent DFA, because different se-
quences may define different transitions for the
same states and modifiers. After doing a merge,
new merges may be required, so the procedure
must be repeated until a DFA is obtained.
For this analysis, we took the spectral model
with 9 states, and built DFA from the non-
deterministic automata corresponding to heads
and directions where we saw largest improve-
ments in accuracy with respect to the baselines.
A DFA for the automaton (NN, LEFT) is shown
in Figure 3. The vectors were originally divided
in ten clusters, but the DFA construction required
two state mergings, leading to a eight state au-
tomaton. The state named I is the initial state.
Clearly, we can see that there are special states
for punctuation (state 9) and coordination (states
1 and 5). States 0 and 2 are harder to interpret.
To understand them better, we computed an esti-
mation of the probabilities of the transitions, by
counting the number of times each of them is
used. We found that our estimation of generating
STOP from state 0 is 0.67, and from state 2 it is
0.15. Interestingly, state 2 can transition to state 0
generating prp$, POS or DT, that are usual end-
ings of modifier sequences for nouns (recall that
modifiers are generated head-outwards, so for a
left automaton the final modifier is the left-most
modifier in the sentence).
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.991110413793103">
Our main contribution is a basic tool for inducing
sequential hidden structure in dependency gram-
mars. Most of the recent work in dependency
parsing has explored explicit feature engineering.
In part, this may be attributed to the high cost of
using tools such as EM to induce representations.
Our experiments have shown that adding hidden-
structure improves parsing accuracy, and that our
spectral algorithm is highly scalable.
Our methods may be used to enrich the rep-
resentational power of more sophisticated depen-
dency models. For example, future work should
consider enhancing lexicalized dependency gram-
mars with hidden states that summarize lexical
dependencies. Another line for future research
should extend the learning algorithm to be able
to capture vertical hidden relations in the depen-
dency tree, in addition to sequential relations.
Acknowledgements We are grateful to Gabriele
Musillo and the anonymous reviewers for providing us
with helpful comments. This work was supported by
a Google Research Award and by the European Com-
mission (PASCAL2 NoE FP7-216886, XLike STREP
FP7-288342). Borja Balle was supported by an FPU
fellowship (AP2008-02064) of the Spanish Ministry
of Education. The Spanish Ministry of Science and
Innovation supported Ariadna Quattoni (JCI-2009-
04240) and Xavier Carreras (RYC-2008-02223 and
“KNOW2” TIN2009-14715-C04-04).
</bodyText>
<figure confidence="0.986239125">
nns
STOP
I
,
prp$ vbg jjs
rb vbn pos
jj in dt cd
$ nn
jjr nnp
cd
cc
9
$ nnp
prp$ nn pos
jj dt nnp
2
nn
cc
,
,
STOP
nn
1 0 cc
5
cc
cd nns
prp$ rb pos
jj dt nnp
STOP
3
STOP
7
</figure>
<page confidence="0.994659">
417
</page>
<sectionHeader confidence="0.978285" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999466336283186">
Raphael Bailly. 2011. Quadratic weighted automata:
Spectral algorithm and likelihood maximization.
JMLR Workshop and Conference Proceedings –
ACML.
James K. Baker. 1979. Trainable grammars for speech
recognition. In D. H. Klatt and J. J. Wolf, editors,
Speech Communication Papers for the 97th Meeting
of the Acoustical Society of America, pages 547–
550.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2012. Local loss optimization in operator models:
A new insight into spectral learning. Technical Re-
port LSI-12-5-R, Departament de Llenguatges i Sis-
temes Inform`atics (LSI), Universitat Polit`ecnica de
Catalunya (UPC).
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 957–961, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Stephen Clark and James R. Curran. 2004. Parsing
the wsj using ccg and log-linear models. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 103–110, Barcelona, Spain, July.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the royal sta-
tistical society, Series B, 39(1):1–38.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 457–464, Univer-
sity of Maryland, June.
Jason Eisner and Noah A. Smith. 2010. Favor
short dependencies: Parsing with soft and hard con-
straints on dependency length. In Harry Bunt, Paola
Merlo, and Joakim Nivre, editors, Trends in Parsing
Technology: Dependency Parsing, Domain Adapta-
tion, and Deep Parsing, chapter 8, pages 121–150.
Springer.
Jason Eisner. 2000. Bilexical grammars and their
cubic-time parsing algorithms. In Harry Bunt and
Anton Nijholt, editors, Advances in Probabilis-
tic and Other Parsing Technologies, pages 29–62.
Kluwer Academic Publishers, October.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 177–183, Santa Cruz, California, USA, June.
Association for Computational Linguistics.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009.
A spectral algorithm for learning hidden markov
models. In COLT 2009 - The 22nd Conference on
Learning Theory.
Gabriel Infante-Lopez and Maarten de Rijke. 2004.
Alternative approaches for generating bodies of
grammar rules. In Proceedings of the 42nd Meet-
ing of the Association for Computational Lin-
guistics (ACL’04), Main Volume, pages 454–461,
Barcelona, Spain, July.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1–11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
first edition, July.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 342–
350, Suntec, Singapore, August. Association for
Computational Linguistics.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL’05),
pages 75–82, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 81–88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523–530, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Gabriele Antonio Musillo and Paola Merlo. 2008. Un-
lexicalised hidden variable models of split depen-
dency grammars. In Proceedings of ACL-08: HLT,
Short Papers, pages 213–216, Columbus, Ohio,
June. Association for Computational Linguistics.
James D. Park and Adnan Darwiche. 2004. Com-
plexity results and approximation strategies for map
</reference>
<page confidence="0.981294">
418
</page>
<reference confidence="0.999803303030303">
explanations. Journal of Artificial Intelligence Re-
search, 21:101–133.
Mark Paskin. 2001. Cubic-time parsing and learning
algorithms for grammatical bigram models. Techni-
cal Report UCB/CSD-01-1148, University of Cali-
fornia, Berkeley.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 433–
440, Sydney, Australia, July. Association for Com-
putational Linguistics.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
2006 Conference on Empirical Methods in Natu-
ral Language Processing, pages 560–567, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the Tenth International Conference
on Parsing Technologies, pages 144–155, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.998669">
419
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.598977">
<title confidence="0.999909">Spectral Learning for Non-Deterministic Dependency Parsing</title>
<author confidence="0.892409">M Luque Ariadna Quattoni Balle Carreras</author>
<affiliation confidence="0.832381">Universidad Nacional de C´ordoba Universitat Polit`ecnica de Catalunya and CONICET Barcelona E-08034</affiliation>
<address confidence="0.988812">C´ordoba X5000HUA, Argentina</address>
<email confidence="0.996849">francolq@famaf.unc.edu.ar</email>
<abstract confidence="0.9968848125">In this paper we study spectral learning methods for non-deterministic split headautomata grammars, a powerful hiddenstate formalism for dependency parsing. We present a learning algorithm that, like other spectral methods, is efficient and nonsusceptible to local minima. We show how this algorithm can be formulated as a technique for inducing hidden structure from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Raphael Bailly</author>
</authors>
<title>Quadratic weighted automata: Spectral algorithm and likelihood maximization.</title>
<date>2011</date>
<booktitle>JMLR Workshop and Conference Proceedings – ACML.</booktitle>
<contexts>
<context position="4204" citStr="Bailly, 2011" startWordPosition="626" endWordPosition="627">or409 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics malism, head-modifier sequences are generated by a collection of finite-state automata. In our case, the underlying machines are probabilistic non-deterministic finite state automata (PNFA), which we parameterize using the operator model representation. This representation allows the use of simple spectral algorithms for estimating the model parameters from data (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012). In all previous work, the algorithms used to induce hidden structure require running repeated inference on training data—e.g. Expectation-Maximization (Dempster et al., 1977), or split-merge algorithms. In contrast, spectral methods are simple and very efficient —parameter estimation is reduced to computing some data statistics, performing SVD, and inverting matrices. The main contributions of this paper are: • We present a spectral learning algorithm for inducing PNFA with applications to headautomata dependency grammars. Our formulation is based on thinking about the d</context>
<context position="11707" citStr="Bailly (2011)" startWordPosition="1980" endWordPosition="1981">ollection of operator models to compute probabilities. Assume that for each possible head h in the vocabulary X and each direction d E {LEFT, RIGHT} we have an operator model that computes probabilities of modifier sequences as follows: P(x1:T |h, d) = (αh,d T h,d . h,d h,d ,,,, ) AxT Ax1 α1 . Then, this collection of operator models defines an operator SHAG that assigns a probability to each y E Y according to (1). To learn the model parameters, namely �αh,d 1 , αh,d,,,, , {Ah,d a }aEX� for h E X and d E {LEFT, RIGHT}, we use spectral learning methods based on the works of Hsu et al. (2009), Bailly (2011) and Balle et al. (2012). The main challenge of learning an operator model is to infer a hidden-state space from observable quantities, i.e. quantities that can be computed from the distribution of sequences that we observe. As it turns out, we cannot recover the actual hidden-state space used by the operators we wish to learn. The key insight of the spectral learning method is that we can recover a hiddenstate space that corresponds to a projection of the original hidden space. Such projected space is equivalent to the original one in the sense that we 411 can find operators in the projected </context>
</contexts>
<marker>Bailly, 2011</marker>
<rawString>Raphael Bailly. 2011. Quadratic weighted automata: Spectral algorithm and likelihood maximization. JMLR Workshop and Conference Proceedings – ACML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition. In</title>
<date>1979</date>
<booktitle>Speech Communication Papers for the 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<editor>D. H. Klatt and J. J. Wolf, editors,</editor>
<contexts>
<context position="20333" citStr="Baker, 1979" startWordPosition="3597" endWordPosition="3598">rministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. = α = ¯α &gt; ∞ ¯Ax1 ¯α1 AxT ··· statistics matrices bp1, bp∞, Pb, and { 7: bAh,d a 413 Let (si, sj) denote a dependency between head word i and modifier word j. The posterior or marginal probability of a dependency (si, sj) given a sentence s0:N is defined as µi,j = P((si, sj) |s0:N) = X P(y) . yEY(s0:N) : (si,sj)Ey To compute marginals, the sum over derivations can be decomposed into a product of inside and outside quantities (Baker, 1979). Below we describe an inside-outside algorithm for our grammars. Given a sentence s0:N and marginal scores µi,j, we compute the parse tree for s0:N as X y� = argmax log µi,j (14) yEY(s0:N) (si,sj)Ey using the standard projective parsing algorithm for arc-factored models (Eisner, 2000). Overall we use a two-pass parsing process, first to compute marginals and then to compute the best tree. 4.1 An Inside-Outside Algorithm In this section we sketch an algorithm to compute marginal probabilities of dependencies. Our algorithm is an adaptation of the parsing algorithm for SHAG by Eisner and Satta </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In D. H. Klatt and J. J. Wolf, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, pages 547– 550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Borja Balle</author>
<author>Ariadna Quattoni</author>
<author>Xavier Carreras</author>
</authors>
<title>Local loss optimization in operator models: A new insight into spectral learning.</title>
<date>2012</date>
<booktitle>Llenguatges i Sistemes Inform`atics (LSI), Universitat Polit`ecnica de Catalunya (UPC).</booktitle>
<tech>Technical Report LSI-12-5-R, Departament de</tech>
<contexts>
<context position="4225" citStr="Balle et al., 2012" startWordPosition="628" endWordPosition="631">ngs of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics malism, head-modifier sequences are generated by a collection of finite-state automata. In our case, the underlying machines are probabilistic non-deterministic finite state automata (PNFA), which we parameterize using the operator model representation. This representation allows the use of simple spectral algorithms for estimating the model parameters from data (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012). In all previous work, the algorithms used to induce hidden structure require running repeated inference on training data—e.g. Expectation-Maximization (Dempster et al., 1977), or split-merge algorithms. In contrast, spectral methods are simple and very efficient —parameter estimation is reduced to computing some data statistics, performing SVD, and inverting matrices. The main contributions of this paper are: • We present a spectral learning algorithm for inducing PNFA with applications to headautomata dependency grammars. Our formulation is based on thinking about the distribution generated</context>
<context position="11731" citStr="Balle et al. (2012)" startWordPosition="1983" endWordPosition="1986">tor models to compute probabilities. Assume that for each possible head h in the vocabulary X and each direction d E {LEFT, RIGHT} we have an operator model that computes probabilities of modifier sequences as follows: P(x1:T |h, d) = (αh,d T h,d . h,d h,d ,,,, ) AxT Ax1 α1 . Then, this collection of operator models defines an operator SHAG that assigns a probability to each y E Y according to (1). To learn the model parameters, namely �αh,d 1 , αh,d,,,, , {Ah,d a }aEX� for h E X and d E {LEFT, RIGHT}, we use spectral learning methods based on the works of Hsu et al. (2009), Bailly (2011) and Balle et al. (2012). The main challenge of learning an operator model is to infer a hidden-state space from observable quantities, i.e. quantities that can be computed from the distribution of sequences that we observe. As it turns out, we cannot recover the actual hidden-state space used by the operators we wish to learn. The key insight of the spectral learning method is that we can recover a hiddenstate space that corresponds to a projection of the original hidden space. Such projected space is equivalent to the original one in the sense that we 411 can find operators in the projected space that parameterize </context>
<context position="15276" citStr="Balle et al., 2012" startWordPosition="2646" endWordPosition="2649">we assume that the true values of f(x) for bigrams are known, though in practice the algorithm will work with empirical estimates of these. The information about f known by the algorithm is organized in matrix form as follows. Let P E Rlxl be a matrix containing the value of f(x) for all strings of length two, i.e. bigrams.2. That is, each entry in P E Rlxl contains the expected number of occurrences of a given bigram: P(b, a) = E(ab E] x) . (6) 2In fact, while we restrict ourselves to strings of length two, an analogous algorithm can be derived that considers longer strings to define P. See (Balle et al., 2012) for details. pEX* Conversely, we also define a backward matrix B E Rlxn whose ath row contains the probability of generating a from any possible state: B(a, :) = α 00 X AsAa = FAa . (10) sEX* By plugging the forward and backward matrices into (8) one obtains the factorization P = BF. With similar arguments it is easy to see that one also has Pb = BAbF, p1 = B α1, and p 00 = α F. Hence, if B and F were known, one 00 could in principle invert these expressions in order to recover the operators of the model from empirical estimations computed from a sample. In the next section we show that in fa</context>
<context position="16672" citStr="Balle et al., 2012" startWordPosition="2922" endWordPosition="2925">e shown that an operator model A computing P induces a factorization of the matrix P, namely P = BF. More generally, it turns out that when the rank of P equals the minimal number of states of an operator model that computes P, then one can prove a duality relation between operators and factorizations of P. In particular, one can show that, for any rank factorization P = QR, the operators given by a1 = Q+p1, aT� = pT R+, and Aa = Q+PaR+, yield an operator model for P. A key fact in proving this result is that the function P is invariant to the basis chosen to represent operator matrices. See (Balle et al., 2012) for further details. Thus, we can recover an operator model for P from any rank factorization of P, provided a rank assumption on P holds (which hereafter we assume to be the case). Since we only have access to an approximation of P, it seems reasonable to choose a factorization which is robust to estimation errors. A natural such choice is the thin SVD decomposition of P (i.e. using top n singular vectors), given by: P = U(EVT) = U(UTP). Intuitively, we can think of U and UTP as projected backward and forward matrices. Now that we have a factorization of P we can construct an operator model </context>
</contexts>
<marker>Balle, Quattoni, Carreras, 2012</marker>
<rawString>Borja Balle, Ariadna Quattoni, and Xavier Carreras. 2012. Local loss optimization in operator models: A new insight into spectral learning. Technical Report LSI-12-5-R, Departament de Llenguatges i Sistemes Inform`atics (LSI), Universitat Polit`ecnica de Catalunya (UPC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLPCoNLL</booktitle>
<pages>957--961</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1288" citStr="Carreras, 2007" startWordPosition="173" endWordPosition="175">cing hidden structure from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In Proceedings of the CoNLL Shared Task Session of EMNLPCoNLL 2007, pages 957–961, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the wsj using ccg and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>103--110</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="19635" citStr="Clark and Curran, 2004" startWordPosition="3471" endWordPosition="3475">his problem, known as MAP inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. = α = ¯α &gt; ∞ ¯Ax1 ¯α1 AxT ··· statistics matrices bp1, bp∞, Pb, and { 7: bAh,d a 413 Let (si, sj) denote a dependency between head word i and modifier word j. The posterior or marginal probability of a dependency (si, sj) given a sentence s0:N is defined as µi,j = P((si, sj) |s0:N) = X P(y) . yEY(s0:N) : (si,sj)Ey To compute marginals, the su</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the wsj using ccg and log-linear models. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 103–110, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="29419" citStr="Collins (1999)" startWordPosition="5174" endWordPosition="5175">zed deterministic grammars with the unlexicalized grammars obtained in the previous experiment using the spectral algorithm. The goal behind this experiment is to show that the information captured in hidden states is complimentary to head-modifier lexical preferences. In this case X consists of lexical items, and we assume access to the PoS tag of each lexical item. We will denote as ta and wa the PoS tag and word of a symbol a ∈ X. We will estimate conditional distributions P(a |h, d, Q), where a ∈ X is a modifier, h ∈ X is a head, d is a direction, and Q is a deterministic state. Following Collins (1999), we use three configurations of deterministic states: • LEX: a single state. • LEX+F: two distinct states for first modifier and rest of modifiers. • LEX+FCP: four distinct states, encoding: first modifier, previous modifier was a coordination, previous modifier was punctuation, and previous modifier was some other word. To estimate P we use a back-off strategy: P(a|h, d, Q) = PA(ta|h, d, Q)PB(wa|ta, h, d, 6) To estimate PA we use two back-off levels, the fine level conditions on {wh, d, Q} and the 2 3 4 5 6 7 8 9 10 number of states Figure 2: Accuracy curve on English development set for lex</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the royal statistical society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="4401" citStr="Dempster et al., 1977" startWordPosition="653" endWordPosition="657">or Computational Linguistics malism, head-modifier sequences are generated by a collection of finite-state automata. In our case, the underlying machines are probabilistic non-deterministic finite state automata (PNFA), which we parameterize using the operator model representation. This representation allows the use of simple spectral algorithms for estimating the model parameters from data (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012). In all previous work, the algorithms used to induce hidden structure require running repeated inference on training data—e.g. Expectation-Maximization (Dempster et al., 1977), or split-merge algorithms. In contrast, spectral methods are simple and very efficient —parameter estimation is reduced to computing some data statistics, performing SVD, and inverting matrices. The main contributions of this paper are: • We present a spectral learning algorithm for inducing PNFA with applications to headautomata dependency grammars. Our formulation is based on thinking about the distribution generated by a PNFA in terms of the forward-backward recursions. • Spectral learning algorithms in previous work only use statistics of prefixes of sequences. In contrast, our algorithm</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and headautomaton grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>457--464</pages>
<institution>University of Maryland,</institution>
<contexts>
<context position="3581" citStr="Eisner and Satta, 1999" startWordPosition="537" endWordPosition="540">hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de Rijke, 2004). Finally, a third line of work has induced hidden structure from the history of actions of a parser (Titov and Henderson, 2007). However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem. As a result, training a hidden-variable model is both expensive and prone to local minima issues. In this paper we present a learning algorithm for hidden-state split head-automata grammars (SHAG) (Eisner and Satta, 1999). In this for409 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics malism, head-modifier sequences are generated by a collection of finite-state automata. In our case, the underlying machines are probabilistic non-deterministic finite state automata (PNFA), which we parameterize using the operator model representation. This representation allows the use of simple spectral algorithms for estimating the model parameters from data (Hsu et </context>
<context position="5830" citStr="Eisner and Satta, 1999" startWordPosition="878" endWordPosition="881"> nondeterminism improves the accuracy of several baselines. When we compare our algorithm to EM we observe a reduction of two orders of magnitude in training time. The paper is organized as follows. Next section describes the necessary background on SHAG and operator models. Section 3 introduces Operator SHAG for parsing, and presents a spectral learning algorithm. Section 4 presents a parsing algorithm. Section 5 presents experiments and analysis of results, and section 6 concludes. 2 Preliminaries 2.1 Head-Automata Dependency Grammars In this work we use split head-automata grammars (SHAG) (Eisner and Satta, 1999; Eisner, 2000), a context-free grammatical formalism whose derivations are projective dependency trees. We will use xi:j = xixi+1 · · · xj to denote a sequence of symbols xt with i &lt; t &lt; j. A SHAG generates sentences s0:N, where symbols st E X with 1 &lt; t &lt; N are regular words and s0 = * E� X is a special root symbol. Let X= X U {*}. A derivation y, i.e. a dependency tree, is a collection of head-modifier sequences (h, d, x1:T), where h E X is a word, d E {LEFT, RIGHT} is a direction, and x1:T is a sequence of T words, where each xt E X is a modifier of h in direction d. We say that h is the h</context>
<context position="20939" citStr="Eisner and Satta (1999)" startWordPosition="3696" endWordPosition="3699">ies (Baker, 1979). Below we describe an inside-outside algorithm for our grammars. Given a sentence s0:N and marginal scores µi,j, we compute the parse tree for s0:N as X y� = argmax log µi,j (14) yEY(s0:N) (si,sj)Ey using the standard projective parsing algorithm for arc-factored models (Eisner, 2000). Overall we use a two-pass parsing process, first to compute marginals and then to compute the best tree. 4.1 An Inside-Outside Algorithm In this section we sketch an algorithm to compute marginal probabilities of dependencies. Our algorithm is an adaptation of the parsing algorithm for SHAG by Eisner and Satta (1999) to the case of non-deterministic head-automata, and has a runtime cost of O(n2N3), where n is the number of states of the model, and N is the length of the input sentence. Hence the algorithm maintains the standard cubic cost on the sentence length, while the quadratic cost on n is inherent to the computations defined by our model in Eq. (3). The main insight behind our extension is that, because the computations of our model involve state-distribution vectors, we need to extend the standard inside/outside quantities to be in the form of such state-distribution quantities.4 Throughout this se</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and headautomaton grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pages 457–464, University of Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Noah A Smith</author>
</authors>
<title>Favor short dependencies: Parsing with soft and hard constraints on dependency length.</title>
<date>2010</date>
<booktitle>Trends in Parsing Technology: Dependency Parsing, Domain Adaptation, and Deep Parsing, chapter 8,</booktitle>
<pages>121--150</pages>
<editor>In Harry Bunt, Paola Merlo, and Joakim Nivre, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="25269" citStr="Eisner and Smith, 2010" startWordPosition="4441" endWordPosition="4444"> learned by the spectral algorithm to see the information that is captured in the hidden state space. 5.1 Fully Unlexicalized Grammars We trained fully unlexicalized dependency grammars from dependency treebanks, that is, X are PoS tags and we parse PoS tag sequences. In all cases, our modifier sequences include special START and STOP symbols at the boundaries. 5 6 We compare the following SHAG models: • DET: a baseline deterministic grammar with a single state. • DET+F: a deterministic grammar with two states, one emitting the first modifier of a sequence, and another emitting the rest (see (Eisner and Smith, 2010) for a similar deterministic baseline). • SPECTRAL: a non-deterministic grammar with n hidden states trained with the spectral algorithm. n is a parameter of the model. • EM: a non-deterministic grammar with n states trained with EM. Here, we estimate bA�,d � � using forwardbackward for the E step. To initialize, we mimicked an HMM initialization: (1) we set b�1 and b�,,,, randomly; (2) we created a random transition matrix T E Rnxn; (3) we 5Even though the operators α1 and α∞ of a PNFA account for start and stop probabilities, in preliminary experiments we found that having explicit START and</context>
<context position="28517" citStr="Eisner and Smith (2010)" startWordPosition="5014" endWordPosition="5017">r of states unlabeled attachment score 82 80 78 76 74 72 70 68 Det Det+F pectral S EM (5) EM (10) EM (25) EM (100) operators (b�1, b�., 415 DET DET+F SPECTRAL EM WSJ 69.45% 75.91% 80.44% 81.68% Table 1: Unlabeled Attachment Score of fully unlexicalized models on the WSJ test set. Table 1 shows results on WSJ test data, selecting the models that obtain peak performances in development. We observe the same behavior: hidden-states largely improve over deterministic baselines, and EM obtains a slight improvement over the spectral algorithm. Comparing to previous work on parsing WSJ PoS sequences, Eisner and Smith (2010) obtained an accuracy of 75.6% using a deterministic SHAG that uses information about dependency lengths. However, they used Viterbi inference, which we found to perform worse than MBR inference (see footnote 7). 5.2 Experiments with Lexicalized Grammars We now turn to combining lexicalized deterministic grammars with the unlexicalized grammars obtained in the previous experiment using the spectral algorithm. The goal behind this experiment is to show that the information captured in hidden states is complimentary to head-modifier lexical preferences. In this case X consists of lexical items, </context>
</contexts>
<marker>Eisner, Smith, 2010</marker>
<rawString>Jason Eisner and Noah A. Smith. 2010. Favor short dependencies: Parsing with soft and hard constraints on dependency length. In Harry Bunt, Paola Merlo, and Joakim Nivre, editors, Trends in Parsing Technology: Dependency Parsing, Domain Adaptation, and Deep Parsing, chapter 8, pages 121–150. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and their cubic-time parsing algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<editor>In Harry Bunt and Anton Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="1244" citStr="Eisner, 2000" startWordPosition="167" endWordPosition="168"> can be formulated as a technique for inducing hidden structure from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity </context>
<context position="5845" citStr="Eisner, 2000" startWordPosition="882" endWordPosition="884"> the accuracy of several baselines. When we compare our algorithm to EM we observe a reduction of two orders of magnitude in training time. The paper is organized as follows. Next section describes the necessary background on SHAG and operator models. Section 3 introduces Operator SHAG for parsing, and presents a spectral learning algorithm. Section 4 presents a parsing algorithm. Section 5 presents experiments and analysis of results, and section 6 concludes. 2 Preliminaries 2.1 Head-Automata Dependency Grammars In this work we use split head-automata grammars (SHAG) (Eisner and Satta, 1999; Eisner, 2000), a context-free grammatical formalism whose derivations are projective dependency trees. We will use xi:j = xixi+1 · · · xj to denote a sequence of symbols xt with i &lt; t &lt; j. A SHAG generates sentences s0:N, where symbols st E X with 1 &lt; t &lt; N are regular words and s0 = * E� X is a special root symbol. Let X= X U {*}. A derivation y, i.e. a dependency tree, is a collection of head-modifier sequences (h, d, x1:T), where h E X is a word, d E {LEFT, RIGHT} is a direction, and x1:T is a sequence of T words, where each xt E X is a modifier of h in direction d. We say that h is the head of each xt.</context>
<context position="7747" citStr="Eisner, 2000" startWordPosition="1261" endWordPosition="1262">he strings L(y, *) for any y E Y. In this paper we use probabilistic versions of SHAG where probabilities of head-modifier sequences in a derivation are independent of each other: P(y) = � P(x1:T |h, d) (1) (h,d,x1:T)Ey In the literature, standard arc-factored models further assume that T+ 1 P(x1:T |h, d) = H P(xt|h, d, at) , (2) t=1 where xT+1 is always a special STOP word, and at is the state of a deterministic automaton generating x1:T+1. For example, setting a1 = FIRST and at&gt;1 = REST corresponds to first-order models, while setting a1 = NULL and at&gt;1 = xt−1 corresponds to sibling models (Eisner, 2000; McDonald et al., 2005; McDonald and Pereira, 2006). 1Throughout the paper we assume we can distinguish the words in a derivation, irrespective of whether two words at different positions correspond to the same symbol. 410 2.2 Operator Models An operator model A with n states is a tuple (α1, α�,,,,, {Aa}aEX), where Aa E Rnxn is an operator matrix and α1, α,,,, E Rn are vectors. A computes a function f : X* —* R as follows: f(x1:T) = α� AxT ... Ax1 α1 . (3) One intuitive way of understanding operator models is to consider the case where f computes a probability distribution over strings. Such </context>
<context position="20619" citStr="Eisner, 2000" startWordPosition="3644" endWordPosition="3645">cy between head word i and modifier word j. The posterior or marginal probability of a dependency (si, sj) given a sentence s0:N is defined as µi,j = P((si, sj) |s0:N) = X P(y) . yEY(s0:N) : (si,sj)Ey To compute marginals, the sum over derivations can be decomposed into a product of inside and outside quantities (Baker, 1979). Below we describe an inside-outside algorithm for our grammars. Given a sentence s0:N and marginal scores µi,j, we compute the parse tree for s0:N as X y� = argmax log µi,j (14) yEY(s0:N) (si,sj)Ey using the standard projective parsing algorithm for arc-factored models (Eisner, 2000). Overall we use a two-pass parsing process, first to compute marginals and then to compute the best tree. 4.1 An Inside-Outside Algorithm In this section we sketch an algorithm to compute marginal probabilities of dependencies. Our algorithm is an adaptation of the parsing algorithm for SHAG by Eisner and Satta (1999) to the case of non-deterministic head-automata, and has a runtime cost of O(n2N3), where n is the number of states of the model, and N is the length of the input sentence. Hence the algorithm maintains the standard cubic cost on the sentence length, while the quadratic cost on n</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Jason Eisner. 2000. Bilexical grammars and their cubic-time parsing algorithms. In Harry Bunt and Anton Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies, pages 29–62. Kluwer Academic Publishers, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--183</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Santa Cruz, California, USA,</location>
<contexts>
<context position="19611" citStr="Goodman, 1996" startWordPosition="3469" endWordPosition="3470">Y(30:N) P(y). This problem, known as MAP inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. = α = ¯α &gt; ∞ ¯Ax1 ¯α1 AxT ··· statistics matrices bp1, bp∞, Pb, and { 7: bAh,d a 413 Let (si, sj) denote a dependency between head word i and modifier word j. The posterior or marginal probability of a dependency (si, sj) given a sentence s0:N is defined as µi,j = P((si, sj) |s0:N) = X P(y) . yEY(s0:N) : (si,sj)Ey To c</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 177–183, Santa Cruz, California, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
<author>Tong Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden markov models.</title>
<date>2009</date>
<booktitle>In COLT 2009 - The 22nd Conference on Learning Theory.</booktitle>
<contexts>
<context position="4190" citStr="Hsu et al., 2009" startWordPosition="622" endWordPosition="625">, 1999). In this for409 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics malism, head-modifier sequences are generated by a collection of finite-state automata. In our case, the underlying machines are probabilistic non-deterministic finite state automata (PNFA), which we parameterize using the operator model representation. This representation allows the use of simple spectral algorithms for estimating the model parameters from data (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012). In all previous work, the algorithms used to induce hidden structure require running repeated inference on training data—e.g. Expectation-Maximization (Dempster et al., 1977), or split-merge algorithms. In contrast, spectral methods are simple and very efficient —parameter estimation is reduced to computing some data statistics, performing SVD, and inverting matrices. The main contributions of this paper are: • We present a spectral learning algorithm for inducing PNFA with applications to headautomata dependency grammars. Our formulation is based on thinki</context>
<context position="8969" citStr="Hsu et al., 2009" startWordPosition="1469" endWordPosition="1472">tribution can be described in two equivalent ways: by making some independence assumptions and providing the corresponding parameters, or by explaining the process used to compute f. This is akin to describing the distribution defined by an HMM in terms of a factorization and its corresponding transition and emission parameters, or using the inductive equations of the forward algorithm. The operator model representation takes the latter approach. Operator models have had numerous applications. For example, they can be used as an alternative parameterization of the function computed by an HMM (Hsu et al., 2009). Consider an HMM with n hidden states and initial-state probabilities 7r E Rn, transition probabilities T E Rnxn, and observation probabilities Oa E Rnxn for each a E X, with the following meaning: • 7r(i) is the probability of starting at state i, • T(i, j) is the probability of transitioning from state j to state i, • Oa is a diagonal matrix, such that Oa(i, i) is the probability of generating symbol a from state i. Given an HMM, an equivalent operator model can be defined by setting α1 = 7r, Aa = TOa and α,,,, = 1. To see this, let us show that the forward algorithm computes the expression</context>
<context position="10716" citStr="Hsu et al., 2009" startWordPosition="1786" endWordPosition="1789"> are at state j. HMM are only one example of distributions that can be parameterized by operator models. In general, operator models can parameterize any PNFA, where the parameters of the model correspond to probabilities of emitting a symbol from a state and moving to the next state. The advantage of working with operator models is that, under certain mild assumptions on the operator parameters, there exist algorithms that can estimate the operators from observable statistics of the input sequences. These algorithms are extremely efficient and are not susceptible to local minima issues. See (Hsu et al., 2009) for theoretical proofs of the learnability of HMM under the operator model representation. In the following, we write x = xi:j E X* to denote sequences of symbols, and use Axi,j as a shorthand for Axj • • • Axi. Also, for convenience we assume X = {1, ... , l}, so that we can index vectors and matrices by symbols in X. 3 Learning Operator SHAG We will define a SHAG using a collection of operator models to compute probabilities. Assume that for each possible head h in the vocabulary X and each direction d E {LEFT, RIGHT} we have an operator model that computes probabilities of modifier sequenc</context>
<context position="12780" citStr="Hsu et al. (2009)" startWordPosition="2169" endWordPosition="2172"> the original hidden space. Such projected space is equivalent to the original one in the sense that we 411 can find operators in the projected space that parameterize the same probability distribution over sequences. In the rest of this section we describe an algorithm for learning an operator model. We will assume a fixed head word and direction, and drop h and d from all terms. Hence, our goal is to learn the following distribution, parameterized by operators α1, {Aa}aEX, and α00: P(x1:T) = α 00 AxT ··· Ax α1 . (4) Our algorithm shares many features with the previous spectral algorithms of Hsu et al. (2009) and Bailly (2011), though the derivation given here is based upon the general formulation of Balle et al. (2012). The main difference is that our algorithm is able to learn operator models from substring statistics, while algorithms in previous works were restricted to statistics on prefixes. In principle, our algorithm should extract much more information from a sample. 3.1 Preliminary Definitions The spectral learning algorithm will use statistics estimated from samples of the target distribution. More specifically, consider the function that computes the expected number of occurrences of a</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A spectral algorithm for learning hidden markov models. In COLT 2009 - The 22nd Conference on Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Infante-Lopez</author>
<author>Maarten de Rijke</author>
</authors>
<title>Alternative approaches for generating bodies of grammar rules.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>454--461</pages>
<location>Barcelona, Spain,</location>
<marker>Infante-Lopez, de Rijke, 2004</marker>
<rawString>Gabriel Infante-Lopez and Maarten de Rijke. 2004. Alternative approaches for generating bodies of grammar rules. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 454–461, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1334" citStr="Koo and Collins, 2010" startWordPosition="180" endWordPosition="183">ns computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, first edition,</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, Cambridge, first edition, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="26446" citStr="Marcus et al., 1994" startWordPosition="4654" endWordPosition="4658">nts we found that having explicit START and STOP symbols results in more accurate models. 6Note that, for parsing, the operators for the START and STOP symbols can be packed into α1 and α∞ respectively. One just defines α01 = ASTART α1 and α0&gt;∞ = α&gt;∞ ASTOP. Figure 1: Accuracy curve on English development set for fully unlexicalized models. created a diagonal matrix O�,d � E Rnxn, where O�,d � (i, i) is the probability of generating symbol a from hb and d (estimated from training); (4) we set Aa,d = T O�,d � . We trained SHAG models using the standard WSJ sections of the English Penn Treebank (Marcus et al., 1994). Figure 1 shows the Unlabeled Attachment Score (UAS) curve on the development set, in terms of the number of hidden states for the spectral and EM models. We can see that DET+F largely outperforms DET7, while the hidden-state models obtain much larger improvements. For the EM model, we show the accuracy curve after 5, 10, 25 and 100 iterations.8 In terms of peak accuracies, EM gives a slightly better result than the spectral method (80.51% for EM with 15 states versus 79.75% for the spectral method with 9 states). However, the spectral algorithm is much faster to train. With our Matlab implem</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1994. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>342--350</pages>
<institution>Suntec, Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="1310" citStr="Martins et al., 2009" startWordPosition="176" endWordPosition="179">cture from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andre Martins, Noah Smith, and Eric Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 342– 350, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>75--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1423" citStr="Matsuzaki et al., 2005" startWordPosition="195" endWordPosition="198"> algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered. This can be achieved by means of feature engineering, but compressing such i</context>
<context position="2778" citStr="Matsuzaki et al., 2005" startWordPosition="407" endWordPosition="410">he size of the factors generally results in polynomial increases in the parsing cost. In principle, hidden variable models could solve some of the problems of feature engineering in higher-order factorizations, since they could automatically induce the information in a derivation history that should be passed across factors. Potentially, they would require less feature engineering since they can learn from an annotated corpus an optimal way to compress derivations into hidden states. For example, one line of work has added hidden annotations to the non-terminals of a phrase-structure grammar (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008), resulting in compact grammars that obtain parsing accuracies comparable to lexicalized grammars. A second line of work has modeled hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de Rijke, 2004). Finally, a third line of work has induced hidden structure from the history of actions of a parser (Titov and Henderson, 2007). However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem. As a result, train</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 75–82, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1272" citStr="McDonald and Pereira, 2006" startWordPosition="169" endWordPosition="172">ated as a technique for inducing hidden structure from distributions computed by forwardbackward recursions. Furthermore, we also present an inside-outside algorithm for the parsing model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tel</context>
<context position="7799" citStr="McDonald and Pereira, 2006" startWordPosition="1268" endWordPosition="1271">this paper we use probabilistic versions of SHAG where probabilities of head-modifier sequences in a derivation are independent of each other: P(y) = � P(x1:T |h, d) (1) (h,d,x1:T)Ey In the literature, standard arc-factored models further assume that T+ 1 P(x1:T |h, d) = H P(xt|h, d, at) , (2) t=1 where xT+1 is always a special STOP word, and at is the state of a deterministic automaton generating x1:T+1. For example, setting a1 = FIRST and at&gt;1 = REST corresponds to first-order models, while setting a1 = NULL and at&gt;1 = xt−1 corresponds to sibling models (Eisner, 2000; McDonald et al., 2005; McDonald and Pereira, 2006). 1Throughout the paper we assume we can distinguish the words in a derivation, irrespective of whether two words at different positions correspond to the same symbol. 410 2.2 Operator Models An operator model A with n states is a tuple (α1, α�,,,,, {Aa}aEX), where Aa E Rnxn is an operator matrix and α1, α,,,, E Rn are vectors. A computes a function f : X* —* R as follows: f(x1:T) = α� AxT ... Ax1 α1 . (3) One intuitive way of understanding operator models is to consider the case where f computes a probability distribution over strings. Such a distribution can be described in two equivalent wa</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="7770" citStr="McDonald et al., 2005" startWordPosition="1263" endWordPosition="1267">, *) for any y E Y. In this paper we use probabilistic versions of SHAG where probabilities of head-modifier sequences in a derivation are independent of each other: P(y) = � P(x1:T |h, d) (1) (h,d,x1:T)Ey In the literature, standard arc-factored models further assume that T+ 1 P(x1:T |h, d) = H P(xt|h, d, at) , (2) t=1 where xT+1 is always a special STOP word, and at is the state of a deterministic automaton generating x1:T+1. For example, setting a1 = FIRST and at&gt;1 = REST corresponds to first-order models, while setting a1 = NULL and at&gt;1 = xt−1 corresponds to sibling models (Eisner, 2000; McDonald et al., 2005; McDonald and Pereira, 2006). 1Throughout the paper we assume we can distinguish the words in a derivation, irrespective of whether two words at different positions correspond to the same symbol. 410 2.2 Operator Models An operator model A with n states is a tuple (α1, α�,,,,, {Aa}aEX), where Aa E Rnxn is an operator matrix and α1, α,,,, E Rn are vectors. A computes a function f : X* —* R as follows: f(x1:T) = α� AxT ... Ax1 α1 . (3) One intuitive way of understanding operator models is to consider the case where f computes a probability distribution over strings. Such a distribution can be d</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 523–530, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Antonio Musillo</author>
<author>Paola Merlo</author>
</authors>
<title>Unlexicalised hidden variable models of split dependency grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>213--216</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1470" citStr="Musillo and Merlo, 2008" startWordPosition="203" endWordPosition="206"> cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered. This can be achieved by means of feature engineering, but compressing such information into a state of bounded size will ty</context>
<context position="2825" citStr="Musillo and Merlo, 2008" startWordPosition="415" endWordPosition="418">olynomial increases in the parsing cost. In principle, hidden variable models could solve some of the problems of feature engineering in higher-order factorizations, since they could automatically induce the information in a derivation history that should be passed across factors. Potentially, they would require less feature engineering since they can learn from an annotated corpus an optimal way to compress derivations into hidden states. For example, one line of work has added hidden annotations to the non-terminals of a phrase-structure grammar (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008), resulting in compact grammars that obtain parsing accuracies comparable to lexicalized grammars. A second line of work has modeled hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de Rijke, 2004). Finally, a third line of work has induced hidden structure from the history of actions of a parser (Titov and Henderson, 2007). However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem. As a result, training a hidden-variable model is both expensive a</context>
</contexts>
<marker>Musillo, Merlo, 2008</marker>
<rawString>Gabriele Antonio Musillo and Paola Merlo. 2008. Unlexicalised hidden variable models of split dependency grammars. In Proceedings of ACL-08: HLT, Short Papers, pages 213–216, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James D Park</author>
<author>Adnan Darwiche</author>
</authors>
<title>Complexity results and approximation strategies for map explanations.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>21--101</pages>
<contexts>
<context position="19405" citStr="Park and Darwiche, 2004" startWordPosition="3437" endWordPosition="3440">wever, note that when dealing with sparse matrices many of these operations can be performed more efficiently. 4 Parsing Algorithms Given a sentence s0.N we would like to find its most likely derivation, y� = argmaxyEY(30:N) P(y). This problem, known as MAP inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. = α = ¯α &gt; ∞ ¯Ax1 ¯α1 AxT ··· statistics matrices bp1, bp∞, Pb, and { 7: bAh,d a 413 Let (si, sj) denote a depende</context>
</contexts>
<marker>Park, Darwiche, 2004</marker>
<rawString>James D. Park and Adnan Darwiche. 2004. Complexity results and approximation strategies for map explanations. Journal of Artificial Intelligence Research, 21:101–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Paskin</author>
</authors>
<title>Cubic-time parsing and learning algorithms for grammatical bigram models.</title>
<date>2001</date>
<tech>Technical Report UCB/CSD-01-1148,</tech>
<institution>University of California, Berkeley.</institution>
<contexts>
<context position="23524" citStr="Paskin, 2001" startWordPosition="4138" endWordPosition="4139">βsi,R(xt+1:T) E Rn is a backward statedistribution vector —the qth coordinate is the probability of being at state q of the right automaton of si and generating xt+1:T. Analogous inside-outside expressions can be defined for the rest of structures (left/right triangles and trapezoids). With these quantities, we can compute marginals as ( (φI,R i,j)T θI,R i,j Z−1 if i &lt; j , j µi = (φI,L i,j)TθI,L i,j Z−1 if j &lt; i , (17) where Z =PyEY(s0:N)P(y) = (α?,R� )T θC,R 0,N. Finally, we sketch the equations for computing inside scores in O(N3) time. The outside equations can be derived analogously (see (Paskin, 2001)). For 0 &lt; i &lt; j &lt; N: θC,R i,i = αsi,R (18) 1 � � θI,R αsk,R T θC,R (19) i,k ( oo ) k,j si,R CR sj,L T C,L Asj Bi,k ((α0° ) θk+1,j (20) X P(y�)αsi,R(x1:t) . (15) θI,R i,j = yEY(si:j) : (si,sj)Ey , y={(si,R,x1:t)} U yi , xt=sj X P(y�)βsi,R(xt+1:T) , (16) I,R = φi,j yEY(s0:isj:n) : root(y)=s0, y={�si,R,xt:T U y , xt=sj C,R = X j θi,j k=i+1 X j k=i I,R θi,j = 414 5 Experiments The goal of our experiments is to show that incorporating hidden states in a SHAG using operator models can consistently improve parsing accuracy. A second goal is to compare the spectral learning algorithm to EM, a standar</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>Mark Paskin. 2001. Cubic-time parsing and learning algorithms for grammatical bigram models. Technical Report UCB/CSD-01-1148, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="19687" citStr="Petrov and Klein, 2007" startWordPosition="3480" endWordPosition="3483"> intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. = α = ¯α &gt; ∞ ¯Ax1 ¯α1 AxT ··· statistics matrices bp1, bp∞, Pb, and { 7: bAh,d a 413 Let (si, sj) denote a dependency between head word i and modifier word j. The posterior or marginal probability of a dependency (si, sj) given a sentence s0:N is defined as µi,j = P((si, sj) |s0:N) = X P(y) . yEY(s0:N) : (si,sj)Ey To compute marginals, the sum over derivations can be decomposed into a product </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1444" citStr="Petrov et al., 2006" startWordPosition="199" endWordPosition="202">ng model that runs in cubic time, hence maintaining the standard parsing costs for context-free grammars. 1 Introduction Dependency structures of natural language sentences exhibit a significant amount of non-local phenomena. Historically, there have been two main approaches to model non-locality: (1) increasing the order of the factors of a dependency model (e.g. with sibling and grandparent relations (Eisner, 2000; McDonald and Pereira, 2006; Carreras, 2007; Martins et al., 2009; Koo and Collins, 2010)), and (2) using hidden states to pass information across factors (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008). Higher-order models have the advantage that they are relatively easy to train, because estimating the parameters of the model can be expressed as a convex optimization. However, they have two main drawbacks. (1) The number of parameters grows significantly with the size of the factors, leading to potential data-sparsity problems. A solution to address the data-sparsity problem is to explicitly tell the model what properties of higher-order factors need to be remembered. This can be achieved by means of feature engineering, but compressing such information into a sta</context>
<context position="2799" citStr="Petrov et al., 2006" startWordPosition="411" endWordPosition="414">enerally results in polynomial increases in the parsing cost. In principle, hidden variable models could solve some of the problems of feature engineering in higher-order factorizations, since they could automatically induce the information in a derivation history that should be passed across factors. Potentially, they would require less feature engineering since they can learn from an annotated corpus an optimal way to compress derivations into hidden states. For example, one line of work has added hidden annotations to the non-terminals of a phrase-structure grammar (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008), resulting in compact grammars that obtain parsing accuracies comparable to lexicalized grammars. A second line of work has modeled hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de Rijke, 2004). Finally, a third line of work has induced hidden structure from the history of actions of a parser (Titov and Henderson, 2007). However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem. As a result, training a hidden-variable</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433– 440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Loss minimization in parse reranking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>560--567</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="19662" citStr="Titov and Henderson, 2006" startWordPosition="3476" endWordPosition="3479">P inference, is known to be intractable for hidden-state structure prediction models, as it involves finding the most likely tree structure while summing out over hidden states. We use a common approximation to MAP based on first computing posterior marginals of tree edges (i.e. dependencies) and then maximizing over the tree structure (see (Park and Darwiche, 2004) for complexity of general MAP inference and approximations). For parsing, this strategy is sometimes known as MBR decoding; previous work has shown that empirically it gives good performance (Goodman, 1996; Clark and Curran, 2004; Titov and Henderson, 2006; Petrov and Klein, 2007). In our case, we use the non-deterministic SHAG to compute posterior marginals of dependencies. We first explain the general strategy of MBR decoding, and then present an algorithm to compute marginals. = α = ¯α &gt; ∞ ¯Ax1 ¯α1 AxT ··· statistics matrices bp1, bp∞, Pb, and { 7: bAh,d a 413 Let (si, sj) denote a dependency between head word i and modifier word j. The posterior or marginal probability of a dependency (si, sj) given a sentence s0:N is defined as µi,j = P((si, sj) |s0:N) = X P(y) . yEY(s0:N) : (si,sj)Ey To compute marginals, the sum over derivations can be d</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>Ivan Titov and James Henderson. 2006. Loss minimization in parse reranking. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 560–567, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Tenth International Conference on Parsing Technologies,</booktitle>
<pages>144--155</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3182" citStr="Titov and Henderson, 2007" startWordPosition="472" endWordPosition="475">earn from an annotated corpus an optimal way to compress derivations into hidden states. For example, one line of work has added hidden annotations to the non-terminals of a phrase-structure grammar (Matsuzaki et al., 2005; Petrov et al., 2006; Musillo and Merlo, 2008), resulting in compact grammars that obtain parsing accuracies comparable to lexicalized grammars. A second line of work has modeled hidden sequential structure, like in our case, but using PDFA (Infante-Lopez and de Rijke, 2004). Finally, a third line of work has induced hidden structure from the history of actions of a parser (Titov and Henderson, 2007). However, the main drawback of the hidden variable approach to parsing is that, to the best of our knowledge, there has not been any convex formulation of the learning problem. As a result, training a hidden-variable model is both expensive and prone to local minima issues. In this paper we present a learning algorithm for hidden-state split head-automata grammars (SHAG) (Eisner and Satta, 1999). In this for409 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409–419, Avignon, France, April 23 - 27 2012. c�2012 Association for </context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan Titov and James Henderson. 2007. A latent variable model for generative dependency parsing. In Proceedings of the Tenth International Conference on Parsing Technologies, pages 144–155, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>