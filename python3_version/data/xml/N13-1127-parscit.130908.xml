<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000266">
<title confidence="0.988231">
Emergence of Gricean Maxims from Multi-Agent Decision Theory
</title>
<author confidence="0.999276">
Adam Vogel, Max Bodoia, Christopher Potts, and Dan Jurafsky
</author>
<affiliation confidence="0.994759">
Stanford University
</affiliation>
<address confidence="0.801939">
Stanford, CA, USA
</address>
<email confidence="0.999036">
{acvogel,mbodoia,cgpotts,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999198272727273">
Grice characterized communication in terms
of the cooperative principle, which enjoins
speakers to make only contributions that serve
the evolving conversational goals. We show
that the cooperative principle and the associ-
ated maxims of relevance, quality, and quan-
tity emerge from multi-agent decision theory.
We utilize the Decentralized Partially Observ-
able Markov Decision Process (Dec-POMDP)
model of multi-agent decision making which
relies only on basic definitions of rationality
and the ability of agents to reason about each
other’s beliefs in maximizing joint utility. Our
model uses cognitively-inspired heuristics to
simplify the otherwise intractable task of rea-
soning jointly about actions, the environment,
and the nested beliefs of other actors. Our
experiments on a cooperative language task
show that reasoning about others’ belief states,
and the resulting emergent Gricean commu-
nicative behavior, leads to significantly im-
proved task performance.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961704545455">
Grice (1975) famously characterized communica-
tion among rational agents in terms of an overarch-
ing cooperative principle and a set of more specific
maxims, which enjoin speakers to make contribu-
tions that are truthful, informative, relevant, clear,
and concise. Since then, there have been many at-
tempts to derive the maxims (or perhaps just their ef-
fects) from more basic cognitive principles concern-
ing how people make decisions, formulate plans,
and collaborate to achieve goals. This research
traces to early work by Lewis (1969) on signaling
systems. It has recently been the subject of ex-
tensive theoretical discussion (Clark, 1996; Merin,
1997; Blutner, 1998; Parikh, 2001; Beaver, 2002;
van Rooy, 2003; Benz et al., 2005; Franke, 2009)
and has been tested experimentally using one-step
games in which the speaker produces a message and
the hearer ventures a guess as to its intended refer-
ent (Rosenberg and Cohen, 1964; Dale and Reiter,
1995; Golland et al., 2010; Stiller et al., 2011; Frank
and Goodman, 2012; Krahmer and van Deemter,
2012; Degen and Franke, 2012; Rohde et al., 2012).
To date, however, these theoretical models and ex-
periments have not been extended to multi-step in-
teractions extending over time and involving both
language and action together, which leaves this work
relatively disconnected from research on planning
and goal-orientation in artificial agents (Perrault
and Allen, 1980; Allen, 1991; Grosz and Sidner,
1986; Bratman, 1987; Hobbs et al., 1993; Allen
et al., 2007; DeVault et al., 2005; Stone et al.,
2007; DeVault, 2008). We attribute this in large
part to the complexity of Gricean reasoning itself,
which requires agents to model each other’s belief
states. Tracking these as they evolve over time in re-
sponse to experiences is extremely demanding. Our
approach complements slot-filling dialog systems,
where the focus is on managing speech recogni-
tion uncertainty (Young et al., 2010; Thomson and
Young, 2010).
However, recent years have seen significant ad-
vances in multi-agent decision-theoretic models and
their efficient implementation. With the current pa-
per, we seek to show that the Decentralized Par-
</bodyText>
<page confidence="0.970715">
1072
</page>
<note confidence="0.473282">
Proceedings of NAACL-HLT 2013, pages 1072–1081,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.998650147058824">
tially Observable Markov Decision Process (Dec-
POMDP) provides a robust, flexible foundation for
implementing agents that communicate in a Gricean
manner. Dec-POMDPs are multi-agent, partially-
observable models in which agents maintain be-
lief distributions over the underlying, hidden world
state, including the beliefs of the other players, and
speech actions change those beliefs. In this setting,
informative, relevant communication emerges as the
best way to maximize joint utility.
The complexity of pragmatic reasoning is still
forbidding, though. Correspondingly, optimal de-
cision making in Dec-POMDPs is NEXP complete
(Bernstein et al., 2002). To manage this issue, we
introduce several cognitively-plausible approxima-
tions which allow us to simplify the Dec-POMDP to
a single-agent POMDP, for which relatively efficient
solvers exist (Spaan and Vlassis, 2005). We demon-
strate our algorithms on a variation of the Cards task,
a partially-observable collaborative search problem
(Potts, 2012). Spatial language comprises the bulk
of communication in the Cards task, and we dis-
cuss a model of spatial semantics in Section 3. Us-
ing this task and a model of the meaning of spatial
language, we next discuss two agents that play the
game: ListenerBot (Section 4) makes decisions us-
ing a single-agent POMDP that does not take into
account the beliefs or actions of its partner, whereas
DialogBot (Section 5) maintains a model of its part-
ner’s beliefs. As a result of the cooperative structure
of the underlying model and the effects of commu-
nication within it, DialogBot’s contributions are rel-
evant, truthful, and informative, which leads to sig-
nificantly improved task performance.
</bodyText>
<sectionHeader confidence="0.981771" genericHeader="introduction">
2 The Cards Task and Corpus
</sectionHeader>
<bodyText confidence="0.999578111111111">
The Cards corpus consists of 1,266 transcripts1 from
an online, two-person collaborative game in which
two players explore a maze-like environment, com-
municating with each other via a text chat window
(Figure 1). A deck of playing cards has been dis-
tributed randomly around the environment, and the
players’ task is to find six consecutive cards of the
same suit. Our implemented agents solve a sim-
plified version of this task in which the two agents
</bodyText>
<footnote confidence="0.9805625">
1Released by Potts (2012) at http://cardscorpus.
christopherpotts.net
</footnote>
<figureCaption confidence="0.97414">
Figure 1: The Cards corpus gameboard. Player 1’s
location is marked “P1”. The nearby yellow boxes
mark card locations. The dialogue history and chat
window are at the top. This board, the one we use
throughout, consists of 231 open grid squares.
</figureCaption>
<bodyText confidence="0.9995609">
must both end up co-located with a single card, the
Ace of Spades (AS). This is much simpler than the
six-card version from the human–human corpus, but
it involves the same kind of collaborative goal and
forces our agents to deal with the same kind of par-
tial knowledge about the world as the humans did.
Each agent knows its own location, but not his part-
ner’s, and a player can see the AS only when co-
located with it. The agents use (simplified) English
to communicate with each other.
</bodyText>
<sectionHeader confidence="0.9912" genericHeader="method">
3 Spatial Semantics
</sectionHeader>
<bodyText confidence="0.9999114">
Much of the communication in the Cards task in-
volves referring to spatial locations on the board.
Accordingly, we focus on spatial language for our
artificial agents. In this section, we present a model
of spatial semantics, which we create by leveraging
the human–human Cards transcripts. We discuss the
spatial semantic representation, how we classify the
semantics of new locative expressions, and our use
of spatial semantics to form a high-level state space
for decision making.
</bodyText>
<subsectionHeader confidence="0.999673">
3.1 Semantic Representation
</subsectionHeader>
<bodyText confidence="0.998518714285714">
Potts (2012) released annotations, derived from the
Cards corpus, which reduce 599 of the players’
statements about their locations to formulae of the
form δ(ϕ1 n ··· n ϕk), where δ is a domain and
ϕ1�����ϕk are semantic literals. For example, the ut-
terance “(I’m) at the top right of the board” is anno-
tated as BOARD(top n right), and “(I’m) in bottom
</bodyText>
<page confidence="0.989492">
1073
</page>
<bodyText confidence="0.99896475">
of the C room” is annotated as C room(bottom). Ta-
ble 1 lists the full set of semantic primitives that ap-
pear as domain expressions and literals.
Because the Cards transcripts are so highly struc-
tured, we can interpret these expressions in terms
of the Cards world itself. For a given formula
σ = δ(ϕ1 n ··· n ϕk), we compute the number of
times that a player identified its location with (an
utterance translated as) σ while standing on grid
square (x,y). These counts are smoothed using
a simple 2D-smoothing scheme, detailed in (Potts,
2012), and normalized in the usual manner to form a
distribution overboard squares Pr((x,y)|σ). These
grounded interpretations are the basis for commu-
nication between the artificial agents we define in
Section 4.
BOARD, SQUARE, right, middle, top, left, bot-
tom, corner, approx, precise, entrance, C room,
hall, room, sideways C, loop, reverse C,
U room, T room, deadend, wall, sideways F
</bodyText>
<tableCaption confidence="0.996884">
Table 1: The spatial semantic primitives.
</tableCaption>
<subsectionHeader confidence="0.999947">
3.2 Semantics Classifier
</subsectionHeader>
<bodyText confidence="0.999983090909091">
Using the corpus examples of utterances paired with
their spatial semantic representations, we learn a set
of classifiers to predict a spatial utterance’s semantic
representation. We train a binary classifier for each
semantic primitive ϕi using a log-linear model with
simple bag of words features. The words are not
normalized or stemmed and we use whitespace tok-
enization. We additionally train a multi-class clas-
sifier for all possible domains δ. At test time, we
use the domain classifier and each primitive binary
classifier to produce a semantic representation.
</bodyText>
<subsectionHeader confidence="0.999903">
3.3 Semantic State Space
</subsectionHeader>
<bodyText confidence="0.999202888888889">
The decision making algorithms that we discuss in
Section 4 are highly sensitive to the size of the state
space. The full representation of the game board
consists of 231 squares. Representing the location
of both players and the location of the card requires
3233 = 12,326,391 states, well beyond the capabil-
ities of current decision-making algorithms.
To ameliorate this difficulty, we cluster squares
together using the spatial referring expression cor-
</bodyText>
<figureCaption confidence="0.968027">
Figure 2: Semantic state space clusters with k = 16.
</figureCaption>
<bodyText confidence="0.999454434782609">
pus. This approach follows from research that
shows that humans’ mental spatial representations
are influenced by their language (Hayward and Tarr,
1995). Our intuition is that human players do not
consider all possible locations of the card and play-
ers, but instead lump them into semantically coher-
ent states, such as “the card is in the top right cor-
ner.” Following this intuition, we cluster states to-
gether which have similar referring expressions, al-
lowing our agents to use language as a cognitive
technology and not just a tool for communication.
For each board square (x,y) we form a vector
φ(x,y) with φi(x,y) = Pr((x,y)|σi), where σi is the
ith distinct semantic representation in the corpus.
This forms a 136-dimensional vector for each board
square. We then use k-means clustering with a Eu-
clidean distance metric in this semantic space to
cluster states which are referred to similarly.
Figure 2 shows a clustering for k = 16 which we
utilize for the remainder of the paper. Denoting the
board regions by {1,...,Nregions}, we compute the
probability of an expression σ referring to a region
r by averaging over the squares in the region:
</bodyText>
<equation confidence="0.9943975">
Pr((x,y)|σi)
|{(x,y)|(x,y) E region r}|
</equation>
<sectionHeader confidence="0.99316" genericHeader="method">
4 ListenerBot
</sectionHeader>
<bodyText confidence="0.99955025">
We first introduce ListenerBot, an agent that does
not take into account the actions or beliefs of its
partner. ListenerBot decides what actions to take
using a Partially Observable Markov Decision Pro-
cess (POMDP). This allows ListenerBot to track its
beliefs about the location of the card and to incor-
porate linguistic advice. However, ListenerBot does
not produce utterances.
</bodyText>
<equation confidence="0.9646325">
Pr(r|σi) « Y
(x,y)E region r
</equation>
<page confidence="0.914395">
1074
</page>
<bodyText confidence="0.999853454545455">
A POMDP is defined by a tuple
(S,A,T,O,a,R,b0,γ). We explicate each com-
ponent with examples from our task. Figure 3(a)
provides the POMDP influence diagram.
States S is the finite state space of the world. The
state space S of ListenerBot consists of the location
of the player p and the location of the card c. As
discussed above in Section 3.3, we cluster squares of
the board into Nregions semantically coherent regions,
denoted by {1,...,Nregions}. The state space over
these regions is defined as
</bodyText>
<equation confidence="0.99073">
S := {(p,c)|p,c ∈ {1,...,Nregions}}
</equation>
<bodyText confidence="0.992059857142857">
Two regions r1 and r2 are called adjacent, written
adj(r1,r2), if any of their constituent squares touch.
Actions A is the set of actions available to the
agent. ListenerBot can only take physical actions
and has no communicative ability. Physical actions
in our region-based state space are composed of two
types: traveling to a region and searching a region.
</bodyText>
<listItem confidence="0.992293666666667">
• travel(r): travel to region r
• search: player exhaustively searches the cur-
rent region
</listItem>
<bodyText confidence="0.992779571428571">
Transition Distributions The transition distribu-
tion T(s0|a,s) models the dynamics of the world.
This represents the ramifications of physical actions
such as moving around the map. For a state s =
(p,c) and action a = travel(r), the player moves to
region r if it is adjacent to p, and otherwise stays in
the same place:
</bodyText>
<equation confidence="0.993394">
{ 1 adj(r, p)∧ p0 = r
∧c = c0
1 ¬adj(r, p) ∧ p = p0
∧c = c0
0 otherwise
</equation>
<bodyText confidence="0.965027944444445">
Search actions are only concerned with observations
and do not change the state of the world:2
T((p0,c0)|search,(p,c)) = ✶[p0 = p ∧ c0 = c]
The travel and search high-level actions are trans-
lated into low-level (up, down, left, right) actions
using a simple A∗ path planner.
Observations Agents receive observations from
a set O according to an observation distribution
2✶[Q] is the indicator function, which is 1 if proposition Q
is true and 0 otherwise.
a(o|s0,a). Observations include properties of the
physical world, such as the location of the card, and
also natural language utterances, which serve to in-
directly change agents’ beliefs about the world and
the beliefs of their interlocutors.
Search actions generate two possible observa-
tions: ohere and o¬here, which denote the presence
or absence of the card from the current region.
</bodyText>
<equation confidence="0.9998505">
a(ohere|(p0,c0),search) = ✶[p0 = c0]
a(o¬here|(p0,c0),search) = ✶[p0 =6 c0]
</equation>
<bodyText confidence="0.5462145">
Travel actions do not generate meaningful observa-
tions:
</bodyText>
<equation confidence="0.969822">
a(o¬here|(p0,c0),travel) = 1
</equation>
<bodyText confidence="0.9998005">
Linguistic Advice We model linguistic advice as
another form of observation. Agents receive mes-
sages from a finite set E, and each message σ ∈ E
has a semantics, or distribution over the state space
Pr(s|σ). In the Cards task, we use the semantic dis-
tributions defined in Section 3. To combine the se-
mantics of language with the standard POMDP ob-
servation model, we apply Bayes’ rule:
</bodyText>
<equation confidence="0.998903333333333">
Pr(s|σ)Pr(σ)
Pr(σ|s) = (1)
�σ0 Pr(s|σ0)Pr(σ0)
</equation>
<bodyText confidence="0.999903764705882">
The prior, Pr(σ), can be derived from corpus data.
By treating language as just another form of ob-
servation, we are able to leverage existing POMDP
solution algorithms. This approach contrasts with
previous work on communication in Dec-POMDPs,
where agents directly share their perceptual obser-
vations (Pynadath and Tambe, 2002; Spaan et al.,
2008), an assumption which does not fit natural lan-
guage.
Reward The reward function R(s,a) : S → R rep-
resents the goals of the agent, who chooses actions
to maximize reward. The goal of the Cards task is
for both players to be on top of the card, so any ac-
tion that leads to this state receives a high reward R+.
All other actions receive a small negative reward R−,
which gives agents an incentive to finish the task as
quickly as possible.
</bodyText>
<equation confidence="0.995404">
R((p,c),a) = R+ p = c
R− p =6c
</equation>
<bodyText confidence="0.9704885">
Lastly, γ ∈ [0,1) is the discount factor, specifying
the trade-off between immediate and future rewards.
</bodyText>
<equation confidence="0.87437">
T((p0,c0)|travel(r),(p,c)) =
</equation>
<page confidence="0.87489">
1075
</page>
<figure confidence="0.999556090909091">
o
s s0
R
a
o0
o2
o1
s s0
a2
a1
R
o0
o 1
2
0
o
s¯ ¯s0
s s0
R
a
o0
(a) ListenerBot POMDP (b) Full Dec-POMDP (c) DialogBot POMDP
</figure>
<figureCaption confidence="0.975964">
Figure 3: The decision diagram for the ListenerBot POMDP, the full Dec-POMDP, and the DialogBot ap-
proximation POMDP. The ListenerBot (a) only considers his own location p and the card location c. In the
</figureCaption>
<bodyText confidence="0.948339444444445">
full Dec-POMDP (b), both agents receive individual observations and choose actions independently. Opti-
mal decision making requires tracking all possible histories of beliefs of the other agent. In diagram (c), Di-
alogBot approximates the full Dec-POMDP as single-agent POMDP. At each time step, DialogBot marginal-
izes out the possible observations o¯ that ListenerBot received, yielding an expected belief state ¯b.
Initial Belief State The initial belief state, b0 ∈
Δ(S), is a distribution over the state space S. Lis-
tenerBot begins each game with a known initial lo-
cation p0 but a uniform distribution over the location
of the card c:
</bodyText>
<equation confidence="0.97668225">
1
Nregions p = p0
b0(p,c) ∝
0 otherwise
</equation>
<bodyText confidence="0.999746166666667">
Belief Update and Decision Making The key de-
cision making problem in POMDPs is the construc-
tion of a policy π : Δ(S) → A, a function from beliefs
to actions which dictates how the agent acts. Deci-
sion making in POMDPs proceeds as follows. The
world starts in a hidden state s0 ∼ b0. The agent
executes action a0 = π(b0). The underlying hid-
den world state transitions to s1 ∼ T(s0|a0,s0), the
world generates observation o0 ∼ Ω(o|s1,a0), and
the agent receives reward R(s0,a0). Using the obser-
vation o0, the agent constructs anew belief b1 ∈ Δ(S)
using Bayes’ rule:
</bodyText>
<equation confidence="0.988732">
bat,ot t+1(s0) = Pr(s0|at,ot,bt)
Pr(ot|at,s0,bt)Pr(s0|at,bt)
=
Pr(ot|bt,at)
Ω(ot|s0,at)∑s∈S T(s0|at,s)bt(s)
∑s00 Ω(ot|s00,at)∑s∈S T(s00|at,s)bt(s)
</equation>
<bodyText confidence="0.9991955">
This process is referred to as belief update and is
analogous to the forward algorithm in HMMs. To in-
corporate communication into the standard POMDP
model, we consider observations (o,σ) ∈ O × Σ
which are a combination of a perceptual observation
o and a received message σ. The semantics of the
message σ is included in the belief update equation
using Pr(s|σ), derived in Equation 1:
</bodyText>
<equation confidence="0.997301166666667">
bat,ot,σt
t+1 (s0) =
Ω(o|s0,a) Pr(s0|σ)Pr(σ)
∑σ0∈Σ Pr(s0|σ0)Pr(σ0) ∑s∈ST(s0|a,s)bt(s)
∑s00∈SΩ(o|s00,a) Pr(s00|σ)Pr(σ)
∑σ0∈Σ Pr(s00|σ0)Pr(σ0) ∑s∈S T(s00|a,s)bt(s)
</equation>
<bodyText confidence="0.992990142857143">
Using this new belief state b1, the agent selects an
action a1 = π(b1), and the process continues.
An initial belief state b0 and a policy π to-
gether define a Markov chain over pairs of states
and actions. For a given policy π, we define a
value function Vπ : Δ(S) → R which represents the
expected discounted reward with respect to that
</bodyText>
<equation confidence="0.97410375">
Markov chain:
∞
Vπ(b0) = ∑ γt E[R(bt,at)|b0,π]
t=0
</equation>
<bodyText confidence="0.999648">
The goal of the agent is find a policy π∗ which max-
imizes the value of the initial belief state:
</bodyText>
<equation confidence="0.924172">
π∗ = argmax
π
</equation>
<bodyText confidence="0.9998988">
Exact computation of π∗ is PSPACE-complete (Pa-
padimitriou and Tsitsiklis, 1987), making approx-
imation algorithms necessary for all but the sim-
plest problems. We use Perseus (Spaan and Vlassis,
2005), an anytime approximate point-based value it-
</bodyText>
<equation confidence="0.823599">
Vπ(b0)
</equation>
<page confidence="0.868638">
1076
</page>
<bodyText confidence="0.61235">
eration algorithm.
</bodyText>
<sectionHeader confidence="0.994835" genericHeader="method">
5 DialogBot
</sectionHeader>
<bodyText confidence="0.996872657142857">
We now introduce DialogBot, a Cards agent which
is capable of producing linguistic advice. To decide
when and how to speak, DialogBot maintains a dis-
tribution over its partner’s beliefs and reasons about
the effects his utterances will have on those beliefs.
To handle these complexities, DialogBot models
the world as a Decentralized Partially Observable
Markov Decision Process (Dec-POMDP) (Bernstein
et al., 2002). See Figure 3(b) for the influence dia-
gram. The definition of Dec-POMDPs mirrors that
of the POMDP, with the following changes.
There is a finite set I of agents, which we re-
strict to two. Each agent takes an action ai at
each time step, forming a joint action a~ = (a1,a2).
Each agent receives its own observation oi accord-
ing to Ω(o1,o2|a1,a2,s0). The transition distribu-
tions T(s0|a1,a2,s) and the reward R(s,a1,a2) both
depend on both agents’ actions.
Optimal decision making in Dec-POMDPs re-
quires maintaining a probability distribution over
all possible sequences of actions and observations
( ¯a1, ¯o1,..., ¯at, ¯ot)that the other player might have
received. As t increases, we have an exponential in-
crease in the belief states an agent must consider.
Confirming this informal intuition, decision mak-
ing in Dec-POMDPs is NEXP-complete, a complex-
ity class above P-SPACE (Bernstein et al., 2002).
This computational complexity limits the applica-
tion of Dec-POMDPs to very small problems. To
address this difficulty we make several simplifying
assumptions, allowing us to construct a single-agent
POMDP which approximates the full Dec-POMDP.
Firstly, we assume that other agents do not take
into account our own beliefs, i.e., the other agent
acts like a ListenerBot. This bypasses the infinitely
nested belief problem by assuming that other agents
track one less level of nested beliefs, a common
approach (Goodman and Stuhlm¨uller, 2012; Gmy-
trasiewicz and Doshi, 2005).
Secondly, instead of tracking the full tree of pos-
sible observation histories, we maintain a point es-
timate b¯ of the other agent’s beliefs, which we
term the expected belief state. Rather than track-
ing each possible observation/action history of the
other agent, at each time step we marginalize out
the observations they could have received. Figure 4
compares this approach with exact belief update.
Thirdly, we assume that the other agent acts ac-
cording to a variant of the QMDP approximation
(Littman et al., 1995). Under this approximation, the
other agent solves a fully-observable MDP version
of the ListenerBot POMDP, yielding an MDP pol-
icy it: S → A. This critically allows us to approxi-
mate the other agent’s belief update using a specially
formed POMDP, which we detail next.
State Space To construct the approximate single-
agent POMDP from the full Dec-POMDP problem,
we formulate the state space as S × S. (See Figure
3(c) for the influence diagram.) We write a state
(s, ¯s) ∈ S × S, where s is DialogBot’s beliefs about
the true state of the world, and s¯ is DialogBot’s esti-
mate of the other agent’s beliefs.
Transition Distribution The main difficulty
in constructing the approximate single-agent
POMDP is specifying the transition distribu-
tion T((s0, ¯s0)|a,(s, ¯s)). To address this, we
break this distribution into two components:
T((s0, ¯s0)|a,(s, ¯s)) = ¯T(¯s0|s0,a,(s, ¯s))T(s0|a,s, ¯s).
The first term dictates how DialogBot updates its
beliefs about the other agent’s beliefs:
</bodyText>
<equation confidence="0.979237857142857">
¯T(¯s0|s0,a,(s, ¯s)) = Pr(¯s0|s0,a,(s, ¯s))
= ∑ Pr(¯s0|a, ¯o, ¯s,s)Pr(¯o|s0,a, ¯�(¯s))
¯o∈O
� Ω( ¯o|¯s0,a, ¯�(¯s))T(¯s0|a, ¯�(¯s), ¯s)
= ∑ ∑¯s00 Ω( ¯o|¯s00,a, ¯�(¯s))T(¯s00|a, ¯�(¯s), ¯s)
¯o∈O
)× Ω(¯o|s0,a, ¯�(¯s))
</equation>
<bodyText confidence="0.996355833333333">
We sum over all observations o¯ the other agent could
have received, updating our probability of ¯s0 as Lis-
tenerBot would have, multiplied by the probability
that ListenerBot would have received that observa-
tion, Ω(¯o|s0, ¯�(¯s)). The QMDP approximation al-
lows us to simulate ListenerBot’s belief update in
¯T(¯s0|s0,a,(s, ¯s)). Exact belief update would require
access to ¯b: by using ¯�(¯s) we can estimate the action
that ListenerBot would have taken.
In cases where s¯ contradicts s such that for all o¯ ei-
ther Ω(¯o|s0, ¯�(¯s)) = 0 or Ω(¯o|¯s0, ¯�(¯s)) = 0, we redis-
tribute the belief mass uniformly: ¯T(¯s0|s0,a,(s, ¯s)) ∝
</bodyText>
<page confidence="0.985264">
1077
</page>
<figure confidence="0.99355653125">
o1
o2
o1
o2
bt
¯bo1
t +1
o1
t+ 1
¯bo2
o2
¯bo1,o1
t+2
¯bo1,o2
t+2
¯bo2,o1
t+2
¯bo2,o2
t+2
(a) Exact multi-agent belief tracking
o1
o2
o2
o1
o1
o2
o2
o1
bt
¯bt+1
¯bt+2
(b) Approximate multi-agent belief tracking
</figure>
<figureCaption confidence="0.999175">
Figure 4: Exact multi-agent belief tracking compared with our approximate approach. Each node represents
</figureCaption>
<bodyText confidence="0.9562658">
a belief state. In exact tracking (a), the agent tracks every possible history of observations that its partner
could have received, which grows exponentially in time. In approximate update (b), the agent considers each
possible observation and then averages the resulting belief states, weighted by the probability the other agent
received that observation, resulting in a single summary belief state ¯bt+1. Under the QMDP approximation,
the agent considers what action the other agent would have taken if it completely believed the world was in
a certain state. Thus, there are four belief states resulting from ¯bt, as opposed to two in the exact case.
1 ∀¯s0 =6 ¯s. This approach to managing contradiction
is analogous to logical belief revision (Alchourron´on
et al., 1985; G¨ardenfors, 1988; Ferm´e and Hansson,
2011).
Speech Actions Speech actions are modeled by
how they change the beliefs of the other agent.
The effects of a speech actions are modeled in
¯T(¯s0|s0,a,(s, ¯s)), our model of how ListenerBot’s be-
liefs change. For a speech action a = say(σ) with
</bodyText>
<equation confidence="0.913526666666667">
σ ∈ Σ,
¯T(¯s0|s0,a,(s, ¯s)) =
� Ω( ¯o|¯s0,a, ¯π(¯s))Pr(σ|¯s0)T(¯s0|a, ¯π(¯s), ¯s)
∑ ∑¯s00 Ω( ¯o|¯s00,a, ¯π(¯s))Pr(σ|¯s00)T(¯s00|a, ¯π(¯s), ¯s)
¯o∈O
)× Ω(¯o|s0,a, ¯π(¯s))
</equation>
<bodyText confidence="0.997286444444444">
DialogBot is equipped with the five most
frequent speech actions: BOARD(middle),
BOARD(top), BOARD(bottom), BOARD(left),
and BOARD(right). It produces concrete utterances
by selecting a sentence from the training corpus
with the desired semantics.
Reward DialogBot receives a large reward when
both it and its partner are located on the card, and a
negative cost when moving or speaking:
</bodyText>
<equation confidence="0.9706045">
�
R+ p = c∧ p¯= c
R((p,c,¯p, ¯c),a) =P � P �
R− c V - c
</equation>
<bodyText confidence="0.990872333333333">
DialogBot’s reward is not dependent on the beliefs
of the other player, only the true underlying state of
the world.
</bodyText>
<sectionHeader confidence="0.989281" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999944">
We now experimentally evaluate our semantic clas-
sifiers and the agents’ task performance.
</bodyText>
<subsectionHeader confidence="0.999562">
6.1 Spatial Semantics Classifiers
</subsectionHeader>
<bodyText confidence="0.999827555555556">
We report the performance of our spatial seman-
tics classifiers, although their accuracy is not the fo-
cus of this paper. We use 10-fold cross validation
on a corpus of 577 annotated utterances. We used
simple bag-of-words features, so overfitting the data
with cross validation is not a pressing concern. Of
the 577 utterances, our classifiers perfectly labeled
325 (56.3% accuracy). The classifiers correctly pre-
dicted the domain δ of 515 (89.3%) utterances. The
</bodyText>
<page confidence="0.991662">
1078
</page>
<bodyText confidence="0.9752568">
precision of our binary semantic primitive classifiers
was 969
1126 = .861 and recall 969
1242 = .780, yielding F1
measure .818.
</bodyText>
<subsectionHeader confidence="0.999635">
6.2 Cards Task Evaluation
</subsectionHeader>
<bodyText confidence="0.999923">
We evaluated our ListenerBot and DialogBot agents
in the Cards task. Using 500 randomly generated
initial player and card locations, we tested each
combination of ListenerBot and DialogBot partners.
Agents succeeded at a given initial position if they
both reached the card within 50 moves. Table 2
shows how many trials each dyad won and how
many high-level actions they took to do so.
</bodyText>
<table confidence="0.99875675">
Agents % Success Moves
LB &amp; LB 84.4% 19.8
LB &amp; DB 87.2% 17.5
DB &amp; DB 90.6% 16.6
</table>
<tableCaption confidence="0.9514655">
Table 2: The evaluation for each combination of
agents. LB = ListenerBot; DB = DialogBot.
</tableCaption>
<bodyText confidence="0.999047166666667">
Collaborating DialogBots performed the best,
completing more trials and using fewer moves than
the ListenerBots. The DialogBots initially explore
the space in a similar manner to the ListenerBots,
but then share card location information. This leads
to shorter interactions, as once the DialogBot finds
the card, the other player can find it more quickly.
In the combination of ListenerBot and DialogBot,
we see about half of the improvement over two Lis-
tenerBots. Roughly 50% of the time, the Listener-
Bot finds the card first, which doesn’t help the Di-
alogBot find the card any faster.
</bodyText>
<sectionHeader confidence="0.980039" genericHeader="method">
7 Emergent Pragmatics
</sectionHeader>
<bodyText confidence="0.999979176470588">
Grice’s original model of pragmatics (Grice, 1975)
involves the cooperative principle and four maxims:
quality (“say only what you know to be true”), rela-
tion (“be relevant”), quantity (“be as informative as
is required; do not say more than is required”), and
manner (roughly, be clear and concise).
In most interactions, DialogBot searches for the
card and then reports its location to the other agent.
These reports obey quality in that they are made only
when based on actual observations. The behavior
is not hard-coded, but rather emerges, because only
accurate information serves the agents’ goals. In
contrast, sub-optimal policies generated early in the
POMDP solving process sometimes lie about card
locations. Since this behavior confuses the other
agent and thus has a lower utility, it gets replaced
by truthful communication as the policies improve.
We also capture the effects of relation and the first
clause of quantity, because the nature of the reward
function and the nested belief structures ensure that
DialogBot offers only relevant, informative informa-
tion. For instance, when DialogBot finds the card in
the lower left corner, it alternates saying “left” and
“bottom”, effectively overcoming its limited gener-
ation capabilities. Again, early sub-optimal policies
sometimes do not report the location of the card at
all, thereby failing to fulfill these maxims.
We expect these models to produce behavior con-
sistent with manner and the second clause of quan-
tity, but evaluating this claim will require a richer ex-
perimental paradigm. For example, if DialogBot had
a larger and more structured vocabulary, it would
have to choose between levels of specificity as well
as more or less economical forms.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999984636363636">
We have shown that cooperative pragmatic behavior
can arise from multi-agent decision-theoretic mod-
els in which the agents share a joint utility func-
tion and reason about each other’s belief states.
Decision-making in these models is intractable,
which has been a major obstacle to achieving exper-
imental results in this area. We introduced a series
of approximations to manage this intractability: (i)
combining low-level states into semantically coher-
ent high-level ones; (ii) tracking only an averaged
summary of the other agent’s potential beliefs; (iii)
limiting belief state nesting to one level, and (iv)
simplifying each agent’s model of the other’s be-
liefs so as to reduce uncertainty. These approxima-
tions bring the problems under sufficient control that
they can be solved with current POMDP approxi-
mation algorithms. Our experimental results high-
light the rich pragmatic behavior this gives rise to
and quantify the communicative value of such be-
havior. While there remain insights from earlier the-
oretical proposals and logic-based methods that we
have not fully captured, our current results support
</bodyText>
<page confidence="0.992723">
1079
</page>
<bodyText confidence="0.9997705">
the notion that probabilistic decision-making meth-
ods can yield robust, widely applicable models that
address the real-world difficulties of partial observ-
ability and uncertainty.
</bodyText>
<sectionHeader confidence="0.994559" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997040333333333">
This research was supported in part by ONR
grants N00014-10-1-0109 and N00014-13-1-0287
and ARO grant W911NF-07-1-0216.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999851461538462">
Carlos E. Alchourron´on, Peter G¨ardenfors, and David
Makinson. 1985. On the logic of theory change: Par-
tial meets contradiction and revision functions. Jour-
nal of Symbolic Logic, 50(2):510–530.
James F. Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. PLOW: A collaborative
task learning agent. In Proceedings of the Twenty-
Second AAAI Conference on Artificial Intelligence,
pages 1514–1519. AAAI Press, Vancouver, British
Columbia, Canada.
James F. Allen. 1991. Reasoning About Plans. Morgan
Kaufmann, San Francisco.
David Beaver. 2002. Pragmatics, and that’s an order. In
David Barker-Plummer, David Beaver, Johan van Ben-
them, and Patrick Scotto di Luzio, editors, Logic, Lan-
guage, and Visual Information, pages 192–215. CSLI,
Stanford, CA.
Anton Benz, Gerhard J¨ager, and Robert van Rooij, edi-
tors. 2005. Game Theory and Pragmatics. Palgrave
McMillan, Basingstoke, Hampshire.
Daniel S. Bernstein, Robert Givan, Neil Immerman, and
Shlomo Zilberstein. 2002. The complexity of decen-
tralized control of Markov decision processes. Mathe-
matics of Operations Research, 27(4):819–840.
Reinhard Blutner. 1998. Lexical pragmatics. Journal of
Semantics, 15(2):115–162.
Michael Bratman. 1987. Intentions, Plans, and Practical
Reason. Harvard University Press.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.
Judith Degen and Michael Franke. 2012. Optimal rea-
soning about referential expressions. In Proceedings
of SemDIAL 2012, Paris, September.
David DeVault, Natalia Kariaeva, Anubha Kothari, Iris
Oved, and Matthew Stone. 2005. An information-
state approach to collaborative reference. In Proceed-
ings of the ACL Interactive Poster and Demonstration
Sessions, pages 1–4, Ann Arbor, MI, June. Association
for Computational Linguistics.
David DeVault. 2008. Contribution Tracking: Partici-
pating in Task-Oriented Dialogue under Uncertainty.
Ph.D. thesis, Rutgers University, New Brunswick, NJ.
Eduardo Ferm´e and Sven Ove Hansson. 2011. AGM 25
years: Twenty-five years of research in belief change.
Journal of Philosophical Logic, 40(2):295–331.
Michael C. Frank and Noah D. Goodman. 2012. Predict-
ing pragmatic reasoning in language games. Science,
336(6084):998.
Michael Franke. 2009. Signal to Act: Game Theory
in Pragmatics. ILLC Dissertation Series. Institute for
Logic, Language and Computation, University of Am-
sterdam.
Peter G¨ardenfors. 1988. Knowledge in Flux: Modeling
the Dynamics of Epistemic States. MIT Press.
Piotr J. Gmytrasiewicz and Prashant Doshi. 2005. A
framework for sequential planning in multi-agent set-
tings. Journal of Artificial Intelligence Research,
24:24–49.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 410–419, Cambridge, MA, October. ACL.
Noah D. Goodman and Andreas Stuhlm¨uller. 2012.
Knowledge and implicature: Modeling language un-
derstanding as social cognition. In Proceedings of the
Thirty-Fourth Annual Conference of the Cognitive Sci-
ence Society.
H. Paul Grice. 1975. Logic and conversation. In Peter
Cole and Jerry Morgan, editors, Syntax and Semantics,
volume 3: Speech Acts, pages 43–58. Academic Press,
New York.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175–204, July.
William G. Hayward and Michael J. Tarr. 1995. Spa-
tial language and spatial representation. Cognition,
55:39–84.
Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1–2):69–142.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
Computational Linguistics, 38(1):173–218.
David Lewis. 1969. Convention. Harvard University
Press, Cambridge, MA. Reprinted 2002 by Blackwell.
</reference>
<page confidence="0.829164">
1080
</page>
<reference confidence="0.999891575757576">
Michael L. Littman, Anthony R. Cassandra, and
Leslie Pack Kaelbling. 1995. Learning policies for
partially observable environments: Scaling up. In Ar-
mand Prieditis and Stuart J. Russell, editors, ICML,
pages 362–370. Morgan Kaufmann.
Arthur Merin. 1997. If all our arguments had to be con-
clusive, there would be few of them. Arbeitspapiere
SFB 340 101, University of Stuttgart, Stuttgart.
Christos Papadimitriou and John N. Tsitsiklis. 1987. The
complexity of markov decision processes. Math. Oper.
Res., 12(3):441–450, August.
Prashant Parikh. 2001. The Use of Language. CSLI,
Stanford, CA.
C. Raymond Perrault and James F. Allen. 1980. A plan-
based analysis of indirect speech acts. American Jour-
nal of Computational Linguistics, 6(3–4):167–182.
Christopher Potts. 2012. Goal-driven answers in the
Cards dialogue corpus. In Nathan Arnett and Ryan
Bennett, editors, Proceedings of the 30th West Coast
Conference on Formal Linguistics, Somerville, MA.
Cascadilla Press.
David V. Pynadath and Milind Tambe. 2002. The com-
municative multiagent team decision problem: Ana-
lyzing teamwork theories and models. Journal of Ar-
tificial Intelligence Research, 16:2002.
Hannah Rohde, Scott Seyfarth, Brady Clark, Gerhard
J¨ager, and Stefan Kaufmann. 2012. Communicat-
ing with cost-based implicature: A game-theoretic ap-
proach to ambiguity. In The 16th Workshop on the Se-
mantics and Pragmatics of Dialogue, Paris, Septem-
ber.
Robert van Rooy. 2003. Questioning to resolve decision
problems. Linguistics and Philosophy, 26(6):727–
763.
Seymour Rosenberg and Bertram D. Cohen. 1964.
Speakers’ and listeners’ processes in a word commu-
nication task. Science, 145:1201–1203.
Matthijs T. J. Spaan and Nikos Vlassis. 2005. Perseus:
Randomized point-based value iteration for POMDPs.
Journal ofArtificial Intelligence Research, 24(1):195–
220, August.
Matthijs T. J. Spaan, Frans A. Oliehoek, and Nikos Vlas-
sis. 2008. Multiagent planning under uncertainty with
stochastic communication delays. In In Proc. of the
18th Int. Conf. on Automated Planning and Schedul-
ing, pages 338–345.
Alex Stiller, Noah D. Goodman, and Michael C. Frank.
2011. Ad-hoc scalar implicature in adults and chil-
dren. In Proceedings of the 33rd Annual Meeting of
the Cognitive Science Society, Boston, July.
Matthew Stone, Richmond Thomason, and David De-
Vault. 2007. Enlightened update: A computational
architecture for presupposition and other pragmatic
phenomena. To appear in Donna K. Byron; Craige
Roberts; and Scott Schwenter, Presupposition Accom-
modation.
Blaise Thomson and Steve Young. 2010. Bayesian up-
date of dialogue state: A pomdp framework for spoken
dialogue systems. Comput. Speech Lang., 24(4):562–
588, October.
Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The hidden information state model: A
practical framework for pomdp-based spoken dialogue
management. Comput. Speech Lang., 24(2):150–174,
April.
</reference>
<page confidence="0.995394">
1081
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.452158">
<title confidence="0.950865">Emergence of Gricean Maxims from Multi-Agent Decision Theory</title>
<author confidence="0.916429">Max Bodoia Vogel</author>
<author confidence="0.916429">Christopher Potts</author>
<affiliation confidence="0.590502">Stanford</affiliation>
<address confidence="0.740968">Stanford, CA,</address>
<abstract confidence="0.996031565217391">Grice characterized communication in terms the which enjoins speakers to make only contributions that serve the evolving conversational goals. We show that the cooperative principle and the associated maxims of relevance, quality, and quantity emerge from multi-agent decision theory. We utilize the Decentralized Partially Observable Markov Decision Process (Dec-POMDP) model of multi-agent decision making which relies only on basic definitions of rationality and the ability of agents to reason about each other’s beliefs in maximizing joint utility. Our model uses cognitively-inspired heuristics to simplify the otherwise intractable task of reasoning jointly about actions, the environment, and the nested beliefs of other actors. Our experiments on a cooperative language task show that reasoning about others’ belief states, and the resulting emergent Gricean communicative behavior, leads to significantly improved task performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carlos E Alchourron´on</author>
<author>Peter G¨ardenfors</author>
<author>David Makinson</author>
</authors>
<title>On the logic of theory change: Partial meets contradiction and revision functions.</title>
<date>1985</date>
<journal>Journal of Symbolic Logic,</journal>
<volume>50</volume>
<issue>2</issue>
<marker>Alchourron´on, G¨ardenfors, Makinson, 1985</marker>
<rawString>Carlos E. Alchourron´on, Peter G¨ardenfors, and David Makinson. 1985. On the logic of theory change: Partial meets contradiction and revision functions. Journal of Symbolic Logic, 50(2):510–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>Nathanael Chambers</author>
<author>George Ferguson</author>
<author>Lucian Galescu</author>
<author>Hyuckchul Jung</author>
<author>Mary Swift</author>
<author>William Taysom</author>
</authors>
<title>PLOW: A collaborative task learning agent.</title>
<date>2007</date>
<booktitle>In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1514--1519</pages>
<publisher>AAAI Press,</publisher>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="2705" citStr="Allen et al., 2007" startWordPosition="399" endWordPosition="402">ed referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their efficient implementation. With the current p</context>
</contexts>
<marker>Allen, Chambers, Ferguson, Galescu, Jung, Swift, Taysom, 2007</marker>
<rawString>James F. Allen, Nathanael Chambers, George Ferguson, Lucian Galescu, Hyuckchul Jung, Mary Swift, and William Taysom. 2007. PLOW: A collaborative task learning agent. In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence, pages 1514–1519. AAAI Press, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Reasoning About Plans.</title>
<date>1991</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="2626" citStr="Allen, 1991" startWordPosition="387" endWordPosition="388">aker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent d</context>
</contexts>
<marker>Allen, 1991</marker>
<rawString>James F. Allen. 1991. Reasoning About Plans. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Beaver</author>
</authors>
<title>Pragmatics, and that’s an order. In</title>
<date>2002</date>
<booktitle>Language, and Visual Information,</booktitle>
<pages>192--215</pages>
<editor>David Barker-Plummer, David Beaver, Johan van Benthem, and Patrick Scotto di Luzio, editors, Logic,</editor>
<publisher>CSLI,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="1892" citStr="Beaver, 2002" startWordPosition="270" endWordPosition="271">of an overarching cooperative principle and a set of more specific maxims, which enjoin speakers to make contributions that are truthful, informative, relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this w</context>
</contexts>
<marker>Beaver, 2002</marker>
<rawString>David Beaver. 2002. Pragmatics, and that’s an order. In David Barker-Plummer, David Beaver, Johan van Benthem, and Patrick Scotto di Luzio, editors, Logic, Language, and Visual Information, pages 192–215. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>Game Theory and Pragmatics. Palgrave McMillan,</booktitle>
<editor>Anton Benz, Gerhard J¨ager, and Robert van Rooij, editors.</editor>
<location>Basingstoke, Hampshire.</location>
<marker>2005</marker>
<rawString>Anton Benz, Gerhard J¨ager, and Robert van Rooij, editors. 2005. Game Theory and Pragmatics. Palgrave McMillan, Basingstoke, Hampshire.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Bernstein</author>
<author>Robert Givan</author>
<author>Neil Immerman</author>
<author>Shlomo Zilberstein</author>
</authors>
<title>The complexity of decentralized control of Markov decision processes.</title>
<date>2002</date>
<journal>Mathematics of Operations Research,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="4137" citStr="Bernstein et al., 2002" startWordPosition="607" endWordPosition="610">Decision Process (DecPOMDP) provides a robust, flexible foundation for implementing agents that communicate in a Gricean manner. Dec-POMDPs are multi-agent, partiallyobservable models in which agents maintain belief distributions over the underlying, hidden world state, including the beliefs of the other players, and speech actions change those beliefs. In this setting, informative, relevant communication emerges as the best way to maximize joint utility. The complexity of pragmatic reasoning is still forbidding, though. Correspondingly, optimal decision making in Dec-POMDPs is NEXP complete (Bernstein et al., 2002). To manage this issue, we introduce several cognitively-plausible approximations which allow us to simplify the Dec-POMDP to a single-agent POMDP, for which relatively efficient solvers exist (Spaan and Vlassis, 2005). We demonstrate our algorithms on a variation of the Cards task, a partially-observable collaborative search problem (Potts, 2012). Spatial language comprises the bulk of communication in the Cards task, and we discuss a model of spatial semantics in Section 3. Using this task and a model of the meaning of spatial language, we next discuss two agents that play the game: Listener</context>
<context position="18301" citStr="Bernstein et al., 2002" startWordPosition="2959" endWordPosition="2962">, making approximation algorithms necessary for all but the simplest problems. We use Perseus (Spaan and Vlassis, 2005), an anytime approximate point-based value itVπ(b0) 1076 eration algorithm. 5 DialogBot We now introduce DialogBot, a Cards agent which is capable of producing linguistic advice. To decide when and how to speak, DialogBot maintains a distribution over its partner’s beliefs and reasons about the effects his utterances will have on those beliefs. To handle these complexities, DialogBot models the world as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002). See Figure 3(b) for the influence diagram. The definition of Dec-POMDPs mirrors that of the POMDP, with the following changes. There is a finite set I of agents, which we restrict to two. Each agent takes an action ai at each time step, forming a joint action a~ = (a1,a2). Each agent receives its own observation oi according to Ω(o1,o2|a1,a2,s0). The transition distributions T(s0|a1,a2,s) and the reward R(s,a1,a2) both depend on both agents’ actions. Optimal decision making in Dec-POMDPs requires maintaining a probability distribution over all possible sequences of actions and observations (</context>
</contexts>
<marker>Bernstein, Givan, Immerman, Zilberstein, 2002</marker>
<rawString>Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. 2002. The complexity of decentralized control of Markov decision processes. Mathematics of Operations Research, 27(4):819–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Blutner</author>
</authors>
<title>Lexical pragmatics.</title>
<date>1998</date>
<journal>Journal of Semantics,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="1864" citStr="Blutner, 1998" startWordPosition="266" endWordPosition="267">ong rational agents in terms of an overarching cooperative principle and a set of more specific maxims, which enjoin speakers to make contributions that are truthful, informative, relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action t</context>
</contexts>
<marker>Blutner, 1998</marker>
<rawString>Reinhard Blutner. 1998. Lexical pragmatics. Journal of Semantics, 15(2):115–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bratman</author>
</authors>
<title>Intentions, Plans, and Practical Reason.</title>
<date>1987</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="2665" citStr="Bratman, 1987" startWordPosition="393" endWordPosition="394">r ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their effi</context>
</contexts>
<marker>Bratman, 1987</marker>
<rawString>Michael Bratman. 1987. Intentions, Plans, and Practical Reason. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1836" citStr="Clark, 1996" startWordPosition="262" endWordPosition="263">acterized communication among rational agents in terms of an overarching cooperative principle and a set of more specific maxims, which enjoin speakers to make contributions that are truthful, informative, relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involvin</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2148" citStr="Dale and Reiter, 1995" startWordPosition="312" endWordPosition="315"> perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007;</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Degen</author>
<author>Michael Franke</author>
</authors>
<title>Optimal reasoning about referential expressions.</title>
<date>2012</date>
<booktitle>In Proceedings of SemDIAL 2012,</booktitle>
<location>Paris,</location>
<contexts>
<context position="2272" citStr="Degen and Franke, 2012" startWordPosition="333" endWordPosition="336">d collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to mod</context>
</contexts>
<marker>Degen, Franke, 2012</marker>
<rawString>Judith Degen and Michael Franke. 2012. Optimal reasoning about referential expressions. In Proceedings of SemDIAL 2012, Paris, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Natalia Kariaeva</author>
<author>Anubha Kothari</author>
<author>Iris Oved</author>
<author>Matthew Stone</author>
</authors>
<title>An informationstate approach to collaborative reference.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Interactive Poster and Demonstration Sessions,</booktitle>
<pages>1--4</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="2727" citStr="DeVault et al., 2005" startWordPosition="403" endWordPosition="406">rg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their efficient implementation. With the current paper, we seek to show </context>
</contexts>
<marker>DeVault, Kariaeva, Kothari, Oved, Stone, 2005</marker>
<rawString>David DeVault, Natalia Kariaeva, Anubha Kothari, Iris Oved, and Matthew Stone. 2005. An informationstate approach to collaborative reference. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 1–4, Ann Arbor, MI, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
</authors>
<title>Contribution Tracking: Participating in Task-Oriented Dialogue under Uncertainty.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Rutgers University,</institution>
<location>New Brunswick, NJ.</location>
<contexts>
<context position="2763" citStr="DeVault, 2008" startWordPosition="411" endWordPosition="412"> Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their efficient implementation. With the current paper, we seek to show that the Decentralized Par1072 Proce</context>
</contexts>
<marker>DeVault, 2008</marker>
<rawString>David DeVault. 2008. Contribution Tracking: Participating in Task-Oriented Dialogue under Uncertainty. Ph.D. thesis, Rutgers University, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduardo Ferm´e</author>
<author>Sven Ove Hansson</author>
</authors>
<title>years: Twenty-five years of research in belief change.</title>
<date>2011</date>
<journal>AGM</journal>
<volume>25</volume>
<marker>Ferm´e, Hansson, 2011</marker>
<rawString>Eduardo Ferm´e and Sven Ove Hansson. 2011. AGM 25 years: Twenty-five years of research in belief change. Journal of Philosophical Logic, 40(2):295–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C Frank</author>
<author>Noah D Goodman</author>
</authors>
<title>Predicting pragmatic reasoning in language games.</title>
<date>2012</date>
<journal>Science,</journal>
<volume>336</volume>
<issue>6084</issue>
<contexts>
<context position="2217" citStr="Frank and Goodman, 2012" startWordPosition="324" endWordPosition="327">oncerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of</context>
</contexts>
<marker>Frank, Goodman, 2012</marker>
<rawString>Michael C. Frank and Noah D. Goodman. 2012. Predicting pragmatic reasoning in language games. Science, 336(6084):998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Franke</author>
</authors>
<title>Signal to Act: Game Theory in Pragmatics. ILLC Dissertation Series. Institute for Logic, Language and Computation,</title>
<date>2009</date>
<location>University of Amsterdam.</location>
<contexts>
<context position="1942" citStr="Franke, 2009" startWordPosition="279" endWordPosition="280"> of more specific maxims, which enjoin speakers to make contributions that are truthful, informative, relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on plann</context>
</contexts>
<marker>Franke, 2009</marker>
<rawString>Michael Franke. 2009. Signal to Act: Game Theory in Pragmatics. ILLC Dissertation Series. Institute for Logic, Language and Computation, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter G¨ardenfors</author>
</authors>
<title>Knowledge in Flux: Modeling the Dynamics of Epistemic States.</title>
<date>1988</date>
<publisher>MIT Press.</publisher>
<marker>G¨ardenfors, 1988</marker>
<rawString>Peter G¨ardenfors. 1988. Knowledge in Flux: Modeling the Dynamics of Epistemic States. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr J Gmytrasiewicz</author>
<author>Prashant Doshi</author>
</authors>
<title>A framework for sequential planning in multi-agent settings.</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>24--24</pages>
<contexts>
<context position="19776" citStr="Gmytrasiewicz and Doshi, 2005" startWordPosition="3191" endWordPosition="3195"> complexity class above P-SPACE (Bernstein et al., 2002). This computational complexity limits the application of Dec-POMDPs to very small problems. To address this difficulty we make several simplifying assumptions, allowing us to construct a single-agent POMDP which approximates the full Dec-POMDP. Firstly, we assume that other agents do not take into account our own beliefs, i.e., the other agent acts like a ListenerBot. This bypasses the infinitely nested belief problem by assuming that other agents track one less level of nested beliefs, a common approach (Goodman and Stuhlm¨uller, 2012; Gmytrasiewicz and Doshi, 2005). Secondly, instead of tracking the full tree of possible observation histories, we maintain a point estimate b¯ of the other agent’s beliefs, which we term the expected belief state. Rather than tracking each possible observation/action history of the other agent, at each time step we marginalize out the observations they could have received. Figure 4 compares this approach with exact belief update. Thirdly, we assume that the other agent acts according to a variant of the QMDP approximation (Littman et al., 1995). Under this approximation, the other agent solves a fully-observable MDP versio</context>
</contexts>
<marker>Gmytrasiewicz, Doshi, 2005</marker>
<rawString>Piotr J. Gmytrasiewicz and Prashant Doshi. 2005. A framework for sequential planning in multi-agent settings. Journal of Artificial Intelligence Research, 24:24–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>410--419</pages>
<publisher>ACL.</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="2170" citStr="Golland et al., 2010" startWordPosition="316" endWordPosition="319">ects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We at</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, Cambridge, MA, October. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah D Goodman</author>
<author>Andreas Stuhlm¨uller</author>
</authors>
<title>Knowledge and implicature: Modeling language understanding as social cognition.</title>
<date>2012</date>
<booktitle>In Proceedings of the Thirty-Fourth Annual Conference of the Cognitive Science Society.</booktitle>
<marker>Goodman, Stuhlm¨uller, 2012</marker>
<rawString>Noah D. Goodman and Andreas Stuhlm¨uller. 2012. Knowledge and implicature: Modeling language understanding as social cognition. In Proceedings of the Thirty-Fourth Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics,</booktitle>
<volume>volume</volume>
<pages>43--58</pages>
<editor>In Peter Cole and Jerry Morgan, editors,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="1211" citStr="Grice (1975)" startWordPosition="164" endWordPosition="165">s (Dec-POMDP) model of multi-agent decision making which relies only on basic definitions of rationality and the ability of agents to reason about each other’s beliefs in maximizing joint utility. Our model uses cognitively-inspired heuristics to simplify the otherwise intractable task of reasoning jointly about actions, the environment, and the nested beliefs of other actors. Our experiments on a cooperative language task show that reasoning about others’ belief states, and the resulting emergent Gricean communicative behavior, leads to significantly improved task performance. 1 Introduction Grice (1975) famously characterized communication among rational agents in terms of an overarching cooperative principle and a set of more specific maxims, which enjoin speakers to make contributions that are truthful, informative, relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretica</context>
<context position="26229" citStr="Grice, 1975" startWordPosition="4243" endWordPosition="4244">mpleting more trials and using fewer moves than the ListenerBots. The DialogBots initially explore the space in a similar manner to the ListenerBots, but then share card location information. This leads to shorter interactions, as once the DialogBot finds the card, the other player can find it more quickly. In the combination of ListenerBot and DialogBot, we see about half of the improvement over two ListenerBots. Roughly 50% of the time, the ListenerBot finds the card first, which doesn’t help the DialogBot find the card any faster. 7 Emergent Pragmatics Grice’s original model of pragmatics (Grice, 1975) involves the cooperative principle and four maxims: quality (“say only what you know to be true”), relation (“be relevant”), quantity (“be as informative as is required; do not say more than is required”), and manner (roughly, be clear and concise). In most interactions, DialogBot searches for the card and then reports its location to the other agent. These reports obey quality in that they are made only when based on actual observations. The behavior is not hard-coded, but rather emerges, because only accurate information serves the agents’ goals. In contrast, sub-optimal policies generated </context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. Paul Grice. 1975. Logic and conversation. In Peter Cole and Jerry Morgan, editors, Syntax and Semantics, volume 3: Speech Acts, pages 43–58. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Comput. Linguist.,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="2650" citStr="Grosz and Sidner, 1986" startWordPosition="389" endWordPosition="392"> a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Comput. Linguist., 12(3):175–204, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William G Hayward</author>
<author>Michael J Tarr</author>
</authors>
<title>Spatial language and spatial representation.</title>
<date>1995</date>
<journal>Cognition,</journal>
<pages>55--39</pages>
<contexts>
<context position="9594" citStr="Hayward and Tarr, 1995" startWordPosition="1499" endWordPosition="1502"> discuss in Section 4 are highly sensitive to the size of the state space. The full representation of the game board consists of 231 squares. Representing the location of both players and the location of the card requires 3233 = 12,326,391 states, well beyond the capabilities of current decision-making algorithms. To ameliorate this difficulty, we cluster squares together using the spatial referring expression corFigure 2: Semantic state space clusters with k = 16. pus. This approach follows from research that shows that humans’ mental spatial representations are influenced by their language (Hayward and Tarr, 1995). Our intuition is that human players do not consider all possible locations of the card and players, but instead lump them into semantically coherent states, such as “the card is in the top right corner.” Following this intuition, we cluster states together which have similar referring expressions, allowing our agents to use language as a cognitive technology and not just a tool for communication. For each board square (x,y) we form a vector φ(x,y) with φi(x,y) = Pr((x,y)|σi), where σi is the ith distinct semantic representation in the corpus. This forms a 136-dimensional vector for each boar</context>
</contexts>
<marker>Hayward, Tarr, 1995</marker>
<rawString>William G. Hayward and Michael J. Tarr. 1995. Spatial language and spatial representation. Cognition, 55:39–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
<author>Mark Stickel</author>
<author>Douglas Appelt</author>
<author>Paul Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="2685" citStr="Hobbs et al., 1993" startWordPosition="395" endWordPosition="398">ess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their efficient implementation</context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>Jerry Hobbs, Mark Stickel, Douglas Appelt, and Paul Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63(1–2):69–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Krahmer, van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1):173–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
</authors>
<title>Convention.</title>
<date>1969</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA. Reprinted</location>
<contexts>
<context position="1732" citStr="Lewis (1969)" startWordPosition="246" endWordPosition="247">ive behavior, leads to significantly improved task performance. 1 Introduction Grice (1975) famously characterized communication among rational agents in terms of an overarching cooperative principle and a set of more specific maxims, which enjoin speakers to make contributions that are truthful, informative, relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical m</context>
</contexts>
<marker>Lewis, 1969</marker>
<rawString>David Lewis. 1969. Convention. Harvard University Press, Cambridge, MA. Reprinted 2002 by Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Littman</author>
<author>Anthony R Cassandra</author>
<author>Leslie Pack Kaelbling</author>
</authors>
<title>Learning policies for partially observable environments: Scaling up.</title>
<date>1995</date>
<booktitle>In Armand Prieditis and</booktitle>
<pages>362--370</pages>
<editor>Stuart J. Russell, editors, ICML,</editor>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="20296" citStr="Littman et al., 1995" startWordPosition="3278" endWordPosition="3281"> of nested beliefs, a common approach (Goodman and Stuhlm¨uller, 2012; Gmytrasiewicz and Doshi, 2005). Secondly, instead of tracking the full tree of possible observation histories, we maintain a point estimate b¯ of the other agent’s beliefs, which we term the expected belief state. Rather than tracking each possible observation/action history of the other agent, at each time step we marginalize out the observations they could have received. Figure 4 compares this approach with exact belief update. Thirdly, we assume that the other agent acts according to a variant of the QMDP approximation (Littman et al., 1995). Under this approximation, the other agent solves a fully-observable MDP version of the ListenerBot POMDP, yielding an MDP policy it: S → A. This critically allows us to approximate the other agent’s belief update using a specially formed POMDP, which we detail next. State Space To construct the approximate singleagent POMDP from the full Dec-POMDP problem, we formulate the state space as S × S. (See Figure 3(c) for the influence diagram.) We write a state (s, ¯s) ∈ S × S, where s is DialogBot’s beliefs about the true state of the world, and s¯ is DialogBot’s estimate of the other agent’s bel</context>
</contexts>
<marker>Littman, Cassandra, Kaelbling, 1995</marker>
<rawString>Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. 1995. Learning policies for partially observable environments: Scaling up. In Armand Prieditis and Stuart J. Russell, editors, ICML, pages 362–370. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Merin</author>
</authors>
<title>If all our arguments had to be conclusive, there would be few of them.</title>
<date>1997</date>
<journal>Arbeitspapiere SFB</journal>
<volume>340</volume>
<institution>University of Stuttgart,</institution>
<location>Stuttgart.</location>
<contexts>
<context position="1849" citStr="Merin, 1997" startWordPosition="264" endWordPosition="265">munication among rational agents in terms of an overarching cooperative principle and a set of more specific maxims, which enjoin speakers to make contributions that are truthful, informative, relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both langua</context>
</contexts>
<marker>Merin, 1997</marker>
<rawString>Arthur Merin. 1997. If all our arguments had to be conclusive, there would be few of them. Arbeitspapiere SFB 340 101, University of Stuttgart, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Papadimitriou</author>
<author>John N Tsitsiklis</author>
</authors>
<title>The complexity of markov decision processes.</title>
<date>1987</date>
<journal>Math. Oper. Res.,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="17678" citStr="Papadimitriou and Tsitsiklis, 1987" startWordPosition="2864" endWordPosition="2868">s00|σ)Pr(σ) ∑σ0∈Σ Pr(s00|σ0)Pr(σ0) ∑s∈S T(s00|a,s)bt(s) Using this new belief state b1, the agent selects an action a1 = π(b1), and the process continues. An initial belief state b0 and a policy π together define a Markov chain over pairs of states and actions. For a given policy π, we define a value function Vπ : Δ(S) → R which represents the expected discounted reward with respect to that Markov chain: ∞ Vπ(b0) = ∑ γt E[R(bt,at)|b0,π] t=0 The goal of the agent is find a policy π∗ which maximizes the value of the initial belief state: π∗ = argmax π Exact computation of π∗ is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), making approximation algorithms necessary for all but the simplest problems. We use Perseus (Spaan and Vlassis, 2005), an anytime approximate point-based value itVπ(b0) 1076 eration algorithm. 5 DialogBot We now introduce DialogBot, a Cards agent which is capable of producing linguistic advice. To decide when and how to speak, DialogBot maintains a distribution over its partner’s beliefs and reasons about the effects his utterances will have on those beliefs. To handle these complexities, DialogBot models the world as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (</context>
</contexts>
<marker>Papadimitriou, Tsitsiklis, 1987</marker>
<rawString>Christos Papadimitriou and John N. Tsitsiklis. 1987. The complexity of markov decision processes. Math. Oper. Res., 12(3):441–450, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Parikh</author>
</authors>
<title>The Use of Language.</title>
<date>2001</date>
<location>CSLI, Stanford, CA.</location>
<contexts>
<context position="1878" citStr="Parikh, 2001" startWordPosition="268" endWordPosition="269">ents in terms of an overarching cooperative principle and a set of more specific maxims, which enjoin speakers to make contributions that are truthful, informative, relevant, clear, and concise. Since then, there have been many attempts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which</context>
</contexts>
<marker>Parikh, 2001</marker>
<rawString>Prashant Parikh. 2001. The Use of Language. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond Perrault</author>
<author>James F Allen</author>
</authors>
<title>A planbased analysis of indirect speech acts.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics,</journal>
<pages>6--3</pages>
<contexts>
<context position="2613" citStr="Perrault and Allen, 1980" startWordPosition="383" endWordPosition="386">tep games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in </context>
</contexts>
<marker>Perrault, Allen, 1980</marker>
<rawString>C. Raymond Perrault and James F. Allen. 1980. A planbased analysis of indirect speech acts. American Journal of Computational Linguistics, 6(3–4):167–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Potts</author>
</authors>
<title>Goal-driven answers in the Cards dialogue corpus.</title>
<date>2012</date>
<booktitle>Proceedings of the 30th West Coast Conference on Formal Linguistics,</booktitle>
<editor>In Nathan Arnett and Ryan Bennett, editors,</editor>
<publisher>Cascadilla Press.</publisher>
<location>Somerville, MA.</location>
<contexts>
<context position="4486" citStr="Potts, 2012" startWordPosition="659" endWordPosition="660">his setting, informative, relevant communication emerges as the best way to maximize joint utility. The complexity of pragmatic reasoning is still forbidding, though. Correspondingly, optimal decision making in Dec-POMDPs is NEXP complete (Bernstein et al., 2002). To manage this issue, we introduce several cognitively-plausible approximations which allow us to simplify the Dec-POMDP to a single-agent POMDP, for which relatively efficient solvers exist (Spaan and Vlassis, 2005). We demonstrate our algorithms on a variation of the Cards task, a partially-observable collaborative search problem (Potts, 2012). Spatial language comprises the bulk of communication in the Cards task, and we discuss a model of spatial semantics in Section 3. Using this task and a model of the meaning of spatial language, we next discuss two agents that play the game: ListenerBot (Section 4) makes decisions using a single-agent POMDP that does not take into account the beliefs or actions of its partner, whereas DialogBot (Section 5) maintains a model of its partner’s beliefs. As a result of the cooperative structure of the underlying model and the effects of communication within it, DialogBot’s contributions are releva</context>
<context position="7000" citStr="Potts (2012)" startWordPosition="1079" endWordPosition="1080">The agents use (simplified) English to communicate with each other. 3 Spatial Semantics Much of the communication in the Cards task involves referring to spatial locations on the board. Accordingly, we focus on spatial language for our artificial agents. In this section, we present a model of spatial semantics, which we create by leveraging the human–human Cards transcripts. We discuss the spatial semantic representation, how we classify the semantics of new locative expressions, and our use of spatial semantics to form a high-level state space for decision making. 3.1 Semantic Representation Potts (2012) released annotations, derived from the Cards corpus, which reduce 599 of the players’ statements about their locations to formulae of the form δ(ϕ1 n ··· n ϕk), where δ is a domain and ϕ1�����ϕk are semantic literals. For example, the utterance “(I’m) at the top right of the board” is annotated as BOARD(top n right), and “(I’m) in bottom 1073 of the C room” is annotated as C room(bottom). Table 1 lists the full set of semantic primitives that appear as domain expressions and literals. Because the Cards transcripts are so highly structured, we can interpret these expressions in terms of the Ca</context>
</contexts>
<marker>Potts, 2012</marker>
<rawString>Christopher Potts. 2012. Goal-driven answers in the Cards dialogue corpus. In Nathan Arnett and Ryan Bennett, editors, Proceedings of the 30th West Coast Conference on Formal Linguistics, Somerville, MA. Cascadilla Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David V Pynadath</author>
<author>Milind Tambe</author>
</authors>
<title>The communicative multiagent team decision problem: Analyzing teamwork theories and models.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>16--2002</pages>
<contexts>
<context position="14204" citStr="Pynadath and Tambe, 2002" startWordPosition="2266" endWordPosition="2269">sage σ ∈ E has a semantics, or distribution over the state space Pr(s|σ). In the Cards task, we use the semantic distributions defined in Section 3. To combine the semantics of language with the standard POMDP observation model, we apply Bayes’ rule: Pr(s|σ)Pr(σ) Pr(σ|s) = (1) �σ0 Pr(s|σ0)Pr(σ0) The prior, Pr(σ), can be derived from corpus data. By treating language as just another form of observation, we are able to leverage existing POMDP solution algorithms. This approach contrasts with previous work on communication in Dec-POMDPs, where agents directly share their perceptual observations (Pynadath and Tambe, 2002; Spaan et al., 2008), an assumption which does not fit natural language. Reward The reward function R(s,a) : S → R represents the goals of the agent, who chooses actions to maximize reward. The goal of the Cards task is for both players to be on top of the card, so any action that leads to this state receives a high reward R+. All other actions receive a small negative reward R−, which gives agents an incentive to finish the task as quickly as possible. R((p,c),a) = R+ p = c R− p =6c Lastly, γ ∈ [0,1) is the discount factor, specifying the trade-off between immediate and future rewards. T((p0</context>
</contexts>
<marker>Pynadath, Tambe, 2002</marker>
<rawString>David V. Pynadath and Milind Tambe. 2002. The communicative multiagent team decision problem: Analyzing teamwork theories and models. Journal of Artificial Intelligence Research, 16:2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannah Rohde</author>
<author>Scott Seyfarth</author>
<author>Brady Clark</author>
<author>Gerhard J¨ager</author>
<author>Stefan Kaufmann</author>
</authors>
<title>Communicating with cost-based implicature: A game-theoretic approach to ambiguity.</title>
<date>2012</date>
<booktitle>In The 16th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<location>Paris,</location>
<marker>Rohde, Seyfarth, Clark, J¨ager, Kaufmann, 2012</marker>
<rawString>Hannah Rohde, Scott Seyfarth, Brady Clark, Gerhard J¨ager, and Stefan Kaufmann. 2012. Communicating with cost-based implicature: A game-theoretic approach to ambiguity. In The 16th Workshop on the Semantics and Pragmatics of Dialogue, Paris, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert van Rooy</author>
</authors>
<title>Questioning to resolve decision problems.</title>
<date>2003</date>
<journal>Linguistics and Philosophy,</journal>
<volume>26</volume>
<issue>6</issue>
<pages>763</pages>
<marker>van Rooy, 2003</marker>
<rawString>Robert van Rooy. 2003. Questioning to resolve decision problems. Linguistics and Philosophy, 26(6):727– 763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seymour Rosenberg</author>
<author>Bertram D Cohen</author>
</authors>
<title>Speakers’ and listeners’ processes in a word communication task.</title>
<date>1964</date>
<journal>Science,</journal>
<pages>145--1201</pages>
<contexts>
<context position="2125" citStr="Rosenberg and Cohen, 1964" startWordPosition="308" endWordPosition="311">ts to derive the maxims (or perhaps just their effects) from more basic cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 20</context>
</contexts>
<marker>Rosenberg, Cohen, 1964</marker>
<rawString>Seymour Rosenberg and Bertram D. Cohen. 1964. Speakers’ and listeners’ processes in a word communication task. Science, 145:1201–1203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthijs T J Spaan</author>
<author>Nikos Vlassis</author>
</authors>
<title>Perseus: Randomized point-based value iteration for POMDPs.</title>
<date>2005</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>220</pages>
<contexts>
<context position="4355" citStr="Spaan and Vlassis, 2005" startWordPosition="638" endWordPosition="641">istributions over the underlying, hidden world state, including the beliefs of the other players, and speech actions change those beliefs. In this setting, informative, relevant communication emerges as the best way to maximize joint utility. The complexity of pragmatic reasoning is still forbidding, though. Correspondingly, optimal decision making in Dec-POMDPs is NEXP complete (Bernstein et al., 2002). To manage this issue, we introduce several cognitively-plausible approximations which allow us to simplify the Dec-POMDP to a single-agent POMDP, for which relatively efficient solvers exist (Spaan and Vlassis, 2005). We demonstrate our algorithms on a variation of the Cards task, a partially-observable collaborative search problem (Potts, 2012). Spatial language comprises the bulk of communication in the Cards task, and we discuss a model of spatial semantics in Section 3. Using this task and a model of the meaning of spatial language, we next discuss two agents that play the game: ListenerBot (Section 4) makes decisions using a single-agent POMDP that does not take into account the beliefs or actions of its partner, whereas DialogBot (Section 5) maintains a model of its partner’s beliefs. As a result of</context>
<context position="17797" citStr="Spaan and Vlassis, 2005" startWordPosition="2884" endWordPosition="2887">d the process continues. An initial belief state b0 and a policy π together define a Markov chain over pairs of states and actions. For a given policy π, we define a value function Vπ : Δ(S) → R which represents the expected discounted reward with respect to that Markov chain: ∞ Vπ(b0) = ∑ γt E[R(bt,at)|b0,π] t=0 The goal of the agent is find a policy π∗ which maximizes the value of the initial belief state: π∗ = argmax π Exact computation of π∗ is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), making approximation algorithms necessary for all but the simplest problems. We use Perseus (Spaan and Vlassis, 2005), an anytime approximate point-based value itVπ(b0) 1076 eration algorithm. 5 DialogBot We now introduce DialogBot, a Cards agent which is capable of producing linguistic advice. To decide when and how to speak, DialogBot maintains a distribution over its partner’s beliefs and reasons about the effects his utterances will have on those beliefs. To handle these complexities, DialogBot models the world as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) (Bernstein et al., 2002). See Figure 3(b) for the influence diagram. The definition of Dec-POMDPs mirrors that of the PO</context>
</contexts>
<marker>Spaan, Vlassis, 2005</marker>
<rawString>Matthijs T. J. Spaan and Nikos Vlassis. 2005. Perseus: Randomized point-based value iteration for POMDPs. Journal ofArtificial Intelligence Research, 24(1):195– 220, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthijs T J Spaan</author>
<author>Frans A Oliehoek</author>
<author>Nikos Vlassis</author>
</authors>
<title>Multiagent planning under uncertainty with stochastic communication delays. In</title>
<date>2008</date>
<booktitle>In Proc. of the 18th Int. Conf. on Automated Planning and Scheduling,</booktitle>
<pages>338--345</pages>
<contexts>
<context position="14225" citStr="Spaan et al., 2008" startWordPosition="2270" endWordPosition="2273">, or distribution over the state space Pr(s|σ). In the Cards task, we use the semantic distributions defined in Section 3. To combine the semantics of language with the standard POMDP observation model, we apply Bayes’ rule: Pr(s|σ)Pr(σ) Pr(σ|s) = (1) �σ0 Pr(s|σ0)Pr(σ0) The prior, Pr(σ), can be derived from corpus data. By treating language as just another form of observation, we are able to leverage existing POMDP solution algorithms. This approach contrasts with previous work on communication in Dec-POMDPs, where agents directly share their perceptual observations (Pynadath and Tambe, 2002; Spaan et al., 2008), an assumption which does not fit natural language. Reward The reward function R(s,a) : S → R represents the goals of the agent, who chooses actions to maximize reward. The goal of the Cards task is for both players to be on top of the card, so any action that leads to this state receives a high reward R+. All other actions receive a small negative reward R−, which gives agents an incentive to finish the task as quickly as possible. R((p,c),a) = R+ p = c R− p =6c Lastly, γ ∈ [0,1) is the discount factor, specifying the trade-off between immediate and future rewards. T((p0,c0)|travel(r),(p,c))</context>
</contexts>
<marker>Spaan, Oliehoek, Vlassis, 2008</marker>
<rawString>Matthijs T. J. Spaan, Frans A. Oliehoek, and Nikos Vlassis. 2008. Multiagent planning under uncertainty with stochastic communication delays. In In Proc. of the 18th Int. Conf. on Automated Planning and Scheduling, pages 338–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Stiller</author>
<author>Noah D Goodman</author>
<author>Michael C Frank</author>
</authors>
<title>Ad-hoc scalar implicature in adults and children.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Cognitive Science Society,</booktitle>
<location>Boston,</location>
<contexts>
<context position="2192" citStr="Stiller et al., 2011" startWordPosition="320" endWordPosition="323">cognitive principles concerning how people make decisions, formulate plans, and collaborate to achieve goals. This research traces to early work by Lewis (1969) on signaling systems. It has recently been the subject of extensive theoretical discussion (Clark, 1996; Merin, 1997; Blutner, 1998; Parikh, 2001; Beaver, 2002; van Rooy, 2003; Benz et al., 2005; Franke, 2009) and has been tested experimentally using one-step games in which the speaker produces a message and the hearer ventures a guess as to its intended referent (Rosenberg and Cohen, 1964; Dale and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large </context>
</contexts>
<marker>Stiller, Goodman, Frank, 2011</marker>
<rawString>Alex Stiller, Noah D. Goodman, and Michael C. Frank. 2011. Ad-hoc scalar implicature in adults and children. In Proceedings of the 33rd Annual Meeting of the Cognitive Science Society, Boston, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Richmond Thomason</author>
<author>David DeVault</author>
</authors>
<title>Enlightened update: A computational architecture for presupposition and other pragmatic phenomena.</title>
<date>2007</date>
<note>To appear in</note>
<contexts>
<context position="2747" citStr="Stone et al., 2007" startWordPosition="407" endWordPosition="410">le and Reiter, 1995; Golland et al., 2010; Stiller et al., 2011; Frank and Goodman, 2012; Krahmer and van Deemter, 2012; Degen and Franke, 2012; Rohde et al., 2012). To date, however, these theoretical models and experiments have not been extended to multi-step interactions extending over time and involving both language and action together, which leaves this work relatively disconnected from research on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their efficient implementation. With the current paper, we seek to show that the Decentraliz</context>
</contexts>
<marker>Stone, Thomason, DeVault, 2007</marker>
<rawString>Matthew Stone, Richmond Thomason, and David DeVault. 2007. Enlightened update: A computational architecture for presupposition and other pragmatic phenomena. To appear in Donna K. Byron; Craige Roberts; and Scott Schwenter, Presupposition Accommodation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems.</title>
<date>2010</date>
<journal>Comput. Speech Lang.,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>588</pages>
<contexts>
<context position="3155" citStr="Thomson and Young, 2010" startWordPosition="470" endWordPosition="473">ch on planning and goal-orientation in artificial agents (Perrault and Allen, 1980; Allen, 1991; Grosz and Sidner, 1986; Bratman, 1987; Hobbs et al., 1993; Allen et al., 2007; DeVault et al., 2005; Stone et al., 2007; DeVault, 2008). We attribute this in large part to the complexity of Gricean reasoning itself, which requires agents to model each other’s belief states. Tracking these as they evolve over time in response to experiences is extremely demanding. Our approach complements slot-filling dialog systems, where the focus is on managing speech recognition uncertainty (Young et al., 2010; Thomson and Young, 2010). However, recent years have seen significant advances in multi-agent decision-theoretic models and their efficient implementation. With the current paper, we seek to show that the Decentralized Par1072 Proceedings of NAACL-HLT 2013, pages 1072–1081, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics tially Observable Markov Decision Process (DecPOMDP) provides a robust, flexible foundation for implementing agents that communicate in a Gricean manner. Dec-POMDPs are multi-agent, partiallyobservable models in which agents maintain belief distributions over the un</context>
</contexts>
<marker>Thomson, Young, 2010</marker>
<rawString>Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems. Comput. Speech Lang., 24(4):562– 588, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gaˇsi´c</author>
<author>Simon Keizer</author>
<author>Franc¸ois Mairesse</author>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>The hidden information state model: A practical framework for pomdp-based spoken dialogue management.</title>
<date>2010</date>
<journal>Comput. Speech Lang.,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The hidden information state model: A practical framework for pomdp-based spoken dialogue management. Comput. Speech Lang., 24(2):150–174, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>