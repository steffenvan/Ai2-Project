<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000156">
<title confidence="0.999788666666667">
A Geometric Interpretation of
Non-Target-Normalized Maximum Cross-channel Correlation
for Vocal Activity Detection in Meetings
</title>
<author confidence="0.980912">
Kornel Laskowski Tanja Schultz
</author>
<affiliation confidence="0.892194">
interACT, Universitat Karlsruhe interACT, Carnegie Mellon University
Karlsruhe, Germany Pittsburgh PA, USA
</affiliation>
<email confidence="0.9971">
kornel@ira.uka.de tanja@cs.cmu.edu
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833470588235">
Vocal activity detection is an impor-
tant technology for both automatic speech
recognition and automatic speech under-
standing. In meetings, standard vocal
activity detection algorithms have been
shown to be ineffective, because partici-
pants typically vocalize for only a frac-
tion of the recorded time and because,
while they are not vocalizing, their channels
are frequently dominated by crosstalk from
other participants. In the present work,
we review a particular type of normaliza-
tion of maximum cross-channel correlation,
a feature recently introduced to address the
crosstalk problem. We derive a plausible
geometric interpretation and show how the
frame size affects performance.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731">
Vocal activity detection (VAD) is an important tech-
nology for any application with an automatic speech
recognition (ASR) front end. In meetings, partic-
ipants typically vocalize for only a fraction of the
recorded time. Their temporally contiguous contri-
butions should be identified prior to ASR in order to
leverage speaker adaptation schemes and language
model constraints, and to associate recognized out-
put with specific speakers (who said what). Segmen-
tation into such contributions is informed primarily
by VAD on a frame-by-frame basis.
Individual head-mounted microphone (IHM)
recordings of meetings present a particular challenge
for VAD, due to crosstalk from other participants.
Most state-of-the-art VAD systems for meetings rely
on decoding in a binary speech/non-speech space,
assuming independence among participants, but are
</bodyText>
<page confidence="0.997471">
89
</page>
<bodyText confidence="0.999426333333333">
increasingly relying on features specifically designed
to address the crosstalk issue (Wrigley et al., 2005).
A feature which has attracted attention since its
use in VAD post-processing in (Pfau et al., 2001)
is the maximum cross-channel correlation (XC),
maxτ Ojk (T), between channels j and k, where T is
the lag. When designing features descriptive of the
kth channel, XC is frequently normalized by the en-
ergy in the target&apos; channel k (Wrigley et al., 2003).
Alternately, XC can be normalized by the energy in
the non-target channel j (Laskowski et al., 2004),
a normalization which we refer to here as NT-Norm,
extending the Norm and S-Norm naming conventions
in (Wrigley et al., 2005). Table 1 shows several types
of normalizations which have been explored.
</bodyText>
<table confidence="0.7291265">
Normalization of XC Mean Min Max
(none) maxj#k φjk(τ) [2][4] [2][4] [2][4]
Norm [2][4] [2][4] [2][4]
maxj��k φjk(τ)
φkk(0)
S-Norm maxji4k φjk(τ) [2] [4] [5] [2] [4] [1] [2] [4]
√φjj(0)φkk(0)
NT-Norm maxj�/,,` φjk(τ) 3 6 6
[ ] [ ] [ ]
`Vj7 (0)
</table>
<tableCaption confidence="0.95283">
Table 1: Normalizations and statistics of cross-
</tableCaption>
<bodyText confidence="0.870273111111111">
channel correlation features to describe channel k.
In [1], a median-smoothed version was used in post-
processing. In [3], the sum (JMXC) was used in-
stead of the mean. In [5], cross-correlation was com-
puted over samples and features. In [6], the mini-
mum and the maximum were jointly referred to as
NMXC. References in bold depict features selected
by an automatic feature selection algorithm in [2] and
[4]. (1:(Pfau et al., 2001), 2:(Wrigley et al., 2003),
</bodyText>
<note confidence="0.669048">
3:(Laskowski et al., 2004), 4:(Wrigley et al., 2005),
5:(Huang, 2005), 6:(Boakye and Stolcke, 2006))
</note>
<tableCaption confidence="0.5263385">
&apos;The target/non-target terms are due to (Boakye and
Stolcke, 2006).
</tableCaption>
<subsubsectionHeader confidence="0.330937">
Proceedings of NAACL HLT 2007, Companion Volume, pages 89–92,
</subsubsectionHeader>
<bodyText confidence="0.9279366">
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
The present work revisits NT-Norm normalization,
which has been successfully used in a threshold de-
tector (Laskowski et al., 2004), in automatic initial
label assignment (Laskowski and Schultz, 2006), and
as part of a two-state decoder feature vector (Boakye
and Stolcke, 2006). Our main contribution is a geo-
metric interpretation of NT-Norm XC, in Section 2.
We also describe, in Section 3, several contrastive
experiments, and discuss the results in Section 4.
</bodyText>
<equation confidence="0.955519">
ξk,j = ,
φ
∀j6=k (1)
jj
</equation>
<bodyText confidence="0.999554">
We assume the simplified response in the kth IHM
microphone at a distance dk from a single point
source s (t) to be
</bodyText>
<equation confidence="0.9771442">
� 1 � � �
mk (t) . t − dk
= Ak s + ηk (t) , (2)
c
dk
</equation>
<bodyText confidence="0.9676188">
where c, Ak and ηk (t) are the speed of sound, the
gain of microphone k, an
d source-uncorrelated noise
at microphone k, respectively. Cross-channel corre-
lation is approximated over a frame of size 52 by
</bodyText>
<equation confidence="0.981236111111111">
Pηk≡RΩη2k (t) dt,
!,2
1 (4)
j d2j
max
j (
) = dj
k Ps
(5)
</equation>
<bodyText confidence="0.988003">
respectively, as the maximum of
</bodyText>
<equation confidence="0.929243928571429">
φ
τ
dk
,
τ
φjk (τ) occurs at
τ* = (dk − dj) /c. In consequence,
maxτφjk(τ)φjj (0) ≈ dj , (6)
dk
provided that  
Ak Pη�
1 −  ≈ 1 , (7)
1 d� Ps + Pη�
�
</equation>
<bodyText confidence="0.9585024">
i.e., under assumptions of similar microphone gains,
a non-negligible farfield signal-to-noise ratio at each
microphone, and the simplifications embodied in
Equation 2, NT-Norm XC approximates the relative
τ*
</bodyText>
<subsectionHeader confidence="0.452311">
ame-level energy flooring
</subsectionHeader>
<bodyText confidence="0.997053541666667">
followed by cross-channel normalization (NLED).
90 distances of 2 microphones to the single point source
s (t). We stress that this approximation requires no
side knowledge about the true positions of the par-
ticipants or of their microphones.
Importantly, this interpretation is valid only if
lies within the integration window 52 in Equation 3.
In (Boakye and Stolcke, 2006), the authors showed
that when the analysis window is 25 ms, the NMXC
feature is not as robust as fr
non-speech
activity of each partipant indepen-
dently, the system implements a Viterbi search for
the best path through a 2K-state vocal interac-
tion space, where K is the number of participants.
Segmentation consists of three passes: initial la-
bel assignment (ILA), described in the next subsec-
tion, for acoustic model training; simultaneous multi-
participant Viterbi decoding; and smoothing to pro-
duce segments for ASR. In the current work, during
decoding, we limit the maximum number of simulta-
neously vocalizing participants to 3.
This system is an improved version of that fielded
in the NIST Rich Tran
</bodyText>
<equation confidence="0.543216">
(N)
</equation>
<bodyText confidence="0.8445032">
scription 2006 Meeting Recog-
nition evaluation (RT06s)2, to produce automatic
segmentation in the IHM condition on conference
meetings. The ASR system which we use in this
paper is as described in (Fiigen et al., 2007).
</bodyText>
<subsectionHeader confidence="0.995255">
3.2 Unsupervised ILA
</subsectionHeader>
<bodyText confidence="0.9826785">
For unsupervised labeling of the test audio, prior to
acoustic model training, we employ the criterion
</bodyText>
<equation confidence="0.984786">
�max� φ�k(τ) �
log &gt; 0
φ��(0)
N otherwise .
</equation>
<bodyText confidence="0.990060333333333">
Assuming equality in Equation 6, this corresponds
to declaring a participant as vocalizing when the dis-
tance between the location of the dominant sound
source and that participant’s microphone is smaller
than the geometric mean of the distances from the
source to the remaining microphones, ie. when
</bodyText>
<equation confidence="0.7333755">
sY
dj &gt; dk
�−�
j�=k
2http://www.nist.gov/speech/tests/rt/
2 Geometric Interpretation
</equation>
<bodyText confidence="0.97523">
We propose an interpretable geometric approxima-
tion to NT-Norm XC for channel k,
</bodyText>
<figure confidence="0.997441357142857">
maxτ φjk (τ)



[k] =
54
X
j
k
(g)
φjk (τ) = ZΩdj dkks(t)s(t − τ)dt ,
(3)
j
where
τ≡
(dj
−
dk) /c. Letting
Ps≡RΩs2
(t) dt and
Aj
3 Experimental Setup
3.1 VAD and ASR Systems
Our multispeaker VAD system, shown in Figure 1,
was introduced in (Laskowski and Schultz, 2006).
Rather than detecting the 2-state speech (V) vs.
El
V if
(9)
AM
ILA
AM
TRAINING
VITERBI
DECODING
multichannel audio
REFRAMING
ASR
Pass 1
SMOOTHING
0
4F 4∗ ° (4∗) WER
</figure>
<figureCaption confidence="0.999978">
Figure 1: VAD system architecture, with 4 error measurement points. Symbols as in the text.
</figureCaption>
<bodyText confidence="0.999762777777778">
We refer to this algorithm as ILAave. For contrast we
also consider ILAmin, with the sum in Equation 8 re-
placed by the minimum over j#k. This corresponds
to declaring a participant as vocalizing when the dis-
tance between the location of the dominant sound
source and that participant’s microphone is smaller
than the distance from the source to any other mi-
crophone. We do not consider ILAmax, whose inter-
pretation in light of Equation 6 is not useful.
</bodyText>
<subsectionHeader confidence="0.993174">
3.3 Data
</subsectionHeader>
<bodyText confidence="0.999957555555556">
The data used in the described experiments con-
sist of two datasets from the NIST RT-05s and
RT-06s evaluations. The data which had been
used for VAD system improvement, rt05s eval*,
is the complete rt05s eval set less one meeting,
NIST 20050412-1303. This meeting was excluded
as it contains a participant without a microphone, a
condition known a priori to be absent in rt06s eval;
we use the latter in its entirety.
</bodyText>
<subsectionHeader confidence="0.998601">
3.4 Description of Experiments
</subsectionHeader>
<bodyText confidence="0.999962228571428">
The experiments we present aim to compare ILAave
and ILAmin, and to show how the size of the inte-
gration window, Q, affects system performance. As
our VAD decoder operates at a frame size of 100ms,
we introduce a reframing step between the ILA com-
ponent and both AM training and decoding; see Fig-
ure 1. V is assigned to each 100ms frame if 50% or
more of the frame duration is assigned V by ILA;
otherwise, the 100ms frame is assigned an N label.
We measure performance in four locations within
the combined VAD+ASR system architecture, also
shown in Figure 1. We compute a VAD frame er-
ror just after reframing (CIF), just after decoding
(q*), and just after smoothing (u (q*)). This er-
ror is the sum of the miss rate (MS), and the false
alarm rate excluding intervals of all-participant si-
lence (FAX), computed against unsmoothed word-
level forced alignment references. We use this met-
ric for comparative purposes only, across the vari-
ous measurement points. We also use first-pass ASR
word error rates (WERs), after lattice rescoring, as
a final measure of performance impact.
We evaluate, over a range of ILA frame sizes, the
performance of ILAave(3), with a maximum number
of simultaneously vocalizing participants of 3, and
for the contrastive ILAmin. We note that ILAmin
is capable of declaring at most one microphone at a
time as being worn by a current speaker. As a re-
sult, construction of acoustic models for overlapped
vocal activity states, described in (Laskowski and
Schultz, 2006), results in states of at most 2 simul-
taneously vocalizing participants. We therefore refer
to ILAmin as ILAmin(2), and additionally consider
ILAave(2), in which states with 3 simultaneously vo-
calizing participants are removed.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="introduction">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999721192307692">
We show the results of our experiments in Ta-
ble 2. First-pass WERs, using reference segmenta-
tion (.stm), vary by 1.3% absolute (abs) between
rt05s eval and rt06s eval. We also note that re-
moving the one meeting with a participant without
a microphone reduces the rt05s eval manual seg-
mentation WER by 1.7% abs. WERs obtained with
automatic segmentation should be compared to the
manual segmentation WERs for each set.
As the qF columns shows, ILAmin(2) entails sig-
nificantly more VAD errors than ILAave. Notably,
although we do not show the breakdown, ILAmin(2)
is characterized by fewer false alarms, but misses
much more speech than ILAave(2). This is due in
part to its inability to identify simultaneous talk-
ers. However, following acoustic model training and
use (q*), the VAD error rates between the two algo-
rithms are approximately equal.
In studying the WERs for each ILA algorithm in-
dependently, the variation across ILA frame sizes in
the range 25–100 ms can be significant: for example,
it is 1.2% abs for ILAmin(2) on rt06s eval, com-
pared to the difference with manual segmentation of
3.1% abs. Error curves, as a function of ILA frame
size, are predominantly shallow parabolas, except at
75 ms (notably for ILAmin(2) at qF); we believe that
</bodyText>
<page confidence="0.99454">
91
</page>
<table confidence="0.999890666666667">
ILA Q VAD, rt05s WER, 1st pass
CIF q* u (q*) 05 05* 06
a 100 31.3 16.7 16.0 39.0 34.1 39.6
v 75 33.6 16.6 15.9 38.9 34.1 39.9
e 50 35.2 16.7 16.0 38.8 34.0 39.3
3 25 36.8 17.3 16.3 39.6 34.2 39.7
a 100 31.3 15.8 15.2 37.8 34.4 39.7
v 75 33.6 15.6 15.0 37.9 34.4 39.6
e 50 35.2 15.8 15.2 37.6 34.3 39.3
2 25 36.8 16.4 15.6 38.1 34.3 39.5
m 100 43.4 15.8 14.7 38.2 35.2 39.3
i 75 51.9 15.6 14.6 38.1 35.2 39.3
n 50 47.1 15.7 14.6 37.9 35.1 40.1
2 25 47.7 16.2 14.9 38.1 35.4 40.5
refs 9.5 9.5 9.5 36.1 34.4 37.4
</table>
<tableCaption confidence="0.986699">
Table 2: VAD errors, measured at three points in our
</tableCaption>
<bodyText confidence="0.9928326">
system, and first-pass WERs for rt05s eval (05),
as well as first-pass WERs for rt05s eval* (05*)
and rt06s eval (06). Results are shown for 3 con-
trastive VAD systems (ILAave(3), ILAave(2) and
ILAmin(2)), and 4 ILA frame sizes (100ms, 75ms,
50ms, and 25ms).
this is because 75 ms does not divide evenly into the
decoder frame size of 100 ms, causing more deletions
across the reframing step than for other ILA frame
sizes. Error minima appear for an ILA frame size
somewhere between 50 ms and 75 ms, for both ASR
and post-decoding VAD errors.
Although (Pfau et al., 2001) considered a maxi-
mum lag of 250 samples (15.6ms, or 5m at the speed
of sound), their computation of S-Norm XC used
a rectangular window. Here, as in (Laskowski and
Schultz, 2006) and (Boakye and Stolcke, 2006), we
use a Hamming window. Our results suggest that a
large, broadly tapered window is important for Equa-
tion 6 to hold.
The table also shows that for datasets with-
out uninstrumented participants, rt05s eval*
and rt06s eval, ILAmin(2) is outperformed by
ILAave(2) by as much as 1.1% abs in WER, espe-
cially at small frame sizes. The difference for the full
rt05s eval dataset is smaller. The results also sug-
gest that reducing the maximum degree of simulta-
neous vocalization from 3 to 2 during decoding is an
effective means of reducing errors (ASR insertions,
not shown) for uninstrumented participants.
</bodyText>
<sectionHeader confidence="0.999715" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999995625">
We have derived a geometric approximation for a
particular type of normalization of maximum cross-
channel correlation, NT-Norm XC, recently intro-
duced for multispeaker vocal activity detection. Our
derivation suggests that it is effectively comparing
the distance between each speaker’s mouth and each
microphone. This is novel, as geometry is most often
inferred using the lag of the crosscorrelation maxi-
mum, rather than its amplitude.
Our experiments suggest that frame sizes of 50–75
ms lead to WERs which are lower than those for ei-
ther 100 ms or 25 ms by as much as 1.2% abs; that
ILAave outperforms ILAmin as an initial label as-
signment criterion; and that reducing the degree of
simultaneous vocalization during decoding may ad-
dress problems due to uninstrumented participants.
</bodyText>
<sectionHeader confidence="0.999746" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.998603666666667">
This work was partly supported by the European
Union under the integrated project CHIL (IST-
506909), Computers in the Human Interaction Loop.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999291636363636">
K. Boakye and A. Stolcke. 2006. Improved Speech
Activity Detection Using Cross-Channel Features for
Recognition of Multiparty Meetings. Proc. of INTER-
SPEECH, Pittsburgh PA, USA, pp1962–1965.
C. Fdgen, S. Ikbal, F. Kraft, K. Kumatani, K. Laskowski,
J. McDonough, M. Ostendorf, S. Striker, and M.
Wolfel. 2007. The ISL RT-06S Speech-to-Text Evalu-
ation System. Proc. of MLMI, Springer Lecture Notes
in Computer Science 4299, pp407–418.
Z. Huang and M. Harper. 2005. Speech Activity Detec-
tion on Multichannels of Meeting Recordings. Proc. of
MLMI, Springer Lecture Notes in Computer Science
3869, pp415–427.
K. Laskowski, Q. Jin, and T. Schultz. 2004.
Crosscorrelation-based Multispeaker Speech Activity
Detection. Proc. of INTERSPEECH, Jeju Island,
South Korea, pp973–976.
K. Laskowski and T. Schultz. 2006. Unsupervised Learn-
ing of Overlapped Speech Model Parameters for Multi-
channel Speech Activity Detection in Meetings. Proc.
of ICASSP, Toulouse, France, I:993–996.
T. Pfau and D. Ellis and A. Stolcke. 2001. Multi-
speaker Speech Activity Detection for the ICSI Meet-
ing Recorder. Proc. of ASRU, Madonna di Campiglio,
Italy, pp107–110.
S. Wrigley, G. Brown, V. Wan, and S. Renals. 2003.
Feature Selection for the Classification of Crosstalk
in Multi-Channel Audio. Proc. of EUROSPEECH,
Geneva, Switzerland, pp469–472.
S. Wrigley, G. Brown, V. Wan, and S. Renals. 2005.
Speech and Crosstalk Detection in Multichannel Au-
dio. IEEE Trans. on Speech and Audio Processing,
13:1, pp84–91.
</reference>
<page confidence="0.99591">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980011">
<title confidence="0.998381666666667">A Geometric Interpretation of Non-Target-Normalized Maximum Cross-channel for Vocal Activity Detection in Meetings</title>
<author confidence="0.999275">Kornel Laskowski Tanja Schultz</author>
<affiliation confidence="0.999607">interACT, Universitat Karlsruhe interACT, Carnegie Mellon University</affiliation>
<address confidence="0.999959">Karlsruhe, Germany Pittsburgh PA, USA</address>
<email confidence="0.999332">kornel@ira.uka.detanja@cs.cmu.edu</email>
<abstract confidence="0.999237277777778">Vocal activity detection is an important technology for both automatic speech recognition and automatic speech understanding. In meetings, standard vocal activity detection algorithms have been shown to be ineffective, because participants typically vocalize for only a fraction of the recorded time and because, while they are not vocalizing, their channels are frequently dominated by crosstalk from other participants. In the present work, we review a particular type of normalization of maximum cross-channel correlation, a feature recently introduced to address the crosstalk problem. We derive a plausible geometric interpretation and show how the frame size affects performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Boakye</author>
<author>A Stolcke</author>
</authors>
<title>Improved Speech Activity Detection Using Cross-Channel Features for Recognition of Multiparty Meetings.</title>
<date>2006</date>
<booktitle>Proc. of INTERSPEECH,</booktitle>
<pages>1962--1965</pages>
<location>Pittsburgh PA, USA,</location>
<contexts>
<context position="3464" citStr="Boakye and Stolcke, 2006" startWordPosition="522" endWordPosition="525"> [ ] [ ] [ ] `Vj7 (0) Table 1: Normalizations and statistics of crosschannel correlation features to describe channel k. In [1], a median-smoothed version was used in postprocessing. In [3], the sum (JMXC) was used instead of the mean. In [5], cross-correlation was computed over samples and features. In [6], the minimum and the maximum were jointly referred to as NMXC. References in bold depict features selected by an automatic feature selection algorithm in [2] and [4]. (1:(Pfau et al., 2001), 2:(Wrigley et al., 2003), 3:(Laskowski et al., 2004), 4:(Wrigley et al., 2005), 5:(Huang, 2005), 6:(Boakye and Stolcke, 2006)) &apos;The target/non-target terms are due to (Boakye and Stolcke, 2006). Proceedings of NAACL HLT 2007, Companion Volume, pages 89–92, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics The present work revisits NT-Norm normalization, which has been successfully used in a threshold detector (Laskowski et al., 2004), in automatic initial label assignment (Laskowski and Schultz, 2006), and as part of a two-state decoder feature vector (Boakye and Stolcke, 2006). Our main contribution is a geometric interpretation of NT-Norm XC, in Section 2. We also describe, in Section 3, </context>
<context position="5386" citStr="Boakye and Stolcke, 2006" startWordPosition="864" endWordPosition="867"> d� Ps + Pη� � i.e., under assumptions of similar microphone gains, a non-negligible farfield signal-to-noise ratio at each microphone, and the simplifications embodied in Equation 2, NT-Norm XC approximates the relative τ* ame-level energy flooring followed by cross-channel normalization (NLED). 90 distances of 2 microphones to the single point source s (t). We stress that this approximation requires no side knowledge about the true positions of the participants or of their microphones. Importantly, this interpretation is valid only if lies within the integration window 52 in Equation 3. In (Boakye and Stolcke, 2006), the authors showed that when the analysis window is 25 ms, the NMXC feature is not as robust as fr non-speech activity of each partipant independently, the system implements a Viterbi search for the best path through a 2K-state vocal interaction space, where K is the number of participants. Segmentation consists of three passes: initial label assignment (ILA), described in the next subsection, for acoustic model training; simultaneous multiparticipant Viterbi decoding; and smoothing to produce segments for ASR. In the current work, during decoding, we limit the maximum number of simultaneous</context>
<context position="12740" citStr="Boakye and Stolcke, 2006" startWordPosition="2147" endWordPosition="2150">AD systems (ILAave(3), ILAave(2) and ILAmin(2)), and 4 ILA frame sizes (100ms, 75ms, 50ms, and 25ms). this is because 75 ms does not divide evenly into the decoder frame size of 100 ms, causing more deletions across the reframing step than for other ILA frame sizes. Error minima appear for an ILA frame size somewhere between 50 ms and 75 ms, for both ASR and post-decoding VAD errors. Although (Pfau et al., 2001) considered a maximum lag of 250 samples (15.6ms, or 5m at the speed of sound), their computation of S-Norm XC used a rectangular window. Here, as in (Laskowski and Schultz, 2006) and (Boakye and Stolcke, 2006), we use a Hamming window. Our results suggest that a large, broadly tapered window is important for Equation 6 to hold. The table also shows that for datasets without uninstrumented participants, rt05s eval* and rt06s eval, ILAmin(2) is outperformed by ILAave(2) by as much as 1.1% abs in WER, especially at small frame sizes. The difference for the full rt05s eval dataset is smaller. The results also suggest that reducing the maximum degree of simultaneous vocalization from 3 to 2 during decoding is an effective means of reducing errors (ASR insertions, not shown) for uninstrumented participan</context>
</contexts>
<marker>Boakye, Stolcke, 2006</marker>
<rawString>K. Boakye and A. Stolcke. 2006. Improved Speech Activity Detection Using Cross-Channel Features for Recognition of Multiparty Meetings. Proc. of INTERSPEECH, Pittsburgh PA, USA, pp1962–1965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fdgen</author>
<author>S Ikbal</author>
<author>F Kraft</author>
<author>K Kumatani</author>
<author>K Laskowski</author>
<author>J McDonough</author>
<author>M Ostendorf</author>
<author>S Striker</author>
<author>M Wolfel</author>
</authors>
<date>2007</date>
<booktitle>The ISL RT-06S Speech-to-Text Evaluation System. Proc. of MLMI, Springer Lecture Notes in Computer Science 4299,</booktitle>
<pages>407--418</pages>
<marker>Fdgen, Ikbal, Kraft, Kumatani, Laskowski, McDonough, Ostendorf, Striker, Wolfel, 2007</marker>
<rawString>C. Fdgen, S. Ikbal, F. Kraft, K. Kumatani, K. Laskowski, J. McDonough, M. Ostendorf, S. Striker, and M. Wolfel. 2007. The ISL RT-06S Speech-to-Text Evaluation System. Proc. of MLMI, Springer Lecture Notes in Computer Science 4299, pp407–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Harper</author>
</authors>
<title>Speech Activity Detection on Multichannels of Meeting Recordings.</title>
<date>2005</date>
<booktitle>Proc. of MLMI, Springer Lecture Notes in Computer Science 3869,</booktitle>
<pages>415--427</pages>
<marker>Huang, Harper, 2005</marker>
<rawString>Z. Huang and M. Harper. 2005. Speech Activity Detection on Multichannels of Meeting Recordings. Proc. of MLMI, Springer Lecture Notes in Computer Science 3869, pp415–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Laskowski</author>
<author>Q Jin</author>
<author>T Schultz</author>
</authors>
<title>Crosscorrelation-based Multispeaker Speech Activity Detection.</title>
<date>2004</date>
<booktitle>Proc. of INTERSPEECH, Jeju Island, South Korea,</booktitle>
<pages>973--976</pages>
<contexts>
<context position="2417" citStr="Laskowski et al., 2004" startWordPosition="347" endWordPosition="350">pace, assuming independence among participants, but are 89 increasingly relying on features specifically designed to address the crosstalk issue (Wrigley et al., 2005). A feature which has attracted attention since its use in VAD post-processing in (Pfau et al., 2001) is the maximum cross-channel correlation (XC), maxτ Ojk (T), between channels j and k, where T is the lag. When designing features descriptive of the kth channel, XC is frequently normalized by the energy in the target&apos; channel k (Wrigley et al., 2003). Alternately, XC can be normalized by the energy in the non-target channel j (Laskowski et al., 2004), a normalization which we refer to here as NT-Norm, extending the Norm and S-Norm naming conventions in (Wrigley et al., 2005). Table 1 shows several types of normalizations which have been explored. Normalization of XC Mean Min Max (none) maxj#k φjk(τ) [2][4] [2][4] [2][4] Norm [2][4] [2][4] [2][4] maxj��k φjk(τ) φkk(0) S-Norm maxji4k φjk(τ) [2] [4] [5] [2] [4] [1] [2] [4] √φjj(0)φkk(0) NT-Norm maxj�/,,` φjk(τ) 3 6 6 [ ] [ ] [ ] `Vj7 (0) Table 1: Normalizations and statistics of crosschannel correlation features to describe channel k. In [1], a median-smoothed version was used in postprocess</context>
<context position="3802" citStr="Laskowski et al., 2004" startWordPosition="570" endWordPosition="573">re jointly referred to as NMXC. References in bold depict features selected by an automatic feature selection algorithm in [2] and [4]. (1:(Pfau et al., 2001), 2:(Wrigley et al., 2003), 3:(Laskowski et al., 2004), 4:(Wrigley et al., 2005), 5:(Huang, 2005), 6:(Boakye and Stolcke, 2006)) &apos;The target/non-target terms are due to (Boakye and Stolcke, 2006). Proceedings of NAACL HLT 2007, Companion Volume, pages 89–92, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics The present work revisits NT-Norm normalization, which has been successfully used in a threshold detector (Laskowski et al., 2004), in automatic initial label assignment (Laskowski and Schultz, 2006), and as part of a two-state decoder feature vector (Boakye and Stolcke, 2006). Our main contribution is a geometric interpretation of NT-Norm XC, in Section 2. We also describe, in Section 3, several contrastive experiments, and discuss the results in Section 4. ξk,j = , φ ∀j6=k (1) jj We assume the simplified response in the kth IHM microphone at a distance dk from a single point source s (t) to be � 1 � � � mk (t) . t − dk = Ak s + ηk (t) , (2) c dk where c, Ak and ηk (t) are the speed of sound, the gain of microphone k, a</context>
</contexts>
<marker>Laskowski, Jin, Schultz, 2004</marker>
<rawString>K. Laskowski, Q. Jin, and T. Schultz. 2004. Crosscorrelation-based Multispeaker Speech Activity Detection. Proc. of INTERSPEECH, Jeju Island, South Korea, pp973–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Laskowski</author>
<author>T Schultz</author>
</authors>
<title>Unsupervised Learning of Overlapped Speech Model Parameters for Multichannel Speech Activity Detection in Meetings.</title>
<date>2006</date>
<booktitle>Proc. of ICASSP,</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="3871" citStr="Laskowski and Schultz, 2006" startWordPosition="579" endWordPosition="582">es selected by an automatic feature selection algorithm in [2] and [4]. (1:(Pfau et al., 2001), 2:(Wrigley et al., 2003), 3:(Laskowski et al., 2004), 4:(Wrigley et al., 2005), 5:(Huang, 2005), 6:(Boakye and Stolcke, 2006)) &apos;The target/non-target terms are due to (Boakye and Stolcke, 2006). Proceedings of NAACL HLT 2007, Companion Volume, pages 89–92, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics The present work revisits NT-Norm normalization, which has been successfully used in a threshold detector (Laskowski et al., 2004), in automatic initial label assignment (Laskowski and Schultz, 2006), and as part of a two-state decoder feature vector (Boakye and Stolcke, 2006). Our main contribution is a geometric interpretation of NT-Norm XC, in Section 2. We also describe, in Section 3, several contrastive experiments, and discuss the results in Section 4. ξk,j = , φ ∀j6=k (1) jj We assume the simplified response in the kth IHM microphone at a distance dk from a single point source s (t) to be � 1 � � � mk (t) . t − dk = Ak s + ηk (t) , (2) c dk where c, Ak and ηk (t) are the speed of sound, the gain of microphone k, an d source-uncorrelated noise at microphone k, respectively. Cross-ch</context>
<context position="7221" citStr="Laskowski and Schultz, 2006" startWordPosition="1173" endWordPosition="1176">etween the location of the dominant sound source and that participant’s microphone is smaller than the geometric mean of the distances from the source to the remaining microphones, ie. when sY dj &gt; dk �−� j�=k 2http://www.nist.gov/speech/tests/rt/ 2 Geometric Interpretation We propose an interpretable geometric approximation to NT-Norm XC for channel k, maxτ φjk (τ)    [k] = 54 X j k (g) φjk (τ) = ZΩdj dkks(t)s(t − τ)dt , (3) j where τ≡ (dj − dk) /c. Letting Ps≡RΩs2 (t) dt and Aj 3 Experimental Setup 3.1 VAD and ASR Systems Our multispeaker VAD system, shown in Figure 1, was introduced in (Laskowski and Schultz, 2006). Rather than detecting the 2-state speech (V) vs. El V if (9) AM ILA AM TRAINING VITERBI DECODING multichannel audio REFRAMING ASR Pass 1 SMOOTHING 0 4F 4∗ ° (4∗) WER Figure 1: VAD system architecture, with 4 error measurement points. Symbols as in the text. We refer to this algorithm as ILAave. For contrast we also consider ILAmin, with the sum in Equation 8 replaced by the minimum over j#k. This corresponds to declaring a participant as vocalizing when the distance between the location of the dominant sound source and that participant’s microphone is smaller than the distance from the sourc</context>
<context position="9886" citStr="Laskowski and Schultz, 2006" startWordPosition="1633" endWordPosition="1636">es. We use this metric for comparative purposes only, across the various measurement points. We also use first-pass ASR word error rates (WERs), after lattice rescoring, as a final measure of performance impact. We evaluate, over a range of ILA frame sizes, the performance of ILAave(3), with a maximum number of simultaneously vocalizing participants of 3, and for the contrastive ILAmin. We note that ILAmin is capable of declaring at most one microphone at a time as being worn by a current speaker. As a result, construction of acoustic models for overlapped vocal activity states, described in (Laskowski and Schultz, 2006), results in states of at most 2 simultaneously vocalizing participants. We therefore refer to ILAmin as ILAmin(2), and additionally consider ILAave(2), in which states with 3 simultaneously vocalizing participants are removed. 4 Results and Discussion We show the results of our experiments in Table 2. First-pass WERs, using reference segmentation (.stm), vary by 1.3% absolute (abs) between rt05s eval and rt06s eval. We also note that removing the one meeting with a participant without a microphone reduces the rt05s eval manual segmentation WER by 1.7% abs. WERs obtained with automatic segment</context>
<context position="12709" citStr="Laskowski and Schultz, 2006" startWordPosition="2142" endWordPosition="2145">ults are shown for 3 contrastive VAD systems (ILAave(3), ILAave(2) and ILAmin(2)), and 4 ILA frame sizes (100ms, 75ms, 50ms, and 25ms). this is because 75 ms does not divide evenly into the decoder frame size of 100 ms, causing more deletions across the reframing step than for other ILA frame sizes. Error minima appear for an ILA frame size somewhere between 50 ms and 75 ms, for both ASR and post-decoding VAD errors. Although (Pfau et al., 2001) considered a maximum lag of 250 samples (15.6ms, or 5m at the speed of sound), their computation of S-Norm XC used a rectangular window. Here, as in (Laskowski and Schultz, 2006) and (Boakye and Stolcke, 2006), we use a Hamming window. Our results suggest that a large, broadly tapered window is important for Equation 6 to hold. The table also shows that for datasets without uninstrumented participants, rt05s eval* and rt06s eval, ILAmin(2) is outperformed by ILAave(2) by as much as 1.1% abs in WER, especially at small frame sizes. The difference for the full rt05s eval dataset is smaller. The results also suggest that reducing the maximum degree of simultaneous vocalization from 3 to 2 during decoding is an effective means of reducing errors (ASR insertions, not shown</context>
</contexts>
<marker>Laskowski, Schultz, 2006</marker>
<rawString>K. Laskowski and T. Schultz. 2006. Unsupervised Learning of Overlapped Speech Model Parameters for Multichannel Speech Activity Detection in Meetings. Proc. of ICASSP, Toulouse, France, I:993–996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pfau</author>
<author>D Ellis</author>
<author>A Stolcke</author>
</authors>
<title>Multispeaker Speech Activity Detection for the ICSI Meeting Recorder.</title>
<date>2001</date>
<booktitle>Proc. of ASRU, Madonna di Campiglio, Italy,</booktitle>
<pages>107--110</pages>
<contexts>
<context position="2062" citStr="Pfau et al., 2001" startWordPosition="286" endWordPosition="289">who said what). Segmentation into such contributions is informed primarily by VAD on a frame-by-frame basis. Individual head-mounted microphone (IHM) recordings of meetings present a particular challenge for VAD, due to crosstalk from other participants. Most state-of-the-art VAD systems for meetings rely on decoding in a binary speech/non-speech space, assuming independence among participants, but are 89 increasingly relying on features specifically designed to address the crosstalk issue (Wrigley et al., 2005). A feature which has attracted attention since its use in VAD post-processing in (Pfau et al., 2001) is the maximum cross-channel correlation (XC), maxτ Ojk (T), between channels j and k, where T is the lag. When designing features descriptive of the kth channel, XC is frequently normalized by the energy in the target&apos; channel k (Wrigley et al., 2003). Alternately, XC can be normalized by the energy in the non-target channel j (Laskowski et al., 2004), a normalization which we refer to here as NT-Norm, extending the Norm and S-Norm naming conventions in (Wrigley et al., 2005). Table 1 shows several types of normalizations which have been explored. Normalization of XC Mean Min Max (none) maxj</context>
<context position="3337" citStr="Pfau et al., 2001" startWordPosition="504" endWordPosition="507">maxj��k φjk(τ) φkk(0) S-Norm maxji4k φjk(τ) [2] [4] [5] [2] [4] [1] [2] [4] √φjj(0)φkk(0) NT-Norm maxj�/,,` φjk(τ) 3 6 6 [ ] [ ] [ ] `Vj7 (0) Table 1: Normalizations and statistics of crosschannel correlation features to describe channel k. In [1], a median-smoothed version was used in postprocessing. In [3], the sum (JMXC) was used instead of the mean. In [5], cross-correlation was computed over samples and features. In [6], the minimum and the maximum were jointly referred to as NMXC. References in bold depict features selected by an automatic feature selection algorithm in [2] and [4]. (1:(Pfau et al., 2001), 2:(Wrigley et al., 2003), 3:(Laskowski et al., 2004), 4:(Wrigley et al., 2005), 5:(Huang, 2005), 6:(Boakye and Stolcke, 2006)) &apos;The target/non-target terms are due to (Boakye and Stolcke, 2006). Proceedings of NAACL HLT 2007, Companion Volume, pages 89–92, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics The present work revisits NT-Norm normalization, which has been successfully used in a threshold detector (Laskowski et al., 2004), in automatic initial label assignment (Laskowski and Schultz, 2006), and as part of a two-state decoder feature vector (Boakye and St</context>
<context position="12530" citStr="Pfau et al., 2001" startWordPosition="2110" endWordPosition="2113">2: VAD errors, measured at three points in our system, and first-pass WERs for rt05s eval (05), as well as first-pass WERs for rt05s eval* (05*) and rt06s eval (06). Results are shown for 3 contrastive VAD systems (ILAave(3), ILAave(2) and ILAmin(2)), and 4 ILA frame sizes (100ms, 75ms, 50ms, and 25ms). this is because 75 ms does not divide evenly into the decoder frame size of 100 ms, causing more deletions across the reframing step than for other ILA frame sizes. Error minima appear for an ILA frame size somewhere between 50 ms and 75 ms, for both ASR and post-decoding VAD errors. Although (Pfau et al., 2001) considered a maximum lag of 250 samples (15.6ms, or 5m at the speed of sound), their computation of S-Norm XC used a rectangular window. Here, as in (Laskowski and Schultz, 2006) and (Boakye and Stolcke, 2006), we use a Hamming window. Our results suggest that a large, broadly tapered window is important for Equation 6 to hold. The table also shows that for datasets without uninstrumented participants, rt05s eval* and rt06s eval, ILAmin(2) is outperformed by ILAave(2) by as much as 1.1% abs in WER, especially at small frame sizes. The difference for the full rt05s eval dataset is smaller. The</context>
</contexts>
<marker>Pfau, Ellis, Stolcke, 2001</marker>
<rawString>T. Pfau and D. Ellis and A. Stolcke. 2001. Multispeaker Speech Activity Detection for the ICSI Meeting Recorder. Proc. of ASRU, Madonna di Campiglio, Italy, pp107–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wrigley</author>
<author>G Brown</author>
<author>V Wan</author>
<author>S Renals</author>
</authors>
<title>Feature Selection for the Classification of Crosstalk in Multi-Channel Audio.</title>
<date>2003</date>
<booktitle>Proc. of EUROSPEECH,</booktitle>
<pages>469--472</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2315" citStr="Wrigley et al., 2003" startWordPosition="330" endWordPosition="333">nts. Most state-of-the-art VAD systems for meetings rely on decoding in a binary speech/non-speech space, assuming independence among participants, but are 89 increasingly relying on features specifically designed to address the crosstalk issue (Wrigley et al., 2005). A feature which has attracted attention since its use in VAD post-processing in (Pfau et al., 2001) is the maximum cross-channel correlation (XC), maxτ Ojk (T), between channels j and k, where T is the lag. When designing features descriptive of the kth channel, XC is frequently normalized by the energy in the target&apos; channel k (Wrigley et al., 2003). Alternately, XC can be normalized by the energy in the non-target channel j (Laskowski et al., 2004), a normalization which we refer to here as NT-Norm, extending the Norm and S-Norm naming conventions in (Wrigley et al., 2005). Table 1 shows several types of normalizations which have been explored. Normalization of XC Mean Min Max (none) maxj#k φjk(τ) [2][4] [2][4] [2][4] Norm [2][4] [2][4] [2][4] maxj��k φjk(τ) φkk(0) S-Norm maxji4k φjk(τ) [2] [4] [5] [2] [4] [1] [2] [4] √φjj(0)φkk(0) NT-Norm maxj�/,,` φjk(τ) 3 6 6 [ ] [ ] [ ] `Vj7 (0) Table 1: Normalizations and statistics of crosschannel</context>
</contexts>
<marker>Wrigley, Brown, Wan, Renals, 2003</marker>
<rawString>S. Wrigley, G. Brown, V. Wan, and S. Renals. 2003. Feature Selection for the Classification of Crosstalk in Multi-Channel Audio. Proc. of EUROSPEECH, Geneva, Switzerland, pp469–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wrigley</author>
<author>G Brown</author>
<author>V Wan</author>
<author>S Renals</author>
</authors>
<title>Speech and Crosstalk Detection in Multichannel Audio.</title>
<date>2005</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>13</volume>
<pages>84--91</pages>
<contexts>
<context position="1961" citStr="Wrigley et al., 2005" startWordPosition="269" endWordPosition="272">tion schemes and language model constraints, and to associate recognized output with specific speakers (who said what). Segmentation into such contributions is informed primarily by VAD on a frame-by-frame basis. Individual head-mounted microphone (IHM) recordings of meetings present a particular challenge for VAD, due to crosstalk from other participants. Most state-of-the-art VAD systems for meetings rely on decoding in a binary speech/non-speech space, assuming independence among participants, but are 89 increasingly relying on features specifically designed to address the crosstalk issue (Wrigley et al., 2005). A feature which has attracted attention since its use in VAD post-processing in (Pfau et al., 2001) is the maximum cross-channel correlation (XC), maxτ Ojk (T), between channels j and k, where T is the lag. When designing features descriptive of the kth channel, XC is frequently normalized by the energy in the target&apos; channel k (Wrigley et al., 2003). Alternately, XC can be normalized by the energy in the non-target channel j (Laskowski et al., 2004), a normalization which we refer to here as NT-Norm, extending the Norm and S-Norm naming conventions in (Wrigley et al., 2005). Table 1 shows s</context>
<context position="3417" citStr="Wrigley et al., 2005" startWordPosition="516" endWordPosition="519">φjj(0)φkk(0) NT-Norm maxj�/,,` φjk(τ) 3 6 6 [ ] [ ] [ ] `Vj7 (0) Table 1: Normalizations and statistics of crosschannel correlation features to describe channel k. In [1], a median-smoothed version was used in postprocessing. In [3], the sum (JMXC) was used instead of the mean. In [5], cross-correlation was computed over samples and features. In [6], the minimum and the maximum were jointly referred to as NMXC. References in bold depict features selected by an automatic feature selection algorithm in [2] and [4]. (1:(Pfau et al., 2001), 2:(Wrigley et al., 2003), 3:(Laskowski et al., 2004), 4:(Wrigley et al., 2005), 5:(Huang, 2005), 6:(Boakye and Stolcke, 2006)) &apos;The target/non-target terms are due to (Boakye and Stolcke, 2006). Proceedings of NAACL HLT 2007, Companion Volume, pages 89–92, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics The present work revisits NT-Norm normalization, which has been successfully used in a threshold detector (Laskowski et al., 2004), in automatic initial label assignment (Laskowski and Schultz, 2006), and as part of a two-state decoder feature vector (Boakye and Stolcke, 2006). Our main contribution is a geometric interpretation of NT-Norm XC,</context>
</contexts>
<marker>Wrigley, Brown, Wan, Renals, 2005</marker>
<rawString>S. Wrigley, G. Brown, V. Wan, and S. Renals. 2005. Speech and Crosstalk Detection in Multichannel Audio. IEEE Trans. on Speech and Audio Processing, 13:1, pp84–91.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>