<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.047261">
<title confidence="0.97091">
Language identification of names with SVMs
</title>
<author confidence="0.979899">
Aditya Bhargava and Grzegorz Kondrak
</author>
<affiliation confidence="0.9988525">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.594972">
Edmonton, Alberta, Canada, T6G 2E8
</address>
<email confidence="0.999057">
{abhargava,kondrak}@cs.ualberta.ca
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818928571429">
The task of identifying the language of text
or utterances has a number of applications in
natural language processing. Language iden-
tification has traditionally been approached
with character-level language models. How-
ever, the language model approach crucially
depends on the length of the text in ques-
tion. In this paper, we consider the problem
of language identification of names. We show
that an approach based on SVMs with n-gram
counts as features performs much better than
language models. We also experiment with
applying the method to pre-process transliter-
ation data for the training of separate models.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941434782609">
The task of identifying the language of text or utter-
ances has a number of applications in natural lan-
guage processing. Font Llitj´os and Black (2001)
show that language identification can improve the
accuracy of letter-to-phoneme conversion. Li et
al. (2007) use language identification in a translit-
eration system to account for different semantic
transliteration rules between languages when the tar-
get language is Chinese. Huang (2005) improves the
accuracy of machine transliteration by clustering his
training data according to the source language.
Language identification has traditionally been
approached using character-level n-gram language
models. In this paper, we propose the use of sup-
port vector machines (SVMs) for the language iden-
tification of very short texts such as proper nouns.
We show that SVMs outperform language models
on two different data sets consisting of personal
names. Furthermore, we test the hypothesis that lan-
guage identification can improve transliteration by
pre-processing the source data and training separate
models using a state-of-the-art transliteration sys-
tem.
</bodyText>
<sectionHeader confidence="0.998273" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999959642857143">
N-gram approaches have proven very popular for
language identification in general. Cavnar and Tren-
kle (1994) apply n-gram language models to general
text categorization. They construct character-level
language models using n-grams up to a certain max-
imum length from each class in their training cor-
pora. To classify new text, they generate an n-gram
frequency profile from the text and then assign it to
the class having the most similar language model,
which is determined by summing the differences in
n-gram ranks. Given 14 languages, text of 300 char-
acters or more, and retaining the 400 most common
n-grams up to length 5, they achieve an overall accu-
racy of 99.8%. However, the accuracy of the n-gram
approach strongly depends on the length of the texts.
Kruengkrai et al. (2005) report that, on a language
identification task of 17 languages with average text
length 50 bytes, the accuracy drops to 90.2%. When
SVMs were used for the same task, they achieved
99.7% accuracy.
Konstantopoulos (2007) looks particularly at the
task of identifying the language of proper nouns. He
focuses on a data set of soccer player names coming
from 13 possible national languages. He finds that
using general n-gram language models yields an av-
erage F1 score of only 27%, but training the models
specifically to these smaller data gives significantly
better results: 50% average F1 score for last names
</bodyText>
<page confidence="0.983713">
693
</page>
<subsubsectionHeader confidence="0.577982">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 693–696,
</subsubsectionHeader>
<subsectionHeader confidence="0.276783">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.99986975">
only, and 60% for full names.
On the other hand, Li et al. (2007) report some
good results for single-name language identification
using n-gram language models. For the task of sepa-
rating single Chinese, English, and Japanese names,
they achieve an overall accuracy of 94.8%. One rea-
son that they do better is because of the smaller num-
ber of classes. We can further see that the languages
in question are very dissimilar, making the problem
easier; for example, the character “x” appears only
in the list of Chinese names, and the bigram “kl” ap-
pears only in the list of English names.
</bodyText>
<sectionHeader confidence="0.979034" genericHeader="method">
3 Language identification with SVMs
</sectionHeader>
<bodyText confidence="0.975550909090909">
Rather than using language models to determine the
language of a name, we propose to count charac-
ter n-gram occurrences in the given name, for n up
to some maximum length, and use these counts as
the features in an SVM. We choose SVMs because
they can take a large number of features and learn to
weigh them appropriately. When counting n-grams,
we include space characters at the beginning and
end of each word, so that prefixes and suffixes are
counted appropriately. In addition to n-gram counts,
we also include word length as a feature.
In our initial experiments, we tested several dif-
ferent kernels. The kernels that performed the best
were the linear, sigmoid, and radial basis function
(RBF) kernels. We tested various maximum n-gram
lengths; Figure 1 shows the accuracy of the linear
kernel as a function of maximum n-gram length.
Polynomial kernels, a substring match–count string
kernel, and a string kernel based on the edit distance
all performed poorly in comparison. We also exper-
imented with other modifications such as normaliz-
ing the feature vectors, and decreasing the weights
of frequent n-gram counts to avoid larger counts
dominating smaller counts. Since the effects were
negligible, we exclude these results from this paper.
In our experiments, we used the LIBLINEAR
(Fan et al., 2008) package for the linear kernel and
the LIBSVM (Chang and Lin, 2001) package for the
RBF and sigmoid kernels. We discarded any peri-
ods and parentheses, but kept apostrophes and hy-
phens, and we converted all letters to lower case.
We removed very short names of length less than
two. For all data sets, we held out 10% of the data
</bodyText>
<figureCaption confidence="0.9839815">
Figure 1: Cross-validation accuracy of the linear kernel
on the Transfermarkt full names corpus.
</figureCaption>
<bodyText confidence="0.999882375">
as the test set. We then found optimal parameters
for each kernel type using 10-fold cross-validation
on the remaining training set. This yielded optimum
maximum n-gram lengths of four for single names
and five for full names. Using the optimal parame-
ters, we constructed models from the entire training
data and then tested the models on the held-out test
set.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="method">
4 Intrinsic evaluation
</sectionHeader>
<bodyText confidence="0.999979">
We used two corpora to test our SVM-based ap-
proach: the Transfermarkt corpus of soccer player
names, and the Chinese-English-Japanese (CEJ)
corpus of first names and surnames. These corpora
are described in further detail below.
</bodyText>
<subsectionHeader confidence="0.975818">
4.1 Transfermarkt corpus
</subsectionHeader>
<bodyText confidence="0.99996">
The Transfermarkt corpus (Konstantopoulos, 2007)
consists of European soccer player names annotated
with one of 13 possible national languages, with sep-
arate lists provided for last names and full names.
Diacritics were removed in order to avoid trivializ-
ing the task. There are 14914 full names, with aver-
age length 14.8, and 12051 last names, with average
length 7.8. It should be noted that these data are
noisy; the fact that a player plays for a certain na-
tion’s team does not necessarily indicate that his or
her name is of that nation’s language. For example,
Dario Dakovic was born in Bosnia but plays for the
Austrian national team; his name is therefore anno-
tated as German.
Table 1 shows our results on the Transfermarkt
corpus. Because Konstantopoulos (2007) provides
only F1 scores, we used his scripts to generate new
results using language models and calculate the ac-
curacy instead, which allows us to be consistent with
our tests on other data sets. Our results show that us-
</bodyText>
<figure confidence="0.975905125">
40
1 2 3 4 5 6
Maximum n-gram length
80
Accuracy (%)
70
60
50
</figure>
<page confidence="0.979595">
694
</page>
<table confidence="0.9997648">
Method Last names Full names
Language models 44.7 54.2
Linear SVM 56.4 79.9
RBF SVM 55.7 78.9
Sigmoid SVM 56.2 78.7
</table>
<tableCaption confidence="0.9959445">
Table 1: Language identification accuracy on the Trans-
fermarkt corpus. Language models have n = 5.
</tableCaption>
<bodyText confidence="0.9993618">
ing SVMs clearly outperforms using language mod-
els on the Transfermarkt corpus; in fact, SVMs yield
better accuracy on last names than language models
on full names. Differences between kernels are not
statistically significant.
</bodyText>
<subsectionHeader confidence="0.943589">
4.2 CEJ corpus
</subsectionHeader>
<bodyText confidence="0.999991166666667">
The CEJ corpus (Li et al., 2007) provides a com-
bined list of first names and surnames, each classi-
fied as Chinese, English, or Japanese. There are a
total of 97115 names with an average length of 7.6
characters. This corpus was used for the semantic
transliteration of personal names into Chinese.
We found that the RBF and sigmoid kernels were
very slow—presumably due to the large size of the
corpus—so we tested only the linear kernel. Table 2
shows our results in comparison to those of language
models reported in (Li et al., 2007); we reduce the
error rate by over 50%.
</bodyText>
<sectionHeader confidence="0.966716" genericHeader="method">
5 Application to machine transliteration
</sectionHeader>
<bodyText confidence="0.9999155">
Machine transliteration is one of the primary poten-
tial applications of language identification because
the language of a word often determines its pronun-
ciation. We therefore tested language identification
to see if results could indeed be improved by using
language identification as a pre-processing step.
</bodyText>
<subsectionHeader confidence="0.96747">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999882142857143">
The English-Hindi corpus of names (Li et al., 2009;
MSRI, 2009) contains a test set of 1000 names rep-
resented in both the Latin and Devanagari scripts.
We manually classified these names as being of ei-
ther Indian or non-Indian origin, occasionally resort-
ing to web searches to help disambiguate them.1 We
discarded those names that fell into both categories
</bodyText>
<footnote confidence="0.992435">
1Our tagged data are available online at http://www.
cs.ualberta.ca/˜ab31/langid/.
</footnote>
<table confidence="0.990615333333333">
Method Ch. Eng. Jap. All
Lang. model 96.4 89.9 96.5 94.8
Linear SVM 99.0 94.8 97.6 97.6
</table>
<tableCaption confidence="0.9970425">
Table 2: Language identification accuracy on the CEJ
corpus. Language models have n = 4.
</tableCaption>
<bodyText confidence="0.999928411764706">
(e.g. “Maya”) as well as those that we could not
confidently classify. In total, we discarded 95 of
these names, and randomly selected 95 names from
the training set that we could confidently classify to
complete our corpus of 1000 names. Of the 1000
names, 546 were classified as being of Indian origin
and the remaining 454 were classified as being of
non-Indian origin; the names have an average length
of 7.0 characters.
We trained our language identification approach
on 900 names, with the remaining 100 names serv-
ing as the test set. The resulting accuracy was 80%
with the linear kernel, 84% with the RBF kernel,
and 83% with the sigmoid kernel. In this case, the
performance of the RBF kernel was found to be sig-
nificantly better than that of the linear kernel accord-
ing to the McNemar test with p &lt; 0.05.
</bodyText>
<subsectionHeader confidence="0.997951">
5.2 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999934428571429">
We tested a simple method of combining language
identification with transliteration. We use a lan-
guage identification model to split the training, de-
velopment, and test sets into disjoint classes. We
train a transliteration model on each separate class,
and then combine the results.
Our transliteration system was DIRECTL (Ji-
ampojamarn et al., 2009). We trained the language
identification model over the entire set of 1000
tagged names using the parameters from above. Be-
cause these names comprised most of the test set
and were now being used as the training set for the
language identification model, we swapped various
names between sets such that none of the words used
for training the language identification model were
in the final transliteration test set.
Using this language identification model, we split
the data. After splitting, the “Indian” training, de-
velopment, and testing sets had 5032, 575, and 483
words respectively while the “non-Indian” sets had
11081, 993, and 517 words respectively.
</bodyText>
<page confidence="0.997643">
695
</page>
<subsectionHeader confidence="0.805094">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.99999">
Splitting the data and training two separate mod-
els yielded a combined top-1 accuracy of 46.0%, as
compared to 47.0% achieved by a single translitera-
tion model trained over the full data; this difference
is not statistically significant. Somewhat counter-
intuitively, using language identification as a pre-
processing step for machine transliteration yields no
improvement in performance for our particular data
and transliteration system.
While it could be argued that our language identi-
fication accuracy of 84% is too low to be useful here,
we believe that the principal reason for this perfor-
mance decrease is the reduction in the amount of
data available for the training of the separate mod-
els. We performed an experiment to confirm this
hypothesis: we randomly split the full data into two
sets, matching the sizes of the Indian and non-Indian
sets. We then trained two separate models and com-
bined the results; this yielded a top-1 accuracy of
41.5%. The difference between this and the 46.0%
result above is statistically significant with p &lt; 0.01.
From this we conclude that the reduction in data size
was a significant factor in the previously described
null result, and that language identification does pro-
vide useful information to the transliteration system.
In addition, we believe that the transliteration system
may implicitly leverage the language origin infor-
mation. Whether a closer coupling of the two mod-
ules could produce an increase in accuracy remains
an open question.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993111111111">
We have proposed a novel approach to the task of
language identification of names. We have shown
that applying SVMs with n-gram counts as fea-
tures outperforms the predominant approach based
on language models. We also tested language identi-
fication in one of its potential applications, machine
transliteration, and found that a simple method of
splitting the data by language yields no significant
change in accuracy, although there is an improve-
ment in comparison to a random split.
In the future, we plan to investigate other methods
of incorporating language identification in machine
transliteration. Options to explore include the use
of language identification probabilities as features in
the transliteration system (Li et al., 2007), as well as
splitting the data into sets that are not necessarily
disjoint, allowing separate transliteration models to
learn from potentially useful common information.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999596666666667">
We thank Sittichai Jiampojamarn for his assistance
with the DIRECTL transliteration system, Shane
Bergsma for his advice, and Stasinos Konstantopou-
los for providing us with his scripts and data. This
research was supported by the Natural Sciences and
Engineering Research Council of Canada.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999796297297297">
W. B. Cavnar and J. M. Trenkle. 1994. N-gram-based
text categorization. In Proc. of the Third Annual Sym-
posium on Document Analysis and Information Re-
trieval, pages 161–175.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a li-
brary for support vector machines. Software available
at http://www.csie.ntu.edu.tw/˜cjlin/
libsvm.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. Journal of Machine Learning Re-
search, 9:1871–1874.
A. Font Llitj´os and A. W. Black. 2001. Knowledge of
language origin improves pronunciation accuracy of
proper names. In Proc. of Eurospeech, pages 1919–
1922.
F. Huang. 2005. Cluster-specific named entity transliter-
ation. In Proc. of HLT-EMNLP, pages 435–442.
S. Jiampojamarn, A. Bhargava, Q. Dou, K. Dwyer, and
G. Kondrak. 2009. DirecTL: a language independent
approach to transliteration. In Proc. of ACL-IJCNLP
Named Entities Workshop, pages 28–31.
S. Konstantopoulos. 2007. What’s in a name? In Proc.
of RANLP Computational Phonology Workshop.
C. Kruengkrai, P. Srichaivattana, V. Sornlertlamvanich,
and H. Isahara. 2005. Language identification based
on string kernels. In Proc. of International Symposium
on Communications and Information Technologies.
H. Li, K. C. Sim, J.-S. Kuo, and M. Dong. 2007. Seman-
tic transliteration of personal names. In Proc. of ACL,
pages 120–127.
H. Li, A. Kumaran, V. Pervouchine, and M. Zhang. 2009.
Report of NEWS 2009 machine transliteration shared
task. In Proc. of Named Entities Workshop: Shared
Task on Transliteration, pages 1–18.
MSRI, 2009. Microsoft Research India. http://
research.microsoft.com/india.
</reference>
<page confidence="0.998775">
696
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933050">
<title confidence="0.999577">Language identification of names with SVMs</title>
<author confidence="0.990863">Aditya Bhargava</author>
<author confidence="0.990863">Grzegorz</author>
<affiliation confidence="0.999076">Department of Computing University of</affiliation>
<address confidence="0.958301">Edmonton, Alberta, Canada, T6G</address>
<abstract confidence="0.998883333333333">The task of identifying the language of text or utterances has a number of applications in natural language processing. Language identification has traditionally been approached with character-level language models. However, the language model approach crucially depends on the length of the text in question. In this paper, we consider the problem of language identification of names. We show that an approach based on SVMs with n-gram counts as features performs much better than language models. We also experiment with applying the method to pre-process transliteration data for the training of separate models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W B Cavnar</author>
<author>J M Trenkle</author>
</authors>
<title>N-gram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proc. of the Third Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<contexts>
<context position="2076" citStr="Cavnar and Trenkle (1994)" startWordPosition="300" endWordPosition="304">ached using character-level n-gram language models. In this paper, we propose the use of support vector machines (SVMs) for the language identification of very short texts such as proper nouns. We show that SVMs outperform language models on two different data sets consisting of personal names. Furthermore, we test the hypothesis that language identification can improve transliteration by pre-processing the source data and training separate models using a state-of-the-art transliteration system. 2 Previous work N-gram approaches have proven very popular for language identification in general. Cavnar and Trenkle (1994) apply n-gram language models to general text categorization. They construct character-level language models using n-grams up to a certain maximum length from each class in their training corpora. To classify new text, they generate an n-gram frequency profile from the text and then assign it to the class having the most similar language model, which is determined by summing the differences in n-gram ranks. Given 14 languages, text of 300 characters or more, and retaining the 400 most common n-grams up to length 5, they achieve an overall accuracy of 99.8%. However, the accuracy of the n-gram </context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>W. B. Cavnar and J. M. Trenkle. 1994. N-gram-based text categorization. In Proc. of the Third Annual Symposium on Document Analysis and Information Retrieval, pages 161–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/˜cjlin/ libsvm.</title>
<date>2001</date>
<contexts>
<context position="5566" citStr="Chang and Lin, 2001" startWordPosition="879" endWordPosition="882">e 1 shows the accuracy of the linear kernel as a function of maximum n-gram length. Polynomial kernels, a substring match–count string kernel, and a string kernel based on the edit distance all performed poorly in comparison. We also experimented with other modifications such as normalizing the feature vectors, and decreasing the weights of frequent n-gram counts to avoid larger counts dominating smaller counts. Since the effects were negligible, we exclude these results from this paper. In our experiments, we used the LIBLINEAR (Fan et al., 2008) package for the linear kernel and the LIBSVM (Chang and Lin, 2001) package for the RBF and sigmoid kernels. We discarded any periods and parentheses, but kept apostrophes and hyphens, and we converted all letters to lower case. We removed very short names of length less than two. For all data sets, we held out 10% of the data Figure 1: Cross-validation accuracy of the linear kernel on the Transfermarkt full names corpus. as the test set. We then found optimal parameters for each kernel type using 10-fold cross-validation on the remaining training set. This yielded optimum maximum n-gram lengths of four for single names and five for full names. Using the opti</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.-C. Chang and C.-J. Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/˜cjlin/ libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="5499" citStr="Fan et al., 2008" startWordPosition="867" endWordPosition="870">n (RBF) kernels. We tested various maximum n-gram lengths; Figure 1 shows the accuracy of the linear kernel as a function of maximum n-gram length. Polynomial kernels, a substring match–count string kernel, and a string kernel based on the edit distance all performed poorly in comparison. We also experimented with other modifications such as normalizing the feature vectors, and decreasing the weights of frequent n-gram counts to avoid larger counts dominating smaller counts. Since the effects were negligible, we exclude these results from this paper. In our experiments, we used the LIBLINEAR (Fan et al., 2008) package for the linear kernel and the LIBSVM (Chang and Lin, 2001) package for the RBF and sigmoid kernels. We discarded any periods and parentheses, but kept apostrophes and hyphens, and we converted all letters to lower case. We removed very short names of length less than two. For all data sets, we held out 10% of the data Figure 1: Cross-validation accuracy of the linear kernel on the Transfermarkt full names corpus. as the test set. We then found optimal parameters for each kernel type using 10-fold cross-validation on the remaining training set. This yielded optimum maximum n-gram lengt</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Font Llitj´os</author>
<author>A W Black</author>
</authors>
<title>Knowledge of language origin improves pronunciation accuracy of proper names.</title>
<date>2001</date>
<booktitle>In Proc. of Eurospeech,</booktitle>
<pages>pages</pages>
<marker>Llitj´os, Black, 2001</marker>
<rawString>A. Font Llitj´os and A. W. Black. 2001. Knowledge of language origin improves pronunciation accuracy of proper names. In Proc. of Eurospeech, pages 1919– 1922.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
</authors>
<title>Cluster-specific named entity transliteration.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP,</booktitle>
<pages>435--442</pages>
<contexts>
<context position="1283" citStr="Huang (2005)" startWordPosition="188" endWordPosition="189">rforms much better than language models. We also experiment with applying the method to pre-process transliteration data for the training of separate models. 1 Introduction The task of identifying the language of text or utterances has a number of applications in natural language processing. Font Llitj´os and Black (2001) show that language identification can improve the accuracy of letter-to-phoneme conversion. Li et al. (2007) use language identification in a transliteration system to account for different semantic transliteration rules between languages when the target language is Chinese. Huang (2005) improves the accuracy of machine transliteration by clustering his training data according to the source language. Language identification has traditionally been approached using character-level n-gram language models. In this paper, we propose the use of support vector machines (SVMs) for the language identification of very short texts such as proper nouns. We show that SVMs outperform language models on two different data sets consisting of personal names. Furthermore, we test the hypothesis that language identification can improve transliteration by pre-processing the source data and train</context>
</contexts>
<marker>Huang, 2005</marker>
<rawString>F. Huang. 2005. Cluster-specific named entity transliteration. In Proc. of HLT-EMNLP, pages 435–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jiampojamarn</author>
<author>A Bhargava</author>
<author>Q Dou</author>
<author>K Dwyer</author>
<author>G Kondrak</author>
</authors>
<title>DirecTL: a language independent approach to transliteration.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP Named Entities Workshop,</booktitle>
<pages>28--31</pages>
<contexts>
<context position="10820" citStr="Jiampojamarn et al., 2009" startWordPosition="1759" endWordPosition="1763">y was 80% with the linear kernel, 84% with the RBF kernel, and 83% with the sigmoid kernel. In this case, the performance of the RBF kernel was found to be significantly better than that of the linear kernel according to the McNemar test with p &lt; 0.05. 5.2 Experimental setup We tested a simple method of combining language identification with transliteration. We use a language identification model to split the training, development, and test sets into disjoint classes. We train a transliteration model on each separate class, and then combine the results. Our transliteration system was DIRECTL (Jiampojamarn et al., 2009). We trained the language identification model over the entire set of 1000 tagged names using the parameters from above. Because these names comprised most of the test set and were now being used as the training set for the language identification model, we swapped various names between sets such that none of the words used for training the language identification model were in the final transliteration test set. Using this language identification model, we split the data. After splitting, the “Indian” training, development, and testing sets had 5032, 575, and 483 words respectively while the </context>
</contexts>
<marker>Jiampojamarn, Bhargava, Dou, Dwyer, Kondrak, 2009</marker>
<rawString>S. Jiampojamarn, A. Bhargava, Q. Dou, K. Dwyer, and G. Kondrak. 2009. DirecTL: a language independent approach to transliteration. In Proc. of ACL-IJCNLP Named Entities Workshop, pages 28–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Konstantopoulos</author>
</authors>
<title>What’s in a name?</title>
<date>2007</date>
<booktitle>In Proc. of RANLP Computational Phonology Workshop.</booktitle>
<contexts>
<context position="2973" citStr="Konstantopoulos (2007)" startWordPosition="452" endWordPosition="453">and then assign it to the class having the most similar language model, which is determined by summing the differences in n-gram ranks. Given 14 languages, text of 300 characters or more, and retaining the 400 most common n-grams up to length 5, they achieve an overall accuracy of 99.8%. However, the accuracy of the n-gram approach strongly depends on the length of the texts. Kruengkrai et al. (2005) report that, on a language identification task of 17 languages with average text length 50 bytes, the accuracy drops to 90.2%. When SVMs were used for the same task, they achieved 99.7% accuracy. Konstantopoulos (2007) looks particularly at the task of identifying the language of proper nouns. He focuses on a data set of soccer player names coming from 13 possible national languages. He finds that using general n-gram language models yields an average F1 score of only 27%, but training the models specifically to these smaller data gives significantly better results: 50% average F1 score for last names 693 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 693–696, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics only, a</context>
<context position="6612" citStr="Konstantopoulos, 2007" startWordPosition="1050" endWordPosition="1051"> type using 10-fold cross-validation on the remaining training set. This yielded optimum maximum n-gram lengths of four for single names and five for full names. Using the optimal parameters, we constructed models from the entire training data and then tested the models on the held-out test set. 4 Intrinsic evaluation We used two corpora to test our SVM-based approach: the Transfermarkt corpus of soccer player names, and the Chinese-English-Japanese (CEJ) corpus of first names and surnames. These corpora are described in further detail below. 4.1 Transfermarkt corpus The Transfermarkt corpus (Konstantopoulos, 2007) consists of European soccer player names annotated with one of 13 possible national languages, with separate lists provided for last names and full names. Diacritics were removed in order to avoid trivializing the task. There are 14914 full names, with average length 14.8, and 12051 last names, with average length 7.8. It should be noted that these data are noisy; the fact that a player plays for a certain nation’s team does not necessarily indicate that his or her name is of that nation’s language. For example, Dario Dakovic was born in Bosnia but plays for the Austrian national team; his na</context>
</contexts>
<marker>Konstantopoulos, 2007</marker>
<rawString>S. Konstantopoulos. 2007. What’s in a name? In Proc. of RANLP Computational Phonology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kruengkrai</author>
<author>P Srichaivattana</author>
<author>V Sornlertlamvanich</author>
<author>H Isahara</author>
</authors>
<title>Language identification based on string kernels.</title>
<date>2005</date>
<booktitle>In Proc. of International Symposium on Communications and Information Technologies.</booktitle>
<contexts>
<context position="2754" citStr="Kruengkrai et al. (2005)" startWordPosition="415" endWordPosition="418">zation. They construct character-level language models using n-grams up to a certain maximum length from each class in their training corpora. To classify new text, they generate an n-gram frequency profile from the text and then assign it to the class having the most similar language model, which is determined by summing the differences in n-gram ranks. Given 14 languages, text of 300 characters or more, and retaining the 400 most common n-grams up to length 5, they achieve an overall accuracy of 99.8%. However, the accuracy of the n-gram approach strongly depends on the length of the texts. Kruengkrai et al. (2005) report that, on a language identification task of 17 languages with average text length 50 bytes, the accuracy drops to 90.2%. When SVMs were used for the same task, they achieved 99.7% accuracy. Konstantopoulos (2007) looks particularly at the task of identifying the language of proper nouns. He focuses on a data set of soccer player names coming from 13 possible national languages. He finds that using general n-gram language models yields an average F1 score of only 27%, but training the models specifically to these smaller data gives significantly better results: 50% average F1 score for l</context>
</contexts>
<marker>Kruengkrai, Srichaivattana, Sornlertlamvanich, Isahara, 2005</marker>
<rawString>C. Kruengkrai, P. Srichaivattana, V. Sornlertlamvanich, and H. Isahara. 2005. Language identification based on string kernels. In Proc. of International Symposium on Communications and Information Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>K C Sim</author>
<author>J-S Kuo</author>
<author>M Dong</author>
</authors>
<title>Semantic transliteration of personal names.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>120--127</pages>
<contexts>
<context position="1103" citStr="Li et al. (2007)" startWordPosition="160" endWordPosition="163">the length of the text in question. In this paper, we consider the problem of language identification of names. We show that an approach based on SVMs with n-gram counts as features performs much better than language models. We also experiment with applying the method to pre-process transliteration data for the training of separate models. 1 Introduction The task of identifying the language of text or utterances has a number of applications in natural language processing. Font Llitj´os and Black (2001) show that language identification can improve the accuracy of letter-to-phoneme conversion. Li et al. (2007) use language identification in a transliteration system to account for different semantic transliteration rules between languages when the target language is Chinese. Huang (2005) improves the accuracy of machine transliteration by clustering his training data according to the source language. Language identification has traditionally been approached using character-level n-gram language models. In this paper, we propose the use of support vector machines (SVMs) for the language identification of very short texts such as proper nouns. We show that SVMs outperform language models on two differ</context>
<context position="3631" citStr="Li et al. (2007)" startWordPosition="557" endWordPosition="560">ying the language of proper nouns. He focuses on a data set of soccer player names coming from 13 possible national languages. He finds that using general n-gram language models yields an average F1 score of only 27%, but training the models specifically to these smaller data gives significantly better results: 50% average F1 score for last names 693 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 693–696, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics only, and 60% for full names. On the other hand, Li et al. (2007) report some good results for single-name language identification using n-gram language models. For the task of separating single Chinese, English, and Japanese names, they achieve an overall accuracy of 94.8%. One reason that they do better is because of the smaller number of classes. We can further see that the languages in question are very dissimilar, making the problem easier; for example, the character “x” appears only in the list of Chinese names, and the bigram “kl” appears only in the list of English names. 3 Language identification with SVMs Rather than using language models to deter</context>
<context position="8111" citStr="Li et al., 2007" startWordPosition="1309" endWordPosition="1312">with our tests on other data sets. Our results show that us40 1 2 3 4 5 6 Maximum n-gram length 80 Accuracy (%) 70 60 50 694 Method Last names Full names Language models 44.7 54.2 Linear SVM 56.4 79.9 RBF SVM 55.7 78.9 Sigmoid SVM 56.2 78.7 Table 1: Language identification accuracy on the Transfermarkt corpus. Language models have n = 5. ing SVMs clearly outperforms using language models on the Transfermarkt corpus; in fact, SVMs yield better accuracy on last names than language models on full names. Differences between kernels are not statistically significant. 4.2 CEJ corpus The CEJ corpus (Li et al., 2007) provides a combined list of first names and surnames, each classified as Chinese, English, or Japanese. There are a total of 97115 names with an average length of 7.6 characters. This corpus was used for the semantic transliteration of personal names into Chinese. We found that the RBF and sigmoid kernels were very slow—presumably due to the large size of the corpus—so we tested only the linear kernel. Table 2 shows our results in comparison to those of language models reported in (Li et al., 2007); we reduce the error rate by over 50%. 5 Application to machine transliteration Machine transli</context>
</contexts>
<marker>Li, Sim, Kuo, Dong, 2007</marker>
<rawString>H. Li, K. C. Sim, J.-S. Kuo, and M. Dong. 2007. Semantic transliteration of personal names. In Proc. of ACL, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>A Kumaran</author>
<author>V Pervouchine</author>
<author>M Zhang</author>
</authors>
<title>machine transliteration shared task.</title>
<date>2009</date>
<journal>Report of NEWS</journal>
<booktitle>In Proc. of Named Entities Workshop: Shared Task on Transliteration,</booktitle>
<pages>1--18</pages>
<contexts>
<context position="9064" citStr="Li et al., 2009" startWordPosition="1466" endWordPosition="1469">to the large size of the corpus—so we tested only the linear kernel. Table 2 shows our results in comparison to those of language models reported in (Li et al., 2007); we reduce the error rate by over 50%. 5 Application to machine transliteration Machine transliteration is one of the primary potential applications of language identification because the language of a word often determines its pronunciation. We therefore tested language identification to see if results could indeed be improved by using language identification as a pre-processing step. 5.1 Data The English-Hindi corpus of names (Li et al., 2009; MSRI, 2009) contains a test set of 1000 names represented in both the Latin and Devanagari scripts. We manually classified these names as being of either Indian or non-Indian origin, occasionally resorting to web searches to help disambiguate them.1 We discarded those names that fell into both categories 1Our tagged data are available online at http://www. cs.ualberta.ca/˜ab31/langid/. Method Ch. Eng. Jap. All Lang. model 96.4 89.9 96.5 94.8 Linear SVM 99.0 94.8 97.6 97.6 Table 2: Language identification accuracy on the CEJ corpus. Language models have n = 4. (e.g. “Maya”) as well as those t</context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>H. Li, A. Kumaran, V. Pervouchine, and M. Zhang. 2009. Report of NEWS 2009 machine transliteration shared task. In Proc. of Named Entities Workshop: Shared Task on Transliteration, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MSRI</author>
</authors>
<date>2009</date>
<institution>Microsoft Research India.</institution>
<note>http:// research.microsoft.com/india.</note>
<contexts>
<context position="9077" citStr="MSRI, 2009" startWordPosition="1470" endWordPosition="1471"> of the corpus—so we tested only the linear kernel. Table 2 shows our results in comparison to those of language models reported in (Li et al., 2007); we reduce the error rate by over 50%. 5 Application to machine transliteration Machine transliteration is one of the primary potential applications of language identification because the language of a word often determines its pronunciation. We therefore tested language identification to see if results could indeed be improved by using language identification as a pre-processing step. 5.1 Data The English-Hindi corpus of names (Li et al., 2009; MSRI, 2009) contains a test set of 1000 names represented in both the Latin and Devanagari scripts. We manually classified these names as being of either Indian or non-Indian origin, occasionally resorting to web searches to help disambiguate them.1 We discarded those names that fell into both categories 1Our tagged data are available online at http://www. cs.ualberta.ca/˜ab31/langid/. Method Ch. Eng. Jap. All Lang. model 96.4 89.9 96.5 94.8 Linear SVM 99.0 94.8 97.6 97.6 Table 2: Language identification accuracy on the CEJ corpus. Language models have n = 4. (e.g. “Maya”) as well as those that we could </context>
</contexts>
<marker>MSRI, 2009</marker>
<rawString>MSRI, 2009. Microsoft Research India. http:// research.microsoft.com/india.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>