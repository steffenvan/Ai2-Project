<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001689">
<title confidence="0.990232">
Classifier Combination Techniques Applied to Coreference Resolution
</title>
<author confidence="0.999316">
Smita Vemulapalli1, Xiaoqiang Luo2, John F. Pitrelli2 and Imed Zitouni2
</author>
<affiliation confidence="0.99528">
1Center for Signal and Image Processing (CSIP) 2IBM T. J. Watson Research Center
School of ECE, Georgia Institute of Technology 1101 Kitchawan Road
</affiliation>
<address confidence="0.808716">
Atlanta, GA 30332, USA Yorktown Heights, NY 10598, USA
</address>
<email confidence="0.998622">
smita@ece.gatech.edu {xiaoluo,pitrelli,izitouni}@us.ibm.com
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999623533333333">
This paper examines the applicability of clas-
sifier combination approaches such as bagging
and boosting for coreference resolution. To
the best of our knowledge, this is the first ef-
fort that utilizes such techniques for corefer-
ence resolution. In this paper, we provide ex-
perimental evidence which indicates that the
accuracy of the coreference engine can po-
tentially be increased by use of bagging and
boosting methods, without any additional fea-
tures or training data. We implement and eval-
uate combination techniques at the mention,
entity and document level, and also address is-
sues like entity alignment, that are specific to
coreference resolution.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991791">
Coreference resolution is the task of partitioning a
set of mentions (i.e. person, organization and loca-
tion) into entities. A mention is an instance of textual
reference to an object, which can be either named
(e.g. Barack Obama), nominal (e.g. the president) or
pronominal (e.g. he, his, it). An entity is an aggre-
gate of all the mentions (of any level) which refer to
one conceptual entity. For example, in the following
sentence:
John said Mary was his sister.
there are four mentions: John, Mary, his, and
sister.
John and his belong to the one entity since they
refer to the same person; Mary and sister both
refer to another person entity. Furthermore, John
and Mary are named mentions, sister is a nomi-
nal mention and his is a pronominal mention.
</bodyText>
<page confidence="0.845771">
1
</page>
<bodyText confidence="0.999577461538462">
In this paper, we present a potential approach for
improving the performance of coreference resolu-
tion by using classifier combination techniques such
as bagging and boosting. To the best of our knowl-
edge, this is the first effort that utilizes classifier
combination for improving coreference resolution.
Combination methods have been applied to many
problems in natural-language processing (NLP). Ex-
amples include the ROVER system (Fiscus, 1997)
for speech recognition, the Multi-Engine Machine
Translation (MEMT) system (Jayaraman and Lavie,
2005), and part-of-speech tagging (Brill and Wu,
1998; Halteren et al., 2001). Most of these tech-
niques have shown a considerable improvement over
the performance of a single classifier and, therefore,
lead us to consider implementing such a multiple-
classifier system for coreference resolution as well.
Using classifier combination techniques one can
potentially achieve a classification accuracy that is
superior to that of the single best classifier. This
is based on the assumption that the errors made by
each of the classifiers are not identical, and there-
fore if we intelligently combine multiple classifier
outputs, we may be able to correct some of these er-
rors.
The main contributions of this paper are:
</bodyText>
<listItem confidence="0.988244333333333">
• Demonstrating the potential for improvement in
the baseline – By implementing a system that
behaves like an oracle, we have shown that the
output of the combination of multiple classifiers
has the potential to be significantly higher in ac-
curacy than any of the individual classifiers.
• Adapting traditional bagging techniques – Mul-
tiple classifiers, generated using bagging tech-
niques, were combined using an entity-level sum
</listItem>
<note confidence="0.932106">
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 1–6,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<listItem confidence="0.849133272727273">
rule and mention-level majority voting.
• Implementing a document-level boosting algo-
rithm – A boosting algorithm was implemented
in which a coreference resolution classifier was
iteratively trained using a re-weighted training
set, where the reweighting was done at the doc-
ument level.
• Addressing the problem of entity alignment –
In order to apply combination techniques to
multiple classifiers, we need to address entity-
alignment issues, explained later in this paper.
</listItem>
<bodyText confidence="0.9976647">
The baseline coreference system we use is sim-
ilar to the one described by Luo et al. (Luo et al.,
2004). In such a system, mentions are processed
sequentially, and at each step, a mention is either
linked to one of existing entities, or used to create a
new entity. At the end of this process, each possible
partition of the mentions corresponds to a unique se-
quence of link or creation actions, each of which is
scored by a statistical model. The one with the high-
est score is output as the final coreference result.
</bodyText>
<sectionHeader confidence="0.983984" genericHeader="introduction">
2 Classifier Combination Techniques
</sectionHeader>
<subsectionHeader confidence="0.995086">
2.1 Bagging
</subsectionHeader>
<bodyText confidence="0.999977166666667">
One way to obtain multiple classifiers is via bagging
or bootstrap aggregating (Breiman, 1996). These
classifiers, obtained using randomly-sampled train-
ing sets, may be combined to improve classification.
We generated several classifiers by two tech-
niques. In the first technique, we randomly sample
the set of documents (training set) to generate a few
classifiers. In the second technique, we need to re-
duce the feature set and this is not done in a random
fashion. Instead, we use our understanding of the in-
dividual features and also their relation to other fea-
tures to decide which features may be dropped.
</bodyText>
<subsectionHeader confidence="0.997823">
2.2 Oracle
</subsectionHeader>
<bodyText confidence="0.999945090909091">
In this paper, we refer to an oracle system which
uses knowledge of the truth. Here, truth, called the
gold standard henceforth, refers to mention detec-
tion and coreference resolution done by a human for
each document. It is possible that the gold standard
may have errors and is not perfect truth, but, as in
most NLP systems, it is considered the reference for
evaluating computer-based coreference resolution.
To understand the oracle, consider an example in
which the outputs of two classifiers for the same in-
put document are C1 and C2, as shown in Figure 1.
</bodyText>
<equation confidence="0.723334">
ile X File X File X File X
</equation>
<figureCaption confidence="0.999651">
Figure 1: Working of the oracle
</figureCaption>
<bodyText confidence="0.99995762962963">
The number of entities in C1 and C2 may not be the
same and even in cases where they are, the number
of mentions in corresponding entities may not be the
same. In fact, even finding the corresponding entity
in the other classifier output or in the gold standard
output G is not a trivial problem and requires us to
be able to align any two classifier outputs.
The alignment between any two coreference la-
belings, say C1 and G, for a document is the best
one-to-one map (Luo, 2005) between the entities of
C1 and G. To align the entities of C1 with those of
G, under the assumption that an entity in C1 may
be aligned with at most only one entity in G and
vice versa, we need to generate a bipartite graph
between the entities of C1 and G. Now the align-
ment task is a maximum bipartite matching prob-
lem. This is solved by using the Kuhn-Munkres al-
gorithm (Kuhn, 1955; Munkres, 1957). The weights
of the edges of the graph are entity-level alignment
measures. The metric we use is a relative mea-
sure of the similarity between the two entities. To
compute the similarity metric O (Luo, 2005) for the
entity pair (R, S), we use the formula shown in
Equation 1, where (n) represents the commonal-
ity with attribute-weighted partial scores. Attributes
are things such as (ACE) entity type, subtype, entity
class, etc.
</bodyText>
<footnote confidence="0.9790976">
1A mention may be repeated across multiple output entities,
which is not an unwarranted advantage as the scorer insists on
one-to-one entity alignment. So if there are two entities con-
taining mention A, at most one mention A is credited and the
other will hurt the score.
</footnote>
<figure confidence="0.943244625">
Classifier C1 Gold Classifier C2 Oracle Output
0.88
0.72
0.85
G-E1
G-E2
G-E3
G-E4
0.78
0.75
0.66
1.0
C2-EQ
C2-EP
C2-ER
C2-ES
</figure>
<equation confidence="0.984337333333333">
2 |R n S|
O(R, S) = (1)
|R |+ |S|
</equation>
<bodyText confidence="0.999931857142857">
The oracle output is a combination of the entities
in C1 and C2 with the highest entity-pair alignment
measures with the entities in G.1 We can see in Fig-
ure 1 that the entity G-E1 is aligned with entities C1-
EA and C2-EP. We pick the entity with the highest
entity-pair alignment measure (highlighted in gray)
which, in this case, is C1-EA. This is repeated for
</bodyText>
<page confidence="0.958097">
2
</page>
<subsectionHeader confidence="0.992766">
2.3 Preliminary Combination Approaches
</subsectionHeader>
<bodyText confidence="0.999986352941176">
Imitating the oracle. Making use of the existing
framework of the oracle, we implement a combina-
tion technique that imitates the oracle except that in
this case, we do not have the gold standard. If we
have N classifiers Ci, i = 1 to N, then we replace
the gold standard by each of the N classifiers in suc-
cession, to get N outputs Combi, i = 1 to N.
The task of generating multiple classifier combi-
nation outputs that have a higher accuracy than the
original classifiers is often considered to be easier
than the task of determining the best of these out-
puts. We used the formulas in Equations 2, 3 and 4
to assign a score Si to each of the N combination
outputs Combi, and then we pick the one with the
highest score. The function Sc (which corresponds
to the function φ in Equation 1) gives the similarity
between the entities in the pair (R, S).
</bodyText>
<equation confidence="0.9993175">
1 � Si = N − 1 j = 1 to N Sc(Combi, Cj) (2)
j =� %
Si = Sc(Combi, Ci) (3)
Si = 1 E
N − 1 j = 1 to N Sc(Combi, Combj) (4)
j =� %
</equation>
<bodyText confidence="0.999567181818182">
Entity-level sum-rule. We implemented a basic sum-
rule at the entity level, where we generate only one
combination classifier output by aligning the entities
in the N classifiers and picking only one entity at
each level of alignment. In the oracle, the reference
for entity-alignment was the gold standard. Here,
we use the baseline/full system (generated using the
entire training and feature set) to do this. The entity-
level alignment is represented as a table in Figure 2.
Let Ai, i = 1 to M be the aligned entities in one
row of the table in Figure 2. Here, M G N if
</bodyText>
<figureCaption confidence="0.996821">
Figure 3: Mention-level majority voting
</figureCaption>
<bodyText confidence="0.997686166666667">
we exclude the baseline from the combination and
M G N + 1 if we include it. To pick one entity
out of these M entities, we use the traditional sum
rule (Tulyakov et al., 2008), shown in Equation 5, to
compute the S(Ai) for each Ai and pick the entity
with the highest S(Ai) value.
</bodyText>
<equation confidence="0.999092333333333">
S(Ai) = � Sc(Ai, Aj) (5)
j=1toN
j =� %
</equation>
<subsectionHeader confidence="0.991793">
2.4 Mention-level Majority Voting
</subsectionHeader>
<bodyText confidence="0.999890923076923">
In the previous techniques, entities are either picked
or rejected as a whole but never broken down fur-
ther. In the mention-level majority voting technique,
we work at the mention level, so the entities created
after combination may be different from the entities
of all the classifiers that are being combined.
In the entity-level alignment table (shown in Fig-
ure 3), A, B, C and D refer to the entities in the base-
line system and A1, A2, ..., D4 represent the enti-
ties of the input classifiers that are aligned with each
of the baseline classifier entities. Majority voting is
done by counting the number of times a mention is
found in a set of aligned entities. So for every row
in the table, we have a mention count. The row with
the highest mention count is assigned the mention in
the output. This is repeated for each mention in the
document. In Figure 3, we are voting for the men-
tion m1, which is found to have a voting count of 3
(the majority vote) at the entity-level A and a count
of 1 at the entity-level C, so the mention is assigned
to the entity A. It is important to note that some clas-
sifier entities may not align with any baseline clas-
sifier entity as we allow only a one-to-one mapping
during alignment. Such entities will not be a part of
the alignment table. If this number is large, it may
have a considerable effect on the combination.
</bodyText>
<subsectionHeader confidence="0.922687">
2.5 Document-level Boosting
</subsectionHeader>
<bodyText confidence="0.9994582">
Boosting techniques (Schapire, 1999) combine mul-
tiple classifiers, built iteratively and trained on
re-weighted data, to improve classification accu-
racy. Since coreference resolution is done for a
whole document, we can not split a document fur-
</bodyText>
<figure confidence="0.7588485">
Classifier Ct Full F Classifier C2
File X File X File X
</figure>
<figureCaption confidence="0.982512">
Figure 2: Entity alignment between classifier outputs
</figureCaption>
<bodyText confidence="0.474215">
every entity in G. The oracle output can be seen in
the right-hand side of Figure 1. This technique can
be scaled up to work for any number of classifiers.
</bodyText>
<figure confidence="0.996760563636364">
Entity-level
Alignment Table
F-El
Cl-EA C2-EP
F-E2
C1-EB C2-ER
F-E3
Cl-ED C2-EQ
F-E4
C2-ES
0.88
0.72
0.85
F-El
F-E2
F-E3
F-E4
0.78
0.75
1.0
0.6
C2-EQ
C2-ER
C2-EP
C2-ES
C1-EA
C1-EB
Cl-EC
Cl-ED
Majority
voting for
mention m1
ention
m1
Entity-level
Alignment Table
Mention
Output
Count for m1
A{m1,m2,m6}
A
3
A1 A2 A3 A4 ...
B{ m3}
B
0
B1 B2 B4 ...
C{ m4,m5}
1
C
C1 C2 C3 C4 ...
D{m7 }
0
D
D2 D3 D4 ...
</figure>
<page confidence="0.974439">
3
</page>
<tableCaption confidence="0.989431">
Table 1: Statistics of ACE 2005 data
</tableCaption>
<table confidence="0.9910855">
DataSet #Docs #Words #Mentions #Entities
Training 499 253771 46646 16102
Test 100 45659 8178 2709
Total 599 299430 54824 18811
</table>
<tableCaption confidence="0.842526">
Table 2: Accuracy of generated and baseline classifiers
</tableCaption>
<figure confidence="0.9769402">
Classifier Accuracy (%)
C1 − C15 Average 77.52
Highest 79.16
Lowest 75.81
C0 Baseline 78.53
</figure>
<figureCaption confidence="0.999876">
Figure 4: Document-level boosting
</figureCaption>
<bodyText confidence="0.998165464285715">
ther. So when we re-weight the training set, we
are actually re-weighting the documents (hence the
name document-level boosting). Figure 4 shows an
overview of this technique.
The decision of which documents to boost is
made using two thresholds: percentile threshold
Pthresh and the F-measure threshold Fthresh. Doc-
uments in the test set that are in the lowest Pthresh
percentile and that have a document F-measure less
than Fthresh will be boosted in the training set for
the next iteration. We shuffle the training set to cre-
ate some randomness and then divide it into groups
of training and test sets in a round-robin fashion such
that a predetermined ratio of the number of training
documents to the number of test documents is main-
tained. In Figure 4, the light gray regions refer to
training documents and the dark gray regions refer
to test documents. Another important consideration
is that it is difficult to achieve good coreference res-
olution performance on documents of some genres
compared to others, even if they are boosted signif-
icantly. In an iterative process, it is likely that doc-
uments of such genres will get repeatedly boosted.
Also our training set has more documents of some
genres and fewer of others. So we try to maintain, to
some extent, the ratio of documents from different
genres in the training set while splitting this training
set further into groups of training and test sets.
</bodyText>
<sectionHeader confidence="0.999381" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.987997195652174">
This section describes the general setup used to con-
duct the experiments and presents an evaluation of
the combination techniques that were implemented.
Experimental setup. The coreference resolution
system used in our experiments makes use of a Max-
imum Entropy model which has lexical, syntacti-
cal, semantic and discourse features (Luo et al.,
2004). Experiments are conducted on ACE 2005
data (NIST, 2005), which consists of 599 documents
from rich and diversified sources. We reserve the
last 16% documents of each source as the test set,
and use the rest of the documents as the training set.
The ACE 2005 data split is tabulated in Table 1.
Bagging A total of 15 classifiers (C1 to C15) were
generated, 12 of which were obtained by sampling
the training set and the remaining 3 by sampling
the feature set. We also make use of the base-
line classifier C0. The accuracy of C0 to C15 has
been summarized in Table 2. The agreement be-
tween the classifiers’ output was found to be in the
range of 93% to 95%. In this paper, the metric used
to compute the accuracy of the coreference resolu-
tion is the Constrained Entity-Alignment F-Measure
(CEAF) (Luo, 2005) with the entity-pair similarity
measure in Equation 1.
Oracle. To conduct the oracle experiment, we train
1 to 15 classifiers and align their output to the gold
standard. For all entities aligned with a gold entity,
we pick the one with the highest score as the output.
We measure the performance for varying number of
classifiers, and the result is plotted in Figure 5.
First, we observe a steady and significant increase
in CEAF for every additional classifier, because ad-
ditional classifiers can only improve the alignment
score. Second, we note that the oracle accuracy is
87.58% for a single input classifier C1, i.e. an abso-
lute gain of 9% compared to C0. This is because the
availability of gold entities makes it possible to re-
move many false-alarm entities. Finally, the oracle
accuracy when all 15 classifiers are used as input is
94.59%, a 16.06% absolute improvement.
This experiment helps us to understand the perfor-
mance bound of combining multiple classifiers and
the contribution of every additional classifier.
Preliminary combination approaches. While the
oracle results are encouraging, a natural question is
</bodyText>
<page confidence="0.995668">
4
</page>
<figureCaption confidence="0.993579666666667">
Figure 5: Oracle performance vs. number of classifiers
Figure 6: A real example showing the working of
mention-level majority voting
</figureCaption>
<bodyText confidence="0.999745">
how much performance gain can be attained if the
gold standard is not available. To answer this ques-
tion, we replace the gold standard with one of the
classifiers C1 to C15, and align the classifiers. This
is done in a round robin fashion as described in Sec-
tion 2.3. The best performance of this procedure is
77.93%. The sum-rule combination output had an
accuracy of 78.65% with a slightly different base-
line of 78.81%. These techniques do not yield a sta-
tistically significant increase in CEAF but this is not
surprising as C1 to C15 are highly correlated.
Mention-level majority voting. This experiment is
conducted to evaluate the mention-level majority
voting technique. The results are not statistically
better than the baseline, but they give us valuable
insight into the working of the combination tech-
nique. The example in Figure 6 shows a single
entity-alignment level for the baseline C0 and 3 clas-
sifiers C1, C2, and C3 and the combination output
by mention-level majority voting. The mentions are
denoted by the notation ‘EntityID - MentionID’, for
example 7-10 is the mention with EntityID=7 and
MentionID=10. Here, we use the EntityID in the
gold file. The mentions with EntityID=7 are “cor-
rect” i.e. they belong in this entity, and the others
are “wrong” i.e. they do not belong in this entity.
The aligned mentions are of four types:
</bodyText>
<listItem confidence="0.995854">
• Type I mentions – These mentions have a highest
voting count of 2 or more at the same entity-level
alignment and hence appear in the output.
• Type II mentions – These mentions have a high-
</listItem>
<bodyText confidence="0.987551583333333">
est voting count of 1. But they are present in
more than one input classifier and there is a tie
between the mention counts at different entity-
level alignments. The rule to break the tie is
that mentions are included if they are also seen
in the full system C0. As can been seen, this rule
brings in correct mentions such as 7-61, 7-63,
7-64, but it also admits 20-33,20-39 and 20-62.
In the oracle, the gold standard helps to remove
entities with false-alarm mentions, whereas the
full system output is noisy and it is not strong
enough to reliably remove undesired mentions.
</bodyText>
<listItem confidence="0.9816696">
• Type III mentions – There is only one mention
20-66 which is of this type. It is selected in the
combination output since it is present in C2 and
the baseline C0, although it has been rejected as
a false-alarm in C1 and C3.
• Type IV mentions – These false-alarm mentions
(relative to C0) are rejected in the output. As can
be seen, this correctly rejects mentions such as
15-22 and 20-68, but it also rejects correct men-
tions 7-18, 7-19 and 7-30.
</listItem>
<bodyText confidence="0.999981">
In summary, the current implementation of this
technique has a limited ability to distinguish correct
mentions from wrong ones due to the noisy nature
of C0 which is used for alignment. We also observe
that mentions spread across different alignments of-
ten have low-count and they are often tied in count.
Therefore, it is important to set a minimum thresh-
old for accepting these low-count majority votes and
also investigate better tie-breaking techniques.
Document-level Boosting This experiment is con-
ducted to evaluate the document-level boosting tech-
nique. Table 3 shows the results with the ratio
of the number of training documents to the num-
ber of test documents equal to 80:20, F-measure
threshold Fthresh = 74% and percentile threshold
Pthresh = 25%. The accuracy increases by 0.7%,
relative to the baseline. Due to computational com-
plexity considerations, we used fixed values for the
parameters. Therefore, these values may be sub-
optimal and may not correspond to the best possible
increase in accuracy.
</bodyText>
<sectionHeader confidence="0.999991" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.970477333333333">
A large body of literature related to statistical meth-
ods for coreference resolution is available (Ng and
Cardie, 2003; Yang et al., 2003; Ng, 2008; Poon and
</bodyText>
<figure confidence="0.999003203703703">
Baseline Classifier Classifier Classifier Combination
Cl Cz Ca Output
7-10
7-17
7-27
7-61
7-63
7-64
20-39
20-62
20-66
7-17
7-27
7-30
15-22
20-33
20-68
37-56
7-10
7-17
7-18
7-19
7-27
legend.
Type I
mentions
Type II
mentions
Type III
mentions
Type IV
mentions
7-10
7-17
7-27
7-61
7-63
7-64
20-33
20-39
20-62
20-66
37-56
7-10
7-17
7-27
7-61
7-63
7-64
20-33
20-39
20-62
20-66
37-56
</figure>
<page confidence="0.91774">
5
</page>
<tableCaption confidence="0.995036">
Table 3: Results of document-level boosting
</tableCaption>
<table confidence="0.872499">
Iteration Accuracy (%)
1 78.53
2 78.82
3 79.08
4 78.37
</table>
<bodyText confidence="0.96655325">
Domingos, 2008; McCallum and Wellner, 2003).
Poon and Domingos (Poon and Domingos, 2008)
use an unsupervised technique based on joint infer-
ence across mentions and Markov logic as a repre-
sentation language for their system on both MUC
and ACE data. Ng (Ng, 2008) proposed a genera-
tive model for unsupervised coreference resolution
that views coreference as an EM clustering process.
In this paper, we make use of a coreference engine
similar to the one described by Luo et al. (Luo et al.,
2004), where a Bell tree representation and a Maxi-
mum entropy framework are used to provide a natu-
rally incremental framework for coreference resolu-
tion. To the best of our knowledge, this is the first ef-
fort that utilizes classifier combination techniques to
improve coreference resolution. Combination tech-
niques have earlier been applied to various applica-
tions including machine translation (Jayaraman and
Lavie, 2005), part-of-speech tagging (Brill and Wu,
1998) and base noun phrase identification (Sang et
al., 2000). However, the use of these techniques for
coreference resolution presents a unique set of chal-
lenges, such as the issue of entity alignment between
the multiple classifier outputs.
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999975857142857">
In this paper, we examined and evaluated the ap-
plicability of bagging and boosting techniques to
coreference resolution. We also provided empir-
ical evidence that coreference resolution accuracy
can potentially be improved by using multiple clas-
sifiers. In future, we plan to improve (1) the entity-
alignment strategy, (2) the majority voting technique
by setting a minimum threshold for the majority-
vote and better tie-breaking, and (3) the boosting
algorithm to automatically optimize the parameters
that have been manually set in this paper. Another
possible avenue for future work would be to test
these combination techniques with other coreference
resolution systems.
</bodyText>
<sectionHeader confidence="0.999552" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998564666666667">
The authors would like to acknowledge Ganesh N.
Ramaswamy for his guidance and support in con-
ducting the research presented in this paper.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662404255319">
L. Breiman. 1996. Bagging predictors. In Machine
Learning.
E. Brill and J. Wu. 1998. Classifier combination for im-
proved lexical disambiguation. In Proc. of COLING.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recogniser output voting error
reduction (rover). In Proc. of ASRU.
H. V. Halteren et al. 2001. Improving accuracy in
word class tagging through the combination of ma-
chine learning systems. Computational Linguistics,
27.
S. Jayaraman and A. Lavie. 2005. Multi-engine machine
translation guided by explicit word matching. In Proc.
of ACL.
H. W. Kuhn. 1955. The hungarian method for the assign-
ment problem. Naval Research Logistics Quarterly, 2.
X. Luo et al. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proc. of
ACL.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. of EMNLP.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proc. of IJCAI/IIWeb.
J. Munkres. 1957. Algorithms for the assignment and
transportation problems. Journal of the Society of In-
dustrial and Applied Mathematics, 5(1).
V. Ng and C. Cardie. 2003. Bootstrapping coreference
classifiers with multiple machine learning algorithms.
In Proc. of EMNLP.
V. Ng. 2008. Unsupervised models for coreference reso-
lution. In Proc. of EMNLP.
NIST. 2005. ACE’05 evaluation. www.nist.gov/
speech/tests/ace/ace05/index.html.
H. Poon and P. Domingos. 2008. Joint unsupervised
coreference resolution with Markov Logic. In Proc.
of EMNLP.
E. F. T. K. Sang et al. 2000. Applying system combi-
nation to base noun phrase identification. In Proc. of
COLING 2000.
R.E. Schapire. 1999. A brief introduction to boosting. In
Proc. of IJCAI.
S. Tulyakov et al. 2008. Review of classifier combi-
nation methods. In Machine Learning in Document
Analysis and Recognition.
X. Yang et al. 2003. Coreference resolution using com-
petition learning approach. In Proc. of ACL.
</reference>
<page confidence="0.998772">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.575235">
<title confidence="0.996599">Classifier Combination Techniques Applied to Coreference Resolution</title>
<author confidence="0.858815">Xiaoqiang John F</author>
<author confidence="0.858815">Imed</author>
<affiliation confidence="0.812797">for Signal and Image Processing (CSIP) T. J. Watson Research Center School of ECE, Georgia Institute of Technology 1101 Kitchawan Road</affiliation>
<address confidence="0.996995">Atlanta, GA 30332, USA Yorktown Heights, NY 10598, USA</address>
<abstract confidence="0.998317875">This paper examines the applicability of classifier combination approaches such as bagging and boosting for coreference resolution. To the best of our knowledge, this is the first effort that utilizes such techniques for coreference resolution. In this paper, we provide experimental evidence which indicates that the accuracy of the coreference engine can potentially be increased by use of bagging and boosting methods, without any additional features or training data. We implement and evaluate combination techniques at the mention, entity and document level, and also address issues like entity alignment, that are specific to coreference resolution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>In Machine Learning.</booktitle>
<contexts>
<context position="4833" citStr="Breiman, 1996" startWordPosition="754" endWordPosition="755">lar to the one described by Luo et al. (Luo et al., 2004). In such a system, mentions are processed sequentially, and at each step, a mention is either linked to one of existing entities, or used to create a new entity. At the end of this process, each possible partition of the mentions corresponds to a unique sequence of link or creation actions, each of which is scored by a statistical model. The one with the highest score is output as the final coreference result. 2 Classifier Combination Techniques 2.1 Bagging One way to obtain multiple classifiers is via bagging or bootstrap aggregating (Breiman, 1996). These classifiers, obtained using randomly-sampled training sets, may be combined to improve classification. We generated several classifiers by two techniques. In the first technique, we randomly sample the set of documents (training set) to generate a few classifiers. In the second technique, we need to reduce the feature set and this is not done in a random fashion. Instead, we use our understanding of the individual features and also their relation to other features to decide which features may be dropped. 2.2 Oracle In this paper, we refer to an oracle system which uses knowledge of the</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman. 1996. Bagging predictors. In Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="2438" citStr="Brill and Wu, 1998" startWordPosition="371" endWordPosition="374">onominal mention. 1 In this paper, we present a potential approach for improving the performance of coreference resolution by using classifier combination techniques such as bagging and boosting. To the best of our knowledge, this is the first effort that utilizes classifier combination for improving coreference resolution. Combination methods have been applied to many problems in natural-language processing (NLP). Examples include the ROVER system (Fiscus, 1997) for speech recognition, the Multi-Engine Machine Translation (MEMT) system (Jayaraman and Lavie, 2005), and part-of-speech tagging (Brill and Wu, 1998; Halteren et al., 2001). Most of these techniques have shown a considerable improvement over the performance of a single classifier and, therefore, lead us to consider implementing such a multipleclassifier system for coreference resolution as well. Using classifier combination techniques one can potentially achieve a classification accuracy that is superior to that of the single best classifier. This is based on the assumption that the errors made by each of the classifiers are not identical, and therefore if we intelligently combine multiple classifier outputs, we may be able to correct som</context>
<context position="21766" citStr="Brill and Wu, 1998" startWordPosition="3722" endWordPosition="3725">ews coreference as an EM clustering process. In this paper, we make use of a coreference engine similar to the one described by Luo et al. (Luo et al., 2004), where a Bell tree representation and a Maximum entropy framework are used to provide a naturally incremental framework for coreference resolution. To the best of our knowledge, this is the first effort that utilizes classifier combination techniques to improve coreference resolution. Combination techniques have earlier been applied to various applications including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al., 2000). However, the use of these techniques for coreference resolution presents a unique set of challenges, such as the issue of entity alignment between the multiple classifier outputs. 5 Conclusions and Future Work In this paper, we examined and evaluated the applicability of bagging and boosting techniques to coreference resolution. We also provided empirical evidence that coreference resolution accuracy can potentially be improved by using multiple classifiers. In future, we plan to improve (1) the entityalignment strategy, (2) the majorit</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recogniser output voting error reduction (rover).</title>
<date>1997</date>
<booktitle>In Proc. of ASRU.</booktitle>
<contexts>
<context position="2287" citStr="Fiscus, 1997" startWordPosition="353" endWordPosition="354">n; Mary and sister both refer to another person entity. Furthermore, John and Mary are named mentions, sister is a nominal mention and his is a pronominal mention. 1 In this paper, we present a potential approach for improving the performance of coreference resolution by using classifier combination techniques such as bagging and boosting. To the best of our knowledge, this is the first effort that utilizes classifier combination for improving coreference resolution. Combination methods have been applied to many problems in natural-language processing (NLP). Examples include the ROVER system (Fiscus, 1997) for speech recognition, the Multi-Engine Machine Translation (MEMT) system (Jayaraman and Lavie, 2005), and part-of-speech tagging (Brill and Wu, 1998; Halteren et al., 2001). Most of these techniques have shown a considerable improvement over the performance of a single classifier and, therefore, lead us to consider implementing such a multipleclassifier system for coreference resolution as well. Using classifier combination techniques one can potentially achieve a classification accuracy that is superior to that of the single best classifier. This is based on the assumption that the errors </context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>J. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recogniser output voting error reduction (rover). In Proc. of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H V Halteren</author>
</authors>
<title>Improving accuracy in word class tagging through the combination of machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<marker>Halteren, 2001</marker>
<rawString>H. V. Halteren et al. 2001. Improving accuracy in word class tagging through the combination of machine learning systems. Computational Linguistics, 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jayaraman</author>
<author>A Lavie</author>
</authors>
<title>Multi-engine machine translation guided by explicit word matching.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2390" citStr="Jayaraman and Lavie, 2005" startWordPosition="364" endWordPosition="367">ed mentions, sister is a nominal mention and his is a pronominal mention. 1 In this paper, we present a potential approach for improving the performance of coreference resolution by using classifier combination techniques such as bagging and boosting. To the best of our knowledge, this is the first effort that utilizes classifier combination for improving coreference resolution. Combination methods have been applied to many problems in natural-language processing (NLP). Examples include the ROVER system (Fiscus, 1997) for speech recognition, the Multi-Engine Machine Translation (MEMT) system (Jayaraman and Lavie, 2005), and part-of-speech tagging (Brill and Wu, 1998; Halteren et al., 2001). Most of these techniques have shown a considerable improvement over the performance of a single classifier and, therefore, lead us to consider implementing such a multipleclassifier system for coreference resolution as well. Using classifier combination techniques one can potentially achieve a classification accuracy that is superior to that of the single best classifier. This is based on the assumption that the errors made by each of the classifiers are not identical, and therefore if we intelligently combine multiple c</context>
<context position="21721" citStr="Jayaraman and Lavie, 2005" startWordPosition="3716" endWordPosition="3719">odel for unsupervised coreference resolution that views coreference as an EM clustering process. In this paper, we make use of a coreference engine similar to the one described by Luo et al. (Luo et al., 2004), where a Bell tree representation and a Maximum entropy framework are used to provide a naturally incremental framework for coreference resolution. To the best of our knowledge, this is the first effort that utilizes classifier combination techniques to improve coreference resolution. Combination techniques have earlier been applied to various applications including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al., 2000). However, the use of these techniques for coreference resolution presents a unique set of challenges, such as the issue of entity alignment between the multiple classifier outputs. 5 Conclusions and Future Work In this paper, we examined and evaluated the applicability of bagging and boosting techniques to coreference resolution. We also provided empirical evidence that coreference resolution accuracy can potentially be improved by using multiple classifiers. In future, we plan to improve (1) </context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>S. Jayaraman and A. Lavie. 2005. Multi-engine machine translation guided by explicit word matching. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Kuhn</author>
</authors>
<title>The hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<volume>2</volume>
<contexts>
<context position="6850" citStr="Kuhn, 1955" startWordPosition="1119" endWordPosition="1120">andard output G is not a trivial problem and requires us to be able to align any two classifier outputs. The alignment between any two coreference labelings, say C1 and G, for a document is the best one-to-one map (Luo, 2005) between the entities of C1 and G. To align the entities of C1 with those of G, under the assumption that an entity in C1 may be aligned with at most only one entity in G and vice versa, we need to generate a bipartite graph between the entities of C1 and G. Now the alignment task is a maximum bipartite matching problem. This is solved by using the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957). The weights of the edges of the graph are entity-level alignment measures. The metric we use is a relative measure of the similarity between the two entities. To compute the similarity metric O (Luo, 2005) for the entity pair (R, S), we use the formula shown in Equation 1, where (n) represents the commonality with attribute-weighted partial scores. Attributes are things such as (ACE) entity type, subtype, entity class, etc. 1A mention may be repeated across multiple output entities, which is not an unwarranted advantage as the scorer insists on one-to-one entity alignment. So</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>H. W. Kuhn. 1955. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Luo, 2004</marker>
<rawString>X. Luo et al. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="6465" citStr="Luo, 2005" startWordPosition="1043" endWordPosition="1044">fiers for the same input document are C1 and C2, as shown in Figure 1. ile X File X File X File X Figure 1: Working of the oracle The number of entities in C1 and C2 may not be the same and even in cases where they are, the number of mentions in corresponding entities may not be the same. In fact, even finding the corresponding entity in the other classifier output or in the gold standard output G is not a trivial problem and requires us to be able to align any two classifier outputs. The alignment between any two coreference labelings, say C1 and G, for a document is the best one-to-one map (Luo, 2005) between the entities of C1 and G. To align the entities of C1 with those of G, under the assumption that an entity in C1 may be aligned with at most only one entity in G and vice versa, we need to generate a bipartite graph between the entities of C1 and G. Now the alignment task is a maximum bipartite matching problem. This is solved by using the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957). The weights of the edges of the graph are entity-level alignment measures. The metric we use is a relative measure of the similarity between the two entities. To compute the similarity metric O (Lu</context>
<context position="15308" citStr="Luo, 2005" startWordPosition="2640" endWordPosition="2641">se the rest of the documents as the training set. The ACE 2005 data split is tabulated in Table 1. Bagging A total of 15 classifiers (C1 to C15) were generated, 12 of which were obtained by sampling the training set and the remaining 3 by sampling the feature set. We also make use of the baseline classifier C0. The accuracy of C0 to C15 has been summarized in Table 2. The agreement between the classifiers’ output was found to be in the range of 93% to 95%. In this paper, the metric used to compute the accuracy of the coreference resolution is the Constrained Entity-Alignment F-Measure (CEAF) (Luo, 2005) with the entity-pair similarity measure in Equation 1. Oracle. To conduct the oracle experiment, we train 1 to 15 classifiers and align their output to the gold standard. For all entities aligned with a gold entity, we pick the one with the highest score as the output. We measure the performance for varying number of classifiers, and the result is plotted in Figure 5. First, we observe a steady and significant increase in CEAF for every additional classifier, because additional classifiers can only improve the alignment score. Second, we note that the oracle accuracy is 87.58% for a single in</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On coreference resolution performance metrics. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In Proc. of IJCAI/IIWeb.</booktitle>
<contexts>
<context position="20852" citStr="McCallum and Wellner, 2003" startWordPosition="3575" endWordPosition="3578">hods for coreference resolution is available (Ng and Cardie, 2003; Yang et al., 2003; Ng, 2008; Poon and Baseline Classifier Classifier Classifier Combination Cl Cz Ca Output 7-10 7-17 7-27 7-61 7-63 7-64 20-39 20-62 20-66 7-17 7-27 7-30 15-22 20-33 20-68 37-56 7-10 7-17 7-18 7-19 7-27 legend. Type I mentions Type II mentions Type III mentions Type IV mentions 7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 5 Table 3: Results of document-level boosting Iteration Accuracy (%) 1 78.53 2 78.82 3 79.08 4 78.37 Domingos, 2008; McCallum and Wellner, 2003). Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. Ng (Ng, 2008) proposed a generative model for unsupervised coreference resolution that views coreference as an EM clustering process. In this paper, we make use of a coreference engine similar to the one described by Luo et al. (Luo et al., 2004), where a Bell tree representation and a Maximum entropy framework are used to provide a naturally incremental framework for coreference resolution.</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In Proc. of IJCAI/IIWeb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>Journal of the Society of Industrial and Applied Mathematics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="6866" citStr="Munkres, 1957" startWordPosition="1121" endWordPosition="1122">t G is not a trivial problem and requires us to be able to align any two classifier outputs. The alignment between any two coreference labelings, say C1 and G, for a document is the best one-to-one map (Luo, 2005) between the entities of C1 and G. To align the entities of C1 with those of G, under the assumption that an entity in C1 may be aligned with at most only one entity in G and vice versa, we need to generate a bipartite graph between the entities of C1 and G. Now the alignment task is a maximum bipartite matching problem. This is solved by using the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957). The weights of the edges of the graph are entity-level alignment measures. The metric we use is a relative measure of the similarity between the two entities. To compute the similarity metric O (Luo, 2005) for the entity pair (R, S), we use the formula shown in Equation 1, where (n) represents the commonality with attribute-weighted partial scores. Attributes are things such as (ACE) entity type, subtype, entity class, etc. 1A mention may be repeated across multiple output entities, which is not an unwarranted advantage as the scorer insists on one-to-one entity alignment. So if there are tw</context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>J. Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society of Industrial and Applied Mathematics, 5(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Bootstrapping coreference classifiers with multiple machine learning algorithms.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="20290" citStr="Ng and Cardie, 2003" startWordPosition="3478" endWordPosition="3481">ument-level boosting technique. Table 3 shows the results with the ratio of the number of training documents to the number of test documents equal to 80:20, F-measure threshold Fthresh = 74% and percentile threshold Pthresh = 25%. The accuracy increases by 0.7%, relative to the baseline. Due to computational complexity considerations, we used fixed values for the parameters. Therefore, these values may be suboptimal and may not correspond to the best possible increase in accuracy. 4 Related Work A large body of literature related to statistical methods for coreference resolution is available (Ng and Cardie, 2003; Yang et al., 2003; Ng, 2008; Poon and Baseline Classifier Classifier Classifier Combination Cl Cz Ca Output 7-10 7-17 7-27 7-61 7-63 7-64 20-39 20-62 20-66 7-17 7-27 7-30 15-22 20-33 20-68 37-56 7-10 7-17 7-18 7-19 7-27 legend. Type I mentions Type II mentions Type III mentions Type IV mentions 7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 5 Table 3: Results of document-level boosting Iteration Accuracy (%) 1 78.53 2 78.82 3 79.08 4 78.37 Domingos, 2008; McCallum and Wellner, 2003). Poon and Domingos (Poon and Domingos</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>V. Ng and C. Cardie. 2003. Bootstrapping coreference classifiers with multiple machine learning algorithms. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Unsupervised models for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="20319" citStr="Ng, 2008" startWordPosition="3486" endWordPosition="3487">shows the results with the ratio of the number of training documents to the number of test documents equal to 80:20, F-measure threshold Fthresh = 74% and percentile threshold Pthresh = 25%. The accuracy increases by 0.7%, relative to the baseline. Due to computational complexity considerations, we used fixed values for the parameters. Therefore, these values may be suboptimal and may not correspond to the best possible increase in accuracy. 4 Related Work A large body of literature related to statistical methods for coreference resolution is available (Ng and Cardie, 2003; Yang et al., 2003; Ng, 2008; Poon and Baseline Classifier Classifier Classifier Combination Cl Cz Ca Output 7-10 7-17 7-27 7-61 7-63 7-64 20-39 20-62 20-66 7-17 7-27 7-30 15-22 20-33 20-68 37-56 7-10 7-17 7-18 7-19 7-27 legend. Type I mentions Type II mentions Type III mentions Type IV mentions 7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 5 Table 3: Results of document-level boosting Iteration Accuracy (%) 1 78.53 2 78.82 3 79.08 4 78.37 Domingos, 2008; McCallum and Wellner, 2003). Poon and Domingos (Poon and Domingos, 2008) use an unsupervised t</context>
</contexts>
<marker>Ng, 2008</marker>
<rawString>V. Ng. 2008. Unsupervised models for coreference resolution. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>ACE’05 evaluation.</title>
<date>2005</date>
<note>www.nist.gov/ speech/tests/ace/ace05/index.html.</note>
<contexts>
<context position="14558" citStr="NIST, 2005" startWordPosition="2503" endWordPosition="2504">hers. So we try to maintain, to some extent, the ratio of documents from different genres in the training set while splitting this training set further into groups of training and test sets. 3 Evaluation This section describes the general setup used to conduct the experiments and presents an evaluation of the combination techniques that were implemented. Experimental setup. The coreference resolution system used in our experiments makes use of a Maximum Entropy model which has lexical, syntactical, semantic and discourse features (Luo et al., 2004). Experiments are conducted on ACE 2005 data (NIST, 2005), which consists of 599 documents from rich and diversified sources. We reserve the last 16% documents of each source as the test set, and use the rest of the documents as the training set. The ACE 2005 data split is tabulated in Table 1. Bagging A total of 15 classifiers (C1 to C15) were generated, 12 of which were obtained by sampling the training set and the remaining 3 by sampling the feature set. We also make use of the baseline classifier C0. The accuracy of C0 to C15 has been summarized in Table 2. The agreement between the classifiers’ output was found to be in the range of 93% to 95%.</context>
</contexts>
<marker>NIST, 2005</marker>
<rawString>NIST. 2005. ACE’05 evaluation. www.nist.gov/ speech/tests/ace/ace05/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
<author>P Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov Logic.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="20897" citStr="Poon and Domingos, 2008" startWordPosition="3582" endWordPosition="3585"> and Cardie, 2003; Yang et al., 2003; Ng, 2008; Poon and Baseline Classifier Classifier Classifier Combination Cl Cz Ca Output 7-10 7-17 7-27 7-61 7-63 7-64 20-39 20-62 20-66 7-17 7-27 7-30 15-22 20-33 20-68 37-56 7-10 7-17 7-18 7-19 7-27 legend. Type I mentions Type II mentions Type III mentions Type IV mentions 7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 7-10 7-17 7-27 7-61 7-63 7-64 20-33 20-39 20-62 20-66 37-56 5 Table 3: Results of document-level boosting Iteration Accuracy (%) 1 78.53 2 78.82 3 79.08 4 78.37 Domingos, 2008; McCallum and Wellner, 2003). Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. Ng (Ng, 2008) proposed a generative model for unsupervised coreference resolution that views coreference as an EM clustering process. In this paper, we make use of a coreference engine similar to the one described by Luo et al. (Luo et al., 2004), where a Bell tree representation and a Maximum entropy framework are used to provide a naturally incremental framework for coreference resolution. To the best of our knowledge, this is the fi</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>H. Poon and P. Domingos. 2008. Joint unsupervised coreference resolution with Markov Logic. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
</authors>
<title>Applying system combination to base noun phrase identification.</title>
<date>2000</date>
<booktitle>In Proc. of COLING</booktitle>
<marker>Sang, 2000</marker>
<rawString>E. F. T. K. Sang et al. 2000. Applying system combination to base noun phrase identification. In Proc. of COLING 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
</authors>
<title>A brief introduction to boosting.</title>
<date>1999</date>
<booktitle>In Proc. of IJCAI.</booktitle>
<contexts>
<context position="11502" citStr="Schapire, 1999" startWordPosition="1983" endWordPosition="1984">ated for each mention in the document. In Figure 3, we are voting for the mention m1, which is found to have a voting count of 3 (the majority vote) at the entity-level A and a count of 1 at the entity-level C, so the mention is assigned to the entity A. It is important to note that some classifier entities may not align with any baseline classifier entity as we allow only a one-to-one mapping during alignment. Such entities will not be a part of the alignment table. If this number is large, it may have a considerable effect on the combination. 2.5 Document-level Boosting Boosting techniques (Schapire, 1999) combine multiple classifiers, built iteratively and trained on re-weighted data, to improve classification accuracy. Since coreference resolution is done for a whole document, we can not split a document furClassifier Ct Full F Classifier C2 File X File X File X Figure 2: Entity alignment between classifier outputs every entity in G. The oracle output can be seen in the right-hand side of Figure 1. This technique can be scaled up to work for any number of classifiers. Entity-level Alignment Table F-El Cl-EA C2-EP F-E2 C1-EB C2-ER F-E3 Cl-ED C2-EQ F-E4 C2-ES 0.88 0.72 0.85 F-El F-E2 F-E3 F-E4 </context>
</contexts>
<marker>Schapire, 1999</marker>
<rawString>R.E. Schapire. 1999. A brief introduction to boosting. In Proc. of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tulyakov</author>
</authors>
<title>Review of classifier combination methods.</title>
<date>2008</date>
<booktitle>In Machine Learning in Document Analysis and Recognition.</booktitle>
<marker>Tulyakov, 2008</marker>
<rawString>S. Tulyakov et al. 2008. Review of classifier combination methods. In Machine Learning in Document Analysis and Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yang</author>
</authors>
<title>Coreference resolution using competition learning approach.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Yang, 2003</marker>
<rawString>X. Yang et al. 2003. Coreference resolution using competition learning approach. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>