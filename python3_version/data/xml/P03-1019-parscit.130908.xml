<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.981545">
A Comparative Study on Reordering Constraints in Statistical Machine
Translation
</title>
<author confidence="0.998406">
Richard Zens and Hermann Ney
</author>
<affiliation confidence="0.9704605">
Chair of Computer Science VI
RWTH Aachen - University of Technology
</affiliation>
<email confidence="0.99535">
{zens,ney}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.995572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987909090909">
In statistical machine translation, the gen-
eration of a translation hypothesis is com-
putationally expensive. If arbitrary word-
reorderings are permitted, the search prob-
lem is NP-hard. On the other hand, if
we restrict the possible word-reorderings
in an appropriate way, we obtain a
polynomial-time search algorithm.
In this paper, we compare two different re-
ordering constraints, namely the ITG con-
straints and the IBM constraints. This
comparison includes a theoretical dis-
cussion on the permitted number of re-
orderings for each of these constraints.
We show a connection between the ITG
constraints and the since 1870 known
Schr¨oder numbers.
We evaluate these constraints on two
tasks: the Verbmobil task and the Cana-
dian Hansards task. The evaluation con-
sists of two parts: First, we check how
many of the Viterbi alignments of the
training corpus satisfy each of these con-
straints. Second, we restrict the search to
each of these constraints and compare the
resulting translation hypotheses.
The experiments will show that the base-
line ITG constraints are not sufficient
on the Canadian Hansards task. There-
fore, we present an extension to the ITG
constraints. These extended ITG con-
straints increase the alignment coverage
from about 87% to 96%.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998577428571429">
In statistical machine translation, we are given
a source language (‘French’) sentence fJ1 =
f1 ... fj ... fJ, which is to be translated into a target
language (‘English’) sentence eI1 = e1 ... ei ... eI.
Among all possible target language sentences, we
will choose the sentence with the highest probabil-
ity:
</bodyText>
<equation confidence="0.966041">
ˆeI1 = argmax {Pr(eI1|fJ1 )} (1)
el 1
= argmax {Pr(eI1) · Pr(fJ1 |eI1)} (2)
el 1
</equation>
<bodyText confidence="0.999416285714286">
The decomposition into two knowledge sources
in Eq. 2 is the so-called source-channel approach
to statistical machine translation (Brown et al.,
1990). It allows an independent modeling of tar-
get language model Pr(eI1) and translation model
Pr(fJ1 |eI1). The target language model describes
the well-formedness of the target language sentence.
The translation model links the source language sen-
tence to the target language sentence. It can be fur-
ther decomposed into alignment and lexicon model.
The argmax operation denotes the search problem,
i.e. the generation of the output sentence in the tar-
get language. We have to maximize over all possible
target language sentences.
In this paper, we will focus on the alignment
problem, i.e. the mapping between source sen-
tence positions and target sentence positions. As
the word order in source and target language may
differ, the search algorithm has to allow certain
word-reorderings. If arbitrary word-reorderings are
allowed, the search problem is NP-hard (Knight,
</bodyText>
<equation confidence="0.534434">
g
</equation>
<bodyText confidence="0.9999235625">
1999). Therefore, we have to restrict the possible
reorderings in some way to make the search prob-
lem feasible. Here, we will discuss two such con-
straints in detail. The first constraints are based on
inversion transduction grammars (ITG) (Wu, 1995;
Wu, 1997). In the following, we will call these the
ITG constraints. The second constraints are the IBM
constraints (Berger et al., 1996). In the next section,
we will describe these constraints from a theoretical
point of view. Then, we will describe the resulting
search algorithm and its extension for word graph
generation. Afterwards, we will analyze the Viterbi
alignments produced during the training of the align-
ment models. Then, we will compare the translation
results when restricting the search to either of these
constraints.
</bodyText>
<sectionHeader confidence="0.996057" genericHeader="method">
2 Theoretical Discussion
</sectionHeader>
<bodyText confidence="0.999971076923077">
In this section, we will discuss the reordering con-
straints from a theoretical point of view. We will
answer the question of how many word-reorderings
are permitted for the ITG constraints as well as for
the IBM constraints. Since we are only interested
in the number of possible reorderings, the specific
word identities are of no importance here. Further-
more, we assume a one-to-one correspondence be-
tween source and target words. Thus, we are inter-
ested in the number of word-reorderings, i.e. permu-
tations, that satisfy the chosen constraints. First, we
will consider the ITG constraints. Afterwards, we
will describe the IBM constraints.
</bodyText>
<subsectionHeader confidence="0.924275">
2.1 ITG Constraints
</subsectionHeader>
<bodyText confidence="0.9999145625">
Let us now consider the ITG constraints. Here, we
interpret the input sentence as a sequence of blocks.
In the beginning, each position is a block of its own.
Then, the permutation process can be seen as fol-
lows: we select two consecutive blocks and merge
them to a single block by choosing between two op-
tions: either keep them in monotone order or invert
the order. This idea is illustrated in Fig. 1. The white
boxes represent the two blocks to be merged.
Now, we investigate, how many permutations are
obtainable with this method. A permutation derived
by the above method can be represented as a binary
tree where the inner nodes are colored either black or
white. At black nodes the resulting sequences of the
children are inverted. At white nodes they are kept in
monotone order. This representation is equivalent to
</bodyText>
<figure confidence="0.819428">
without inversion with inversion
source positions
</figure>
<figureCaption confidence="0.934853">
Figure 1: Illustration of monotone and inverted con-
catenation of two consecutive blocks.
</figureCaption>
<bodyText confidence="0.989667875">
the parse trees of the simple grammar in (Wu, 1997).
We observe that a given permutation may be con-
structed in several ways by the above method. For
instance, let us consider the identity permutation of
1, 2,..., n. Any binary tree with n nodes and all in-
ner nodes colored white (monotone order) is a pos-
sible representation of this permutation. To obtain
a unique representation, we pose an additional con-
straint on the binary trees: if the right son of a node
is an inner node, it has to be colored with the oppo-
site color. With this constraint, each of these binary
trees is unique and equivalent to a parse tree of the
’canonical-form’ grammar in (Wu, 1997).
In (Shapiro and Stephens, 1991), it is shown that
the number of such binary trees with n nodes is
the (n − 1)th large Schr¨oder number Sn−1. The
(small) Schr¨oder numbers have been first described
in (Schr¨oder, 1870) as the number of bracketings of
a given sequence (Schr¨oder’s second problem). The
large Schr¨oder numbers are just twice the Schr¨oder
numbers. Schr¨oder remarked that the ratio between
two consecutive Schr¨oder numbers approaches 3 +
2.\/2 = 5.8284.... A second-order recurrence for
the large Schr¨oder numbers is:
</bodyText>
<equation confidence="0.886271">
(n + 1)Sn = 3(2n − 1)Sn−1 − (n − 2)Sn−2
</equation>
<bodyText confidence="0.99167265">
with n &gt; 2 and S0 = 1, S1 = 2.
The Schr¨oder numbers have many combinatori-
cal interpretations. Here, we will mention only two
of them. The first one is another way of view-
ing at the ITG constraints. The number of permu-
tations of the sequence 1, 2, ..., n, which avoid the
subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large
Schr¨oder number Sn−1. More details on forbidden
subsequences can be found in (West, 1995). The
interesting point is that a search with the ITG con-
straints cannot generate a word-reordering that con-
tains one of these two subsequences. In (Wu, 1997),
these forbidden subsequences are called ’inside-out’
transpositions.
Another interpretation of the Schr¨oder numbers is
given in (Knuth, 1973): The number of permutations
that can be sorted with an output-restricted double-
ended queue (deque) is exactly the large Schr¨oder
number. Additionally, Knuth presents an approxi-
mation for the large Schr¨oder numbers:
</bodyText>
<equation confidence="0.781911">
√
Sn ≈ c · (3 + 8)n · n−32 (3)
</equation>
<bodyText confidence="0.999874">
where c is set to 2 �(3√2 − 4)/π. This approxi-
mation function confirms the result of Schr¨oder, and
we obtain Sn ∈ o((3 + √8)n), i.e. the Schr¨oder
numbers grow like (3 + √8)n ≈ 5.83n.
</bodyText>
<subsectionHeader confidence="0.989563">
2.2 IBM Constraints
</subsectionHeader>
<bodyText confidence="0.999978375">
In this section, we will describe the IBM constraints
(Berger et al., 1996). Here, we mark each position in
the source sentence either as covered or uncovered.
In the beginning, all source positions are uncovered.
Now, the target sentence is produced from bottom to
top. A target position must be aligned to one of the
first k uncovered source positions. The IBM con-
straints are illustrated in Fig. 2.
</bodyText>
<figure confidence="0.39429">
j J
</figure>
<figureCaption confidence="0.99704">
Figure 2: Illustration of the IBM constraints.
</figureCaption>
<bodyText confidence="0.998722571428571">
For most of the target positions there are k per-
mitted source positions. Only towards the end of the
sentence this is reduced to the number of remaining
uncovered source positions. Let n denote the length
of the input sequence and let rn denote the permitted
number of permutations with the IBM constraints.
Then, we obtain:
</bodyText>
<equation confidence="0.98142125">
� kn−k · k! n &gt; k
≤
n! n
k (4)
</equation>
<bodyText confidence="0.998123769230769">
Typically, k is set to 4. In this case, we obtain an
asymptotic upper and lower bound of 4n, i.e. rn ∈
o(4n).
In Tab. 1, the ratio of the number of permitted re-
orderings for the discussed constraints is listed as
a function of the sentence length. We see that for
longer sentences the ITG constraints allow for more
reorderings than the IBM constraints. For sentences
of length 10 words, there are about twice as many
reorderings for the ITG constraints than for the IBM
constraints. This ratio steadily increases. For longer
sentences, the ITG constraints allow for much more
flexibility than the IBM constraints.
</bodyText>
<sectionHeader confidence="0.986358" genericHeader="method">
3 Search
</sectionHeader>
<bodyText confidence="0.999962090909091">
Now, let us get back to more practical aspects. Re-
ordering constraints are more or less useless, if they
do not allow the maximization of Eq. 2 to be per-
formed in an efficient way. Therefore, in this sec-
tion, we will describe different aspects of the search
algorithm for the ITG constraints. First, we will
present the dynamic programming equations and the
resulting complexity. Then, we will describe prun-
ing techniques to accelerate the search. Finally, we
will extend the basic algorithm for the generation of
word graphs.
</bodyText>
<subsectionHeader confidence="0.997531">
3.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999974875">
The ITG constraints allow for a polynomial-time
search algorithm. It is based on the following dy-
namic programming recursion equations. During
the search a table Qjl,jr,eb,et is constructed. Here,
Qjl,jr,eb,et denotes the probability of the best hy-
pothesis translating the source words from position
jl (left) to position jr (right) which begins with the
target language word eb (bottom) and ends with the
word et (top). This is illustrated in Fig. 3.
Here, we initialize this table with monotone trans-
lations of IBM Model 4. Therefore, Q0jl jr eb et de-
notes the probability of the best monotone hypothe-
sis of IBM Model 4. Alternatively, we could use any
other single-word based lexicon as well as phrase-
based models for this initialization. Our choice is
the IBM Model4 to make the results as comparable
</bodyText>
<figure confidence="0.622329">
uncovered position
covered position
uncovered position for extension
rn =
</figure>
<tableCaption confidence="0.993102">
Table 1: Ratio of the number of permitted reorderings with the ITG constraints Sn_1 and the IBM constraints
rn for different sentence lengths n.
</tableCaption>
<figure confidence="0.695662">
n 1 ... 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Sn_1/rn Pz� 1.0 1.2 1.4 1.7 2.1 2.6 3.4 4.3 5.6 7.4 9.8 13.0 17.4 23.3 31.4
et
eb
</figure>
<figureCaption confidence="0.999845">
Figure 3: Illustration of the Q-table.
</figureCaption>
<bodyText confidence="0.99998425">
as possible to the search with the IBM constraints.
We introduce a new parameter pm (m=ˆ monotone),
which denotes the probability of a monotone combi-
nation of two partial hypotheses.
</bodyText>
<equation confidence="0.99416">
Qjl,jr,eb,et = (5)
</equation>
<bodyText confidence="0.999984428571429">
We formulated this equation for a bigram lan-
guage model, but of course, the same method can
also be applied for a trigram language model. The
resulting algorithm is similar to the CYK-parsing al-
gorithm. It has a worst-case complexity of O(J3 &apos;
E4). Here, J is the length of the source sentence
and E is the vocabulary size of the target language.
</bodyText>
<subsectionHeader confidence="0.997872">
3.2 Pruning
</subsectionHeader>
<bodyText confidence="0.97965952173913">
Although the described search algorithm has a
polynomial-time complexity, even with a bigram
language model the search space is very large. A full
search is possible but time consuming. The situation
gets even worse when a trigram language model is
used. Therefore, pruning techniques are obligatory
to reduce the translation time.
Pruning is applied to hypotheses that translate the
same subsequence fjr
jl of the source sentence. We
use pruning in the following two ways. The first
pruning technique is histogram pruning: we restrict
the number of translation hypotheses per sequence
fjr
jl . For each sequence fjr
jl , we keep only a fixed
number of translation hypotheses. The second prun-
ing technique is threshold pruning: the idea is to re-
move all hypotheses that have a low probability rela-
tive to the best hypothesis. Therefore, we introduce
a threshold pruning parameter q, with 0 &lt; q &lt; 1.
Let Q3�l,jr denote the maximum probability of all
translation hypotheses for fjr
</bodyText>
<equation confidence="0.948307333333334">
jl . Then, we prune a
hypothesis iff:
Qjl,jr,eb,et &lt; q &apos; Qjl,jr
</equation>
<bodyText confidence="0.999091333333333">
Applying these pruning techniques the computa-
tional costs can be reduced significantly with almost
no loss in translation quality.
</bodyText>
<subsectionHeader confidence="0.99934">
3.3 Generation of Word Graphs
</subsectionHeader>
<bodyText confidence="0.972198285714286">
The generation of word graphs for a bottom-top
search with the IBM constraints is described in
(Ueffing et al., 2002). These methods cannot be
applied to the CYK-style search for the ITG con-
straints. Here, the idea for the generation of word
graphs is the following: assuming we already have
word graphs for the source sequences fkjl and fjr
</bodyText>
<equation confidence="0.695167">
k+1,
then we can construct a word graph for the sequence
fjr
jl by concatenating the partial word graphs either
</equation>
<bodyText confidence="0.999408642857143">
in monotone or inverted order.
Now, we describe this idea in a more formal way.
A word graph is a directed acyclic graph (dag) with
one start and one end node. The edges are annotated
with target language words or phrases. We also al-
low 2-transitions. These are edges annotated with
the empty word. Additionally, edges may be anno-
tated with probabilities of the language or translation
model. Each path from start node to end node rep-
resents one translation hypothesis. The probability
of this hypothesis is calculated by multiplying the
probabilities along the path.
During the search, we have to combine two word
graphs in either monotone or inverted order. This
</bodyText>
<figure confidence="0.709779833333333">
jl jr
max { Q0jl,jr,eb,et,
jl&lt;k&lt;jr,
e&apos;,e&apos;&apos;
Qjl,k,eb,e&apos; &apos; Qk+1,jr,e&apos;&apos;,et &apos; p(e,,|e,) &apos; pm, lS
Qk+1,jr,eb,e&apos; &apos; Qjl,k,e&apos;&apos; et &apos; p(e ,,|e,) &apos; (1 − pm)1
</figure>
<bodyText confidence="0.987967766666667">
is done in the following way: we are given two
word graphs w1 and w2 with start and end nodes
(s1, g1) and (s2,g2), respectively. First, we add
an 2-transition (g1, s2) from the end node of the
first graph w1 to the start node of the second graph
w2 and annotate this edge with the probability of a
monotone concatenation pm. Second, we create a
copy of each of the original word graphs w1 and w2.
Then, we add an 2-transition (g2, s1) from the end
node of the copied second graph to the start node of
the copied first graph. This edge is annotated with
the probability of a inverted concatenation 1 — pm.
Now, we have obtained two word graphs: one for a
monotone and one for a inverted concatenation. The
final word graphs is constructed by merging the two
start nodes and the two end nodes, respectively.
Let W(jl, jr) denote the word graph for the
source sequence fjr
jl . This graph is constructed
from the word graphs of all subsequences of (jl, jr).
Therefore, we assume, these word graphs have al-
ready been produced. For all source positions k with
jl &lt; k &lt; jr, we combine the word graphs W (jl, k)
and W (k + 1, jr) as described above. Finally, we
merge all start nodes of these graphs as well as all
end nodes. Now, we have obtained the word graph
W(jl, jr) for the source sequence fjr
jl . As initializa-
tion, we use the word graphs of the monotone IBM4
search.
</bodyText>
<subsectionHeader confidence="0.923802">
3.4 Extended ITG constraints
</subsectionHeader>
<bodyText confidence="0.97318156">
In this section, we will extend the ITG constraints
described in Sec. 2.1. This extension will go beyond
basic reordering constraints.
We already mentioned that the use of consecutive
phrases within the ITG approach is straightforward.
The only thing we have to change is the initializa-
tion of the Q-table. Now, we will extend this idea to
phrases that are non-consecutive in the source lan-
guage. For this purpose, we adopt the view of the
ITG constraints as a bilingual grammar as, e.g., in
(Wu, 1997). For the baseline ITG constraints, the
resulting grammar is:
A— [AA]  |(AA)  |f/e  |f/2  |2/e
Here, [AA] denotes a monotone concatenation and
(AA) denotes an inverted concatenation.
Let us now consider the case of a source phrase
consisting of two parts f1 and f2. Let e denote the
corresponding target phrase. We add the productions
A — [e/f1 A 2/f2]  |(e/f1 A 2/f2)
to the grammar. The probabilities of these pro-
ductions are, dependent on the translation direction,
p(e|f1, f2) or p(f1, f2|e), respectively. Obviously,
these productions are not in the normal form of an
ITG, but with the method described in (Wu, 1997),
they can be normalized.
</bodyText>
<sectionHeader confidence="0.972171" genericHeader="method">
4 Corpus Statistics
</sectionHeader>
<bodyText confidence="0.999847666666667">
In the following sections we will present results on
two tasks. Therefore, in this section we will show
the corpus statistics for each of these tasks.
</bodyText>
<subsectionHeader confidence="0.917398">
4.1 Verbmobil
</subsectionHeader>
<bodyText confidence="0.917096571428571">
The first task we will present results on is the Verb-
mobil task (Wahlster, 2000). The domain of this
corpus is appointment scheduling, travel planning,
and hotel reservation. It consists of transcriptions
of spontaneous speech. Table 2 shows the corpus
statistics of this corpus. The training corpus (Train)
was used to train the IBM model parameters. The
remaining free parameters, i.e. pm and the model
scaling factors (Och and Ney, 2002), were adjusted
on the development corpus (Dev). The resulting sys-
tem was evaluated on the test corpus (Test).
Table 2: Statistics of training and test corpus for
the Verbmobil task (PP=perplexity, SL=sentence
length).
</bodyText>
<table confidence="0.999587285714286">
German English
Train Sentences 58 073
Words 519 523 549 921
Vocabulary 7 939 4 672
Singletons 3 453 1698
average SL 8.9 9.5
Dev Sentences 276
Words 3 159 3 438
Trigram PP - 28.1
average SL 11.5 12.5
Test Sentences 251
Words 2 628 2 871
Trigram PP - 30.5
average SL 10.5 11.4
</table>
<tableCaption confidence="0.9496702">
Table 3: Statistics of training and test corpus
for the Canadian Hansards task (PP=perplexity,
SL=sentence length).
Table 4: Coverage on the training corpus for align-
ment constraints for the Verbmobil task (VM) and
</tableCaption>
<table confidence="0.976753454545455">
for the Canadian Hansards task (CH).
French English
Train Sentences 1.5M
Words 24M 22M
Vocabulary 100 269 78 332
Singletons 40199 31319
average SL 16.6 15.1
Test Sentences 5432
Words 97 646 88 773
Trigram PP – 179.8
average SL 18.0 16.3
</table>
<subsectionHeader confidence="0.981042">
4.2 Canadian Hansards
</subsectionHeader>
<bodyText confidence="0.999974555555556">
Additionally, we carried out experiments on the
Canadian Hansards task. This task contains the pro-
ceedings of the Canadian parliament, which are kept
by law in both French and English. About 3 million
parallel sentences of this bilingual data have been
made available by the Linguistic Data Consortium
(LDC). Here, we use a subset of the data containing
only sentences with a maximum length of 30 words.
Table 3 shows the training and test corpus statistics.
</bodyText>
<sectionHeader confidence="0.978521" genericHeader="method">
5 Evaluation in Training
</sectionHeader>
<bodyText confidence="0.999926476190476">
In this section, we will investigate for each of the
constraints the coverage of the training corpus align-
ment. For this purpose, we compute the Viterbi
alignment of IBM Model 5 with GIZA++ (Och and
Ney, 2000). This alignment is produced without any
restrictions on word-reorderings. Then, we check
for every sentence if the alignment satisfies each of
the constraints. The ratio of the number of satisfied
alignments and the total number of sentences is re-
ferred to as coverage. Tab. 4 shows the results for
the Verbmobil task and for the Canadian Hansards
task. It contains the results for both translation direc-
tions German-English (S—*T) and English-German
(T—*S) for the Verbmobil task and French-English
(S—*T) and English-French (T—*S) for the Canadian
Hansards task, respectively.
For the Verbmobil task, the baseline ITG con-
straints and the IBM constraints result in a similar
coverage. It is about 91% for the German-English
translation direction and about 88% for the English-
German translation direction. A significantly higher
</bodyText>
<table confidence="0.998554875">
coverage [%]
task constraint S—*T T—*S
VM IBM 91.0 88.1
ITG baseline 91.6 87.0
extended 96.5 96.9
CH IBM 87.1 86.7
ITG baseline 81.3 73.6
extended 96.1 95.6
</table>
<bodyText confidence="0.999284333333333">
coverage of about 96% is obtained with the extended
ITG constraints. Thus with the extended ITG con-
straints, the coverage increases by about 8% abso-
lute.
For the Canadian Hansards task, the baseline ITG
constraints yield a worse coverage than the IBM
constraints. Especially for the English-French trans-
lation direction, the ITG coverage of 73.6% is very
low. Again, the extended ITG constraints obtained
the best results. Here, the coverage increases from
about 87% for the IBM constraints to about 96% for
the extended ITG constraints.
</bodyText>
<sectionHeader confidence="0.971299" genericHeader="method">
6 Translation Experiments
</sectionHeader>
<subsectionHeader confidence="0.995689">
6.1 Evaluation Criteria
</subsectionHeader>
<bodyText confidence="0.9975985">
In our experiments, we use the following error crite-
ria:
</bodyText>
<listItem confidence="0.897713">
• WER (word error rate):
</listItem>
<bodyText confidence="0.999355">
The WER is computed as the minimum num-
ber of substitution, insertion and deletion oper-
ations that have to be performed to convert the
generated sentence into the target sentence.
</bodyText>
<listItem confidence="0.986638333333333">
• PER (position-independent word error rate):
A shortcoming of the WER is the fact that it
requires a perfect word order. The PER com-
pares the words in the two sentences ignoring
the word order.
• mWER (multi-reference word error rate):
</listItem>
<bodyText confidence="0.987776333333333">
For each test sentence, not only a single refer-
ence translation is used, as for the WER, but a
whole set of reference translations. For each
translation hypothesis, the WER to the most
similar sentence is calculated (Nießen et al.,
2000).
</bodyText>
<listItem confidence="0.830327">
• BLEU score:
</listItem>
<bodyText confidence="0.999599333333333">
This score measures the precision of unigrams,
bigrams, trigrams and fourgrams with respect
to a whole set of reference translations with a
penalty for too short sentences (Papineni et al.,
2001). BLEU measures accuracy, i.e. large
BLEU scores are better.
</bodyText>
<listItem confidence="0.982727">
• SSER (subjective sentence error rate):
</listItem>
<bodyText confidence="0.999229">
For a more detailed analysis, subjective judg-
ments by test persons are necessary. Each
translated sentence was judged by a human ex-
aminer according to an error scale from 0.0 to
1.0 (Nießen et al., 2000).
</bodyText>
<subsectionHeader confidence="0.997252">
6.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.999986571428571">
In this section, we will present the translation results
for both the IBM constraints and the baseline ITG
constraints. We used a single-word based search
with IBM Model 4. The initialization for the ITG
constraints was done with monotone IBM Model 4
translations. So, the only difference between the two
systems are the reordering constraints.
In Tab. 5 the results for the Verbmobil task are
shown. We see that the results on this task are sim-
ilar. The search with the ITG constraints yields
slightly lower error rates.
Some translation examples of the Verbmobil task
are shown in Tab. 6. We have to keep in mind,
that the Verbmobil task consists of transcriptions
of spontaneous speech. Therefore, the source sen-
tences as well as the reference translations may have
an unorthodox grammatical structure. In the first
example, the German verb-group (“w¨urde vorschla-
gen”) is split into two parts. The search with the
ITG constraints is able to produce a correct transla-
tion. With the IBM constraints, it is not possible to
translate this verb-group correctly, because the dis-
tance between the two parts is too large (more than
four words). As we see in the second example, in
German the verb of a subordinate clause is placed at
the end (“¨ubernachten”). The IBM search is not able
to perform the necessary long-range reordering, as it
is done with the ITG search.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999852307692308">
The ITG constraints were introduced in (Wu, 1995).
The applications were, for instance, the segmenta-
tion of Chinese character sequences into Chinese
“words” and the bracketing of the source sentence
into sub-sentential chunks. In (Wu, 1996) the base-
line ITG constraints were used for statistical ma-
chine translation. The resulting algorithm is simi-
lar to the one presented in Sect. 3.1, but here, we
use monotone translation hypotheses of the full IBM
Model 4 as initialization, whereas in (Wu, 1996) a
single-word based lexicon model is used. In (Vilar,
1998) a model similar to Wu’s method was consid-
ered.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999997846153846">
We have described the ITG constraints in detail and
compared them to the IBM constraints. We draw the
following conclusions: especially for long sentences
the ITG constraints allow for higher flexibility in
word-reordering than the IBM constraints. Regard-
ing the Viterbi alignment in training, the baseline
ITG constraints yield a similar coverage as the IBM
constraints on the Verbmobil task. On the Canadian
Hansards task the baseline ITG constraints were not
sufficient. With the extended ITG constraints the
coverage improves significantly on both tasks. On
the Canadian Hansards task the coverage increases
from about 87% to about 96%.
We have presented a polynomial-time search al-
gorithm for statistical machine translation based on
the ITG constraints and its extension for the gen-
eration of word graphs. We have shown the trans-
lation results for the Verbmobil task. On this task,
the translation quality of the search with the base-
line ITG constraints is already competitive with the
results for the IBM constraints. Therefore, we ex-
pect the search with the extended ITG constraints to
outperform the search with the IBM constraints.
Future work will include the automatic extraction
of the bilingual grammar as well as the use of this
grammar for the translation process.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999794625">
A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D. Pietra,
J. R. Gillett, A. S. Kehler, and R. L. Mercer. 1996.
Language translation apparatus and method of using
context-based translation models, United States patent,
patent number 5510981, April.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
</reference>
<tableCaption confidence="0.98987">
Table 5: Translation results on the Verbmobil task.
</tableCaption>
<table confidence="0.98862425">
type automatic human
System WER [%] PER [%] mWER [%] BLEU [%] SSER [%]
IBM 46.2 33.3 40.0 42.5 40.8
ITG 45.6 33.9 40.0 37.1 42.0
</table>
<tableCaption confidence="0.925762">
Table 6: Verbmobil: translation examples.
</tableCaption>
<table confidence="0.62409925">
source ja, ich w¨urde den Flug um viertel nach sieben vorschlagen.
reference yes, I would suggest the flight at a quarter past seven.
ITG yes, I would suggest the flight at seven fifteen.
IBM yes, I would be the flight at quarter to seven suggestion.
source ich schlage vor, dass wir in Hannover im Hotel Gr¨unschnabel ¨ubernachten.
reference I suggest to stay at the hotel Gr¨unschnabel in Hanover.
ITG I suggest that we stay in Hanover at hotel Gr¨unschnabel.
IBM I suggest that we are in Hanover at hotel Gr¨unschnabel stay.
</table>
<reference confidence="0.9994688">
translation. Computational Linguistics, 16(2):79–85,
June.
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607–615, December.
D. E. Knuth. 1973. The Art of Computer Program-
ming, volume 1 - Fundamental Algorithms. Addison-
Wesley, Reading, MA, 2nd edition.
S. Nießen, F. J. Och, G. Leusch, and H. Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In Proc. of the Second Int.
Conf. on Language Resources and Evaluation (LREC),
pages 39–45, Athens, Greece, May.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of the 38th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 440–447, Hong Kong, October.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 295–302, July.
K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176 (W0109-022),
IBM Research Division, Thomas J. Watson Research
Center, September.
E. Schr¨oder. 1870. Vier combinatorische Probleme.
Zeitschrift f¨ur Mathematik und Physik, 15:361–376.
L. Shapiro and A. B. Stephens. 1991. Boostrap percola-
tion, the Schr¨oder numbers, and the n-kings problem.
SIAM Journal on Discrete Mathematics, 4(2):275–
280, May.
N. Ueffing, F. J. Och, and H. Ney. 2002. Generation
of word graphs in statistical machine translation. In
Proc. Conf. on Empirical Methods for Natural Lan-
guage Processing, pages 156–163, Philadelphia, PA,
July.
J. M. Vilar. 1998. Aprendizaje de Transductores Subse-
cuenciales para su empleo en tareas de Dominio Re-
stringido. Ph.D. thesis, Universidad Politecnica de Va-
lencia.
W. Wahlster, editor. 2000. Verbmobil: Foundations
of speech-to-speech translations. Springer Verlag,
Berlin, Germany, July.
J. West. 1995. Generating trees and the Catalan and
Schr¨oder numbers. Discrete Mathematics, 146:247–
262, November.
D. Wu. 1995. Stochastic inversion transduction gram-
mars, with application to segmentation, bracketing,
and alignment of parallel corpora. In Proc. of the 14th
International Joint Conf. on Artificial Intelligence (IJ-
CAI), pages 1328–1334, Montreal, August.
D. Wu. 1996. A polynomial-time algorithm for statis-
tical machine translation. In Proc. of the 34th Annual
Conf. of the Association for Computational Linguistics
(ACL ’96), pages 152–158, Santa Cruz, CA, June.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–403, September.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.821694">
<title confidence="0.999324">A Comparative Study on Reordering Constraints in Statistical Machine Translation</title>
<author confidence="0.995478">Zens Ney</author>
<affiliation confidence="0.9587765">Chair of Computer Science VI RWTH Aachen - University of Technology</affiliation>
<abstract confidence="0.99651096969697">In statistical machine translation, the generation of a translation hypothesis is computationally expensive. If arbitrary wordreorderings are permitted, the search problem is NP-hard. On the other hand, if we restrict the possible word-reorderings in an appropriate way, we obtain a polynomial-time search algorithm. In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints. This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints. We show a connection between the ITG constraints and the since 1870 known We evaluate these constraints on two tasks: the Verbmobil task and the Canadian Hansards task. The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints. Second, we restrict the search to each of these constraints and compare the resulting translation hypotheses. The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task. Therefore, we present an extension to the ITG constraints. These extended ITG constraints increase the alignment coverage from about 87% to 96%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>P F Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>J R Gillett</author>
<author>A S Kehler</author>
<author>R L Mercer</author>
</authors>
<title>Language translation apparatus and method of using context-based translation models, United States patent, patent number 5510981,</title>
<date>1996</date>
<contexts>
<context position="3280" citStr="Berger et al., 1996" startWordPosition="517" endWordPosition="520">d target sentence positions. As the word order in source and target language may differ, the search algorithm has to allow certain word-reorderings. If arbitrary word-reorderings are allowed, the search problem is NP-hard (Knight, g 1999). Therefore, we have to restrict the possible reorderings in some way to make the search problem feasible. Here, we will discuss two such constraints in detail. The first constraints are based on inversion transduction grammars (ITG) (Wu, 1995; Wu, 1997). In the following, we will call these the ITG constraints. The second constraints are the IBM constraints (Berger et al., 1996). In the next section, we will describe these constraints from a theoretical point of view. Then, we will describe the resulting search algorithm and its extension for word graph generation. Afterwards, we will analyze the Viterbi alignments produced during the training of the alignment models. Then, we will compare the translation results when restricting the search to either of these constraints. 2 Theoretical Discussion In this section, we will discuss the reordering constraints from a theoretical point of view. We will answer the question of how many word-reorderings are permitted for the </context>
<context position="7821" citStr="Berger et al., 1996" startWordPosition="1291" endWordPosition="1294">’ transpositions. Another interpretation of the Schr¨oder numbers is given in (Knuth, 1973): The number of permutations that can be sorted with an output-restricted doubleended queue (deque) is exactly the large Schr¨oder number. Additionally, Knuth presents an approximation for the large Schr¨oder numbers: √ Sn ≈ c · (3 + 8)n · n−32 (3) where c is set to 2 �(3√2 − 4)/π. This approximation function confirms the result of Schr¨oder, and we obtain Sn ∈ o((3 + √8)n), i.e. the Schr¨oder numbers grow like (3 + √8)n ≈ 5.83n. 2.2 IBM Constraints In this section, we will describe the IBM constraints (Berger et al., 1996). Here, we mark each position in the source sentence either as covered or uncovered. In the beginning, all source positions are uncovered. Now, the target sentence is produced from bottom to top. A target position must be aligned to one of the first k uncovered source positions. The IBM constraints are illustrated in Fig. 2. j J Figure 2: Illustration of the IBM constraints. For most of the target positions there are k permitted source positions. Only towards the end of the sentence this is reduced to the number of remaining uncovered source positions. Let n denote the length of the input sequ</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Gillett, Kehler, Mercer, 1996</marker>
<rawString>A. L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer. 1996. Language translation apparatus and method of using context-based translation models, United States patent, patent number 5510981, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="2024" citStr="Brown et al., 1990" startWordPosition="318" endWordPosition="321">straints increase the alignment coverage from about 87% to 96%. 1 Introduction In statistical machine translation, we are given a source language (‘French’) sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language (‘English’) sentence eI1 = e1 ... ei ... eI. Among all possible target language sentences, we will choose the sentence with the highest probability: ˆeI1 = argmax {Pr(eI1|fJ1 )} (1) el 1 = argmax {Pr(eI1) · Pr(fJ1 |eI1)} (2) el 1 The decomposition into two knowledge sources in Eq. 2 is the so-called source-channel approach to statistical machine translation (Brown et al., 1990). It allows an independent modeling of target language model Pr(eI1) and translation model Pr(fJ1 |eI1). The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. It can be further decomposed into alignment and lexicon model. The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. We have to maximize over all possible target language sentences. In this paper, we will focus on the alignment problem, i.e. the mapping </context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Decoding complexity in wordreplacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<marker>Knight, 1999</marker>
<rawString>K. Knight. 1999. Decoding complexity in wordreplacement translation models. Computational Linguistics, 25(4):607–615, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>The Art of Computer Programming,</title>
<date>1973</date>
<volume>1</volume>
<publisher>AddisonWesley,</publisher>
<location>Reading, MA,</location>
<note>2nd edition.</note>
<contexts>
<context position="7292" citStr="Knuth, 1973" startWordPosition="1197" endWordPosition="1198">mention only two of them. The first one is another way of viewing at the ITG constraints. The number of permutations of the sequence 1, 2, ..., n, which avoid the subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large Schr¨oder number Sn−1. More details on forbidden subsequences can be found in (West, 1995). The interesting point is that a search with the ITG constraints cannot generate a word-reordering that contains one of these two subsequences. In (Wu, 1997), these forbidden subsequences are called ’inside-out’ transpositions. Another interpretation of the Schr¨oder numbers is given in (Knuth, 1973): The number of permutations that can be sorted with an output-restricted doubleended queue (deque) is exactly the large Schr¨oder number. Additionally, Knuth presents an approximation for the large Schr¨oder numbers: √ Sn ≈ c · (3 + 8)n · n−32 (3) where c is set to 2 �(3√2 − 4)/π. This approximation function confirms the result of Schr¨oder, and we obtain Sn ∈ o((3 + √8)n), i.e. the Schr¨oder numbers grow like (3 + √8)n ≈ 5.83n. 2.2 IBM Constraints In this section, we will describe the IBM constraints (Berger et al., 1996). Here, we mark each position in the source sentence either as covered </context>
</contexts>
<marker>Knuth, 1973</marker>
<rawString>D. E. Knuth. 1973. The Art of Computer Programming, volume 1 - Fundamental Algorithms. AddisonWesley, Reading, MA, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nießen</author>
<author>F J Och</author>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In Proc. of the Second Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>39--45</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="21086" citStr="Nießen et al., 2000" startWordPosition="3568" endWordPosition="3571">m number of substitution, insertion and deletion operations that have to be performed to convert the generated sentence into the target sentence. • PER (position-independent word error rate): A shortcoming of the WER is the fact that it requires a perfect word order. The PER compares the words in the two sentences ignoring the word order. • mWER (multi-reference word error rate): For each test sentence, not only a single reference translation is used, as for the WER, but a whole set of reference translations. For each translation hypothesis, the WER to the most similar sentence is calculated (Nießen et al., 2000). • BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences (Papineni et al., 2001). BLEU measures accuracy, i.e. large BLEU scores are better. • SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 (Nießen et al., 2000). 6.2 Translation Results In this section, we will present the translation results f</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>S. Nießen, F. J. Och, G. Leusch, and H. Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In Proc. of the Second Int. Conf. on Language Resources and Evaluation (LREC), pages 39–45, Athens, Greece, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>440--447</pages>
<location>Hong Kong,</location>
<contexts>
<context position="18776" citStr="Och and Ney, 2000" startWordPosition="3194" endWordPosition="3197">s task contains the proceedings of the Canadian parliament, which are kept by law in both French and English. About 3 million parallel sentences of this bilingual data have been made available by the Linguistic Data Consortium (LDC). Here, we use a subset of the data containing only sentences with a maximum length of 30 words. Table 3 shows the training and test corpus statistics. 5 Evaluation in Training In this section, we will investigate for each of the constraints the coverage of the training corpus alignment. For this purpose, we compute the Viterbi alignment of IBM Model 5 with GIZA++ (Och and Ney, 2000). This alignment is produced without any restrictions on word-reorderings. Then, we check for every sentence if the alignment satisfies each of the constraints. The ratio of the number of satisfied alignments and the total number of sentences is referred to as coverage. Tab. 4 shows the results for the Verbmobil task and for the Canadian Hansards task. It contains the results for both translation directions German-English (S—*T) and English-German (T—*S) for the Verbmobil task and French-English (S—*T) and English-French (T—*S) for the Canadian Hansards task, respectively. For the Verbmobil ta</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL), pages 440–447, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>295--302</pages>
<contexts>
<context position="17115" citStr="Och and Ney, 2002" startWordPosition="2910" endWordPosition="2913">s Statistics In the following sections we will present results on two tasks. Therefore, in this section we will show the corpus statistics for each of these tasks. 4.1 Verbmobil The first task we will present results on is the Verbmobil task (Wahlster, 2000). The domain of this corpus is appointment scheduling, travel planning, and hotel reservation. It consists of transcriptions of spontaneous speech. Table 2 shows the corpus statistics of this corpus. The training corpus (Train) was used to train the IBM model parameters. The remaining free parameters, i.e. pm and the model scaling factors (Och and Ney, 2002), were adjusted on the development corpus (Dev). The resulting system was evaluated on the test corpus (Test). Table 2: Statistics of training and test corpus for the Verbmobil task (PP=perplexity, SL=sentence length). German English Train Sentences 58 073 Words 519 523 549 921 Vocabulary 7 939 4 672 Singletons 3 453 1698 average SL 8.9 9.5 Dev Sentences 276 Words 3 159 3 438 Trigram PP - 28.1 average SL 11.5 12.5 Test Sentences 251 Words 2 628 2 871 Trigram PP - 30.5 average SL 10.5 11.4 Table 3: Statistics of training and test corpus for the Canadian Hansards task (PP=perplexity, SL=sentence</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 295–302, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center,</location>
<contexts>
<context position="21297" citStr="Papineni et al., 2001" startWordPosition="3602" endWordPosition="3605">the WER is the fact that it requires a perfect word order. The PER compares the words in the two sentences ignoring the word order. • mWER (multi-reference word error rate): For each test sentence, not only a single reference translation is used, as for the WER, but a whole set of reference translations. For each translation hypothesis, the WER to the most similar sentence is calculated (Nießen et al., 2000). • BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences (Papineni et al., 2001). BLEU measures accuracy, i.e. large BLEU scores are better. • SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 (Nießen et al., 2000). 6.2 Translation Results In this section, we will present the translation results for both the IBM constraints and the baseline ITG constraints. We used a single-word based search with IBM Model 4. The initialization for the ITG constraints was done with monotone IBM Model 4 translations. So, </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. A. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Schr¨oder</author>
</authors>
<date>1870</date>
<booktitle>Vier combinatorische Probleme. Zeitschrift f¨ur Mathematik und Physik,</booktitle>
<pages>15--361</pages>
<marker>Schr¨oder, 1870</marker>
<rawString>E. Schr¨oder. 1870. Vier combinatorische Probleme. Zeitschrift f¨ur Mathematik und Physik, 15:361–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shapiro</author>
<author>A B Stephens</author>
</authors>
<title>Boostrap percolation, the Schr¨oder numbers, and the n-kings problem.</title>
<date>1991</date>
<journal>SIAM Journal on Discrete Mathematics,</journal>
<volume>4</volume>
<issue>2</issue>
<pages>280</pages>
<contexts>
<context position="6027" citStr="Shapiro and Stephens, 1991" startWordPosition="977" endWordPosition="980">t a given permutation may be constructed in several ways by the above method. For instance, let us consider the identity permutation of 1, 2,..., n. Any binary tree with n nodes and all inner nodes colored white (monotone order) is a possible representation of this permutation. To obtain a unique representation, we pose an additional constraint on the binary trees: if the right son of a node is an inner node, it has to be colored with the opposite color. With this constraint, each of these binary trees is unique and equivalent to a parse tree of the ’canonical-form’ grammar in (Wu, 1997). In (Shapiro and Stephens, 1991), it is shown that the number of such binary trees with n nodes is the (n − 1)th large Schr¨oder number Sn−1. The (small) Schr¨oder numbers have been first described in (Schr¨oder, 1870) as the number of bracketings of a given sequence (Schr¨oder’s second problem). The large Schr¨oder numbers are just twice the Schr¨oder numbers. Schr¨oder remarked that the ratio between two consecutive Schr¨oder numbers approaches 3 + 2.\/2 = 5.8284.... A second-order recurrence for the large Schr¨oder numbers is: (n + 1)Sn = 3(2n − 1)Sn−1 − (n − 2)Sn−2 with n &gt; 2 and S0 = 1, S1 = 2. The Schr¨oder numbers hav</context>
</contexts>
<marker>Shapiro, Stephens, 1991</marker>
<rawString>L. Shapiro and A. B. Stephens. 1991. Boostrap percolation, the Schr¨oder numbers, and the n-kings problem. SIAM Journal on Discrete Mathematics, 4(2):275– 280, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Generation of word graphs in statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. Conf. on Empirical Methods for Natural Language Processing,</booktitle>
<pages>156--163</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="12797" citStr="Ueffing et al., 2002" startWordPosition="2151" endWordPosition="2154">ique is threshold pruning: the idea is to remove all hypotheses that have a low probability relative to the best hypothesis. Therefore, we introduce a threshold pruning parameter q, with 0 &lt; q &lt; 1. Let Q3�l,jr denote the maximum probability of all translation hypotheses for fjr jl . Then, we prune a hypothesis iff: Qjl,jr,eb,et &lt; q &apos; Qjl,jr Applying these pruning techniques the computational costs can be reduced significantly with almost no loss in translation quality. 3.3 Generation of Word Graphs The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al., 2002). These methods cannot be applied to the CYK-style search for the ITG constraints. Here, the idea for the generation of word graphs is the following: assuming we already have word graphs for the source sequences fkjl and fjr k+1, then we can construct a word graph for the sequence fjr jl by concatenating the partial word graphs either in monotone or inverted order. Now, we describe this idea in a more formal way. A word graph is a directed acyclic graph (dag) with one start and one end node. The edges are annotated with target language words or phrases. We also allow 2-transitions. These are e</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>N. Ueffing, F. J. Och, and H. Ney. 2002. Generation of word graphs in statistical machine translation. In Proc. Conf. on Empirical Methods for Natural Language Processing, pages 156–163, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Vilar</author>
</authors>
<title>Aprendizaje de Transductores Subsecuenciales para su empleo en tareas de Dominio Restringido.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Universidad Politecnica de Valencia.</institution>
<contexts>
<context position="23569" citStr="Vilar, 1998" startWordPosition="3982" endWordPosition="3983">, as it is done with the ITG search. 7 Related Work The ITG constraints were introduced in (Wu, 1995). The applications were, for instance, the segmentation of Chinese character sequences into Chinese “words” and the bracketing of the source sentence into sub-sentential chunks. In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation. The resulting algorithm is similar to the one presented in Sect. 3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used. In (Vilar, 1998) a model similar to Wu’s method was considered. 8 Conclusions We have described the ITG constraints in detail and compared them to the IBM constraints. We draw the following conclusions: especially for long sentences the ITG constraints allow for higher flexibility in word-reordering than the IBM constraints. Regarding the Viterbi alignment in training, the baseline ITG constraints yield a similar coverage as the IBM constraints on the Verbmobil task. On the Canadian Hansards task the baseline ITG constraints were not sufficient. With the extended ITG constraints the coverage improves signific</context>
</contexts>
<marker>Vilar, 1998</marker>
<rawString>J. M. Vilar. 1998. Aprendizaje de Transductores Subsecuenciales para su empleo en tareas de Dominio Restringido. Ph.D. thesis, Universidad Politecnica de Valencia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
<author>editor</author>
</authors>
<title>Verbmobil: Foundations of speech-to-speech translations.</title>
<date>2000</date>
<publisher>Springer Verlag,</publisher>
<location>Berlin, Germany,</location>
<marker>Wahlster, editor, 2000</marker>
<rawString>W. Wahlster, editor. 2000. Verbmobil: Foundations of speech-to-speech translations. Springer Verlag, Berlin, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J West</author>
</authors>
<title>Generating trees and the Catalan and Schr¨oder numbers.</title>
<date>1995</date>
<journal>Discrete Mathematics,</journal>
<volume>146</volume>
<pages>262</pages>
<contexts>
<context position="6990" citStr="West, 1995" startWordPosition="1152" endWordPosition="1153">hat the ratio between two consecutive Schr¨oder numbers approaches 3 + 2.\/2 = 5.8284.... A second-order recurrence for the large Schr¨oder numbers is: (n + 1)Sn = 3(2n − 1)Sn−1 − (n − 2)Sn−2 with n &gt; 2 and S0 = 1, S1 = 2. The Schr¨oder numbers have many combinatorical interpretations. Here, we will mention only two of them. The first one is another way of viewing at the ITG constraints. The number of permutations of the sequence 1, 2, ..., n, which avoid the subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large Schr¨oder number Sn−1. More details on forbidden subsequences can be found in (West, 1995). The interesting point is that a search with the ITG constraints cannot generate a word-reordering that contains one of these two subsequences. In (Wu, 1997), these forbidden subsequences are called ’inside-out’ transpositions. Another interpretation of the Schr¨oder numbers is given in (Knuth, 1973): The number of permutations that can be sorted with an output-restricted doubleended queue (deque) is exactly the large Schr¨oder number. Additionally, Knuth presents an approximation for the large Schr¨oder numbers: √ Sn ≈ c · (3 + 8)n · n−32 (3) where c is set to 2 �(3√2 − 4)/π. This approximat</context>
</contexts>
<marker>West, 1995</marker>
<rawString>J. West. 1995. Generating trees and the Catalan and Schr¨oder numbers. Discrete Mathematics, 146:247– 262, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In Proc. of the 14th International Joint Conf. on Artificial Intelligence (IJCAI),</booktitle>
<pages>1328--1334</pages>
<location>Montreal,</location>
<contexts>
<context position="3141" citStr="Wu, 1995" startWordPosition="496" endWordPosition="497">anguage sentences. In this paper, we will focus on the alignment problem, i.e. the mapping between source sentence positions and target sentence positions. As the word order in source and target language may differ, the search algorithm has to allow certain word-reorderings. If arbitrary word-reorderings are allowed, the search problem is NP-hard (Knight, g 1999). Therefore, we have to restrict the possible reorderings in some way to make the search problem feasible. Here, we will discuss two such constraints in detail. The first constraints are based on inversion transduction grammars (ITG) (Wu, 1995; Wu, 1997). In the following, we will call these the ITG constraints. The second constraints are the IBM constraints (Berger et al., 1996). In the next section, we will describe these constraints from a theoretical point of view. Then, we will describe the resulting search algorithm and its extension for word graph generation. Afterwards, we will analyze the Viterbi alignments produced during the training of the alignment models. Then, we will compare the translation results when restricting the search to either of these constraints. 2 Theoretical Discussion In this section, we will discuss t</context>
<context position="23058" citStr="Wu, 1995" startWordPosition="3900" endWordPosition="3901">ample, the German verb-group (“w¨urde vorschlagen”) is split into two parts. The search with the ITG constraints is able to produce a correct translation. With the IBM constraints, it is not possible to translate this verb-group correctly, because the distance between the two parts is too large (more than four words). As we see in the second example, in German the verb of a subordinate clause is placed at the end (“¨ubernachten”). The IBM search is not able to perform the necessary long-range reordering, as it is done with the ITG search. 7 Related Work The ITG constraints were introduced in (Wu, 1995). The applications were, for instance, the segmentation of Chinese character sequences into Chinese “words” and the bracketing of the source sentence into sub-sentential chunks. In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation. The resulting algorithm is similar to the one presented in Sect. 3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used. In (Vilar, 1998) a model similar to Wu’s method was considered. 8 Conclusions We have described the ITG c</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>D. Wu. 1995. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proc. of the 14th International Joint Conf. on Artificial Intelligence (IJCAI), pages 1328–1334, Montreal, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Conf. of the Association for Computational Linguistics (ACL ’96),</booktitle>
<pages>152--158</pages>
<location>Santa Cruz, CA,</location>
<contexts>
<context position="23249" citStr="Wu, 1996" startWordPosition="3928" endWordPosition="3929">possible to translate this verb-group correctly, because the distance between the two parts is too large (more than four words). As we see in the second example, in German the verb of a subordinate clause is placed at the end (“¨ubernachten”). The IBM search is not able to perform the necessary long-range reordering, as it is done with the ITG search. 7 Related Work The ITG constraints were introduced in (Wu, 1995). The applications were, for instance, the segmentation of Chinese character sequences into Chinese “words” and the bracketing of the source sentence into sub-sentential chunks. In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation. The resulting algorithm is similar to the one presented in Sect. 3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used. In (Vilar, 1998) a model similar to Wu’s method was considered. 8 Conclusions We have described the ITG constraints in detail and compared them to the IBM constraints. We draw the following conclusions: especially for long sentences the ITG constraints allow for higher flexibility in word-reorde</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>D. Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. of the 34th Annual Conf. of the Association for Computational Linguistics (ACL ’96), pages 152–158, Santa Cruz, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3152" citStr="Wu, 1997" startWordPosition="498" endWordPosition="499">ntences. In this paper, we will focus on the alignment problem, i.e. the mapping between source sentence positions and target sentence positions. As the word order in source and target language may differ, the search algorithm has to allow certain word-reorderings. If arbitrary word-reorderings are allowed, the search problem is NP-hard (Knight, g 1999). Therefore, we have to restrict the possible reorderings in some way to make the search problem feasible. Here, we will discuss two such constraints in detail. The first constraints are based on inversion transduction grammars (ITG) (Wu, 1995; Wu, 1997). In the following, we will call these the ITG constraints. The second constraints are the IBM constraints (Berger et al., 1996). In the next section, we will describe these constraints from a theoretical point of view. Then, we will describe the resulting search algorithm and its extension for word graph generation. Afterwards, we will analyze the Viterbi alignments produced during the training of the alignment models. Then, we will compare the translation results when restricting the search to either of these constraints. 2 Theoretical Discussion In this section, we will discuss the reorderi</context>
<context position="5384" citStr="Wu, 1997" startWordPosition="863" endWordPosition="864">he white boxes represent the two blocks to be merged. Now, we investigate, how many permutations are obtainable with this method. A permutation derived by the above method can be represented as a binary tree where the inner nodes are colored either black or white. At black nodes the resulting sequences of the children are inverted. At white nodes they are kept in monotone order. This representation is equivalent to without inversion with inversion source positions Figure 1: Illustration of monotone and inverted concatenation of two consecutive blocks. the parse trees of the simple grammar in (Wu, 1997). We observe that a given permutation may be constructed in several ways by the above method. For instance, let us consider the identity permutation of 1, 2,..., n. Any binary tree with n nodes and all inner nodes colored white (monotone order) is a possible representation of this permutation. To obtain a unique representation, we pose an additional constraint on the binary trees: if the right son of a node is an inner node, it has to be colored with the opposite color. With this constraint, each of these binary trees is unique and equivalent to a parse tree of the ’canonical-form’ grammar in </context>
<context position="7148" citStr="Wu, 1997" startWordPosition="1179" endWordPosition="1180">n = 3(2n − 1)Sn−1 − (n − 2)Sn−2 with n &gt; 2 and S0 = 1, S1 = 2. The Schr¨oder numbers have many combinatorical interpretations. Here, we will mention only two of them. The first one is another way of viewing at the ITG constraints. The number of permutations of the sequence 1, 2, ..., n, which avoid the subsequences (3, 1, 4, 2) and (2, 4, 1, 3), is the large Schr¨oder number Sn−1. More details on forbidden subsequences can be found in (West, 1995). The interesting point is that a search with the ITG constraints cannot generate a word-reordering that contains one of these two subsequences. In (Wu, 1997), these forbidden subsequences are called ’inside-out’ transpositions. Another interpretation of the Schr¨oder numbers is given in (Knuth, 1973): The number of permutations that can be sorted with an output-restricted doubleended queue (deque) is exactly the large Schr¨oder number. Additionally, Knuth presents an approximation for the large Schr¨oder numbers: √ Sn ≈ c · (3 + 8)n · n−32 (3) where c is set to 2 �(3√2 − 4)/π. This approximation function confirms the result of Schr¨oder, and we obtain Sn ∈ o((3 + √8)n), i.e. the Schr¨oder numbers grow like (3 + √8)n ≈ 5.83n. 2.2 IBM Constraints In</context>
<context position="15848" citStr="Wu, 1997" startWordPosition="2702" endWordPosition="2703">r jl . As initialization, we use the word graphs of the monotone IBM4 search. 3.4 Extended ITG constraints In this section, we will extend the ITG constraints described in Sec. 2.1. This extension will go beyond basic reordering constraints. We already mentioned that the use of consecutive phrases within the ITG approach is straightforward. The only thing we have to change is the initialization of the Q-table. Now, we will extend this idea to phrases that are non-consecutive in the source language. For this purpose, we adopt the view of the ITG constraints as a bilingual grammar as, e.g., in (Wu, 1997). For the baseline ITG constraints, the resulting grammar is: A— [AA] |(AA) |f/e |f/2 |2/e Here, [AA] denotes a monotone concatenation and (AA) denotes an inverted concatenation. Let us now consider the case of a source phrase consisting of two parts f1 and f2. Let e denote the corresponding target phrase. We add the productions A — [e/f1 A 2/f2] |(e/f1 A 2/f2) to the grammar. The probabilities of these productions are, dependent on the translation direction, p(e|f1, f2) or p(f1, f2|e), respectively. Obviously, these productions are not in the normal form of an ITG, but with the method describ</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>